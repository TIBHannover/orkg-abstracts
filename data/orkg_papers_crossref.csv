uri,title,doi,abstract_source,abstract,processed_abstract
http://orkg.org/orkg/resource/R3000,A model for contextual data sharing in smartphone applications,10.1108/ijpcc-06-2016-0030,crossref,"<jats:sec>\n<jats:title content-type=""abstract-subheading"">Purpose</jats:title>\n<jats:p>The purpose of this paper is to introduce a model for identifying, storing and sharing contextual information across smartphone apps that uses the native device services. The authors present the idea of using user input and interaction within an app as contextual information, and how each app can identify and store contextual information.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>\n<jats:p>Contexts are modeled as hierarchical objects that can be stored and shared by applications using native mechanisms. A proof-of-concept implementation of the model for the Android platform demonstrates contexts modelled as hierarchical objects stored and shared by applications using native mechanisms.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Findings</jats:title>\n<jats:p>The model was found to be practically viable by implemented sample apps that share context and through a performance analysis of the system.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Practical implications</jats:title>\n<jats:p>The contextual data-sharing model enables the creation of smart apps and services without being tied to any vendor’s cloud services.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>\n<jats:p>This paper introduces a new approach for sharing context in smartphone applications that does not require cloud services.</jats:p>\n</jats:sec>","\n purpose \n the purpose of this paper is to introduce a model for identifying, storing and sharing contextual information across smartphone apps that uses the native device services. the authors present the idea of using user input and interaction within an app as contextual information, and how each app can identify and store contextual information. \n \n \n design/methodology/approach \n contexts are modeled as hierarchical objects that can be stored and shared by applications using native mechanisms. a proof-of-concept implementation of the model for the android platform demonstrates contexts modelled as hierarchical objects stored and shared by applications using native mechanisms. \n \n \n findings \n the model was found to be practically viable by implemented sample apps that share context and through a performance analysis of the system. \n \n \n practical implications \n the contextual data-sharing model enables the creation of smart apps and services without being tied to any vendor’s cloud services. \n \n \n originality/value \n this paper introduces a new approach for sharing context in smartphone applications that does not require cloud services. \n"
http://orkg.org/orkg/resource/R4208,You will be…: a study of job advertisements to determine employers' requirements for LIS professionals in the UK in 2007,10.1108/00242530810899595,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to investigate what employers seek when recruiting library and information professionals in the UK and whether professional skills, generic skills or personal qualities are most in demand.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>A content analysis of a sample of 180 advertisements requiring a professional library or information qualification from Chartered Institute of Library and Information Professional\'s Library\u2009+\u2009Information Gazette over the period May 2006‐2007.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The findings reveal that a multitude of skills and qualities are required in the profession. When the results were compared with Information National Training Organisation and Library and Information Management Employability Skills research, customer service, interpersonal and communication skills, and general computing skills emerged as the requirements most frequently sought by employers. Overall, requirements from the generic skills area were most important to employers, but the research also demonstrates that professional skills are still valued. An unanticipated demand for profession related experience was found: this was the single most frequently sought requirement in the advertisements analysed.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>Although the Gazette is the largest source of library and information jobs, it does not provide a complete picture of the employment market.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>The paper contributes to debates about the skillsbase of the profession, and raises awareness of the abilities professionals need to cultivate in order to progress through their careers.</jats:p></jats:sec>","purpose the purpose of this paper is to investigate what employers seek when recruiting library and information professionals in the uk and whether professional skills, generic skills or personal qualities are most in demand. design/methodology/approach a content analysis of a sample of 180 advertisements requiring a professional library or information qualification from chartered institute of library and information professional\'s library\u2009+\u2009information gazette over the period may 2006‐2007. findings the findings reveal that a multitude of skills and qualities are required in the profession. when the results were compared with information national training organisation and library and information management employability skills research, customer service, interpersonal and communication skills, and general computing skills emerged as the requirements most frequently sought by employers. overall, requirements from the generic skills area were most important to employers, but the research also demonstrates that professional skills are still valued. an unanticipated demand for profession related experience was found: this was the single most frequently sought requirement in the advertisements analysed. research limitations/implications although the gazette is the largest source of library and information jobs, it does not provide a complete picture of the employment market. originality/value the paper contributes to debates about the skillsbase of the profession, and raises awareness of the abilities professionals need to cultivate in order to progress through their careers."
http://orkg.org/orkg/resource/R4581,Recruiting Project Managers: A Comparative Analysis of Competencies and Recruitment Signals from Job Advertisements,10.1002/pmj.21366,crossref,"<jats:p> This research addresses the competencies organizations use through project manager job advertisements. We develop a list of project manager job competencies; break down the competency components into knowledge, skills, and abilities; and conduct a comparative analysis of the use of these competencies. We examine the online contents of project manager job advertisements in the public domain. Analysis shows that industry job advertisements emphasize “soft skills” and competencies in a manner different than that in the literature. Additionally, differences are found across countries and between industries. Implications from the findings highlight the incongruent dissemination of project manager competencies, regional and industrial demands, and the recruitment of project managers. </jats:p>","this research addresses the competencies organizations use through project manager job advertisements. we develop a list of project manager job competencies; break down the competency components into knowledge, skills, and abilities; and conduct a comparative analysis of the use of these competencies. we examine the online contents of project manager job advertisements in the public domain. analysis shows that industry job advertisements emphasize “soft skills” and competencies in a manner different than that in the literature. additionally, differences are found across countries and between industries. implications from the findings highlight the incongruent dissemination of project manager competencies, regional and industrial demands, and the recruitment of project managers."
http://orkg.org/orkg/resource/R4337,You will be…: a study of job advertisements to determine employers' requirements for LIS professionals in the UK in 2007,10.1108/00242530810899595,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to investigate what employers seek when recruiting library and information professionals in the UK and whether professional skills, generic skills or personal qualities are most in demand.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>A content analysis of a sample of 180 advertisements requiring a professional library or information qualification from Chartered Institute of Library and Information Professional\'s Library\u2009+\u2009Information Gazette over the period May 2006‐2007.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The findings reveal that a multitude of skills and qualities are required in the profession. When the results were compared with Information National Training Organisation and Library and Information Management Employability Skills research, customer service, interpersonal and communication skills, and general computing skills emerged as the requirements most frequently sought by employers. Overall, requirements from the generic skills area were most important to employers, but the research also demonstrates that professional skills are still valued. An unanticipated demand for profession related experience was found: this was the single most frequently sought requirement in the advertisements analysed.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>Although the Gazette is the largest source of library and information jobs, it does not provide a complete picture of the employment market.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>The paper contributes to debates about the skillsbase of the profession, and raises awareness of the abilities professionals need to cultivate in order to progress through their careers.</jats:p></jats:sec>","purpose the purpose of this paper is to investigate what employers seek when recruiting library and information professionals in the uk and whether professional skills, generic skills or personal qualities are most in demand. design/methodology/approach a content analysis of a sample of 180 advertisements requiring a professional library or information qualification from chartered institute of library and information professional\'s library\u2009+\u2009information gazette over the period may 2006‐2007. findings the findings reveal that a multitude of skills and qualities are required in the profession. when the results were compared with information national training organisation and library and information management employability skills research, customer service, interpersonal and communication skills, and general computing skills emerged as the requirements most frequently sought by employers. overall, requirements from the generic skills area were most important to employers, but the research also demonstrates that professional skills are still valued. an unanticipated demand for profession related experience was found: this was the single most frequently sought requirement in the advertisements analysed. research limitations/implications although the gazette is the largest source of library and information jobs, it does not provide a complete picture of the employment market. originality/value the paper contributes to debates about the skillsbase of the profession, and raises awareness of the abilities professionals need to cultivate in order to progress through their careers."
http://orkg.org/orkg/resource/R4415,Changing patterns in IT skill sets 1988-2003,10.1145/1017114.1017121,crossref,"<jats:p>This paper examines trends in required job skills for IT professionals. Through an empirical study of classified job advertising for IT professionals over the past 17 years, we evaluate whether the observed trends support earlier predictions offered by researchers who sought to anticipate future job and skill demands (Leitheiser 1992; Trauth, Farwell, &amp; Lee 1993). Many of the findings are consistent with previous studies and support the notion that employers are seeking an ever-increasing number and variety of skill sets from the new hires. In addition, we found ongoing evidence of a recruitment gap (Todd, McKeen &amp; Gallupe 1995) where, despite many firms\' stated emphasis on well-rounded individuals with business knowledge and strong ""soft skills,"" the job advertising aspect of the recruiting process continues to focus on ""hard skills"". The changing demand patterns for IT professionals necessitate life-long learning skills not only for IT practitioners but also for the academics who teach them.</jats:p>","this paper examines trends in required job skills for it professionals. through an empirical study of classified job advertising for it professionals over the past 17 years, we evaluate whether the observed trends support earlier predictions offered by researchers who sought to anticipate future job and skill demands (leitheiser 1992; trauth, farwell, &amp; lee 1993). many of the findings are consistent with previous studies and support the notion that employers are seeking an ever-increasing number and variety of skill sets from the new hires. in addition, we found ongoing evidence of a recruitment gap (todd, mckeen &amp; gallupe 1995) where, despite many firms\' stated emphasis on well-rounded individuals with business knowledge and strong ""soft skills,"" the job advertising aspect of the recruiting process continues to focus on ""hard skills"". the changing demand patterns for it professionals necessitate life-long learning skills not only for it practitioners but also for the academics who teach them."
http://orkg.org/orkg/resource/R4534,Knowledge and Skill Requirements for Marketing Jobs in the 21st Century,10.1177/0273475310380881,crossref,"<jats:p> This study examines the skills and conceptual knowledge that employers require for marketing positions at different levels ranging from entry- or lower-level jobs to middle- and senior-level positions. The data for this research are based on a content analysis of 500 marketing jobs posted on Monster.com for Atlanta, Chicago, Los Angeles, New York City, and Seattle. There were notable differences between the skills and conceptual knowledge required for entry-, lower-, middle-, and upper-level marketing jobs. Technical skills appear to be much more important at all levels than what was documented in earlier research. This study discusses the implications of these research findings for the professional school pedagogical model of marketing education. </jats:p>","this study examines the skills and conceptual knowledge that employers require for marketing positions at different levels ranging from entry- or lower-level jobs to middle- and senior-level positions. the data for this research are based on a content analysis of 500 marketing jobs posted on monster.com for atlanta, chicago, los angeles, new york city, and seattle. there were notable differences between the skills and conceptual knowledge required for entry-, lower-, middle-, and upper-level marketing jobs. technical skills appear to be much more important at all levels than what was documented in earlier research. this study discusses the implications of these research findings for the professional school pedagogical model of marketing education."
http://orkg.org/orkg/resource/R4634,SKILL: A System for Skill Identification and Normalization,,crossref,"<jats:p>Named Entity Recognition (NER) and Named Entity Normalization (NEN) refer to the recognition and normalization of raw texts to known entities. From the perspective of recruitment innovation, professional skill characterization and normalization render human capital data more meaningful both commercially and socially. Accurate and detailed normalization of skills is the key for the predictive analysis of labor market dynamics. Such analytics help bridge the skills gap between employers and candidate workers by matching the right talent for the right job and identifying in-demand skills for workforce training programs. This can also work towards the social goal of providing more job opportunities to the community. In this paper we propose an automated approach for skill entity recognition and optimal normalization. The proposed system has two components: 1) Skills taxonomy generation, which employs vocational skill related sections of resumes and Wikipedia categories to define and develop a taxonomy of professional skills; 2) Skills tagging, which leverages properties of semantic word vectors to recognize and normalize relevant skills in input text. By sampling based end-user evaluation, the current system attains 91% accuracy on the taxonomy generation and 82% accuracy on the skills tagging tasks. The beta version of the system is currently applied in various big data and business intelligence applications for workforce analytics and career track projections at CareerBuilder.</jats:p>","named entity recognition (ner) and named entity normalization (nen) refer to the recognition and normalization of raw texts to known entities. from the perspective of recruitment innovation, professional skill characterization and normalization render human capital data more meaningful both commercially and socially. accurate and detailed normalization of skills is the key for the predictive analysis of labor market dynamics. such analytics help bridge the skills gap between employers and candidate workers by matching the right talent for the right job and identifying in-demand skills for workforce training programs. this can also work towards the social goal of providing more job opportunities to the community. in this paper we propose an automated approach for skill entity recognition and optimal normalization. the proposed system has two components: 1) skills taxonomy generation, which employs vocational skill related sections of resumes and wikipedia categories to define and develop a taxonomy of professional skills; 2) skills tagging, which leverages properties of semantic word vectors to recognize and normalize relevant skills in input text. by sampling based end-user evaluation, the current system attains 91% accuracy on the taxonomy generation and 82% accuracy on the skills tagging tasks. the beta version of the system is currently applied in various big data and business intelligence applications for workforce analytics and career track projections at careerbuilder."
http://orkg.org/orkg/resource/R4603,Skill Needs for Early Career Researchers—A Text Mining Approach,10.3390/su11102789,crossref,"<jats:p>Research and development activities are one of the main drivers for progress, economic growth and wellbeing in many societies. This article proposes a text mining approach applied to a large amount of data extracted from job vacancies advertisements, aiming to shed light on the main skills and demands that characterize first stage research positions in Europe. Results show that data handling and processing skills are essential for early career researchers, irrespective of their research field. Also, as many analyzed first stage research positions are connected to universities, they include teaching activities to a great extent. Management of time, risks, projects, and resources plays an important part in the job requirements included in the analyzed advertisements. Such information is relevant not only for early career researchers who perform job selection taking into account the match of possessed skills with the required ones, but also for educational institutions that are responsible for skills development of the future R&amp;D professionals.</jats:p>","research and development activities are one of the main drivers for progress, economic growth and wellbeing in many societies. this article proposes a text mining approach applied to a large amount of data extracted from job vacancies advertisements, aiming to shed light on the main skills and demands that characterize first stage research positions in europe. results show that data handling and processing skills are essential for early career researchers, irrespective of their research field. also, as many analyzed first stage research positions are connected to universities, they include teaching activities to a great extent. management of time, risks, projects, and resources plays an important part in the job requirements included in the analyzed advertisements. such information is relevant not only for early career researchers who perform job selection taking into account the match of possessed skills with the required ones, but also for educational institutions that are responsible for skills development of the future r&amp;d professionals."
http://orkg.org/orkg/resource/R4796,The hierarchy-of-hypotheses approach: A synthesis method for enhancing theory development in ecology and evolution,10.32942/osf.io/6a85f,crossref,"<p>In the current era of Big Data, existing synthesis tools (e.g. formal meta-analysis) are useful for handling the deluge of data and information. However, there is a need for complementary tools that help to (i) structure data and information, (ii) closely connect evidence to theory and (iii) further develop theory. We present the hierarchy-of-hypotheses (HoH) approach to address these issues. In an HoH, hypotheses are conceptually and visually structured in a hierarchically nested way, where the lower branches can be directly connected to empirical results. Used as an evidence-driven, bottom-up approach, it can (i) show connections between empirical results, even when derived through diverse approaches; and (ii) indicate under which circumstances hypotheses are applicable. Used as a theory-driven, top-down method, it helps uncover mechanistic components of hypotheses. We offer guidance on how to build an HoH, provide examples from population and evolutionary biology and propose terminological clarifications.</p>","in the current era of big data, existing synthesis tools (e.g. formal meta-analysis) are useful for handling the deluge of data and information. however, there is a need for complementary tools that help to (i) structure data and information, (ii) closely connect evidence to theory and (iii) further develop theory. we present the hierarchy-of-hypotheses (hoh) approach to address these issues. in an hoh, hypotheses are conceptually and visually structured in a hierarchically nested way, where the lower branches can be directly connected to empirical results. used as an evidence-driven, bottom-up approach, it can (i) show connections between empirical results, even when derived through diverse approaches; and (ii) indicate under which circumstances hypotheses are applicable. used as a theory-driven, top-down method, it helps uncover mechanistic components of hypotheses. we offer guidance on how to build an hoh, provide examples from population and evolutionary biology and propose terminological clarifications."
http://orkg.org/orkg/resource/R4857,How are topics born? Understanding the research dynamics preceding the emergence of new areas,10.7717/peerj-cs.119,crossref,"<jats:p>The ability to promptly recognise new research trends is strategic for many stakeholders, including universities, institutional funding bodies, academic publishers and companies. While the literature describes several approaches which aim to identify the emergence of new research topics early in their lifecycle, these rely on the assumption that the topic in question is already associated with a number of publications and consistently referred to by a community of researchers. Hence, detecting the emergence of a new research area at an <jats:italic>embryonic stage</jats:italic>, i.e., before the topic has been consistently labelled by a community of researchers and associated with a number of publications, is still an open challenge. In this paper, we begin to address this challenge by performing a study of the dynamics preceding the creation of new topics. This study indicates that the emergence of a new topic is anticipated by a significant increase in the pace of collaboration between relevant research areas, which can be seen as the ‘parents’ of the new topic. These initial findings (i) confirm our hypothesis that it is possible in principle to detect the emergence of a new topic at the embryonic stage, (ii) provide new empirical evidence supporting relevant theories in Philosophy of Science, and also (iii) suggest that new topics tend to emerge in an environment in which weakly interconnected research areas begin to cross-fertilise.</jats:p>","the ability to promptly recognise new research trends is strategic for many stakeholders, including universities, institutional funding bodies, academic publishers and companies. while the literature describes several approaches which aim to identify the emergence of new research topics early in their lifecycle, these rely on the assumption that the topic in question is already associated with a number of publications and consistently referred to by a community of researchers. hence, detecting the emergence of a new research area at an embryonic stage , i.e., before the topic has been consistently labelled by a community of researchers and associated with a number of publications, is still an open challenge. in this paper, we begin to address this challenge by performing a study of the dynamics preceding the creation of new topics. this study indicates that the emergence of a new topic is anticipated by a significant increase in the pace of collaboration between relevant research areas, which can be seen as the ‘parents’ of the new topic. these initial findings (i) confirm our hypothesis that it is possible in principle to detect the emergence of a new topic at the embryonic stage, (ii) provide new empirical evidence supporting relevant theories in philosophy of science, and also (iii) suggest that new topics tend to emerge in an environment in which weakly interconnected research areas begin to cross-fertilise."
http://orkg.org/orkg/resource/R5129,PARAMO: A Pipeline for Reconstructing Ancestral Anatomies Using Ontologies and Stochastic Mapping,10.1093/isd/ixz009,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Comparative phylogenetics has been largely lacking a method for reconstructing the evolution of phenotypic entities that consist of ensembles of multiple discrete traits—entire organismal anatomies or organismal body regions. In this study, we provide a new approach named PARAMO (PhylogeneticAncestralReconstruction ofAnatomy byMappingOntologies) that appropriately models anatomical dependencies and uses ontology-informed amalgamation of stochastic maps to reconstruct phenotypic evolution at different levels of anatomical hierarchy including entire phenotypes. This approach provides new opportunities for tracking phenotypic radiations and evolution of organismal anatomies.</jats:p>","abstract \n comparative phylogenetics has been largely lacking a method for reconstructing the evolution of phenotypic entities that consist of ensembles of multiple discrete traits—entire organismal anatomies or organismal body regions. in this study, we provide a new approach named paramo (phylogeneticancestralreconstruction ofanatomy bymappingontologies) that appropriately models anatomical dependencies and uses ontology-informed amalgamation of stochastic maps to reconstruct phenotypic evolution at different levels of anatomical hierarchy including entire phenotypes. this approach provides new opportunities for tracking phenotypic radiations and evolution of organismal anatomies."
http://orkg.org/orkg/resource/R6156,On Graph-Based Name Disambiguation,10.1145/1891879.1891883,crossref,"<jats:p>\n            Name ambiguity stems from the fact that many people or objects share identical names in the real world. Such name ambiguity decreases the performance of document retrieval, Web search, information integration, and may cause confusion in other applications. Due to the same name spellings and lack of information, it is a nontrivial task to distinguish them accurately. In this article, we focus on investigating the problem in digital libraries to distinguish publications written by authors with identical names. We present an effective framework named GHOST (abbreviation for GrapHical framewOrk for name diSambiguaTion), to solve the problem systematically. We devise a novel similarity metric, and utilize only one type of attribute (i.e., coauthorship) in GHOST. Given the similarity matrix, intermediate results are grouped into clusters with a recently introduced powerful clustering algorithm called\n            <jats:italic>Affinity Propagation</jats:italic>\n            . In addition, as a complementary technique, user feedback can be used to enhance the performance. We evaluated the framework on the real DBLP and PubMed datasets, and the experimental results show that GHOST can achieve both high\n            <jats:italic>precision</jats:italic>\n            and\n            <jats:italic>recall</jats:italic>\n            .\n          </jats:p>","\n name ambiguity stems from the fact that many people or objects share identical names in the real world. such name ambiguity decreases the performance of document retrieval, web search, information integration, and may cause confusion in other applications. due to the same name spellings and lack of information, it is a nontrivial task to distinguish them accurately. in this article, we focus on investigating the problem in digital libraries to distinguish publications written by authors with identical names. we present an effective framework named ghost (abbreviation for graphical framework for name disambiguation), to solve the problem systematically. we devise a novel similarity metric, and utilize only one type of attribute (i.e., coauthorship) in ghost. given the similarity matrix, intermediate results are grouped into clusters with a recently introduced powerful clustering algorithm called\n affinity propagation \n . in addition, as a complementary technique, user feedback can be used to enhance the performance. we evaluated the framework on the real dblp and pubmed datasets, and the experimental results show that ghost can achieve both high\n precision \n and\n recall \n .\n"
http://orkg.org/orkg/resource/R5207,Warum brauchen wir eine (neue) Bibliothekswissenschaft?,10.1515/bfp-2018-0046,crossref,"<jats:title>Zusammenfassung</jats:title>\n               <jats:p>Die Medienschwelle, an der wir uns befinden, stellt viele Institutionen infrage. Nicht aber die Bibliothek, wie viele äußerst erfolgreiche neue Bibliotheksprojekte (ÖB und WB) belegen. Der Themenschwerpunkt lässt (auch anlässlich der Next Library Conference in Berlin im September 2018) unterschiedliche Wissenschaftler zu Wort kommen, die dafür plädieren, sich auch wissenschaftlich mit dem Phänomen Bibliothek (wieder) zu befassen, um besser zu verstehen, wie ihre Potentiale den digitalen Wandel positiv begleiten können.</jats:p>","zusammenfassung \n die medienschwelle, an der wir uns befinden, stellt viele institutionen infrage. nicht aber die bibliothek, wie viele äußerst erfolgreiche neue bibliotheksprojekte (öb und wb) belegen. der themenschwerpunkt lässt (auch anlässlich der next library conference in berlin im september 2018) unterschiedliche wissenschaftler zu wort kommen, die dafür plädieren, sich auch wissenschaftlich mit dem phänomen bibliothek (wieder) zu befassen, um besser zu verstehen, wie ihre potentiale den digitalen wandel positiv begleiten können."
http://orkg.org/orkg/resource/R5259,OpenBiodiv: A Knowledge Graph for Literature-Extracted Linked Open Data in Biodiversity Science,10.3390/publications7020038,crossref,"<jats:p>Hundreds of years of biodiversity research have resulted in the accumulation of a substantial pool of communal knowledge; however, most of it is stored in silos isolated from each other, such as published articles or monographs. The need for a system to store and manage collective biodiversity knowledge in a community-agreed and interoperable open format has evolved into the concept of the Open Biodiversity Knowledge Management System (OBKMS). This paper presents OpenBiodiv: An OBKMS that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. It is presented as a Linked Open Dataset generated from scientific literature. OpenBiodiv encompasses data extracted from more than 5000 scholarly articles published by Pensoft and many more taxonomic treatments extracted by Plazi from journals of other publishers. The data from both sources are converted to Resource Description Framework (RDF) and integrated in a graph database using the OpenBiodiv-O ontology and an RDF version of the Global Biodiversity Information Facility (GBIF) taxonomic backbone. Through the application of semantic technologies, the project showcases the value of open publishing of Findable, Accessible, Interoperable, Reusable (FAIR) data towards the establishment of open science practices in the biodiversity domain.</jats:p>","hundreds of years of biodiversity research have resulted in the accumulation of a substantial pool of communal knowledge; however, most of it is stored in silos isolated from each other, such as published articles or monographs. the need for a system to store and manage collective biodiversity knowledge in a community-agreed and interoperable open format has evolved into the concept of the open biodiversity knowledge management system (obkms). this paper presents openbiodiv: an obkms that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. it is presented as a linked open dataset generated from scientific literature. openbiodiv encompasses data extracted from more than 5000 scholarly articles published by pensoft and many more taxonomic treatments extracted by plazi from journals of other publishers. the data from both sources are converted to resource description framework (rdf) and integrated in a graph database using the openbiodiv-o ontology and an rdf version of the global biodiversity information facility (gbif) taxonomic backbone. through the application of semantic technologies, the project showcases the value of open publishing of findable, accessible, interoperable, reusable (fair) data towards the establishment of open science practices in the biodiversity domain."
http://orkg.org/orkg/resource/R6360,A joint model for question answering over multiple knowledge bases,,crossref,"<jats:p>\n      \n        As the amount of knowledge bases (KBs) grows rapidly, the problem of question answering (QA) over multiple KBs has drawn more attention. The most significant distinction between multiple KB-QA and single KB-QA is that the former must consider the alignments between KBs. The pipeline strategy first constructs the alignments independently, and then uses the obtained alignments to construct queries. However, alignment construction is not a trivial task, and the introduced noises would be passed on to query construction. By contrast, we notice that alignment construction and query construction are interactive steps, and jointly considering them would be beneficial. To this end, we present a novel joint model based on integer linear programming (ILP), uniting these two procedures into a uniform framework. The experimental results demonstrate that the proposed approach outperforms state-of-the-art systems, and is able to improve the performance of both alignment construction and query construction.\n      \n    </jats:p>","\n \n as the amount of knowledge bases (kbs) grows rapidly, the problem of question answering (qa) over multiple kbs has drawn more attention. the most significant distinction between multiple kb-qa and single kb-qa is that the former must consider the alignments between kbs. the pipeline strategy first constructs the alignments independently, and then uses the obtained alignments to construct queries. however, alignment construction is not a trivial task, and the introduced noises would be passed on to query construction. by contrast, we notice that alignment construction and query construction are interactive steps, and jointly considering them would be beneficial. to this end, we present a novel joint model based on integer linear programming (ilp), uniting these two procedures into a uniform framework. the experimental results demonstrate that the proposed approach outperforms state-of-the-art systems, and is able to improve the performance of both alignment construction and query construction.\n \n"
http://orkg.org/orkg/resource/R6465,Visualizing Populated Ontologies with OntoTrix,10.4018/ijswis.2013100102,crossref,"<p>Research on visualizing Semantic Web data has yielded many tools that rely on information visualization techniques to better support the user in understanding and editing these data. Most tools structure the visualization according to the concept definitions and interrelations that constitute the ontology’s vocabulary. Instances are often treated as somewhat peripheral information, when considered at all. These instances, that populate ontologies, represent an essential part of any knowledge base. Understanding instance-level data might be easier for users because of their higher concreteness, but instances will often be orders of magnitude more numerous than the concept definitions that give them machine-processable meaning. As such, the visualization of instance-level data poses different but real challenges. The authors present a visualization technique designed to enable users to visualize large instance sets and the relations that connect them. This visualization uses both node-link and adjacency matrix representations of graphs to visualize different parts of the data depending on their semantic and local structural properties. The technique was originally devised for simple social network visualization. The authors extend it to handle the richer and more complex graph structures of populated ontologies, exploiting ontological knowledge to drive the layout of, and navigation in, the representation embedded in a smooth zoomable environment.</p>","research on visualizing semantic web data has yielded many tools that rely on information visualization techniques to better support the user in understanding and editing these data. most tools structure the visualization according to the concept definitions and interrelations that constitute the ontology’s vocabulary. instances are often treated as somewhat peripheral information, when considered at all. these instances, that populate ontologies, represent an essential part of any knowledge base. understanding instance-level data might be easier for users because of their higher concreteness, but instances will often be orders of magnitude more numerous than the concept definitions that give them machine-processable meaning. as such, the visualization of instance-level data poses different but real challenges. the authors present a visualization technique designed to enable users to visualize large instance sets and the relations that connect them. this visualization uses both node-link and adjacency matrix representations of graphs to visualize different parts of the data depending on their semantic and local structural properties. the technique was originally devised for simple social network visualization. the authors extend it to handle the richer and more complex graph structures of populated ontologies, exploiting ontological knowledge to drive the layout of, and navigation in, the representation embedded in a smooth zoomable environment."
http://orkg.org/orkg/resource/R6614,Generating Indicative-Informative Summaries with SumUM,10.1162/089120102762671963,crossref,"""<jats:p> We present and evaluate SumUM, a text summarization system that takes a raw technical text as input and produces an indicative informative summary. The indicative part of the summary identifies the topics of the document, and the informative part elaborates on some of these topics according to the reader's interest. SumUM motivates the topics, describes entities, and defines concepts. It is a first step for exploring the issue of dynamic summarization. This is accomplished through a process of shallow syntactic and semantic analysis, concept identification, and text regeneration. Our method was developed through the study of a corpus of abstracts written by professional abstractors. Relying on human judgment, we have evaluated indicativeness, informativeness, and text acceptability of the automatic summaries. The results thus far indicate good performance when compared with other summarization technologies. </jats:p>""",""" we present and evaluate sumum, a text summarization system that takes a raw technical text as input and produces an indicative informative summary. the indicative part of the summary identifies the topics of the document, and the informative part elaborates on some of these topics according to the reader's interest. sumum motivates the topics, describes entities, and defines concepts. it is a first step for exploring the issue of dynamic summarization. this is accomplished through a process of shallow syntactic and semantic analysis, concept identification, and text regeneration. our method was developed through the study of a corpus of abstracts written by professional abstractors. relying on human judgment, we have evaluated indicativeness, informativeness, and text acceptability of the automatic summaries. the results thus far indicate good performance when compared with other summarization technologies. """
http://orkg.org/orkg/resource/R6736,Towards Unsupervised Learning of Temporal Relations between Events,10.1613/jair.3693,crossref,"<jats:p>Automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as Question Answering, Information Extraction, and Summarization. Since most existing methods are supervised and require large corpora, which for many languages do not exist, we have concentrated our efforts to reduce the need for annotated data as much as possible. This paper presents two different algorithms towards this goal. The first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events. In the first stage, the algorithm learns a general classifier from an annotated corpus. Then, inspired by the hypothesis of ""one type of temporal relation per discourse\'\', it extracts useful information from a cluster of topically related documents. We show that by combining the global information of such a cluster with local decisions of a general classifier, a bootstrapping cross-document classifier can be built to extract temporal relations between events. Our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is higher than that of several previous successful systems. The second proposed method for temporal relation extraction is based on the expectation maximization (EM) algorithm. Within EM, we used different techniques such as a greedy best-first search and integer linear programming for temporal inconsistency removal. We think that the experimental results of our EM based algorithm, as a first step toward a fully unsupervised temporal relation extraction method, is encouraging.</jats:p>","automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as question answering, information extraction, and summarization. since most existing methods are supervised and require large corpora, which for many languages do not exist, we have concentrated our efforts to reduce the need for annotated data as much as possible. this paper presents two different algorithms towards this goal. the first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events. in the first stage, the algorithm learns a general classifier from an annotated corpus. then, inspired by the hypothesis of ""one type of temporal relation per discourse\'\', it extracts useful information from a cluster of topically related documents. we show that by combining the global information of such a cluster with local decisions of a general classifier, a bootstrapping cross-document classifier can be built to extract temporal relations between events. our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is higher than that of several previous successful systems. the second proposed method for temporal relation extraction is based on the expectation maximization (em) algorithm. within em, we used different techniques such as a greedy best-first search and integer linear programming for temporal inconsistency removal. we think that the experimental results of our em based algorithm, as a first step toward a fully unsupervised temporal relation extraction method, is encouraging."
http://orkg.org/orkg/resource/R6943,"Nucleation and growth of new particles in Po Valley, Italy",10.5194/acp-7-355-2007,crossref,"""<jats:p>Abstract. Aerosol number distribution measurements are reported at San Pietro Capofiume (SPC) station (44°39' N, 11°37' E) for the time period 2002–2005. The station is located in Po Valley, the largest industrial, trading and agricultural area in Italy with a high population density. New particle formation was studied based on observations of the particle size distribution, meteorological and gas phase parameters. The nucleation events were classified according to the event clarity based on the particle number concentrations, and the particle formation and growth rates. Out of a total of 769 operational days from 2002 to 2005 clear events were detected on 36% of the days whilst 33% are clearly non-event days. The event frequency was high during spring and summer months with maximum values in May and July, whereas lower frequency was observed in winter and autumn months. The average particle formation and growth rates were estimated as ~6 cm−3 s−1 and ~7 nm h−1, respectively. Such high growth and formation rates are typical for polluted areas. Temperature, wind speed, solar radiation, SO2 and O3 concentrations were on average higher on nucleation days than on non-event days, whereas relative and absolute humidity and NO2 concentration were lower; however, seasonal differences were observed. Backtrajectory analysis suggests that during majority of nucleation event days, the air masses originate from northern to eastern directions. We also study previously developed nucleation event correlations with environmental variables and show that they predict Po Valley nucleation events with variable success.\n                    </jats:p>""",""" abstract. aerosol number distribution measurements are reported at san pietro capofiume (spc) station (44°39' n, 11°37' e) for the time period 2002–2005. the station is located in po valley, the largest industrial, trading and agricultural area in italy with a high population density. new particle formation was studied based on observations of the particle size distribution, meteorological and gas phase parameters. the nucleation events were classified according to the event clarity based on the particle number concentrations, and the particle formation and growth rates. out of a total of 769 operational days from 2002 to 2005 clear events were detected on 36% of the days whilst 33% are clearly non-event days. the event frequency was high during spring and summer months with maximum values in may and july, whereas lower frequency was observed in winter and autumn months. the average particle formation and growth rates were estimated as ~6 cm−3 s−1 and ~7 nm h−1, respectively. such high growth and formation rates are typical for polluted areas. temperature, wind speed, solar radiation, so2 and o3 concentrations were on average higher on nucleation days than on non-event days, whereas relative and absolute humidity and no2 concentration were lower; however, seasonal differences were observed. backtrajectory analysis suggests that during majority of nucleation event days, the air masses originate from northern to eastern directions. we also study previously developed nucleation event correlations with environmental variables and show that they predict po valley nucleation events with variable success.\n """
http://orkg.org/orkg/resource/R8034,An Overview of CMIP5 and the Experiment Design,10.1175/bams-d-11-00094.1,crossref,"""<jats:p>The fifth phase of the Coupled Model Intercomparison Project (CMIP5) will produce a state-of-the- art multimodel dataset designed to advance our knowledge of climate variability and climate change. Researchers worldwide are analyzing the model output and will produce results likely to underlie the forthcoming Fifth Assessment Report by the Intergovernmental Panel on Climate Change. Unprecedented in scale and attracting interest from all major climate modeling groups, CMIP5 includes “long term” simulations of twentieth-century climate and projections for the twenty-first century and beyond. Conventional atmosphere–ocean global climate models and Earth system models of intermediate complexity are for the first time being joined by more recently developed Earth system models under an experiment design that allows both types of models to be compared to observations on an equal footing. Besides the longterm experiments, CMIP5 calls for an entirely new suite of “near term” simulations focusing on recent decades and the future to year 2035. These “decadal predictions” are initialized based on observations and will be used to explore the predictability of climate and to assess the forecast system's predictive skill. The CMIP5 experiment design also allows for participation of stand-alone atmospheric models and includes a variety of idealized experiments that will improve understanding of the range of model responses found in the more complex and realistic simulations. An exceptionally comprehensive set of model output is being collected and made freely available to researchers through an integrated but distributed data archive. For researchers unfamiliar with climate models, the limitations of the models and experiment design are described.</jats:p>""",""" the fifth phase of the coupled model intercomparison project (cmip5) will produce a state-of-the- art multimodel dataset designed to advance our knowledge of climate variability and climate change. researchers worldwide are analyzing the model output and will produce results likely to underlie the forthcoming fifth assessment report by the intergovernmental panel on climate change. unprecedented in scale and attracting interest from all major climate modeling groups, cmip5 includes “long term” simulations of twentieth-century climate and projections for the twenty-first century and beyond. conventional atmosphere–ocean global climate models and earth system models of intermediate complexity are for the first time being joined by more recently developed earth system models under an experiment design that allows both types of models to be compared to observations on an equal footing. besides the longterm experiments, cmip5 calls for an entirely new suite of “near term” simulations focusing on recent decades and the future to year 2035. these “decadal predictions” are initialized based on observations and will be used to explore the predictability of climate and to assess the forecast system's predictive skill. the cmip5 experiment design also allows for participation of stand-alone atmospheric models and includes a variety of idealized experiments that will improve understanding of the range of model responses found in the more complex and realistic simulations. an exceptionally comprehensive set of model output is being collected and made freely available to researchers through an integrated but distributed data archive. for researchers unfamiliar with climate models, the limitations of the models and experiment design are described. """
http://orkg.org/orkg/resource/R8330,An ontology of scientific experiments,10.1098/rsif.2006.0134,crossref,"<jats:p>The formal description of experiments for efficient analysis, annotation and sharing of results is a fundamental part of the practice of science. Ontologies are required to achieve this objective. A few subject-specific ontologies of experiments currently exist. However, despite the unity of scientific experimentation, no general ontology of experiments exists. We propose the ontology EXPO to meet this need. EXPO links the SUMO (the Suggested Upper Merged Ontology) with subject-specific ontologies of experiments by formalizing the generic concepts of experimental design, methodology and results representation. EXPO is expressed in the W3C standard ontology language OWL-DL. We demonstrate the utility of EXPO and its ability to describe different experimental domains, by applying it to two experiments: one in high-energy physics and the other in phylogenetics. The use of EXPO made the goals and structure of these experiments more explicit, revealed ambiguities, and highlighted an unexpected similarity. We conclude that, EXPO is of general value in describing experiments and a step towards the formalization of science.</jats:p>","the formal description of experiments for efficient analysis, annotation and sharing of results is a fundamental part of the practice of science. ontologies are required to achieve this objective. a few subject-specific ontologies of experiments currently exist. however, despite the unity of scientific experimentation, no general ontology of experiments exists. we propose the ontology expo to meet this need. expo links the sumo (the suggested upper merged ontology) with subject-specific ontologies of experiments by formalizing the generic concepts of experimental design, methodology and results representation. expo is expressed in the w3c standard ontology language owl-dl. we demonstrate the utility of expo and its ability to describe different experimental domains, by applying it to two experiments: one in high-energy physics and the other in phylogenetics. the use of expo made the goals and structure of these experiments more explicit, revealed ambiguities, and highlighted an unexpected similarity. we conclude that, expo is of general value in describing experiments and a step towards the formalization of science."
http://orkg.org/orkg/resource/R25131,Evaluation of Six Night Vision Enhancement Systems: Qualitative and Quantitative Support for Intelligent Image Processing,10.1518/001872007x200148,crossref,"<jats:p> Objective: An evaluation study was conducted to answer the question of which system properties of night vision enhancement systems (NVESs) provide a benefit for drivers without increasing their workload. Background: Different infrared sensor, image processing, and display technologies can be integrated into an NVES to support nighttime driving. Because each of these components has its specific strengths and weaknesses, careful testing is required to determine their best combination. Method: Six prototypical systems were assessed in two steps. First, a heuristic evaluation with experts from ergonomics, perception, and traffic psychology was conducted. It produced a broad overview of possible effects of system properties on driving. Based on these results, an experimental field study with 15 experienced drivers was performed. Criteria used to evaluate the development potential of the six prototypes were the usability dimensions of effectiveness, efficiency, and user satisfaction (International Organization for Standardization, 1998). Results: Results showed that the intelligibility of information, the easiness with which obstacles could be located in the environment, and the position of the display presenting the output of the system were of crucial importance for the usability of the NVES and its acceptance. Conclusion: All relevant requirements are met best by NVESs that are positioned at an unobtrusive location and are equipped with functions for the automatic identification of objects and for event-based warnings. Application: These design recommendations and the presented approach to evaluate the systems can be directly incorporated into the development process of future NVESs. </jats:p>","objective: an evaluation study was conducted to answer the question of which system properties of night vision enhancement systems (nvess) provide a benefit for drivers without increasing their workload. background: different infrared sensor, image processing, and display technologies can be integrated into an nves to support nighttime driving. because each of these components has its specific strengths and weaknesses, careful testing is required to determine their best combination. method: six prototypical systems were assessed in two steps. first, a heuristic evaluation with experts from ergonomics, perception, and traffic psychology was conducted. it produced a broad overview of possible effects of system properties on driving. based on these results, an experimental field study with 15 experienced drivers was performed. criteria used to evaluate the development potential of the six prototypes were the usability dimensions of effectiveness, efficiency, and user satisfaction (international organization for standardization, 1998). results: results showed that the intelligibility of information, the easiness with which obstacles could be located in the environment, and the position of the display presenting the output of the system were of crucial importance for the usability of the nves and its acceptance. conclusion: all relevant requirements are met best by nvess that are positioned at an unobtrusive location and are equipped with functions for the automatic identification of objects and for event-based warnings. application: these design recommendations and the presented approach to evaluate the systems can be directly incorporated into the development process of future nvess."
http://orkg.org/orkg/resource/R8345,"Decentralised Authoring, Annotations and Notifications for a Read-Write Web with dokieli",10.1007/978-3-319-60131-1_33,crossref,"<jats:title>Abstract</jats:title><jats:p>While the Web was designed as a decentralised environment, individual authors still lack the ability to conveniently author and publish documents, and to engage in social interactions with documents of others in a truly decentralised fashion. We present <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://dokie.li/"">dokieli</jats:ext-link>, a fully decentralised, browser-based authoring and annotation platform with built-in support for social interactions, through which people retain ownership of and sovereignty over their data. The resulting “living” documents are interoperable and independent of dokieli since they follow standards and best practices, such as HTML+RDFa for a fine-grained semantic structure, Linked Data Platform for personal data storage, and Linked Data Notifications for updates. This article describes dokieli’s architecture and implementation, demonstrating advanced document authoring and interaction without a single point of control. Such an environment provides the right technological conditions for independent publication of scientific articles, news, and other works that benefit from diverse voices and open interactions. To experience the described features please open this document in your Web browser under its canonical URI: <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://csarven.ca/dokieli-rww"">http://csarven.ca/dokieli-rww</jats:ext-link>.</jats:p>","abstract while the web was designed as a decentralised environment, individual authors still lack the ability to conveniently author and publish documents, and to engage in social interactions with documents of others in a truly decentralised fashion. we present dokieli , a fully decentralised, browser-based authoring and annotation platform with built-in support for social interactions, through which people retain ownership of and sovereignty over their data. the resulting “living” documents are interoperable and independent of dokieli since they follow standards and best practices, such as html+rdfa for a fine-grained semantic structure, linked data platform for personal data storage, and linked data notifications for updates. this article describes dokieli’s architecture and implementation, demonstrating advanced document authoring and interaction without a single point of control. such an environment provides the right technological conditions for independent publication of scientific articles, news, and other works that benefit from diverse voices and open interactions. to experience the described features please open this document in your web browser under its canonical uri: http://csarven.ca/dokieli-rww ."
http://orkg.org/orkg/resource/R8348,Research Articles in Simplified HTML: a Web-first format for HTML-based scholarly articles,10.7717/peerj-cs.132,crossref,"<jats:sec><jats:title>Purpose</jats:title><jats:p>This paper introduces the Research Articles in Simplified HTML (or RASH), which is a Web-first format for writing HTML-based scholarly papers; it is accompanied by the RASH Framework, a set of tools for interacting with RASH-based articles. The paper also presents an evaluation that involved authors and reviewers of RASH articles submitted to the SAVE-SD 2015 and SAVE-SD 2016 workshops.</jats:p></jats:sec><jats:sec><jats:title>Design</jats:title><jats:p>RASH has been developed aiming to: be easy to learn and use; share scholarly documents (and embedded semantic annotations) through the Web; support its adoption within the existing publishing workflow.</jats:p></jats:sec><jats:sec><jats:title>Findings</jats:title><jats:p>The evaluation study confirmed that RASH is ready to be adopted in workshops, conferences, and journals and can be quickly learnt by researchers who are familiar with HTML.</jats:p></jats:sec><jats:sec><jats:title>Research Limitations</jats:title><jats:p>The evaluation study also highlighted some issues in the adoption of RASH, and in general of HTML formats, especially by less technically savvy users. Moreover, additional tools are needed, e.g., for enabling additional conversions from/to existing formats such as OpenXML.</jats:p></jats:sec><jats:sec><jats:title>Practical Implications</jats:title><jats:p>RASH (and its Framework) is another step towards enabling the definition of formal representations of the meaning of the content of an article, facilitating its automatic discovery, enabling its linking to semantically related articles, providing access to data within the article in actionable form, and allowing integration of data between papers.</jats:p></jats:sec><jats:sec><jats:title>Social Implications</jats:title><jats:p>RASH addresses the intrinsic needs related to the various users of a scholarly article: researchers (focussing on its content), readers (experiencing new ways for browsing it), citizen scientists (reusing available data formally defined within it through semantic annotations), publishers (using the advantages of new technologies as envisioned by the Semantic Publishing movement).</jats:p></jats:sec><jats:sec><jats:title>Value</jats:title><jats:p>RASH helps authors to focus on the organisation of their texts, supports them in the task of semantically enriching the content of articles, and leaves all the issues about validation, visualisation, conversion, and semantic data extraction to the various tools developed within its Framework.</jats:p></jats:sec>","purpose this paper introduces the research articles in simplified html (or rash), which is a web-first format for writing html-based scholarly papers; it is accompanied by the rash framework, a set of tools for interacting with rash-based articles. the paper also presents an evaluation that involved authors and reviewers of rash articles submitted to the save-sd 2015 and save-sd 2016 workshops. design rash has been developed aiming to: be easy to learn and use; share scholarly documents (and embedded semantic annotations) through the web; support its adoption within the existing publishing workflow. findings the evaluation study confirmed that rash is ready to be adopted in workshops, conferences, and journals and can be quickly learnt by researchers who are familiar with html. research limitations the evaluation study also highlighted some issues in the adoption of rash, and in general of html formats, especially by less technically savvy users. moreover, additional tools are needed, e.g., for enabling additional conversions from/to existing formats such as openxml. practical implications rash (and its framework) is another step towards enabling the definition of formal representations of the meaning of the content of an article, facilitating its automatic discovery, enabling its linking to semantically related articles, providing access to data within the article in actionable form, and allowing integration of data between papers. social implications rash addresses the intrinsic needs related to the various users of a scholarly article: researchers (focussing on its content), readers (experiencing new ways for browsing it), citizen scientists (reusing available data formally defined within it through semantic annotations), publishers (using the advantages of new technologies as envisioned by the semantic publishing movement). value rash helps authors to focus on the organisation of their texts, supports them in the task of semantically enriching the content of articles, and leaves all the issues about validation, visualisation, conversion, and semantic data extraction to the various tools developed within its framework."
http://orkg.org/orkg/resource/R8683,The Uncoordinated Potential of Libraries to Achieve Open Access Now: How the Transition to Open Access Could be Accelerated by Libraries Working Together,10.12685/027.7-2-1-48,crossref,"<jats:p>DOI: 10.12685/027.7-2-1-48The last ten years have shown, that Open Access is not only a vision, but has become real. Libraries are in a good position to push Open Access even further, as they currently fully pay the production costs of the traditional subscription model. SCOAP3 demonstrates that coordination among libraries is not unlikely and could lead to more Open Access immediately.Die letzten zehn Jahren haben gezeigt, dass Open Access nicht nur eine Vision ist, sondern tatsächlich funktioniert. Bibliotheken könnten Open Access ganz zum Durchbruch verhelfen, da sie zurzeit vollständig für die Produktionskosten beim Subskriptionsmodell aufkommen. SCOAP3 zeigt, dass die Koordination zwischen Bibliotheken möglich ist und sofort zu mehr Open Access führen kann.</jats:p>","doi: 10.12685/027.7-2-1-48the last ten years have shown, that open access is not only a vision, but has become real. libraries are in a good position to push open access even further, as they currently fully pay the production costs of the traditional subscription model. scoap3 demonstrates that coordination among libraries is not unlikely and could lead to more open access immediately.die letzten zehn jahren haben gezeigt, dass open access nicht nur eine vision ist, sondern tatsächlich funktioniert. bibliotheken könnten open access ganz zum durchbruch verhelfen, da sie zurzeit vollständig für die produktionskosten beim subskriptionsmodell aufkommen. scoap3 zeigt, dass die koordination zwischen bibliotheken möglich ist und sofort zu mehr open access führen kann."
http://orkg.org/orkg/resource/R9094,Development and evaluation of an Earth-System model – HadGEM2,10.5194/gmd-4-1051-2011,crossref,"<jats:p>Abstract. We describe here the development and evaluation of an Earth system model suitable for centennial-scale climate prediction. The principal new components added to the physical climate model are the terrestrial and ocean ecosystems and gas-phase tropospheric chemistry, along with their coupled interactions.  The individual Earth system components are described briefly and the relevant interactions between the components are explained. Because the multiple interactions could lead to unstable feedbacks, we go through a careful process of model spin up to ensure that all components are stable and the interactions balanced. This spun-up configuration is evaluated against observed data for the Earth system components and is generally found to perform very satisfactorily. The reason for the evaluation phase is that the model is to be used for the core climate simulations carried out by the Met Office Hadley Centre for the Coupled Model Intercomparison Project (CMIP5), so it is essential that addition of the extra complexity does not detract substantially from its climate performance. Localised changes in some specific meteorological variables can be identified, but the impacts on the overall simulation of present day climate are slight.  This model is proving valuable both for climate predictions, and for investigating the strengths of biogeochemical feedbacks.\n                    </jats:p>","abstract. we describe here the development and evaluation of an earth system model suitable for centennial-scale climate prediction. the principal new components added to the physical climate model are the terrestrial and ocean ecosystems and gas-phase tropospheric chemistry, along with their coupled interactions. the individual earth system components are described briefly and the relevant interactions between the components are explained. because the multiple interactions could lead to unstable feedbacks, we go through a careful process of model spin up to ensure that all components are stable and the interactions balanced. this spun-up configuration is evaluated against observed data for the earth system components and is generally found to perform very satisfactorily. the reason for the evaluation phase is that the model is to be used for the core climate simulations carried out by the met office hadley centre for the coupled model intercomparison project (cmip5), so it is essential that addition of the extra complexity does not detract substantially from its climate performance. localised changes in some specific meteorological variables can be identified, but the impacts on the overall simulation of present day climate are slight. this model is proving valuable both for climate predictions, and for investigating the strengths of biogeochemical feedbacks.\n"
http://orkg.org/orkg/resource/R9154,Recent responses to climate change reveal the drivers of species extinction and survival,10.1073/pnas.1913007117,crossref,"<jats:p>Climate change may be a major threat to biodiversity in the next 100 years. Although there has been important work on mechanisms of decline in some species, it generally remains unclear which changes in climate actually cause extinctions, and how many species will likely be lost. Here, we identify the specific changes in climate that are associated with the widespread local extinctions that have already occurred. We then use this information to predict the extent of future biodiversity loss and to identify which processes may forestall extinction. We used data from surveys of 538 plant and animal species over time, 44% of which have already had local extinctions at one or more sites. We found that locations with local extinctions had larger and faster changes in hottest yearly temperatures than those without. Surprisingly, sites with local extinctions had significantly smaller changes in mean annual temperatures, despite the widespread use of mean annual temperatures as proxies for overall climate change. Based on their past rates of dispersal, we estimate that 57–70% of these 538 species will not disperse quickly enough to avoid extinction. However, we show that niche shifts appear to be far more important for avoiding extinction than dispersal, although most studies focus only on dispersal. Specifically, considering both dispersal and niche shifts, we project that only 16–30% of these 538 species may go extinct by 2070. Overall, our results help identify the specific climatic changes that cause extinction and the processes that may help species to survive.</jats:p>","climate change may be a major threat to biodiversity in the next 100 years. although there has been important work on mechanisms of decline in some species, it generally remains unclear which changes in climate actually cause extinctions, and how many species will likely be lost. here, we identify the specific changes in climate that are associated with the widespread local extinctions that have already occurred. we then use this information to predict the extent of future biodiversity loss and to identify which processes may forestall extinction. we used data from surveys of 538 plant and animal species over time, 44% of which have already had local extinctions at one or more sites. we found that locations with local extinctions had larger and faster changes in hottest yearly temperatures than those without. surprisingly, sites with local extinctions had significantly smaller changes in mean annual temperatures, despite the widespread use of mean annual temperatures as proxies for overall climate change. based on their past rates of dispersal, we estimate that 57–70% of these 538 species will not disperse quickly enough to avoid extinction. however, we show that niche shifts appear to be far more important for avoiding extinction than dispersal, although most studies focus only on dispersal. specifically, considering both dispersal and niche shifts, we project that only 16–30% of these 538 species may go extinct by 2070. overall, our results help identify the specific climatic changes that cause extinction and the processes that may help species to survive."
http://orkg.org/orkg/resource/R8735,Calcitonin receptors (version 2019.4) in the IUPHAR/BPS Guide to Pharmacology Database,10.2218/gtopdb/f11/2019.4,crossref,"<jats:p>This receptor family comprises a group of receptors for the calcitonin/CGRP family of peptides. The calcitonin (CT), amylin (AMY), calcitonin gene-related peptide (CGRP) and adrenomedullin (AM) receptors (nomenclature as agreed by the NC-IUPHAR Subcommittee on CGRP, AM, AMY, and CT receptors [122, 67]) are generated by the genes CALCR (which codes for the CT receptor) and CALCRL (which codes for the calcitonin receptor-like receptor, CLR, previously known as CRLR). Their function and pharmacology are altered in the presence of RAMPs (receptor activity-modifying proteins),  which are single TM domain proteins of ca. 130 amino acids, identified as a family of three members; RAMP1, RAMP2 and RAMP3. There are splice variants of the CT receptor; these in turn produce variants of the AMY receptor [122], some of which can be potently activated by CGRP. The endogenous agonists are the peptides calcitonin, α-CGRP (formerly known as CGRP-I), β-CGRP (formerly known as CGRP-II), amylin (occasionally called islet-amyloid polypeptide, diabetes-associated polypeptide), adrenomedullin and adrenomedullin 2/intermedin. There are species differences in peptide sequences, particularly for the CTs. CTR-stimulating peptide (CRSP) is another member of the family with selectivity for the CT receptor but it is not expressed in humans [87]. olcegepant (also known as BIBN4096BS, pKi~10.5) and telcagepant (also known as MK0974, pKi~9) are the most selective antagonists available, showing selectivity for CGRP receptors, with a particular preference for those of primate origin. CLR (calcitonin receptor-like receptor) by itself binds no known endogenous ligand, but in the presence of RAMPs it gives receptors for CGRP, adrenomedullin and adrenomedullin 2/intermedin.</jats:p>","this receptor family comprises a group of receptors for the calcitonin/cgrp family of peptides. the calcitonin (ct), amylin (amy), calcitonin gene-related peptide (cgrp) and adrenomedullin (am) receptors (nomenclature as agreed by the nc-iuphar subcommittee on cgrp, am, amy, and ct receptors [122, 67]) are generated by the genes calcr (which codes for the ct receptor) and calcrl (which codes for the calcitonin receptor-like receptor, clr, previously known as crlr). their function and pharmacology are altered in the presence of ramps (receptor activity-modifying proteins), which are single tm domain proteins of ca. 130 amino acids, identified as a family of three members; ramp1, ramp2 and ramp3. there are splice variants of the ct receptor; these in turn produce variants of the amy receptor [122], some of which can be potently activated by cgrp. the endogenous agonists are the peptides calcitonin, α-cgrp (formerly known as cgrp-i), β-cgrp (formerly known as cgrp-ii), amylin (occasionally called islet-amyloid polypeptide, diabetes-associated polypeptide), adrenomedullin and adrenomedullin 2/intermedin. there are species differences in peptide sequences, particularly for the cts. ctr-stimulating peptide (crsp) is another member of the family with selectivity for the ct receptor but it is not expressed in humans [87]. olcegepant (also known as bibn4096bs, pki~10.5) and telcagepant (also known as mk0974, pki~9) are the most selective antagonists available, showing selectivity for cgrp receptors, with a particular preference for those of primate origin. clr (calcitonin receptor-like receptor) by itself binds no known endogenous ligand, but in the presence of ramps it gives receptors for cgrp, adrenomedullin and adrenomedullin 2/intermedin."
http://orkg.org/orkg/resource/R8624,"Revisiting Style, a Key Concept in Literary Studies",10.1515/jlt-2015-0003,crossref,"<jats:title>Abstract</jats:title><jats:p>Language and literary studies have studied style for centuries, and even since the advent of ›stylistics‹ as a discipline at the beginning of the twentieth century, definitions of ›style‹ have varied heavily across time, space and fields. Today, with increasingly large collections of literary texts being made available in digital form, computational approaches to literary style are proliferating. New methods from disciplines such as corpus linguistics and computer science are being adopted and adapted in interrelated fields such as computational stylistics and corpus stylistics, and are facilitating new approaches to literary style.</jats:p><jats:p>The relation between definitions of style in established linguistic or literary stylistics, and definitions of style in computational or corpus stylistics has not, however, been systematically assessed. This contribution aims to respond to the need to redefine style in the light of this new situation and to establish a clearer perception of both the overlap and the boundaries between ›mainstream‹ and ›computational‹ and/or ›empirical‹ literary stylistics. While stylistic studies of non-literary texts are currently flourishing, our contribution deliberately centers on those approaches relevant to ›literary stylistics‹. It concludes by proposing an operational definition of style that we hope can act as a common ground for diverse approaches to literary style, fostering transdisciplinary research.</jats:p><jats:p>The focus of this contribution is on literary style in linguistics and literary studies (rather than in art history, musicology or fashion), on textual aspects of style (rather than production- or reception-oriented theories of style), and on a descriptive perspective (rather than a prescriptive or didactic one). Even within these limits, however, it appears necessary to build on a broad understanding of the various perspectives on style that have been adopted at different times and in different traditions. For this reason, the contribution first traces the development of the notion of style in three different traditions, those of German, Dutch and French language and literary studies. Despite the numerous links between each other, and between each of them to the British and American traditions, these three traditions each have their proper dynamics, especially with regard to the convergence and/or confrontation between mainstream and computational stylistics. For reasons of space and coherence, the contribution is limited to theoretical developments occurring since 1945.</jats:p><jats:p>The contribution begins by briefly outlining the range of definitions of style that can be encountered across traditions today: style as revealing a higher-order aesthetic value, as the holistic ›gestalt‹ of single texts, as an expression of the individuality of an author, as an artifact presupposing choice among alternatives, as a deviation from a norm or reference, or as any formal property of a text. The contribution then traces the development of definitions of style in each of the three traditions mentioned, with the aim of giving a concise account of how, in each tradition, definitions of style have evolved over time, with special regard to the way such definitions relate to empirical, quantitative or otherwise computational approaches to style in literary texts. It will become apparent how, in each of the three traditions, foundational texts continue to influence current discussions on literary style, but also how stylistics has continuously reacted to broader developments in cultural and literary theory, and how empirical, quantitative or computational approaches have long \xadexisted, usually in parallel to or at the margins of mainstream stylistics. The review will also reflect the lines of discussion around style as a property of literary texts – or of any textual entity in general.</jats:p><jats:p>The perspective on three stylistic traditions is accompanied by a more systematic perspective. The rationale is to work towards a common ground for literary scholars and linguists when talking about (literary) style, across traditions of stylistics, with respect for established definitions of style, but also in light of the digital paradigm. Here, we first show to what extent, at similar or different moments in time, the three traditions have developed comparable positions on style, and which definitions out of the range of possible definitions have been proposed or promoted by which authors in each of the three traditions.</jats:p><jats:p>On the basis of this synthesis, we then conclude by proposing an operational definition of style that is an attempt to provide a common ground for both mainstream and computational literary stylistics. This definition is discussed in some detail in order to explain not only what is meant by each term in the definition, but also how it relates to computational analyses of style – and how this definition aims to avoid some of the pitfalls that can be perceived in earlier definitions of style. Our definition, we hope, will be put to use by a new generation of computational, quantitative, and empirical studies of style in literary texts.</jats:p>","abstract language and literary studies have studied style for centuries, and even since the advent of ›stylistics‹ as a discipline at the beginning of the twentieth century, definitions of ›style‹ have varied heavily across time, space and fields. today, with increasingly large collections of literary texts being made available in digital form, computational approaches to literary style are proliferating. new methods from disciplines such as corpus linguistics and computer science are being adopted and adapted in interrelated fields such as computational stylistics and corpus stylistics, and are facilitating new approaches to literary style. the relation between definitions of style in established linguistic or literary stylistics, and definitions of style in computational or corpus stylistics has not, however, been systematically assessed. this contribution aims to respond to the need to redefine style in the light of this new situation and to establish a clearer perception of both the overlap and the boundaries between ›mainstream‹ and ›computational‹ and/or ›empirical‹ literary stylistics. while stylistic studies of non-literary texts are currently flourishing, our contribution deliberately centers on those approaches relevant to ›literary stylistics‹. it concludes by proposing an operational definition of style that we hope can act as a common ground for diverse approaches to literary style, fostering transdisciplinary research. the focus of this contribution is on literary style in linguistics and literary studies (rather than in art history, musicology or fashion), on textual aspects of style (rather than production- or reception-oriented theories of style), and on a descriptive perspective (rather than a prescriptive or didactic one). even within these limits, however, it appears necessary to build on a broad understanding of the various perspectives on style that have been adopted at different times and in different traditions. for this reason, the contribution first traces the development of the notion of style in three different traditions, those of german, dutch and french language and literary studies. despite the numerous links between each other, and between each of them to the british and american traditions, these three traditions each have their proper dynamics, especially with regard to the convergence and/or confrontation between mainstream and computational stylistics. for reasons of space and coherence, the contribution is limited to theoretical developments occurring since 1945. the contribution begins by briefly outlining the range of definitions of style that can be encountered across traditions today: style as revealing a higher-order aesthetic value, as the holistic ›gestalt‹ of single texts, as an expression of the individuality of an author, as an artifact presupposing choice among alternatives, as a deviation from a norm or reference, or as any formal property of a text. the contribution then traces the development of definitions of style in each of the three traditions mentioned, with the aim of giving a concise account of how, in each tradition, definitions of style have evolved over time, with special regard to the way such definitions relate to empirical, quantitative or otherwise computational approaches to style in literary texts. it will become apparent how, in each of the three traditions, foundational texts continue to influence current discussions on literary style, but also how stylistics has continuously reacted to broader developments in cultural and literary theory, and how empirical, quantitative or computational approaches have long \xadexisted, usually in parallel to or at the margins of mainstream stylistics. the review will also reflect the lines of discussion around style as a property of literary texts – or of any textual entity in general. the perspective on three stylistic traditions is accompanied by a more systematic perspective. the rationale is to work towards a common ground for literary scholars and linguists when talking about (literary) style, across traditions of stylistics, with respect for established definitions of style, but also in light of the digital paradigm. here, we first show to what extent, at similar or different moments in time, the three traditions have developed comparable positions on style, and which definitions out of the range of possible definitions have been proposed or promoted by which authors in each of the three traditions. on the basis of this synthesis, we then conclude by proposing an operational definition of style that is an attempt to provide a common ground for both mainstream and computational literary stylistics. this definition is discussed in some detail in order to explain not only what is meant by each term in the definition, but also how it relates to computational analyses of style – and how this definition aims to avoid some of the pitfalls that can be perceived in earlier definitions of style. our definition, we hope, will be put to use by a new generation of computational, quantitative, and empirical studies of style in literary texts."
http://orkg.org/orkg/resource/R8689,SMDM: enhancing enterprise-wide master data management using semantic web technologies,10.14778/1687553.1687600,crossref,"""<jats:p>Motivated by evolving business requirements and novel enterprise applications, we propose and implement the Semantic Master Data Management (SMDM), a semantics-level enhancement to the existing MDM solutions. The SMDM system publishes relational-based master data as virtual RDF store, and injects instantaneous reasoning capabilities into semantic queries. Two kinds of ontologies are introduced to the system, the core MDM ontology and the external imported domain ontology. SMDM enables data linking among multi-domains, implicit relationship discovery, and declarative definition and extension of business policies and entities. Based on these functions, modern companies can customize their applications and services on demand within the MDM hub. In the demonstration, we build the system environment based on IBM's MDM solution, and run the use cases on the master data of an insurance company.</jats:p>""",""" motivated by evolving business requirements and novel enterprise applications, we propose and implement the semantic master data management (smdm), a semantics-level enhancement to the existing mdm solutions. the smdm system publishes relational-based master data as virtual rdf store, and injects instantaneous reasoning capabilities into semantic queries. two kinds of ontologies are introduced to the system, the core mdm ontology and the external imported domain ontology. smdm enables data linking among multi-domains, implicit relationship discovery, and declarative definition and extension of business policies and entities. based on these functions, modern companies can customize their applications and services on demand within the mdm hub. in the demonstration, we build the system environment based on ibm's mdm solution, and run the use cases on the master data of an insurance company. """
http://orkg.org/orkg/resource/R8707,SMDM: enhancing enterprise-wide master data management using semantic web technologies,10.14778/1687553.1687600,crossref,"""<jats:p>Motivated by evolving business requirements and novel enterprise applications, we propose and implement the Semantic Master Data Management (SMDM), a semantics-level enhancement to the existing MDM solutions. The SMDM system publishes relational-based master data as virtual RDF store, and injects instantaneous reasoning capabilities into semantic queries. Two kinds of ontologies are introduced to the system, the core MDM ontology and the external imported domain ontology. SMDM enables data linking among multi-domains, implicit relationship discovery, and declarative definition and extension of business policies and entities. Based on these functions, modern companies can customize their applications and services on demand within the MDM hub. In the demonstration, we build the system environment based on IBM's MDM solution, and run the use cases on the master data of an insurance company.</jats:p>""",""" motivated by evolving business requirements and novel enterprise applications, we propose and implement the semantic master data management (smdm), a semantics-level enhancement to the existing mdm solutions. the smdm system publishes relational-based master data as virtual rdf store, and injects instantaneous reasoning capabilities into semantic queries. two kinds of ontologies are introduced to the system, the core mdm ontology and the external imported domain ontology. smdm enables data linking among multi-domains, implicit relationship discovery, and declarative definition and extension of business policies and entities. based on these functions, modern companies can customize their applications and services on demand within the mdm hub. in the demonstration, we build the system environment based on ibm's mdm solution, and run the use cases on the master data of an insurance company. """
http://orkg.org/orkg/resource/R9524,An ontology of scientific experiments,10.1098/rsif.2006.0134,crossref,"<jats:p>The formal description of experiments for efficient analysis, annotation and sharing of results is a fundamental part of the practice of science. Ontologies are required to achieve this objective. A few subject-specific ontologies of experiments currently exist. However, despite the unity of scientific experimentation, no general ontology of experiments exists. We propose the ontology EXPO to meet this need. EXPO links the SUMO (the Suggested Upper Merged Ontology) with subject-specific ontologies of experiments by formalizing the generic concepts of experimental design, methodology and results representation. EXPO is expressed in the W3C standard ontology language OWL-DL. We demonstrate the utility of EXPO and its ability to describe different experimental domains, by applying it to two experiments: one in high-energy physics and the other in phylogenetics. The use of EXPO made the goals and structure of these experiments more explicit, revealed ambiguities, and highlighted an unexpected similarity. We conclude that, EXPO is of general value in describing experiments and a step towards the formalization of science.</jats:p>","the formal description of experiments for efficient analysis, annotation and sharing of results is a fundamental part of the practice of science. ontologies are required to achieve this objective. a few subject-specific ontologies of experiments currently exist. however, despite the unity of scientific experimentation, no general ontology of experiments exists. we propose the ontology expo to meet this need. expo links the sumo (the suggested upper merged ontology) with subject-specific ontologies of experiments by formalizing the generic concepts of experimental design, methodology and results representation. expo is expressed in the w3c standard ontology language owl-dl. we demonstrate the utility of expo and its ability to describe different experimental domains, by applying it to two experiments: one in high-energy physics and the other in phylogenetics. the use of expo made the goals and structure of these experiments more explicit, revealed ambiguities, and highlighted an unexpected similarity. we conclude that, expo is of general value in describing experiments and a step towards the formalization of science."
http://orkg.org/orkg/resource/R9557,An ontology of scientific experiments,10.1098/rsif.2006.0134,crossref,"<jats:p>The formal description of experiments for efficient analysis, annotation and sharing of results is a fundamental part of the practice of science. Ontologies are required to achieve this objective. A few subject-specific ontologies of experiments currently exist. However, despite the unity of scientific experimentation, no general ontology of experiments exists. We propose the ontology EXPO to meet this need. EXPO links the SUMO (the Suggested Upper Merged Ontology) with subject-specific ontologies of experiments by formalizing the generic concepts of experimental design, methodology and results representation. EXPO is expressed in the W3C standard ontology language OWL-DL. We demonstrate the utility of EXPO and its ability to describe different experimental domains, by applying it to two experiments: one in high-energy physics and the other in phylogenetics. The use of EXPO made the goals and structure of these experiments more explicit, revealed ambiguities, and highlighted an unexpected similarity. We conclude that, EXPO is of general value in describing experiments and a step towards the formalization of science.</jats:p>","the formal description of experiments for efficient analysis, annotation and sharing of results is a fundamental part of the practice of science. ontologies are required to achieve this objective. a few subject-specific ontologies of experiments currently exist. however, despite the unity of scientific experimentation, no general ontology of experiments exists. we propose the ontology expo to meet this need. expo links the sumo (the suggested upper merged ontology) with subject-specific ontologies of experiments by formalizing the generic concepts of experimental design, methodology and results representation. expo is expressed in the w3c standard ontology language owl-dl. we demonstrate the utility of expo and its ability to describe different experimental domains, by applying it to two experiments: one in high-energy physics and the other in phylogenetics. the use of expo made the goals and structure of these experiments more explicit, revealed ambiguities, and highlighted an unexpected similarity. we conclude that, expo is of general value in describing experiments and a step towards the formalization of science."
http://orkg.org/orkg/resource/R12223,Modelling the epidemic trend of the 2019 novel coronavirus outbreak in China,10.1101/2020.01.23.916726,crossref,"<jats:p>We present a timely evaluation of the Chinese 2019-nCov epidemic in its initial phase, where 2019-nCov demonstrates comparable transmissibility but lower fatality rates than SARS and MERS. A quick diagnosis that leads to case isolation and integrated interventions will have a major impact on its future trend. Nevertheless, as China is facing its Spring Festival travel rush and the epidemic has spread beyond its borders, further investigation on its potential spatiotemporal transmission pattern and novel intervention strategies are warranted.</jats:p>","we present a timely evaluation of the chinese 2019-ncov epidemic in its initial phase, where 2019-ncov demonstrates comparable transmissibility but lower fatality rates than sars and mers. a quick diagnosis that leads to case isolation and integrated interventions will have a major impact on its future trend. nevertheless, as china is facing its spring festival travel rush and the epidemic has spread beyond its borders, further investigation on its potential spatiotemporal transmission pattern and novel intervention strategies are warranted."
http://orkg.org/orkg/resource/R12226,Time-varying transmission dynamics of Novel Coronavirus Pneumonia in China,10.1101/2020.01.25.919787,crossref,"<jats:title>ABSTRACT</jats:title><jats:sec><jats:title>Rationale</jats:title><jats:p>Several studies have estimated basic production number of novel coronavirus pneumonia (NCP). However, the time-varying transmission dynamics of NCP during the outbreak remain unclear.</jats:p></jats:sec><jats:sec><jats:title>Objectives</jats:title><jats:p>We aimed to estimate the basic and time-varying transmission dynamics of NCP across China, and compared them with SARS.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>Data on NCP cases by February 7, 2020 were collected from epidemiological investigations or official websites. Data on severe acute respiratory syndrome (SARS) cases in Guangdong Province, Beijing and Hong Kong during 2002-2003 were also obtained. We estimated the doubling time, basic reproduction number (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>) and time-varying reproduction number (<jats:italic>R<jats:sub>t</jats:sub></jats:italic>) of NCP and SARS.</jats:p></jats:sec><jats:sec><jats:title>Measurements and main results</jats:title><jats:p>As of February 7, 2020, 34,598 NCP cases were identified in China, and daily confirmed cases decreased after February 4. The doubling time of NCP nationwide was 2.4 days which was shorter than that of SARS in Guangdong (14.3 days), Hong Kong (5.7 days) and Beijing (12.4 days). The<jats:italic>R<jats:sub>0</jats:sub></jats:italic>of NCP cases nationwide and in Wuhan were 4.5 and 4.4 respectively, which were higher than<jats:italic>R<jats:sub>0</jats:sub></jats:italic>of SARS in Guangdong (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>=2.3), Hongkong (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>=2.3), and Beijing (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>=2.6). The<jats:italic>R<jats:sub>t</jats:sub></jats:italic>for NCP continuously decreased especially after January 16 nationwide and in Wuhan. The<jats:italic>R<jats:sub>0</jats:sub></jats:italic>for secondary NCP cases in Guangdong was 0.6, and the<jats:italic>R<jats:sub>t</jats:sub></jats:italic>values were less than 1 during the epidemic.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>NCP may have a higher transmissibility than SARS, and the efforts of containing the outbreak are effective. However, the efforts are needed to persist in for reducing time-varying reproduction number below one.</jats:p></jats:sec><jats:sec><jats:title>At a Glance Commentary</jats:title><jats:sec><jats:title>Scientific Knowledge on the Subject</jats:title><jats:p>Since December 29, 2019, pneumonia infection with 2019-nCoV, now named as Novel Coronavirus Pneumonia (NCP), occurred in Wuhan, Hubei Province, China. The disease has rapidly spread from Wuhan to other areas. As a novel virus, the time-varying transmission dynamics of NCP remain unclear, and it is also important to compare it with SARS.</jats:p></jats:sec><jats:sec><jats:title>What This Study Adds to the Field</jats:title><jats:p>We compared the transmission dynamics of NCP with SARS, and found that NCP has a higher transmissibility than SARS. Time-varying production number indicates that rigorous control measures taken by governments are effective across China, and persistent efforts are needed to be taken for reducing instantaneous reproduction number below one.</jats:p></jats:sec></jats:sec>","abstract rationale several studies have estimated basic production number of novel coronavirus pneumonia (ncp). however, the time-varying transmission dynamics of ncp during the outbreak remain unclear. objectives we aimed to estimate the basic and time-varying transmission dynamics of ncp across china, and compared them with sars. methods data on ncp cases by february 7, 2020 were collected from epidemiological investigations or official websites. data on severe acute respiratory syndrome (sars) cases in guangdong province, beijing and hong kong during 2002-2003 were also obtained. we estimated the doubling time, basic reproduction number ( r 0 ) and time-varying reproduction number ( r t ) of ncp and sars. measurements and main results as of february 7, 2020, 34,598 ncp cases were identified in china, and daily confirmed cases decreased after february 4. the doubling time of ncp nationwide was 2.4 days which was shorter than that of sars in guangdong (14.3 days), hong kong (5.7 days) and beijing (12.4 days). the r 0 of ncp cases nationwide and in wuhan were 4.5 and 4.4 respectively, which were higher than r 0 of sars in guangdong ( r 0 =2.3), hongkong ( r 0 =2.3), and beijing ( r 0 =2.6). the r t for ncp continuously decreased especially after january 16 nationwide and in wuhan. the r 0 for secondary ncp cases in guangdong was 0.6, and the r t values were less than 1 during the epidemic. conclusions ncp may have a higher transmissibility than sars, and the efforts of containing the outbreak are effective. however, the efforts are needed to persist in for reducing time-varying reproduction number below one. at a glance commentary scientific knowledge on the subject since december 29, 2019, pneumonia infection with 2019-ncov, now named as novel coronavirus pneumonia (ncp), occurred in wuhan, hubei province, china. the disease has rapidly spread from wuhan to other areas. as a novel virus, the time-varying transmission dynamics of ncp remain unclear, and it is also important to compare it with sars. what this study adds to the field we compared the transmission dynamics of ncp with sars, and found that ncp has a higher transmissibility than sars. time-varying production number indicates that rigorous control measures taken by governments are effective across china, and persistent efforts are needed to be taken for reducing instantaneous reproduction number below one."
http://orkg.org/orkg/resource/R12228,Time-varying transmission dynamics of Novel Coronavirus Pneumonia in China,10.1101/2020.01.25.919787,crossref,"<jats:title>ABSTRACT</jats:title><jats:sec><jats:title>Rationale</jats:title><jats:p>Several studies have estimated basic production number of novel coronavirus pneumonia (NCP). However, the time-varying transmission dynamics of NCP during the outbreak remain unclear.</jats:p></jats:sec><jats:sec><jats:title>Objectives</jats:title><jats:p>We aimed to estimate the basic and time-varying transmission dynamics of NCP across China, and compared them with SARS.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>Data on NCP cases by February 7, 2020 were collected from epidemiological investigations or official websites. Data on severe acute respiratory syndrome (SARS) cases in Guangdong Province, Beijing and Hong Kong during 2002-2003 were also obtained. We estimated the doubling time, basic reproduction number (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>) and time-varying reproduction number (<jats:italic>R<jats:sub>t</jats:sub></jats:italic>) of NCP and SARS.</jats:p></jats:sec><jats:sec><jats:title>Measurements and main results</jats:title><jats:p>As of February 7, 2020, 34,598 NCP cases were identified in China, and daily confirmed cases decreased after February 4. The doubling time of NCP nationwide was 2.4 days which was shorter than that of SARS in Guangdong (14.3 days), Hong Kong (5.7 days) and Beijing (12.4 days). The<jats:italic>R<jats:sub>0</jats:sub></jats:italic>of NCP cases nationwide and in Wuhan were 4.5 and 4.4 respectively, which were higher than<jats:italic>R<jats:sub>0</jats:sub></jats:italic>of SARS in Guangdong (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>=2.3), Hongkong (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>=2.3), and Beijing (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>=2.6). The<jats:italic>R<jats:sub>t</jats:sub></jats:italic>for NCP continuously decreased especially after January 16 nationwide and in Wuhan. The<jats:italic>R<jats:sub>0</jats:sub></jats:italic>for secondary NCP cases in Guangdong was 0.6, and the<jats:italic>R<jats:sub>t</jats:sub></jats:italic>values were less than 1 during the epidemic.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>NCP may have a higher transmissibility than SARS, and the efforts of containing the outbreak are effective. However, the efforts are needed to persist in for reducing time-varying reproduction number below one.</jats:p></jats:sec><jats:sec><jats:title>At a Glance Commentary</jats:title><jats:sec><jats:title>Scientific Knowledge on the Subject</jats:title><jats:p>Since December 29, 2019, pneumonia infection with 2019-nCoV, now named as Novel Coronavirus Pneumonia (NCP), occurred in Wuhan, Hubei Province, China. The disease has rapidly spread from Wuhan to other areas. As a novel virus, the time-varying transmission dynamics of NCP remain unclear, and it is also important to compare it with SARS.</jats:p></jats:sec><jats:sec><jats:title>What This Study Adds to the Field</jats:title><jats:p>We compared the transmission dynamics of NCP with SARS, and found that NCP has a higher transmissibility than SARS. Time-varying production number indicates that rigorous control measures taken by governments are effective across China, and persistent efforts are needed to be taken for reducing instantaneous reproduction number below one.</jats:p></jats:sec></jats:sec>","abstract rationale several studies have estimated basic production number of novel coronavirus pneumonia (ncp). however, the time-varying transmission dynamics of ncp during the outbreak remain unclear. objectives we aimed to estimate the basic and time-varying transmission dynamics of ncp across china, and compared them with sars. methods data on ncp cases by february 7, 2020 were collected from epidemiological investigations or official websites. data on severe acute respiratory syndrome (sars) cases in guangdong province, beijing and hong kong during 2002-2003 were also obtained. we estimated the doubling time, basic reproduction number ( r 0 ) and time-varying reproduction number ( r t ) of ncp and sars. measurements and main results as of february 7, 2020, 34,598 ncp cases were identified in china, and daily confirmed cases decreased after february 4. the doubling time of ncp nationwide was 2.4 days which was shorter than that of sars in guangdong (14.3 days), hong kong (5.7 days) and beijing (12.4 days). the r 0 of ncp cases nationwide and in wuhan were 4.5 and 4.4 respectively, which were higher than r 0 of sars in guangdong ( r 0 =2.3), hongkong ( r 0 =2.3), and beijing ( r 0 =2.6). the r t for ncp continuously decreased especially after january 16 nationwide and in wuhan. the r 0 for secondary ncp cases in guangdong was 0.6, and the r t values were less than 1 during the epidemic. conclusions ncp may have a higher transmissibility than sars, and the efforts of containing the outbreak are effective. however, the efforts are needed to persist in for reducing time-varying reproduction number below one. at a glance commentary scientific knowledge on the subject since december 29, 2019, pneumonia infection with 2019-ncov, now named as novel coronavirus pneumonia (ncp), occurred in wuhan, hubei province, china. the disease has rapidly spread from wuhan to other areas. as a novel virus, the time-varying transmission dynamics of ncp remain unclear, and it is also important to compare it with sars. what this study adds to the field we compared the transmission dynamics of ncp with sars, and found that ncp has a higher transmissibility than sars. time-varying production number indicates that rigorous control measures taken by governments are effective across china, and persistent efforts are needed to be taken for reducing instantaneous reproduction number below one."
http://orkg.org/orkg/resource/R12231,Novel coronavirus 2019-nCoV: early estimation of epidemiological parameters and epidemic predictions,10.1101/2020.01.23.20018549,crossref,"<jats:title>Abstract</jats:title><jats:p>Since first identified, the epidemic scale of the recently emerged novel coronavirus (2019-nCoV) in Wuhan, China, has increased rapidly, with cases arising across China and other countries and regions. using a transmission model, we estimate a basic reproductive number of 3.11 (95%CI, 2.39–4.13); 58–76% of transmissions must be prevented to stop increasing; Wuhan case ascertainment of 5.0% (3.6–7.4); 21022 (11090–33490) total infections in Wuhan 1 to 22 January.</jats:p><jats:sec><jats:title>Changes to previous version</jats:title><jats:list list-type=""bullet""><jats:list-item><jats:p>case data updated to include 22 Jan 2020; we did not use cases reported after this period as cases were reported at the province level hereafter, and large-scale control interventions were initiated on 23 Jan 2020;</jats:p></jats:list-item><jats:list-item><jats:p>improved likelihood function, better accounting for first 41 confirmed cases, and now using all infections (rather than just cases detected) in Wuhan for prediction of infection in international travellers;</jats:p></jats:list-item><jats:list-item><jats:p>improved characterization of uncertainty in parameters, and calculation of epidemic trajectory confidence intervals using a more statistically rigorous method;</jats:p></jats:list-item><jats:list-item><jats:p>extended range of latent period in sensitivity analysis to reflect reports of up to 6 day incubation period in household clusters;</jats:p></jats:list-item><jats:list-item><jats:p>removed travel restriction analysis, as different modelling approaches (e.g. stochastic transmission, rather than deterministic transmission) are more appropriate to such analyses.</jats:p></jats:list-item></jats:list></jats:sec>","abstract since first identified, the epidemic scale of the recently emerged novel coronavirus (2019-ncov) in wuhan, china, has increased rapidly, with cases arising across china and other countries and regions. using a transmission model, we estimate a basic reproductive number of 3.11 (95%ci, 2.39–4.13); 58–76% of transmissions must be prevented to stop increasing; wuhan case ascertainment of 5.0% (3.6–7.4); 21022 (11090–33490) total infections in wuhan 1 to 22 january. changes to previous version case data updated to include 22 jan 2020; we did not use cases reported after this period as cases were reported at the province level hereafter, and large-scale control interventions were initiated on 23 jan 2020; improved likelihood function, better accounting for first 41 confirmed cases, and now using all infections (rather than just cases detected) in wuhan for prediction of infection in international travellers; improved characterization of uncertainty in parameters, and calculation of epidemic trajectory confidence intervals using a more statistically rigorous method; extended range of latent period in sensitivity analysis to reflect reports of up to 6 day incubation period in household clusters; removed travel restriction analysis, as different modelling approaches (e.g. stochastic transmission, rather than deterministic transmission) are more appropriate to such analyses."
http://orkg.org/orkg/resource/R12235,Estimating the effective reproduction number of the 2019-nCoV in China,10.1101/2020.01.27.20018952,crossref,<jats:title>Abstract</jats:title><jats:p>We estimate the effective reproduction number for 2019-nCoV based on the daily reported cases from China CDC. The results indicate that 2019-nCoV has a higher effective reproduction number than SARS with a comparable fatality rate.</jats:p><jats:sec><jats:title>Article Summary Line</jats:title><jats:p>This modeling study indicates that 2019-nCoV has a higher effective reproduction number than SARS with a comparable fatality rate.</jats:p></jats:sec>,abstract we estimate the effective reproduction number for 2019-ncov based on the daily reported cases from china cdc. the results indicate that 2019-ncov has a higher effective reproduction number than sars with a comparable fatality rate. article summary line this modeling study indicates that 2019-ncov has a higher effective reproduction number than sars with a comparable fatality rate.
http://orkg.org/orkg/resource/R12237,"Preliminary estimation of the basic reproduction number of novel coronavirus (2019-nCoV) in China, from 2019 to 2020: A data-driven analysis in the early phase of the outbreak",10.1101/2020.01.23.916395,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Backgrounds</jats:title><jats:p>An ongoing outbreak of a novel coronavirus (2019-nCoV) pneumonia hit a major city of China, Wuhan, December 2019 and subsequently reached other provinces/regions of China and countries. We present estimates of the basic reproduction number, <jats:italic>R</jats:italic><jats:sub>0</jats:sub>, of 2019-nCoV in the early phase of the outbreak.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>Accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019-nCoV cases time series, in mainland China from January 10 to January 24, 2020, through the exponential growth. With the estimated intrinsic growth rate (<jats:italic>γ</jats:italic>), we estimated <jats:italic>R</jats:italic><jats:sub>0</jats:sub> by using the serial intervals (SI) of two other well-known coronavirus diseases, MERS and SARS, as approximations for the true unknown SI.</jats:p></jats:sec><jats:sec><jats:title>Findings</jats:title><jats:p>The early outbreak data largely follows the exponential growth. We estimated that the mean <jats:italic>R</jats:italic><jats:sub>0</jats:sub> ranges from 2.24 (95%CI: 1.96-2.55) to 3.58 (95%CI: 2.89-4.39) associated with 8-fold to 2-fold increase in the reporting rate. We demonstrated that changes in reporting rate substantially affect estimates of <jats:italic>R</jats:italic><jats:sub>0</jats:sub>.</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>The mean estimate of <jats:italic>R</jats:italic><jats:sub>0</jats:sub> for the 2019-nCoV ranges from 2.24 to 3.58, and significantly larger than 1. Our findings indicate the potential of 2019-nCoV to cause outbreaks.</jats:p></jats:sec>","abstract backgrounds an ongoing outbreak of a novel coronavirus (2019-ncov) pneumonia hit a major city of china, wuhan, december 2019 and subsequently reached other provinces/regions of china and countries. we present estimates of the basic reproduction number, r 0 , of 2019-ncov in the early phase of the outbreak. methods accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019-ncov cases time series, in mainland china from january 10 to january 24, 2020, through the exponential growth. with the estimated intrinsic growth rate ( γ ), we estimated r 0 by using the serial intervals (si) of two other well-known coronavirus diseases, mers and sars, as approximations for the true unknown si. findings the early outbreak data largely follows the exponential growth. we estimated that the mean r 0 ranges from 2.24 (95%ci: 1.96-2.55) to 3.58 (95%ci: 2.89-4.39) associated with 8-fold to 2-fold increase in the reporting rate. we demonstrated that changes in reporting rate substantially affect estimates of r 0 . conclusion the mean estimate of r 0 for the 2019-ncov ranges from 2.24 to 3.58, and significantly larger than 1. our findings indicate the potential of 2019-ncov to cause outbreaks."
http://orkg.org/orkg/resource/R12239,"Preliminary estimation of the basic reproduction number of novel coronavirus (2019-nCoV) in China, from 2019 to 2020: A data-driven analysis in the early phase of the outbreak",10.1101/2020.01.23.916395,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Backgrounds</jats:title><jats:p>An ongoing outbreak of a novel coronavirus (2019-nCoV) pneumonia hit a major city of China, Wuhan, December 2019 and subsequently reached other provinces/regions of China and countries. We present estimates of the basic reproduction number, <jats:italic>R</jats:italic><jats:sub>0</jats:sub>, of 2019-nCoV in the early phase of the outbreak.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>Accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019-nCoV cases time series, in mainland China from January 10 to January 24, 2020, through the exponential growth. With the estimated intrinsic growth rate (<jats:italic>γ</jats:italic>), we estimated <jats:italic>R</jats:italic><jats:sub>0</jats:sub> by using the serial intervals (SI) of two other well-known coronavirus diseases, MERS and SARS, as approximations for the true unknown SI.</jats:p></jats:sec><jats:sec><jats:title>Findings</jats:title><jats:p>The early outbreak data largely follows the exponential growth. We estimated that the mean <jats:italic>R</jats:italic><jats:sub>0</jats:sub> ranges from 2.24 (95%CI: 1.96-2.55) to 3.58 (95%CI: 2.89-4.39) associated with 8-fold to 2-fold increase in the reporting rate. We demonstrated that changes in reporting rate substantially affect estimates of <jats:italic>R</jats:italic><jats:sub>0</jats:sub>.</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>The mean estimate of <jats:italic>R</jats:italic><jats:sub>0</jats:sub> for the 2019-nCoV ranges from 2.24 to 3.58, and significantly larger than 1. Our findings indicate the potential of 2019-nCoV to cause outbreaks.</jats:p></jats:sec>","abstract backgrounds an ongoing outbreak of a novel coronavirus (2019-ncov) pneumonia hit a major city of china, wuhan, december 2019 and subsequently reached other provinces/regions of china and countries. we present estimates of the basic reproduction number, r 0 , of 2019-ncov in the early phase of the outbreak. methods accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019-ncov cases time series, in mainland china from january 10 to january 24, 2020, through the exponential growth. with the estimated intrinsic growth rate ( γ ), we estimated r 0 by using the serial intervals (si) of two other well-known coronavirus diseases, mers and sars, as approximations for the true unknown si. findings the early outbreak data largely follows the exponential growth. we estimated that the mean r 0 ranges from 2.24 (95%ci: 1.96-2.55) to 3.58 (95%ci: 2.89-4.39) associated with 8-fold to 2-fold increase in the reporting rate. we demonstrated that changes in reporting rate substantially affect estimates of r 0 . conclusion the mean estimate of r 0 for the 2019-ncov ranges from 2.24 to 3.58, and significantly larger than 1. our findings indicate the potential of 2019-ncov to cause outbreaks."
http://orkg.org/orkg/resource/R12243,Pattern of early human-to-human transmission of Wuhan 2019-nCoV,10.1101/2020.01.23.917351,crossref,"<jats:title>ABSTRACT</jats:title><jats:p>On December 31, 2019, the World Health Organization was notified about a cluster of pneumonia of unknown aetiology in the city of Wuhan, China. Chinese authorities later identified a new coronavirus (2019-nCoV) as the causative agent of the outbreak. As of January 23, 2020, 655 cases have been confirmed in China and several other countries. Understanding the transmission characteristics and the potential for sustained human-to-human transmission of 2019-nCoV is critically important for coordinating current screening and containment strategies, and determining whether the outbreak constitutes a public health emergency of international concern (PHEIC). We performed stochastic simulations of early outbreak trajectories that are consistent with the epidemiological findings to date. We found the basic reproduction number, <jats:italic>R</jats:italic><jats:sub>0</jats:sub>, to be around 2.2 (90% high density interval 1.4—3.8), indicating the potential for sustained human-to-human transmission. Transmission characteristics appear to be of a similar magnitude to severe acute respiratory syndrome-related coronavirus (SARS-CoV) and the 1918 pandemic influenza. These findings underline the importance of heightened screening, surveillance and control efforts, particularly at airports and other travel hubs, in order to prevent further international spread of 2019-nCoV.</jats:p>","abstract on december 31, 2019, the world health organization was notified about a cluster of pneumonia of unknown aetiology in the city of wuhan, china. chinese authorities later identified a new coronavirus (2019-ncov) as the causative agent of the outbreak. as of january 23, 2020, 655 cases have been confirmed in china and several other countries. understanding the transmission characteristics and the potential for sustained human-to-human transmission of 2019-ncov is critically important for coordinating current screening and containment strategies, and determining whether the outbreak constitutes a public health emergency of international concern (pheic). we performed stochastic simulations of early outbreak trajectories that are consistent with the epidemiological findings to date. we found the basic reproduction number, r 0 , to be around 2.2 (90% high density interval 1.4—3.8), indicating the potential for sustained human-to-human transmission. transmission characteristics appear to be of a similar magnitude to severe acute respiratory syndrome-related coronavirus (sars-cov) and the 1918 pandemic influenza. these findings underline the importance of heightened screening, surveillance and control efforts, particularly at airports and other travel hubs, in order to prevent further international spread of 2019-ncov."
http://orkg.org/orkg/resource/R12252,A new coronavirus associated with human respiratory disease in China,10.1038/s41586-020-2008-3,crossref,"<jats:title>Abstract</jats:title><jats:p>Emerging infectious diseases, such as severe acute respiratory syndrome (SARS) and Zika virus disease, present a major threat to public health<jats:sup>1–3</jats:sup>. Despite intense research efforts, how, when and where new diseases appear are still a source of considerable uncertainty. A severe respiratory disease was recently reported in Wuhan, Hubei province, China. As of 25 January 2020, at least 1,975\xa0cases had been reported since the first patient was hospitalized on 12 December 2019. Epidemiological investigations have suggested that the outbreak was associated with a seafood market in Wuhan. Here we study a single patient who was a worker at the market and who was admitted to the Central Hospital of Wuhan on 26 December 2019 while experiencing a severe respiratory syndrome that included fever, dizziness and a cough. Metagenomic RNA sequencing<jats:sup>4</jats:sup> of a sample of bronchoalveolar lavage fluid from the patient identified a new RNA virus strain from the family <jats:italic>Coronaviridae</jats:italic>, which is designated here ‘WH-Human 1’ coronavirus (and has also been referred to as ‘2019-nCoV’). Phylogenetic analysis of the complete viral genome (29,903\xa0nucleotides) revealed that the virus was most closely related (89.1% nucleotide similarity) to a group of SARS-like coronaviruses (genus Betacoronavirus, subgenus Sarbecovirus) that had previously been found in bats in China<jats:sup>5</jats:sup>. This outbreak highlights the ongoing ability of viral spill-over from animals to cause severe disease in humans.</jats:p>","abstract emerging infectious diseases, such as severe acute respiratory syndrome (sars) and zika virus disease, present a major threat to public health 1–3 . despite intense research efforts, how, when and where new diseases appear are still a source of considerable uncertainty. a severe respiratory disease was recently reported in wuhan, hubei province, china. as of 25 january 2020, at least 1,975\xa0cases had been reported since the first patient was hospitalized on 12 december 2019. epidemiological investigations have suggested that the outbreak was associated with a seafood market in wuhan. here we study a single patient who was a worker at the market and who was admitted to the central hospital of wuhan on 26 december 2019 while experiencing a severe respiratory syndrome that included fever, dizziness and a cough. metagenomic rna sequencing 4 of a sample of bronchoalveolar lavage fluid from the patient identified a new rna virus strain from the family coronaviridae , which is designated here ‘wh-human 1’ coronavirus (and has also been referred to as ‘2019-ncov’). phylogenetic analysis of the complete viral genome (29,903\xa0nucleotides) revealed that the virus was most closely related (89.1% nucleotide similarity) to a group of sars-like coronaviruses (genus betacoronavirus, subgenus sarbecovirus) that had previously been found in bats in china 5 . this outbreak highlights the ongoing ability of viral spill-over from animals to cause severe disease in humans."
http://orkg.org/orkg/resource/R57971,"Insect assemblages associated with the exotic riparian shrub Russian olive (Elaeagnaceae), and co-occurring native shrubs in British Columbia, Canada",10.4039/tce.2015.63,crossref,"<jats:title>Abstract</jats:title><jats:p>Russian olive (<jats:italic>Elaeagnus angustifolia</jats:italic> Linnaeus; Elaeagnaceae) is an exotic shrub/tree that has become invasive in many riparian ecosystems throughout semi-arid, western North America, including southern British Columbia, Canada. Despite its prevalence and the potentially dramatic impacts it can have on riparian and aquatic ecosystems, little is known about the insect communities associated with Russian olive within its invaded range. At six sites throughout the Okanagan valley of southern British Columbia, Canada, we compared the diversity of insects associated with Russian olive plants to that of insects associated with two commonly co-occurring native plant species: Woods’ rose (<jats:italic>Rosa woodsii</jats:italic> Lindley; Rosaceae) and Saskatoon (<jats:italic>Amelanchier alnifolia</jats:italic> (Nuttall) Nuttall ex Roemer; Rosaceae). Total abundance did not differ significantly among plant types. Family richness and Shannon diversity differed significantly between Woods’ rose and Saskatoon, but not between either of these plant types and Russian olive. An abundance of Thripidae (Thysanoptera) on Russian olive and Tingidae (Hemiptera) on Saskatoon contributed to significant compositional differences among plant types. The families Chloropidae (Diptera), Heleomyzidae (Diptera), and Gryllidae (Orthoptera) were uniquely associated with Russian olive, albeit in low abundances. Our study provides valuable and novel information about the diversity of insects associated with an emerging plant invader of western Canada.</jats:p>","abstract russian olive ( elaeagnus angustifolia linnaeus; elaeagnaceae) is an exotic shrub/tree that has become invasive in many riparian ecosystems throughout semi-arid, western north america, including southern british columbia, canada. despite its prevalence and the potentially dramatic impacts it can have on riparian and aquatic ecosystems, little is known about the insect communities associated with russian olive within its invaded range. at six sites throughout the okanagan valley of southern british columbia, canada, we compared the diversity of insects associated with russian olive plants to that of insects associated with two commonly co-occurring native plant species: woods’ rose ( rosa woodsii lindley; rosaceae) and saskatoon ( amelanchier alnifolia (nuttall) nuttall ex roemer; rosaceae). total abundance did not differ significantly among plant types. family richness and shannon diversity differed significantly between woods’ rose and saskatoon, but not between either of these plant types and russian olive. an abundance of thripidae (thysanoptera) on russian olive and tingidae (hemiptera) on saskatoon contributed to significant compositional differences among plant types. the families chloropidae (diptera), heleomyzidae (diptera), and gryllidae (orthoptera) were uniquely associated with russian olive, albeit in low abundances. our study provides valuable and novel information about the diversity of insects associated with an emerging plant invader of western canada."
http://orkg.org/orkg/resource/R57986,Herbivory and the success of Ligustrum lucidum: evidence from a comparison between native and novel ranges,10.1071/BT15232,crossref,"<jats:p>\n\nInvasive plant species may benefit from a reduction in herbivory in their introduced range. The reduced herbivory may cause a reallocation of resources from defence to fitness. Here, we evaluated leaf herbivory of an invasive tree species (Ligustrum lucidum Aiton) in its native and novel ranges, and determined the potential changes in leaf traits that may be associated with the patterns of herbivory. We measured forest structure, damage by herbivores and leaf traits in novel and native ranges, and on the basis of the literature, we identified the common natural herbivores of L. lucidum. We also performed an experiment offering leaves from both ranges to a generalist herbivore (Spodoptera frugiperda). L. lucidum was more abundant and experienced significantly less foliar damage in the novel than in the native range, in spite of the occurrence of several natural herbivores. The reduced lignin content and lower lignin\u2009:\u2009N ratio in novel leaves, together with the higher herbivore preference for leaves of this origin in the laboratory experiment, indicated lower herbivore resistance in novel than in native populations. The reduced damage by herbivores is not the only factor explaining invasion success, but it may be an important cause that enhances the invasiveness of L. lucidum.\n</jats:p>","\n\ninvasive plant species may benefit from a reduction in herbivory in their introduced range. the reduced herbivory may cause a reallocation of resources from defence to fitness. here, we evaluated leaf herbivory of an invasive tree species (ligustrum lucidum aiton) in its native and novel ranges, and determined the potential changes in leaf traits that may be associated with the patterns of herbivory. we measured forest structure, damage by herbivores and leaf traits in novel and native ranges, and on the basis of the literature, we identified the common natural herbivores of l. lucidum. we also performed an experiment offering leaves from both ranges to a generalist herbivore (spodoptera frugiperda). l. lucidum was more abundant and experienced significantly less foliar damage in the novel than in the native range, in spite of the occurrence of several natural herbivores. the reduced lignin content and lower lignin\u2009:\u2009n ratio in novel leaves, together with the higher herbivore preference for leaves of this origin in the laboratory experiment, indicated lower herbivore resistance in novel than in native populations. the reduced damage by herbivores is not the only factor explaining invasion success, but it may be an important cause that enhances the invasiveness of l. lucidum.\n"
http://orkg.org/orkg/resource/R57992,Incorporation of an invasive plant into a native insect herbivore food web,10.7717/peerj.1954,crossref,"<jats:p>The integration of invasive species into native food webs represent multifarious dynamics of ecological and evolutionary processes. We document incorporation of<jats:italic>Prunus serotina</jats:italic>(black cherry) into native insect food webs. We find that<jats:italic>P. serotina</jats:italic>harbours a herbivore community less dense but more diverse than its native relative,<jats:italic>P. padus</jats:italic>(bird cherry), with similar proportions of specialists and generalists. While herbivory on<jats:italic>P. padus</jats:italic>remained stable over the past century, that on<jats:italic>P. serotina</jats:italic>gradually doubled. We show that<jats:italic>P. serotina</jats:italic>may have evolved changes in investment in cyanogenic glycosides compared with its native range. In the leaf beetle<jats:italic>Gonioctena quinquepunctata</jats:italic>, recently shifted from native<jats:italic>Sorbus aucuparia</jats:italic>to<jats:italic>P. serotina</jats:italic>, we find divergent host preferences on<jats:italic>Sorbus</jats:italic>- versus<jats:italic>Prunus</jats:italic>-derived populations, and weak host-specific differentiation among 380 individuals genotyped for 119 SNP loci. We conclude that evolutionary processes may generate a specialized herbivore community on an invasive plant, allowing prognoses of reduced invasiveness over time. On the basis of the results presented here, we would like to caution that manual control might have the adverse effect of a slowing down of processes of adaptation, and a delay in the decline of the invasive character of<jats:italic>P. serotina</jats:italic>.</jats:p>","the integration of invasive species into native food webs represent multifarious dynamics of ecological and evolutionary processes. we document incorporation of prunus serotina (black cherry) into native insect food webs. we find that p. serotina harbours a herbivore community less dense but more diverse than its native relative, p. padus (bird cherry), with similar proportions of specialists and generalists. while herbivory on p. padus remained stable over the past century, that on p. serotina gradually doubled. we show that p. serotina may have evolved changes in investment in cyanogenic glycosides compared with its native range. in the leaf beetle gonioctena quinquepunctata , recently shifted from native sorbus aucuparia to p. serotina , we find divergent host preferences on sorbus - versus prunus -derived populations, and weak host-specific differentiation among 380 individuals genotyped for 119 snp loci. we conclude that evolutionary processes may generate a specialized herbivore community on an invasive plant, allowing prognoses of reduced invasiveness over time. on the basis of the results presented here, we would like to caution that manual control might have the adverse effect of a slowing down of processes of adaptation, and a delay in the decline of the invasive character of p. serotina ."
http://orkg.org/orkg/resource/R57994,Can enemy release explain the invasion success of the diploid Leucanthemum vulgare in North America?,10.1007/s10530-016-1152-z,crossref,"<jats:title>Abstract</jats:title><jats:p>Enemy release is a commonly accepted mechanism to explain plant invasions. Both the diploid <jats:italic>Leucanthemum vulgare</jats:italic> and the morphologically very similar tetraploid <jats:italic>Leucanthemum ircutianum</jats:italic> have been introduced into North America. To verify which species is more prevalent in North America we sampled 98 <jats:italic>Leucanthemum</jats:italic> populations and determined their ploidy level. Although polyploidy has repeatedly been proposed to be associated with increased invasiveness in plants, only two of the populations surveyed in North America were the tetraploid <jats:italic>L. ircutianum</jats:italic>. We tested the enemy release hypothesis by first comparing 20 populations of <jats:italic>L. vulgare</jats:italic> and 27 populations of <jats:italic>L. ircutianum</jats:italic> in their native range in Europe, and then comparing the European <jats:italic>L. vulgare</jats:italic> populations with 31 <jats:italic>L. vulgare</jats:italic> populations sampled in North America. Characteristics of the site and associated vegetation, plant performance and invertebrate herbivory were recorded. In Europe, plant height and density of the two species were similar but <jats:italic>L. vulgare</jats:italic> produced more flower heads than <jats:italic>L. ircutianum</jats:italic>. <jats:italic>Leucanthemum vulgare</jats:italic> in North America was 17\xa0% taller, produced twice as many flower heads and grew much denser compared to <jats:italic>L. vulgare</jats:italic> in Europe. Attack rates by root- and leaf-feeding herbivores on <jats:italic>L. vulgare</jats:italic> in Europe (34 and 75\xa0%) was comparable to that on <jats:italic>L. ircutianum</jats:italic> (26 and 71\xa0%) but higher than that on <jats:italic>L. vulgare</jats:italic> in North America (10 and 3\xa0%). However, herbivore load and leaf damage were low in Europe. Cover and height of the co-occurring vegetation was higher in <jats:italic>L. vulgare</jats:italic> populations in the native than in the introduced range, suggesting that a shift in plant competition may more easily explain the invasion success of <jats:italic>L. vulgare</jats:italic> than escape from herbivory.</jats:p>","abstract enemy release is a commonly accepted mechanism to explain plant invasions. both the diploid leucanthemum vulgare and the morphologically very similar tetraploid leucanthemum ircutianum have been introduced into north america. to verify which species is more prevalent in north america we sampled 98 leucanthemum populations and determined their ploidy level. although polyploidy has repeatedly been proposed to be associated with increased invasiveness in plants, only two of the populations surveyed in north america were the tetraploid l. ircutianum . we tested the enemy release hypothesis by first comparing 20 populations of l. vulgare and 27 populations of l. ircutianum in their native range in europe, and then comparing the european l. vulgare populations with 31 l. vulgare populations sampled in north america. characteristics of the site and associated vegetation, plant performance and invertebrate herbivory were recorded. in europe, plant height and density of the two species were similar but l. vulgare produced more flower heads than l. ircutianum . leucanthemum vulgare in north america was 17\xa0% taller, produced twice as many flower heads and grew much denser compared to l. vulgare in europe. attack rates by root- and leaf-feeding herbivores on l. vulgare in europe (34 and 75\xa0%) was comparable to that on l. ircutianum (26 and 71\xa0%) but higher than that on l. vulgare in north america (10 and 3\xa0%). however, herbivore load and leaf damage were low in europe. cover and height of the co-occurring vegetation was higher in l. vulgare populations in the native than in the introduced range, suggesting that a shift in plant competition may more easily explain the invasion success of l. vulgare than escape from herbivory."
http://orkg.org/orkg/resource/R57787,Low prevalence of haemosporidian parasites in the introduced house sparrow (Passer domesticus) in Brazil,,crossref,"<jats:title>Abstract</jats:title><jats:p>Species that are introduced to novel environments can lose their native pathogens and parasites during the process of introduction. The escape from the negative effects associated with these natural enemies is commonly employed as an explanation for the success and expansion of invasive species, which is termed the enemy release hypothesis (ERH). In this study, nested PCR techniques and microscopy were used to determine the prevalence and intensity (respectively) of Plasmodium spp. and Haemoproteus spp. in introduced house sparrows and native urban birds of central Brazil. Generalized linear mixed models were fitted by Laplace approximation considering a binomial error distribution and logit link function. Location and species were considered as random effects and species categorization (native or non-indigenous) as fixed effects. We found that native birds from Brazil presented significantly higher parasite prevalence in accordance with the ERH. We also compared our data with the literature, and found that house sparrows native to Europe exhibited significantly higher parasite prevalence than introduced house sparrows from Brazil, which also supports the ERH. Therefore, it is possible that house sparrows from Brazil might have experienced a parasitic release during the process of introduction, which might also be related to a demographic release (e.g. release from the negative effects of parasites on host population dynamics).</jats:p>","abstract species that are introduced to novel environments can lose their native pathogens and parasites during the process of introduction. the escape from the negative effects associated with these natural enemies is commonly employed as an explanation for the success and expansion of invasive species, which is termed the enemy release hypothesis (erh). in this study, nested pcr techniques and microscopy were used to determine the prevalence and intensity (respectively) of plasmodium spp. and haemoproteus spp. in introduced house sparrows and native urban birds of central brazil. generalized linear mixed models were fitted by laplace approximation considering a binomial error distribution and logit link function. location and species were considered as random effects and species categorization (native or non-indigenous) as fixed effects. we found that native birds from brazil presented significantly higher parasite prevalence in accordance with the erh. we also compared our data with the literature, and found that house sparrows native to europe exhibited significantly higher parasite prevalence than introduced house sparrows from brazil, which also supports the erh. therefore, it is possible that house sparrows from brazil might have experienced a parasitic release during the process of introduction, which might also be related to a demographic release (e.g. release from the negative effects of parasites on host population dynamics)."
http://orkg.org/orkg/resource/R57635,Invasive exotic plants suffer less herbivory than non-invasive exotic plants,,crossref,"<jats:p>We surveyed naturally occurring leaf herbivory in nine invasive and nine non-invasive exotic plant species sampled in natural areas in Ontario, New York and Massachusetts, and found that invasive plants experienced, on average, 96% less leaf damage than non-invasive species. Invasive plants were also more taxonomically isolated than non-invasive plants, belonging to families with 75% fewer native North American genera. However, the relationship between taxonomic isolation at the family level and herbivory was weak. We suggest that invasive plants may possess novel phytochemicals with anti-herbivore properties in addition to allelopathic and anti-microbial characteristics. Herbivory could be employed as an easily measured predictor of the likelihood that recently introduced exotic plants may become invasive.</jats:p>","we surveyed naturally occurring leaf herbivory in nine invasive and nine non-invasive exotic plant species sampled in natural areas in ontario, new york and massachusetts, and found that invasive plants experienced, on average, 96% less leaf damage than non-invasive species. invasive plants were also more taxonomically isolated than non-invasive plants, belonging to families with 75% fewer native north american genera. however, the relationship between taxonomic isolation at the family level and herbivory was weak. we suggest that invasive plants may possess novel phytochemicals with anti-herbivore properties in addition to allelopathic and anti-microbial characteristics. herbivory could be employed as an easily measured predictor of the likelihood that recently introduced exotic plants may become invasive."
http://orkg.org/orkg/resource/R57652,Limited grazing pressure by native herbivores on the invasive seaweed Caulerpa taxifolia in a temperate Australian estuary,,crossref,"<jats:p>\n\nCaulerpa taxifolia is an invasive alga threatening biodiversity in invaded regions. Its proliferation in recipient communities will be due to several factors including limited grazing effects by native herbivores. However, little is known about grazing pressure exerted by native herbivores on C. taxifolia relative to native macrophytes or its attractiveness to them as habitat. The present study determined which herbivores co-occurred with invasive C. taxifolia in a temperate Australian estuary and documented their abundance, relative grazing effects, habitat preference and survivorship on C. taxifolia compared with native macrophytes. Four herbivores co-occurred with C. taxifolia and their densities were often low or zero at the sites studied. Feeding experiments showed that compared with C. taxifolia: the fish, Girella tricuspidata, preferred Ulva sp.; the sea-hare, Aplysia dactylomela, preferred Laurencia sp.; whereas the mesograzers, Cymadusa setosa and Platynereis dumerilii antipoda, both consumed Cystoseira trinodus and Sargassum sp. at higher rates. The two mesograzers also showed strong habitat preference for C. trinodus and Sargassum sp. Cymadusa setosa had poor survivorship on Caulerpa taxifolia whereas P. dumerilii antipoda had 100% survivorship on C. taxifolia after 41 days. We consider that the low diversity and abundance of native herbivores, their weak grazing pressure on C. taxifolia and its low attractiveness as habitat may facilitate further local spread in this estuary, and potentially in other invaded locations.\n</jats:p>","\n\ncaulerpa taxifolia is an invasive alga threatening biodiversity in invaded regions. its proliferation in recipient communities will be due to several factors including limited grazing effects by native herbivores. however, little is known about grazing pressure exerted by native herbivores on c. taxifolia relative to native macrophytes or its attractiveness to them as habitat. the present study determined which herbivores co-occurred with invasive c. taxifolia in a temperate australian estuary and documented their abundance, relative grazing effects, habitat preference and survivorship on c. taxifolia compared with native macrophytes. four herbivores co-occurred with c. taxifolia and their densities were often low or zero at the sites studied. feeding experiments showed that compared with c. taxifolia: the fish, girella tricuspidata, preferred ulva sp.; the sea-hare, aplysia dactylomela, preferred laurencia sp.; whereas the mesograzers, cymadusa setosa and platynereis dumerilii antipoda, both consumed cystoseira trinodus and sargassum sp. at higher rates. the two mesograzers also showed strong habitat preference for c. trinodus and sargassum sp. cymadusa setosa had poor survivorship on caulerpa taxifolia whereas p. dumerilii antipoda had 100% survivorship on c. taxifolia after 41 days. we consider that the low diversity and abundance of native herbivores, their weak grazing pressure on c. taxifolia and its low attractiveness as habitat may facilitate further local spread in this estuary, and potentially in other invaded locations.\n"
http://orkg.org/orkg/resource/R57596,Post-dispersal losses to seed predators: an experimental comparison of native and exotic old field plants,,crossref,"<jats:p> Invasions by exotic plants may be more likely if exotics have low rates of attack by natural enemies, including post-dispersal seed predators (granivores). We investigated this idea with a field experiment conducted near Newmarket, Ontario, in which we experimentally excluded vertebrate and terrestrial insect seed predators from seeds of 43 native and exotic old-field plants. Protection from vertebrates significantly increased recovery of seeds; vertebrate exclusion produced higher recovery than controls for 30 of the experimental species, increasing overall seed recovery from 38.2 to 45.6%. Losses to vertebrates varied among species, significantly increasing with seed mass. In contrast, insect exclusion did not significantly improve seed recovery. There was no evidence that aliens benefitted from a reduced rate of post-dispersal seed predation. The impacts of seed predators did not differ significantly between natives and exotics, which instead showed very similar responses to predator exclusion treatments. These results indicate that while vertebrate granivores had important impacts, especially on large-seeded species, exotics did not generally benefit from reduced rates of seed predation. Instead, differences between natives and exotics were small compared with interspecific variation within these groups.Key words: aliens, exotics, granivores, invaders, old fields, seed predators. </jats:p>","invasions by exotic plants may be more likely if exotics have low rates of attack by natural enemies, including post-dispersal seed predators (granivores). we investigated this idea with a field experiment conducted near newmarket, ontario, in which we experimentally excluded vertebrate and terrestrial insect seed predators from seeds of 43 native and exotic old-field plants. protection from vertebrates significantly increased recovery of seeds; vertebrate exclusion produced higher recovery than controls for 30 of the experimental species, increasing overall seed recovery from 38.2 to 45.6%. losses to vertebrates varied among species, significantly increasing with seed mass. in contrast, insect exclusion did not significantly improve seed recovery. there was no evidence that aliens benefitted from a reduced rate of post-dispersal seed predation. the impacts of seed predators did not differ significantly between natives and exotics, which instead showed very similar responses to predator exclusion treatments. these results indicate that while vertebrate granivores had important impacts, especially on large-seeded species, exotics did not generally benefit from reduced rates of seed predation. instead, differences between natives and exotics were small compared with interspecific variation within these groups.key words: aliens, exotics, granivores, invaders, old fields, seed predators."
http://orkg.org/orkg/resource/R57311,Native plant diversity increases herbivory to non-natives,,crossref,"<jats:p>There is often an inverse relationship between the diversity of a plant community and the invasibility of that community by non-native plants. Native herbivores that colonize novel plants may contribute to diversity–invasibility relationships by limiting the relative success of non-native plants. Here, we show that, in large collections of non-native oak trees at sites across the USA, non-native oaks introduced to regions with greater oak species richness accumulated greater leaf damage than in regions with low oak richness. Underlying this trend was the ability of herbivores to exploit non-native plants that were close relatives to their native host. In diverse oak communities, non-native trees were on average more closely related to native trees and received greater leaf damage than those in depauperate oak communities. Because insect herbivores colonize non-native plants that are similar to their native hosts, in communities with greater native plant diversity, non-natives experience greater herbivory.</jats:p>","there is often an inverse relationship between the diversity of a plant community and the invasibility of that community by non-native plants. native herbivores that colonize novel plants may contribute to diversity–invasibility relationships by limiting the relative success of non-native plants. here, we show that, in large collections of non-native oak trees at sites across the usa, non-native oaks introduced to regions with greater oak species richness accumulated greater leaf damage than in regions with low oak richness. underlying this trend was the ability of herbivores to exploit non-native plants that were close relatives to their native host. in diverse oak communities, non-native trees were on average more closely related to native trees and received greater leaf damage than those in depauperate oak communities. because insect herbivores colonize non-native plants that are similar to their native hosts, in communities with greater native plant diversity, non-natives experience greater herbivory."
http://orkg.org/orkg/resource/R57354,Species diversity and invasion resistance in a marine ecosystem,,crossref,"""<jats:p>Theory predicts that systems that are more diverse should be more resistant to exotic species, but experimental tests are needed to verify this. In experimental communities of sessile marine invertebrates, increased species richness significantly decreased invasion success, apparently because species-rich communities more completely and efficiently used available space, the limiting resource in this system. Declining biodiversity thus facilitates invasion in this system, potentially accelerating the loss of biodiversity and the homogenization of the world's biota.</jats:p>""",""" theory predicts that systems that are more diverse should be more resistant to exotic species, but experimental tests are needed to verify this. in experimental communities of sessile marine invertebrates, increased species richness significantly decreased invasion success, apparently because species-rich communities more completely and efficiently used available space, the limiting resource in this system. declining biodiversity thus facilitates invasion in this system, potentially accelerating the loss of biodiversity and the homogenization of the world's biota. """
http://orkg.org/orkg/resource/R57207,Diversity and biomass of native macrophytes are negatively related to dominance of an invasive Poaceae in Brazilian sub-tropical streams,,crossref,"""<jats:p>Besides exacerbated exploitation, pollution, flow alteration and habitats degradation, freshwater biodiversity is also threatened by biological invasions. This paper addresses how native aquatic macrophyte communities are affected by the non-native species Urochloa arrecta, a current successful invader in Brazilian freshwater systems. We compared the native macrophytes colonizing patches dominated and non-dominated by this invader species. We surveyed eight streams in Northwest Paraná State (Brazil). In each stream, we recorded native macrophytes' richness and biomass in sites where U. arrecta was dominant and in sites where it was not dominant or absent. No native species were found in seven, out of the eight investigated sites where U. arrecta was dominant. Thus, we found higher native species richness, Shannon index and native biomass values in sites without dominance of U. arrecta than in sites dominated by this invader. Although difficult to conclude about causes of such differences, we infer that the elevated biomass production by this grass might be the primary reason for alterations in invaded environments and for the consequent impacts on macrophytes' native communities. However, biotic resistance offered by native richer sites could be an alternative explanation for our results. To mitigate potential impacts and to prevent future environmental perturbations, we propose mechanical removal of the invasive species and maintenance or restoration of riparian vegetation, for freshwater ecosystems have vital importance for the maintenance of ecological services and biodiversity and should be preserved.</jats:p>""",""" besides exacerbated exploitation, pollution, flow alteration and habitats degradation, freshwater biodiversity is also threatened by biological invasions. this paper addresses how native aquatic macrophyte communities are affected by the non-native species urochloa arrecta, a current successful invader in brazilian freshwater systems. we compared the native macrophytes colonizing patches dominated and non-dominated by this invader species. we surveyed eight streams in northwest paraná state (brazil). in each stream, we recorded native macrophytes' richness and biomass in sites where u. arrecta was dominant and in sites where it was not dominant or absent. no native species were found in seven, out of the eight investigated sites where u. arrecta was dominant. thus, we found higher native species richness, shannon index and native biomass values in sites without dominance of u. arrecta than in sites dominated by this invader. although difficult to conclude about causes of such differences, we infer that the elevated biomass production by this grass might be the primary reason for alterations in invaded environments and for the consequent impacts on macrophytes' native communities. however, biotic resistance offered by native richer sites could be an alternative explanation for our results. to mitigate potential impacts and to prevent future environmental perturbations, we propose mechanical removal of the invasive species and maintenance or restoration of riparian vegetation, for freshwater ecosystems have vital importance for the maintenance of ecological services and biodiversity and should be preserved. """
http://orkg.org/orkg/resource/R70546,Machine Learning Models for Analysis of Vital Signs Dynamics: A Case for Sepsis Onset Prediction,,crossref,"<jats:p><jats:italic>Objective</jats:italic>. Achieving accurate prediction of sepsis detection moment based on bedside monitor data in the intensive care unit (ICU). A good clinical outcome is more probable when onset is suspected and treated on time, thus early insight of sepsis onset may save lives and reduce costs. <jats:italic>Methodology</jats:italic>. We present a novel approach for feature extraction, which focuses on the hypothesis that unstable patients are more prone to develop sepsis during ICU stay. These features are used in machine learning algorithms to provide a prediction of a patient’s likelihood to develop sepsis during ICU stay, hours before it is diagnosed. <jats:italic>Results</jats:italic>. Five machine learning algorithms were implemented using R software packages. The algorithms were trained and tested with a set of 4 features which represent the variability in vital signs. These algorithms aimed to calculate a patient’s probability to become septic within the next 4 hours, based on recordings from the last 8 hours. The best area under the curve (AUC) was achieved with Support Vector Machine (SVM) with radial basis function, which was 88.38%. <jats:italic>Conclusions</jats:italic>. The high level of predictive accuracy along with the simplicity and availability of input variables present great potential if applied in ICUs. Variability of a patient’s vital signs proves to be a good indicator of one’s chance to become septic during ICU stay.</jats:p>","objective . achieving accurate prediction of sepsis detection moment based on bedside monitor data in the intensive care unit (icu). a good clinical outcome is more probable when onset is suspected and treated on time, thus early insight of sepsis onset may save lives and reduce costs. methodology . we present a novel approach for feature extraction, which focuses on the hypothesis that unstable patients are more prone to develop sepsis during icu stay. these features are used in machine learning algorithms to provide a prediction of a patient’s likelihood to develop sepsis during icu stay, hours before it is diagnosed. results . five machine learning algorithms were implemented using r software packages. the algorithms were trained and tested with a set of 4 features which represent the variability in vital signs. these algorithms aimed to calculate a patient’s probability to become septic within the next 4 hours, based on recordings from the last 8 hours. the best area under the curve (auc) was achieved with support vector machine (svm) with radial basis function, which was 88.38%. conclusions . the high level of predictive accuracy along with the simplicity and availability of input variables present great potential if applied in icus. variability of a patient’s vital signs proves to be a good indicator of one’s chance to become septic during icu stay."
http://orkg.org/orkg/resource/R57157,Human-related processes drive the richness of exotic birds in Europe,,crossref,"<jats:p>\n            Both human-related and natural factors can affect the establishment and distribution of exotic species. Understanding the relative role of the different factors has important scientific and applied implications. Here, we examined the relative effect of human-related and natural factors in determining the richness of exotic bird species established across Europe. Using hierarchical partitioning, which controls for covariation among factors, we show that the most important factor is the human-related community-level propagule pressure (the number of exotic species introduced), which is often not included in invasion studies due to the lack of information for this early stage in the invasion process. Another, though less important, factor was the human footprint (an index that includes human population size, land use and infrastructure). Biotic and abiotic factors of the environment were of minor importance in shaping the number of established birds when tested at a European extent using 50×50\u200akm\n            <jats:sup>2</jats:sup>\n            grid squares. We provide, to our knowledge, the first map of the distribution of exotic bird richness in Europe. The richest hotspot of established exotic birds is located in southeastern England, followed by areas in Belgium and The Netherlands. Community-level propagule pressure remains the major factor shaping the distribution of exotic birds also when tested for the UK separately. Thus, studies examining the patterns of establishment should aim at collecting the crucial and hard-to-find information on community-level propagule pressure or develop reliable surrogates for estimating this factor. Allowing future introductions of exotic birds into Europe should be reconsidered carefully, as the number of introduced species is basically the main factor that determines the number established.\n          </jats:p>","\n both human-related and natural factors can affect the establishment and distribution of exotic species. understanding the relative role of the different factors has important scientific and applied implications. here, we examined the relative effect of human-related and natural factors in determining the richness of exotic bird species established across europe. using hierarchical partitioning, which controls for covariation among factors, we show that the most important factor is the human-related community-level propagule pressure (the number of exotic species introduced), which is often not included in invasion studies due to the lack of information for this early stage in the invasion process. another, though less important, factor was the human footprint (an index that includes human population size, land use and infrastructure). biotic and abiotic factors of the environment were of minor importance in shaping the number of established birds when tested at a european extent using 50×50\u200akm\n 2 \n grid squares. we provide, to our knowledge, the first map of the distribution of exotic bird richness in europe. the richest hotspot of established exotic birds is located in southeastern england, followed by areas in belgium and the netherlands. community-level propagule pressure remains the major factor shaping the distribution of exotic birds also when tested for the uk separately. thus, studies examining the patterns of establishment should aim at collecting the crucial and hard-to-find information on community-level propagule pressure or develop reliable surrogates for estimating this factor. allowing future introductions of exotic birds into europe should be reconsidered carefully, as the number of introduced species is basically the main factor that determines the number established.\n"
http://orkg.org/orkg/resource/R57067,The role of opportunity in the unintentional introduction of nonnative ants,,crossref,"<jats:p>A longstanding goal in the study of biological invasions is to predict why some species are successful invaders, whereas others are not. To understand this process, detailed information is required concerning the pool of species that have the opportunity to become established. Here we develop an extensive database of ant species unintentionally transported to the continental United States and use these data to test how opportunity and species-level ecological attributes affect the probability of establishment. This database includes an amount of information on failed introductions that may be unparalleled for any group of unintentionally introduced insects. We found a high diversity of species (232 species from 394 records), 12% of which have become established in the continental United States. The probability of establishment increased with the number of times a species was transported (propagule pressure) but was also influenced by nesting habit. Ground nesting species were more likely to become established compared with arboreal species. These results highlight the value of developing similar databases for additional groups of organisms transported by humans to obtain quantitative data on the first stages of the invasion process: opportunity and transport.</jats:p>","a longstanding goal in the study of biological invasions is to predict why some species are successful invaders, whereas others are not. to understand this process, detailed information is required concerning the pool of species that have the opportunity to become established. here we develop an extensive database of ant species unintentionally transported to the continental united states and use these data to test how opportunity and species-level ecological attributes affect the probability of establishment. this database includes an amount of information on failed introductions that may be unparalleled for any group of unintentionally introduced insects. we found a high diversity of species (232 species from 394 records), 12% of which have become established in the continental united states. the probability of establishment increased with the number of times a species was transported (propagule pressure) but was also influenced by nesting habit. ground nesting species were more likely to become established compared with arboreal species. these results highlight the value of developing similar databases for additional groups of organisms transported by humans to obtain quantitative data on the first stages of the invasion process: opportunity and transport."
http://orkg.org/orkg/resource/R57088,The analysis and modelling of British invasions,,crossref,"<jats:p>The SCOPE programme on the ecology of biological invasions addresses three questions: What are the factors that determine whether a species will become an invader or not? What are the site properties which determine whether an ecological system will be relatively prone to or resistant to invasion? How should management systems be developed to best advantage, given the knowledge gained by attempting to answer the first two questions? The answers that have been offered to these questions earlier, and during the course of the programme, are reviewed. The consensus is that, although certain habitat and biological features increase the probability of invasion and establishment, these features are neither necessary nor sufficient, and that the prediction of invasion is not yet feasible. These points are illustrated by examples and generalizations from a survey of British invaders. The probability that an established invader will be a pest in Britain seems to be around 10% . Mathematical modelling may help in understanding and, later, in predicting invasions. Models indicate that establishment may be more critical than spread, and that a successful invader will spread at a constant linear speed. Models and data suggest that both an accelerating rate of spread and occasional major jumps can be expected; consequently, efforts to eliminate an invader at an early stage will be the most effective.</jats:p>","the scope programme on the ecology of biological invasions addresses three questions: what are the factors that determine whether a species will become an invader or not? what are the site properties which determine whether an ecological system will be relatively prone to or resistant to invasion? how should management systems be developed to best advantage, given the knowledge gained by attempting to answer the first two questions? the answers that have been offered to these questions earlier, and during the course of the programme, are reviewed. the consensus is that, although certain habitat and biological features increase the probability of invasion and establishment, these features are neither necessary nor sufficient, and that the prediction of invasion is not yet feasible. these points are illustrated by examples and generalizations from a survey of british invaders. the probability that an established invader will be a pest in britain seems to be around 10% . mathematical modelling may help in understanding and, later, in predicting invasions. models indicate that establishment may be more critical than spread, and that a successful invader will spread at a constant linear speed. models and data suggest that both an accelerating rate of spread and occasional major jumps can be expected; consequently, efforts to eliminate an invader at an early stage will be the most effective."
http://orkg.org/orkg/resource/R56949,Biological control attempts by introductions against pest insects in the field in Canada,,crossref,"<jats:title>Abstract</jats:title><jats:p>This is an analysis of the attempts to colonize at least 208 species of parasites and predators on about 75 species of pest insects in the field in Canada. There was colonization by about 10% of the species that were introduced in totals of under 5,000 individuals, 40% of those introduced in totals of between 5,000 and 31,200, and 78% of those introduced in totals of over 31,200. Indications exist that initial colonizations may be favoured by large releases and by selection of release sites that are semi-isolated and not ecologically complex but that colonizations are hindered when the target species differs taxonomically from the species from which introduced agents originated and when the release site lacks factors needed for introduced agents to survive or when it is subject to potentially-avoidable physical disruptions. There was no evidence that the probability of colonization was increased when the numbers of individuals released were increased by laboratory propagation. About 10% of the attempts were successful from the economic viewpoint. Successes may be overestimated if the influence of causes of coincidental, actual, or supposed changes in pest abundance are overlooked. Most of the successes were by two or more kinds of agents of which at least one attacked species additional to the target pests. Unplanned consequences of colonization have not been sufficiently harmful to warrant precautions to the extent advocated by Turnbull and Chant but are sufficiently potentially dangerous to warrant the restriction of all colonization attempts to biological control experts. It is concluded that most failures were caused by inadequate procedures, rather than by any weaknesses inherent in the method, that those inadequacies can be avoided in the future, and therefore that biological control of pest insects has much unrealized potential for use in Canada.</jats:p>","abstract this is an analysis of the attempts to colonize at least 208 species of parasites and predators on about 75 species of pest insects in the field in canada. there was colonization by about 10% of the species that were introduced in totals of under 5,000 individuals, 40% of those introduced in totals of between 5,000 and 31,200, and 78% of those introduced in totals of over 31,200. indications exist that initial colonizations may be favoured by large releases and by selection of release sites that are semi-isolated and not ecologically complex but that colonizations are hindered when the target species differs taxonomically from the species from which introduced agents originated and when the release site lacks factors needed for introduced agents to survive or when it is subject to potentially-avoidable physical disruptions. there was no evidence that the probability of colonization was increased when the numbers of individuals released were increased by laboratory propagation. about 10% of the attempts were successful from the economic viewpoint. successes may be overestimated if the influence of causes of coincidental, actual, or supposed changes in pest abundance are overlooked. most of the successes were by two or more kinds of agents of which at least one attacked species additional to the target pests. unplanned consequences of colonization have not been sufficiently harmful to warrant precautions to the extent advocated by turnbull and chant but are sufficiently potentially dangerous to warrant the restriction of all colonization attempts to biological control experts. it is concluded that most failures were caused by inadequate procedures, rather than by any weaknesses inherent in the method, that those inadequacies can be avoided in the future, and therefore that biological control of pest insects has much unrealized potential for use in canada."
http://orkg.org/orkg/resource/R56959,Interception frequency of exotic bark and ambrosia beetles (Coleoptera: Scolytinae) and relationship with establishment in New Zealand and worldwide,,crossref,"""<jats:p> Scolytinae species are among the most damaging forest pests, and many of them are invasive. Over 1500 Scolytinae interceptions were recorded at New Zealand's borders between 1950 and 2000. Among the 103 species were Dendroctonus ponderosae, Ips typographus, and other high-risk species, but actual arrivals probably included many more species. Interceptions were primarily associated with dunnage, casewood (crating), and sawn timber, and originated from 59 countries, mainly from Europe, Australasia, northern Asia, and North America. New Zealand and United States interception data were highly correlated, and 7 of the 10 most intercepted species were shared. Interception frequency and establishment in New Zealand were not clearly related. By combining New Zealand and United States interceptions of true bark beetles we obtained data on species found in shipments from around the world. Logistic regression analysis showed that frequently intercepted species were about four times as likely as rarely intercepted species to be established somewhere. Interception records of wood and bark borers are valuable for the prediction of invaders and for our general understanding of invasions. The use of alternatives to solid wood packaging, such as processed wood, should be encouraged to reduce the spread of invasive wood and bark borers. </jats:p>""",""" scolytinae species are among the most damaging forest pests, and many of them are invasive. over 1500 scolytinae interceptions were recorded at new zealand's borders between 1950 and 2000. among the 103 species were dendroctonus ponderosae, ips typographus, and other high-risk species, but actual arrivals probably included many more species. interceptions were primarily associated with dunnage, casewood (crating), and sawn timber, and originated from 59 countries, mainly from europe, australasia, northern asia, and north america. new zealand and united states interception data were highly correlated, and 7 of the 10 most intercepted species were shared. interception frequency and establishment in new zealand were not clearly related. by combining new zealand and united states interceptions of true bark beetles we obtained data on species found in shipments from around the world. logistic regression analysis showed that frequently intercepted species were about four times as likely as rarely intercepted species to be established somewhere. interception records of wood and bark borers are valuable for the prediction of invaders and for our general understanding of invasions. the use of alternatives to solid wood packaging, such as processed wood, should be encouraged to reduce the spread of invasive wood and bark borers. """
http://orkg.org/orkg/resource/R56984,Introduction pathways and establishment rates of invasive aquatic species in Europe,,crossref,"<jats:p>Species invasion is one of the leading mechanisms of global environmental change, particularly in freshwater ecosystems. We used the Food and Agriculture Organization\'s Database of Invasive Aquatic Species to study invasion rates and to analyze invasion pathways within Europe. Of the 123 aquatic species introduced into six contrasting European countries, the average percentage established is 63%, well above the 5%\x9620% suggested by Williamson\'s ""tens"" rule. The introduction and establishment transitions are independent of each other, and species that became widely established did so because their introduction was attempted in many countries, not because of a better establishment capability. The most frequently introduced aquatic species in Europe are freshwater fishes. We describe clear introduction pathways of aquatic species into Europe and three types of country are observed: ""recipient and donor"" (large, midlatitude European countries, such as France, the United Kingdom, and Germany, that give and receive the most introductions), ""recipient"" (most countries, but particularly southern countries, which give few species but receive many), and ""neither recipient nor donor"" (only two countries). A path analysis showed that the numbers of species given and received are mediated by the size (area) of the country and population density, but not gross domestic product per capita.</jats:p>","species invasion is one of the leading mechanisms of global environmental change, particularly in freshwater ecosystems. we used the food and agriculture organization\'s database of invasive aquatic species to study invasion rates and to analyze invasion pathways within europe. of the 123 aquatic species introduced into six contrasting european countries, the average percentage established is 63%, well above the 5%\x9620% suggested by williamson\'s ""tens"" rule. the introduction and establishment transitions are independent of each other, and species that became widely established did so because their introduction was attempted in many countries, not because of a better establishment capability. the most frequently introduced aquatic species in europe are freshwater fishes. we describe clear introduction pathways of aquatic species into europe and three types of country are observed: ""recipient and donor"" (large, midlatitude european countries, such as france, the united kingdom, and germany, that give and receive the most introductions), ""recipient"" (most countries, but particularly southern countries, which give few species but receive many), and ""neither recipient nor donor"" (only two countries). a path analysis showed that the numbers of species given and received are mediated by the size (area) of the country and population density, but not gross domestic product per capita."
http://orkg.org/orkg/resource/R56996,Invasion success of vertebrates in Europe and North America,,crossref,"<jats:p>\n            Species become invasive if they (\n            <jats:italic>i</jats:italic>\n            ) are introduced to a new range, (\n            <jats:italic>ii</jats:italic>\n            ) establish themselves, and (\n            <jats:italic>iii</jats:italic>\n            ) spread. To address the global problems caused by invasive species, several studies investigated steps\n            <jats:italic>ii</jats:italic>\n            and\n            <jats:italic>iii</jats:italic>\n            of this invasion process. However, only one previous study looked at step\n            <jats:italic>i</jats:italic>\n            and examined the proportion of species that have been introduced beyond their native range. We extend this research by investigating all three steps for all freshwater fish, mammals, and birds native to Europe or North America. A higher proportion of European species entered North America than vice versa. However, the introduction rate from Europe to North America peaked in the late 19th century, whereas it is still rising in the other direction. There is no clear difference in invasion success between the two directions, so neither the imperialism dogma (that Eurasian species are exceptionally successful invaders) is supported, nor is the contradictory hypothesis that North America offers more biotic resistance to invaders than Europe because of its less disturbed and richer biota. Our results do not support the tens rule either: that ≈10% of all introduced species establish themselves and that ≈10% of established species spread. We find a success of ≈50% at each step. In comparison, only ≈5% of native vertebrates were introduced in either direction. These figures show that, once a vertebrate is introduced, it has a high potential to become invasive. Thus, it is crucial to minimize the number of species introductions to effectively control invasive vertebrates.\n          </jats:p>","\n species become invasive if they (\n i \n ) are introduced to a new range, (\n ii \n ) establish themselves, and (\n iii \n ) spread. to address the global problems caused by invasive species, several studies investigated steps\n ii \n and\n iii \n of this invasion process. however, only one previous study looked at step\n i \n and examined the proportion of species that have been introduced beyond their native range. we extend this research by investigating all three steps for all freshwater fish, mammals, and birds native to europe or north america. a higher proportion of european species entered north america than vice versa. however, the introduction rate from europe to north america peaked in the late 19th century, whereas it is still rising in the other direction. there is no clear difference in invasion success between the two directions, so neither the imperialism dogma (that eurasian species are exceptionally successful invaders) is supported, nor is the contradictory hypothesis that north america offers more biotic resistance to invaders than europe because of its less disturbed and richer biota. our results do not support the tens rule either: that ≈10% of all introduced species establish themselves and that ≈10% of established species spread. we find a success of ≈50% at each step. in comparison, only ≈5% of native vertebrates were introduced in either direction. these figures show that, once a vertebrate is introduced, it has a high potential to become invasive. thus, it is crucial to minimize the number of species introductions to effectively control invasive vertebrates.\n"
http://orkg.org/orkg/resource/R57000,Ecological predictions and risk assessment for alien fishes in North America,,crossref,"<jats:p>Methods of risk assessment for alien species, especially for nonagricultural systems, are largely qualitative. Using a generalizable risk assessment approach and statistical models of fish introductions into the Great Lakes, North America, we developed a quantitative approach to target prevention efforts on species most likely to cause damage. Models correctly categorized established, quickly spreading, and nuisance fishes with 87 to 94% accuracy. We then identified fishes that pose a high risk to the Great Lakes if introduced from unintentional (ballast water) or intentional pathways (sport, pet, bait, and aquaculture industries).</jats:p>","methods of risk assessment for alien species, especially for nonagricultural systems, are largely qualitative. using a generalizable risk assessment approach and statistical models of fish introductions into the great lakes, north america, we developed a quantitative approach to target prevention efforts on species most likely to cause damage. models correctly categorized established, quickly spreading, and nuisance fishes with 87 to 94% accuracy. we then identified fishes that pose a high risk to the great lakes if introduced from unintentional (ballast water) or intentional pathways (sport, pet, bait, and aquaculture industries)."
http://orkg.org/orkg/resource/R56644,Epiphytic macroinvertebrate communities on Eurasian watermilfoil (Myriophyllum spicatum) and native milfoils Myriophyllum sibericum and Myriophyllum alterniflorum in eastern North America,,crossref,"<jats:p> Aquatic macrophytes play an important role in the survival and proliferation of invertebrates in freshwater ecosystems. Epiphytic invertebrate communities may be altered through the replacement of native macrophytes by exotic macrophytes, even when the macrophytes are close relatives and have similar morphology. We sampled an invasive exotic macrophyte, Eurasian watermilfoil ( Myriophyllum spicatum ), and native milfoils Myriophyllum sibericum and Myriophyllum alterniflorum in four bodies of water in southern Quebec and upstate New York during the summer of 2005. Within each waterbody, we compared the abundance, diversity, and community composition of epiphytic macroinvertebrates on exotic and native Myriophyllum. In general, both M. sibericum and M. alterniflorum had higher invertebrate diversity and higher invertebrate biomass and supported more gastropods than the exotic M. spicatum. In late summer, invertebrate density tended to be higher on M. sibericum than on M. spicatum, but lower on M. alterniflorum than on M. spicatum. Our results demonstrate that M. spicatum supports macroinvertebrate communities that may differ from those on structurally similar native macrophytes, although these differences vary across sites and sampling dates. Thus, the replacement of native milfoils by M. spicatum may have indirect effects on aquatic food webs. </jats:p>","aquatic macrophytes play an important role in the survival and proliferation of invertebrates in freshwater ecosystems. epiphytic invertebrate communities may be altered through the replacement of native macrophytes by exotic macrophytes, even when the macrophytes are close relatives and have similar morphology. we sampled an invasive exotic macrophyte, eurasian watermilfoil ( myriophyllum spicatum ), and native milfoils myriophyllum sibericum and myriophyllum alterniflorum in four bodies of water in southern quebec and upstate new york during the summer of 2005. within each waterbody, we compared the abundance, diversity, and community composition of epiphytic macroinvertebrates on exotic and native myriophyllum. in general, both m. sibericum and m. alterniflorum had higher invertebrate diversity and higher invertebrate biomass and supported more gastropods than the exotic m. spicatum. in late summer, invertebrate density tended to be higher on m. sibericum than on m. spicatum, but lower on m. alterniflorum than on m. spicatum. our results demonstrate that m. spicatum supports macroinvertebrate communities that may differ from those on structurally similar native macrophytes, although these differences vary across sites and sampling dates. thus, the replacement of native milfoils by m. spicatum may have indirect effects on aquatic food webs."
http://orkg.org/orkg/resource/R56680,"Intra-regional transportation of a tugboat fouling community between the ports of Recife and Natal, northeast Brazil",,crossref,"""<jats:p>This study aimed to identify the incrusting and sedentary animals associated with the hull of a tugboat active in the ports of Pernambuco and later loaned to the port of Natal, Rio Grande do Norte. Thus, areas with dense biofouling were scraped and the species then classified in terms of their bioinvasive status for the Brazilian coast. Six were native to Brazil, two were cryptogenic and 16 nonindigenous; nine of the latter were classified as established (Musculus lateralis, Sphenia fragilis, Balanus trigonus, Biflustra savartii, Botrylloides nigrum, Didemnum psammatodes, Herdmania pallida, Microscosmus exasperatus, and Symplegma rubra) and three as invasive (Mytilopsis leucophaeta, Amphibalanus reticulatus, and Striatobalanus amaryllis). The presence of M. leucophaeata, Amphibalanus eburneus and A. reticulatus on the boat's hull propitiated their introduction onto the Natal coast. The occurrence of a great number of tunicate species in Natal reflected the port area's benthic diversity and facilitated the inclusion of two bivalves - Musculus lateralis and Sphenia fragilis - found in their siphons and in the interstices between colonies or individuals, respectively. The results show the role of biofouling on boat hulls in the introduction of nonindigenous species and that the port of Recife acts as a source of some species.</jats:p>""",""" this study aimed to identify the incrusting and sedentary animals associated with the hull of a tugboat active in the ports of pernambuco and later loaned to the port of natal, rio grande do norte. thus, areas with dense biofouling were scraped and the species then classified in terms of their bioinvasive status for the brazilian coast. six were native to brazil, two were cryptogenic and 16 nonindigenous; nine of the latter were classified as established (musculus lateralis, sphenia fragilis, balanus trigonus, biflustra savartii, botrylloides nigrum, didemnum psammatodes, herdmania pallida, microscosmus exasperatus, and symplegma rubra) and three as invasive (mytilopsis leucophaeta, amphibalanus reticulatus, and striatobalanus amaryllis). the presence of m. leucophaeata, amphibalanus eburneus and a. reticulatus on the boat's hull propitiated their introduction onto the natal coast. the occurrence of a great number of tunicate species in natal reflected the port area's benthic diversity and facilitated the inclusion of two bivalves - musculus lateralis and sphenia fragilis - found in their siphons and in the interstices between colonies or individuals, respectively. the results show the role of biofouling on boat hulls in the introduction of nonindigenous species and that the port of recife acts as a source of some species. """
http://orkg.org/orkg/resource/R56569,Recent biological invasion may hasten invasional meltdown by accelerating historical introductions,,crossref,"<jats:p>Biological invasions are rapidly producing planet-wide changes in biodiversity and ecosystem function. In coastal waters of the U.S., &gt;500 invaders have become established, and new introductions continue at an increasing rate. Although most species have little impact on native communities, some initially benign introductions may occasionally turn into damaging invasions, although such introductions are rarely documented. Here, I demonstrate that a recently introduced crab has resulted in the rapid spread and increase of an introduced bivalve that had been rare in the system for nearly 50 yr. This increase has occurred through the positive indirect effects of predation by the introduced crab on native bivalves. I used field and laboratory experiments to show that the mechanism is size-specific predation interacting with the different reproductive life histories of the native (protandrous hermaphrodite) and the introduced (dioecious) bivalves. These results suggest that positive interactions among the hundreds of introduced species that are accumulating in coastal systems could result in the rapid transformation of previously benign introductions into aggressively expanding invasions. Even if future management efforts reduce the number of new introductions, given the large number of species already present, there is a high potential for positive interactions to produce many future management problems. Given that invasional meltdown is now being documented in natural systems, I suggest that coastal systems may be closer to this threshold than currently believed.</jats:p>","biological invasions are rapidly producing planet-wide changes in biodiversity and ecosystem function. in coastal waters of the u.s., &gt;500 invaders have become established, and new introductions continue at an increasing rate. although most species have little impact on native communities, some initially benign introductions may occasionally turn into damaging invasions, although such introductions are rarely documented. here, i demonstrate that a recently introduced crab has resulted in the rapid spread and increase of an introduced bivalve that had been rare in the system for nearly 50 yr. this increase has occurred through the positive indirect effects of predation by the introduced crab on native bivalves. i used field and laboratory experiments to show that the mechanism is size-specific predation interacting with the different reproductive life histories of the native (protandrous hermaphrodite) and the introduced (dioecious) bivalves. these results suggest that positive interactions among the hundreds of introduced species that are accumulating in coastal systems could result in the rapid transformation of previously benign introductions into aggressively expanding invasions. even if future management efforts reduce the number of new introductions, given the large number of species already present, there is a high potential for positive interactions to produce many future management problems. given that invasional meltdown is now being documented in natural systems, i suggest that coastal systems may be closer to this threshold than currently believed."
http://orkg.org/orkg/resource/R56064,Stochastic Behavior of Tropical Convection in Observations and a Multicloud Model,10.1175/jas-d-13-031.1,crossref,"<jats:title>Abstract</jats:title><jats:p>The aim for a more accurate representation of tropical convection in global circulation models is a long-standing issue. Here, the relationships between large and convective scales in observations and a stochastic multicloud model (SMCM) to ultimately support the design of a novel convection parameterization with stochastic elements are investigated. Observations of tropical convection obtained at Darwin and Kwajalein are used here. It is found that the variability of observed tropical convection generally decreases with increasing large-scale forcing, implying a transition from stochastic to more deterministic behavior with increasing forcing. Convection shows a more systematic relationship with measures related to large-scale convergence compared to measures related to energetics (e.g., CAPE). Using the observations, the parameters in the SMCM are adjusted. Then, the SMCM is forced with the time series of the observed large-scale state and the simulated convective behavior is compared to that observed. It is found that the SMCM cloud fields compare better with observations when using predictors related to convergence rather than energetics. Furthermore, the underlying framework of the SMCM is able to reproduce the observed functional dependencies of convective variability on the imposed large-scale state—an encouraging result on the road toward a novel convection parameterization approach. However, establishing sound cause-and-effect relationships between tropical convection and the large-scale environment remains problematic and warrants further research.</jats:p>","abstract the aim for a more accurate representation of tropical convection in global circulation models is a long-standing issue. here, the relationships between large and convective scales in observations and a stochastic multicloud model (smcm) to ultimately support the design of a novel convection parameterization with stochastic elements are investigated. observations of tropical convection obtained at darwin and kwajalein are used here. it is found that the variability of observed tropical convection generally decreases with increasing large-scale forcing, implying a transition from stochastic to more deterministic behavior with increasing forcing. convection shows a more systematic relationship with measures related to large-scale convergence compared to measures related to energetics (e.g., cape). using the observations, the parameters in the smcm are adjusted. then, the smcm is forced with the time series of the observed large-scale state and the simulated convective behavior is compared to that observed. it is found that the smcm cloud fields compare better with observations when using predictors related to convergence rather than energetics. furthermore, the underlying framework of the smcm is able to reproduce the observed functional dependencies of convective variability on the imposed large-scale state—an encouraging result on the road toward a novel convection parameterization approach. however, establishing sound cause-and-effect relationships between tropical convection and the large-scale environment remains problematic and warrants further research."
http://orkg.org/orkg/resource/R56078,Global patterns in threats to vertebrates by biological invasions,,crossref,"<jats:p>Biological invasions as drivers of biodiversity loss have recently been challenged. Fundamentally, we must know where species that are threatened by invasive alien species (IAS) live, and the degree to which they are threatened. We report the first study linking 1372 vertebrates threatened by more than 200 IAS from the completely revised Global Invasive Species Database. New maps of the vulnerability of threatened vertebrates to IAS permit assessments of whether IAS have a major influence on biodiversity, and if so, which taxonomic groups are threatened and where they are threatened. We found that centres of IAS-threatened vertebrates are concentrated in the Americas, India, Indonesia, Australia and New Zealand. The areas in which IAS-threatened species are located do not fully match the current hotspots of invasions, or the current hotspots of threatened species. The relative importance of biological invasions as drivers of biodiversity loss clearly varies across regions and taxa, and changes over time, with mammals from India, Indonesia, Australia and Europe are increasingly being threatened by IAS. The chytrid fungus primarily threatens amphibians, whereas invasive mammals primarily threaten other vertebrates. The differences in IAS threats between regions and taxa can help efficiently target IAS, which is essential for achieving the Strategic Plan 2020 of the Convention on Biological Diversity.</jats:p>","biological invasions as drivers of biodiversity loss have recently been challenged. fundamentally, we must know where species that are threatened by invasive alien species (ias) live, and the degree to which they are threatened. we report the first study linking 1372 vertebrates threatened by more than 200 ias from the completely revised global invasive species database. new maps of the vulnerability of threatened vertebrates to ias permit assessments of whether ias have a major influence on biodiversity, and if so, which taxonomic groups are threatened and where they are threatened. we found that centres of ias-threatened vertebrates are concentrated in the americas, india, indonesia, australia and new zealand. the areas in which ias-threatened species are located do not fully match the current hotspots of invasions, or the current hotspots of threatened species. the relative importance of biological invasions as drivers of biodiversity loss clearly varies across regions and taxa, and changes over time, with mammals from india, indonesia, australia and europe are increasingly being threatened by ias. the chytrid fungus primarily threatens amphibians, whereas invasive mammals primarily threaten other vertebrates. the differences in ias threats between regions and taxa can help efficiently target ias, which is essential for achieving the strategic plan 2020 of the convention on biological diversity."
http://orkg.org/orkg/resource/R56092,Global assessment of establishment success for amphibian and reptile invaders,,crossref,"<jats:p>\n\nContext\nAccording to the tens rule, 10% of introduced species establish themselves.\n\nAims\nWe tested this component of the tens rule for amphibians and reptiles globally, in Europe and North America, where data are presumably of good quality, and on islands versus continents. We also tested whether there was a taxonomic difference in establishment success between amphibians and reptiles.\n\nMethods\nWe examined data comprising 206 successful and 165 failed introduction records for 161 species of amphibians to 55 locations, and 560 successful and 641 failed introduction records for 469 species of reptiles to 116 locations around the world.\n\nKey results\nGlobally, establishment success was not different between amphibians (67%) and reptiles (62%). Both means were well above the 10% value predicted by the tens rule. In Europe and North America, establishment success was lower, although still higher than 10%. For reptiles, establishment success was higher on islands than on continents. Our results question the tens rule and do not show taxonomic differences in establishment success.\n\nImplications\nSimilar to studies on other taxa (birds and mammals), we found that establishment success was generally above 40%. This suggests that we should focus management on reducing the number of herptile species introduced because both reptiles and amphibians have a high likelihood of establishing. As data collection on invasions continue, testing establishment success in light of other factors, including propagule pressure, climate matching and taxonomic classifications, may provide additional insight into which species are most likely to establish in particular areas.\n\n</jats:p>","\n\ncontext\naccording to the tens rule, 10% of introduced species establish themselves.\n\naims\nwe tested this component of the tens rule for amphibians and reptiles globally, in europe and north america, where data are presumably of good quality, and on islands versus continents. we also tested whether there was a taxonomic difference in establishment success between amphibians and reptiles.\n\nmethods\nwe examined data comprising 206 successful and 165 failed introduction records for 161 species of amphibians to 55 locations, and 560 successful and 641 failed introduction records for 469 species of reptiles to 116 locations around the world.\n\nkey results\nglobally, establishment success was not different between amphibians (67%) and reptiles (62%). both means were well above the 10% value predicted by the tens rule. in europe and north america, establishment success was lower, although still higher than 10%. for reptiles, establishment success was higher on islands than on continents. our results question the tens rule and do not show taxonomic differences in establishment success.\n\nimplications\nsimilar to studies on other taxa (birds and mammals), we found that establishment success was generally above 40%. this suggests that we should focus management on reducing the number of herptile species introduced because both reptiles and amphibians have a high likelihood of establishing. as data collection on invasions continue, testing establishment success in light of other factors, including propagule pressure, climate matching and taxonomic classifications, may provide additional insight into which species are most likely to establish in particular areas.\n\n"
http://orkg.org/orkg/resource/R70278,Adaptive behaviour and learning in slime moulds: the role of oscillations,10.1098/rstb.2019.0757,crossref,"""<jats:p>\n            The slime mould\n            <jats:italic>Physarum polycephalum</jats:italic>\n            , an aneural organism, uses information from previous experiences to adjust its behaviour, but the mechanisms by which this is accomplished remain unknown. This article examines the possible role of oscillations in learning and memory in slime moulds. Slime moulds share surprising similarities with the network of synaptic connections in animal brains. First, their topology derives from a network of interconnected, vein-like tubes in which signalling molecules are transported. Second, network motility, which generates slime mould behaviour, is driven by distinct oscillations that organize into spatio-temporal wave patterns. Likewise, neural activity in the brain is organized in a variety of oscillations characterized by different frequencies. Interestingly, the oscillating networks of slime moulds are not precursors of nervous systems but, rather, an alternative architecture. Here, we argue that comparable information-processing operations can be realized on different architectures sharing similar oscillatory properties. After describing learning abilities and oscillatory activities of\n            <jats:italic>P. polycephalum</jats:italic>\n            , we explore the relation between network oscillations and learning, and evaluate the organism's global architecture with respect to information-processing potential. We hypothesize that, as in the brain, modulation of spontaneous oscillations may sustain learning in slime mould.\n          </jats:p>\n          <jats:p>This article is part of the theme issue ‘Basal cognition: conceptual tools and the view from the single cell’.</jats:p>""",""" \n the slime mould\n physarum polycephalum \n , an aneural organism, uses information from previous experiences to adjust its behaviour, but the mechanisms by which this is accomplished remain unknown. this article examines the possible role of oscillations in learning and memory in slime moulds. slime moulds share surprising similarities with the network of synaptic connections in animal brains. first, their topology derives from a network of interconnected, vein-like tubes in which signalling molecules are transported. second, network motility, which generates slime mould behaviour, is driven by distinct oscillations that organize into spatio-temporal wave patterns. likewise, neural activity in the brain is organized in a variety of oscillations characterized by different frequencies. interestingly, the oscillating networks of slime moulds are not precursors of nervous systems but, rather, an alternative architecture. here, we argue that comparable information-processing operations can be realized on different architectures sharing similar oscillatory properties. after describing learning abilities and oscillatory activities of\n p. polycephalum \n , we explore the relation between network oscillations and learning, and evaluate the organism's global architecture with respect to information-processing potential. we hypothesize that, as in the brain, modulation of spontaneous oscillations may sustain learning in slime mould.\n \n this article is part of the theme issue ‘basal cognition: conceptual tools and the view from the single cell’. """
http://orkg.org/orkg/resource/R70618,A diagnostic algorithm for the surveillance of deep surgical site infections after colorectal surgery,,crossref,"<jats:title>Abstract</jats:title><jats:sec id=""S0899823X19000369_as1""><jats:title>Objective:</jats:title><jats:p>Surveillance of surgical site infections (SSIs) is important for infection control and is usually performed through retrospective manual chart review. The aim of this study was to develop an algorithm for the surveillance of deep SSIs based on clinical variables to enhance efficiency of surveillance.</jats:p></jats:sec><jats:sec id=""S0899823X19000369_as2""><jats:title>Design:</jats:title><jats:p>Retrospective cohort study (2012–2015).</jats:p></jats:sec><jats:sec id=""S0899823X19000369_as3""><jats:title>Setting:</jats:title><jats:p>A Dutch teaching hospital.</jats:p></jats:sec><jats:sec id=""S0899823X19000369_as4""><jats:title>Participants:</jats:title><jats:p>We included all consecutive patients who underwent colorectal surgery excluding those with contaminated wounds at the time of surgery. All patients were evaluated for deep SSIs through manual chart review, using the Centers for Disease Control and Prevention (CDC) criteria as the reference standard.</jats:p></jats:sec><jats:sec id=""S0899823X19000369_as5""><jats:title>Analysis:</jats:title><jats:p>We used logistic regression modeling to identify predictors that contributed to the estimation of diagnostic probability. Bootstrapping was applied to increase generalizability, followed by assessment of statistical performance and clinical implications.</jats:p></jats:sec><jats:sec id=""S0899823X19000369_as6""><jats:title>Results:</jats:title><jats:p>In total, 1,606 patients were included, of whom 129 (8.0%) acquired a deep SSI. The final model included postoperative length of stay, wound class, readmission, reoperation, and 30-day mortality. The model achieved 68.7% specificity and 98.5% sensitivity and an area under the receiver operator characteristic (ROC) curve (AUC) of 0.950 (95% CI, 0.932–0.969). Positive and negative predictive values were 21.5% and 99.8%, respectively. Applying the algorithm resulted in a 63.4% reduction in the number of records requiring full manual review (from 1,606 to 590).</jats:p></jats:sec><jats:sec id=""S0899823X19000369_as7""><jats:title>Conclusions:</jats:title><jats:p>This 5-parameter model identified 98.5% of patients with a deep SSI. The model can be used to develop semiautomatic surveillance of deep SSIs after colorectal surgery, which may further improve efficiency and quality of SSI surveillance.</jats:p></jats:sec>","abstract objective: surveillance of surgical site infections (ssis) is important for infection control and is usually performed through retrospective manual chart review. the aim of this study was to develop an algorithm for the surveillance of deep ssis based on clinical variables to enhance efficiency of surveillance. design: retrospective cohort study (2012–2015). setting: a dutch teaching hospital. participants: we included all consecutive patients who underwent colorectal surgery excluding those with contaminated wounds at the time of surgery. all patients were evaluated for deep ssis through manual chart review, using the centers for disease control and prevention (cdc) criteria as the reference standard. analysis: we used logistic regression modeling to identify predictors that contributed to the estimation of diagnostic probability. bootstrapping was applied to increase generalizability, followed by assessment of statistical performance and clinical implications. results: in total, 1,606 patients were included, of whom 129 (8.0%) acquired a deep ssi. the final model included postoperative length of stay, wound class, readmission, reoperation, and 30-day mortality. the model achieved 68.7% specificity and 98.5% sensitivity and an area under the receiver operator characteristic (roc) curve (auc) of 0.950 (95% ci, 0.932–0.969). positive and negative predictive values were 21.5% and 99.8%, respectively. applying the algorithm resulted in a 63.4% reduction in the number of records requiring full manual review (from 1,606 to 590). conclusions: this 5-parameter model identified 98.5% of patients with a deep ssi. the model can be used to develop semiautomatic surveillance of deep ssis after colorectal surgery, which may further improve efficiency and quality of ssi surveillance."
http://orkg.org/orkg/resource/R70905,TeKET: a Tree-Based Unsupervised Keyphrase Extraction Technique ,10.1007/s12559-019-09706-3,crossref,"<jats:title>Abstract</jats:title><jats:p>Automatic keyphrase extraction techniques aim to extract quality keyphrases for higher level summarization of a document. Majority of the existing techniques are mainly domain-specific, which require application domain knowledge and employ higher order statistical methods, and computationally expensive and require large train data, which is rare for many applications. Overcoming these issues, this paper proposes a new unsupervised keyphrase extraction technique. The proposed unsupervised keyphrase extraction technique, named <jats:italic>TeKET</jats:italic> or <jats:italic>Tree-based Keyphrase Extraction Technique</jats:italic>, is a domain-independent technique that employs limited statistical knowledge and requires no train data. This technique also introduces a new variant of a binary tree, called <jats:italic>KeyPhrase Extraction</jats:italic> (<jats:italic>KePhEx</jats:italic>) tree, to extract final keyphrases from candidate keyphrases. In addition, a measure, called <jats:italic>Cohesiveness Index</jats:italic> or <jats:italic>CI</jats:italic>, is derived which denotes a given node’s degree of cohesiveness with respect to the root. The CI is used in flexibly extracting final keyphrases from the KePhEx tree and is co-utilized in the ranking process. The effectiveness of the proposed technique and its domain and language independence are experimentally evaluated using available benchmark corpora, namely SemEval-2010 (a scientific articles dataset), Theses100 (a thesis dataset), and a <jats:italic>German Research Article</jats:italic> dataset, respectively. The acquired results are compared with other relevant unsupervised techniques belonging to both statistical and graph-based techniques. The obtained results demonstrate the improved performance of the proposed technique over other compared techniques in terms of precision, recall, and F1 scores.</jats:p>","abstract automatic keyphrase extraction techniques aim to extract quality keyphrases for higher level summarization of a document. majority of the existing techniques are mainly domain-specific, which require application domain knowledge and employ higher order statistical methods, and computationally expensive and require large train data, which is rare for many applications. overcoming these issues, this paper proposes a new unsupervised keyphrase extraction technique. the proposed unsupervised keyphrase extraction technique, named teket or tree-based keyphrase extraction technique , is a domain-independent technique that employs limited statistical knowledge and requires no train data. this technique also introduces a new variant of a binary tree, called keyphrase extraction ( kephex ) tree, to extract final keyphrases from candidate keyphrases. in addition, a measure, called cohesiveness index or ci , is derived which denotes a given node’s degree of cohesiveness with respect to the root. the ci is used in flexibly extracting final keyphrases from the kephex tree and is co-utilized in the ranking process. the effectiveness of the proposed technique and its domain and language independence are experimentally evaluated using available benchmark corpora, namely semeval-2010 (a scientific articles dataset), theses100 (a thesis dataset), and a german research article dataset, respectively. the acquired results are compared with other relevant unsupervised techniques belonging to both statistical and graph-based techniques. the obtained results demonstrate the improved performance of the proposed technique over other compared techniques in terms of precision, recall, and f1 scores."
http://orkg.org/orkg/resource/R70740,ICT Engagement: a new construct and its assessment in PISA 2015,10.1186/s40536-020-00084-z,crossref,"<jats:title>Abstract</jats:title><jats:p>As a relevant cognitive-motivational aspect of ICT literacy, a new construct <jats:italic>ICT Engagement</jats:italic> is theoretically based on self-determination theory and involves the factors ICT interest, Perceived ICT competence, Perceived autonomy related to ICT use, and ICT as a topic in social interaction. In this manuscript, we present different sources of validity supporting the construct interpretation of test scores in the ICT Engagement scale, which was used in PISA 2015. Specifically, we investigated the internal structure by dimensional analyses and investigated the relation of ICT Engagement aspects to other variables. The analyses are based on public data from PISA 2015 main study from Switzerland (<jats:italic>n</jats:italic>\u2009=\u20095860) and Germany (<jats:italic>n</jats:italic>\u2009=\u20096504). First, we could confirm the four-dimensional structure of ICT Engagement for the Swiss sample using a structural equation modelling approach. Second, ICT Engagement scales explained the highest amount of variance in ICT Use for Entertainment, followed by Practical use. Third, we found significantly lower values for girls in all ICT Engagement scales except ICT Interest. Fourth, we found a small negative correlation between the scores in the subscale “ICT as a topic in social interaction” and reading performance in PISA 2015. We could replicate most results for the German sample. Overall, the obtained results support the construct interpretation of the four ICT Engagement subscales.</jats:p>","abstract as a relevant cognitive-motivational aspect of ict literacy, a new construct ict engagement is theoretically based on self-determination theory and involves the factors ict interest, perceived ict competence, perceived autonomy related to ict use, and ict as a topic in social interaction. in this manuscript, we present different sources of validity supporting the construct interpretation of test scores in the ict engagement scale, which was used in pisa 2015. specifically, we investigated the internal structure by dimensional analyses and investigated the relation of ict engagement aspects to other variables. the analyses are based on public data from pisa 2015 main study from switzerland ( n \u2009=\u20095860) and germany ( n \u2009=\u20096504). first, we could confirm the four-dimensional structure of ict engagement for the swiss sample using a structural equation modelling approach. second, ict engagement scales explained the highest amount of variance in ict use for entertainment, followed by practical use. third, we found significantly lower values for girls in all ict engagement scales except ict interest. fourth, we found a small negative correlation between the scores in the subscale “ict as a topic in social interaction” and reading performance in pisa 2015. we could replicate most results for the german sample. overall, the obtained results support the construct interpretation of the four ict engagement subscales."
http://orkg.org/orkg/resource/R70742,"A PISA-2015 Comparative Meta-Analysis between Singapore and Finland: Relations of Students’ Interest in Science, Perceived ICT Competence, and Environmental Awareness and Optimism",10.3390/ijerph16245157,crossref,"<jats:p>The aim of the present study is twofold: (1) to identify a factor structure between variables-interest in broad science topics, perceived information and communications technology (ICT) competence, environmental awareness and optimism; and (2) to explore the relations between these variables at the country level. The first part of the aim is addressed using exploratory factor analysis with data from the Program for International Student Assessment (PISA) for 15-year-old students from Singapore and Finland. The results show that a comparable structure with four factors was verified in both countries. Correlation analyses and linear regression were used to address the second part of the aim. The results show that adolescents’ interest in broad science topics can predict perceived ICT competence. Their interest in broad science topics and perceived ICT competence can predict environmental awareness in both countries. However, there is difference in predicting environmental optimism. Singaporean students’ interest in broad science topics and their perceived ICT competences are positive predictors, whereas environmental awareness is a negative predictor. Finnish students’ environmental awareness negatively predicted environmental optimism.</jats:p>","the aim of the present study is twofold: (1) to identify a factor structure between variables-interest in broad science topics, perceived information and communications technology (ict) competence, environmental awareness and optimism; and (2) to explore the relations between these variables at the country level. the first part of the aim is addressed using exploratory factor analysis with data from the program for international student assessment (pisa) for 15-year-old students from singapore and finland. the results show that a comparable structure with four factors was verified in both countries. correlation analyses and linear regression were used to address the second part of the aim. the results show that adolescents’ interest in broad science topics can predict perceived ict competence. their interest in broad science topics and perceived ict competence can predict environmental awareness in both countries. however, there is difference in predicting environmental optimism. singaporean students’ interest in broad science topics and their perceived ict competences are positive predictors, whereas environmental awareness is a negative predictor. finnish students’ environmental awareness negatively predicted environmental optimism."
http://orkg.org/orkg/resource/R70603,Learning Data-Driven Patient Risk Stratification Models for Clostridium difficile,,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Background. \u2003Although many risk factors are well known, Clostridium difficile infection (CDI) continues to be a significant problem throughout the world. The purpose of this study was to develop and validate a data-driven, hospital-specific risk stratification procedure for estimating the probability that an inpatient will test positive for C difficile.</jats:p>\n               <jats:p>Methods. \u2003We consider electronic medical record (EMR) data from patients admitted for ≥24 hours to a large urban hospital in the U.S. between April 2011 and April 2013. Predictive models were constructed using L2-regularized logistic regression and data from the first year. The number of observational variables considered varied from a small set of well known risk factors readily available to a physician to over 10 000 variables automatically extracted from the EMR. Each model was evaluated on holdout admission data from the following year. A total of 34 846 admissions with 372 cases of CDI was used to train the model.</jats:p>\n               <jats:p>Results. \u2003Applied to the separate validation set of 34 722 admissions with 355 cases of CDI, the model that made use of the additional EMR data yielded an area under the receiver operating characteristic curve (AUROC) of 0.81 (95% confidence interval [CI], .79–.83), and it significantly outperformed the model that considered only the small set of known clinical risk factors, AUROC of 0.71 (95% CI, .69–.75).</jats:p>\n               <jats:p>Conclusions. \u2003Automated risk stratification of patients based on the contents of their EMRs can be used to accurately ide.jpegy a high-risk population of patients. The proposed method holds promise for enabling the selective allocation of interventions aimed at reducing the rate of CDI.</jats:p>","abstract \n background. \u2003although many risk factors are well known, clostridium difficile infection (cdi) continues to be a significant problem throughout the world. the purpose of this study was to develop and validate a data-driven, hospital-specific risk stratification procedure for estimating the probability that an inpatient will test positive for c difficile. \n methods. \u2003we consider electronic medical record (emr) data from patients admitted for ≥24 hours to a large urban hospital in the u.s. between april 2011 and april 2013. predictive models were constructed using l2-regularized logistic regression and data from the first year. the number of observational variables considered varied from a small set of well known risk factors readily available to a physician to over 10 000 variables automatically extracted from the emr. each model was evaluated on holdout admission data from the following year. a total of 34 846 admissions with 372 cases of cdi was used to train the model. \n results. \u2003applied to the separate validation set of 34 722 admissions with 355 cases of cdi, the model that made use of the additional emr data yielded an area under the receiver operating characteristic curve (auroc) of 0.81 (95% confidence interval [ci], .79–.83), and it significantly outperformed the model that considered only the small set of known clinical risk factors, auroc of 0.71 (95% ci, .69–.75). \n conclusions. \u2003automated risk stratification of patients based on the contents of their emrs can be used to accurately ide.jpegy a high-risk population of patients. the proposed method holds promise for enabling the selective allocation of interventions aimed at reducing the rate of cdi."
http://orkg.org/orkg/resource/R55006,Propagule pressure and persistence in experimental populations,,crossref,"<jats:p>\n            Average inoculum size and number of introductions are known to have positive effects on population persistence. However, whether these factors affect persistence independently or interact is unknown. We conducted a two-factor experiment in which 112 populations of parthenogenetic\n            <jats:italic>Daphnia magna</jats:italic>\n            were maintained for 41 days to study effects of inoculum size and introduction frequency on: (i) population growth, (ii) population persistence and (iii) time-to-extinction. We found that the interaction of inoculum size and introduction frequency—the immigration rate—affected all three dependent variables, while population growth was additionally affected by introduction frequency. We conclude that for this system the most important aspect of propagule pressure is immigration rate, with relatively minor additional effects of introduction frequency and negligible effects of inoculum size.\n          </jats:p>","\n average inoculum size and number of introductions are known to have positive effects on population persistence. however, whether these factors affect persistence independently or interact is unknown. we conducted a two-factor experiment in which 112 populations of parthenogenetic\n daphnia magna \n were maintained for 41 days to study effects of inoculum size and introduction frequency on: (i) population growth, (ii) population persistence and (iii) time-to-extinction. we found that the interaction of inoculum size and introduction frequency—the immigration rate—affected all three dependent variables, while population growth was additionally affected by introduction frequency. we conclude that for this system the most important aspect of propagule pressure is immigration rate, with relatively minor additional effects of introduction frequency and negligible effects of inoculum size.\n"
http://orkg.org/orkg/resource/R54822,"Invasion, competitive dominance, and resource use by exotic and native California grassland species",10.1073/pnas.1835728100,crossref,"<jats:p>The dynamics of invasive species may depend on their abilities to compete for resources and exploit disturbances relative to the abilities of native species. We test this hypothesis and explore its implications for the restoration of native ecosystems in one of the most dramatic ecological invasions worldwide, the replacement of native perennial grasses by exotic annual grasses and forbs in 9.2 million hectares of California grasslands. The long-term persistence of these exotic annuals has been thought to imply that the exotics are superior competitors. However, seed-addition experiments in a southern California grassland revealed that native perennial species, which had lower requirements for deep soil water, soil nitrate, and light, were strong competitors, and they markedly depressed the abundance and fecundity of exotic annuals after overcoming recruitment limitations. Native species reinvaded exotic grasslands across experimentally imposed nitrogen, water, and disturbance gradients. Thus, exotic annuals are not superior competitors but rather may dominate because of prior disturbance and the low dispersal abilities and extreme current rarity of native perennials. If our results prove to be general, it may be feasible to restore native California grassland flora to at least parts of its former range.</jats:p>","the dynamics of invasive species may depend on their abilities to compete for resources and exploit disturbances relative to the abilities of native species. we test this hypothesis and explore its implications for the restoration of native ecosystems in one of the most dramatic ecological invasions worldwide, the replacement of native perennial grasses by exotic annual grasses and forbs in 9.2 million hectares of california grasslands. the long-term persistence of these exotic annuals has been thought to imply that the exotics are superior competitors. however, seed-addition experiments in a southern california grassland revealed that native perennial species, which had lower requirements for deep soil water, soil nitrate, and light, were strong competitors, and they markedly depressed the abundance and fecundity of exotic annuals after overcoming recruitment limitations. native species reinvaded exotic grasslands across experimentally imposed nitrogen, water, and disturbance gradients. thus, exotic annuals are not superior competitors but rather may dominate because of prior disturbance and the low dispersal abilities and extreme current rarity of native perennials. if our results prove to be general, it may be feasible to restore native california grassland flora to at least parts of its former range."
http://orkg.org/orkg/resource/R54884,Past warming trend constrains future warming in CMIP6 models,10.1126/sciadv.aaz9549,crossref,<jats:p>Strong future warming in some new climate models is less likely as their recent warming is inconsistent with observed trends.</jats:p>,strong future warming in some new climate models is less likely as their recent warming is inconsistent with observed trends.
http://orkg.org/orkg/resource/R54729,"The short-term responses of small mammals to wildfire in semiarid mallee shrubland, Australia",10.1071/wr10016,crossref,"<jats:p>\n\nContext. Wildfire is a major driver of the structure and function of mallee eucalypt- and spinifex-dominated landscapes. Understanding how fire influences the distribution of biota in these fire-prone environments is essential for effective ecological and conservation-based management.\nAims. We aimed to (1) determine the effects of an extensive wildfire (118\u2009000\u2009ha) on a small mammal community in the mallee shrublands of semiarid Australia and (2) assess the hypothesis that the fire-response patterns of small mammals can be predicted by their life-history characteristics.\nMethods. Small-mammal surveys were undertaken concurrently at 26 sites: once before the fire and on four occasions following the fire (including 14 sites that remained unburnt). We documented changes in small-mammal occurrence before and after the fire, and compared burnt and unburnt sites. In addition, key components of vegetation structure were assessed at each site.\nKey results. Wildfire had a strong influence on vegetation structure and on the occurrence of small mammals. The mallee ningaui, Ningaui yvonneae, a dasyurid marsupial, showed a marked decline in the immediate post-fire environment, corresponding with a reduction in hummock-grass cover in recently burnt vegetation. Species richness of native small mammals was positively associated with unburnt vegetation, although some species showed no clear response to wildfire.\nConclusions. Our results are consistent with the contention that mammal responses to fire are associated with their known life-history traits. The species most strongly affected by wildfire, N. yvonneae, has the most specific habitat requirements and restricted life history of the small mammals in the study area. The only species positively associated with recently burnt vegetation, the introduced house mouse, Mus domesticus, has a flexible life history and non-specialised resource requirements.\nImplications. Maintaining sources for recolonisation after large-scale wildfires will be vital to the conservation of native small mammals in mallee ecosystems.\n</jats:p>","\n\ncontext. wildfire is a major driver of the structure and function of mallee eucalypt- and spinifex-dominated landscapes. understanding how fire influences the distribution of biota in these fire-prone environments is essential for effective ecological and conservation-based management.\naims. we aimed to (1) determine the effects of an extensive wildfire (118\u2009000\u2009ha) on a small mammal community in the mallee shrublands of semiarid australia and (2) assess the hypothesis that the fire-response patterns of small mammals can be predicted by their life-history characteristics.\nmethods. small-mammal surveys were undertaken concurrently at 26 sites: once before the fire and on four occasions following the fire (including 14 sites that remained unburnt). we documented changes in small-mammal occurrence before and after the fire, and compared burnt and unburnt sites. in addition, key components of vegetation structure were assessed at each site.\nkey results. wildfire had a strong influence on vegetation structure and on the occurrence of small mammals. the mallee ningaui, ningaui yvonneae, a dasyurid marsupial, showed a marked decline in the immediate post-fire environment, corresponding with a reduction in hummock-grass cover in recently burnt vegetation. species richness of native small mammals was positively associated with unburnt vegetation, although some species showed no clear response to wildfire.\nconclusions. our results are consistent with the contention that mammal responses to fire are associated with their known life-history traits. the species most strongly affected by wildfire, n. yvonneae, has the most specific habitat requirements and restricted life history of the small mammals in the study area. the only species positively associated with recently burnt vegetation, the introduced house mouse, mus domesticus, has a flexible life history and non-specialised resource requirements.\nimplications. maintaining sources for recolonisation after large-scale wildfires will be vital to the conservation of native small mammals in mallee ecosystems.\n"
http://orkg.org/orkg/resource/R54778,Six years of plant community development after clearcut harvesting in western Washington,10.1139/x08-170,crossref,"<jats:p> What roles do ruderals and residuals play in early forest succession and how does repeated disturbance affect them? We examined this question by monitoring plant cover and composition on a productive site for 6\xa0years after clearcutting and planting Douglas-fir ( Pseudotsuga menziesii (Mirb.) Franco). The replicated experiment included three treatments: vegetation control with five annual herbicide applications superimposed over two levels of slash removal (bole only or total tree plus most other wood) and an untreated control. Three species groups were analyzed: native forest, native ruderals, and exotic ruderals. Without vegetation control, the understory was rapidly invaded by exotic ruderals but was codominated by native and exotic ruderals by year 6. Douglas-fir cover surpassed covers in the three species group covers at least 3\xa0years sooner with herbicide treatments than without. Species richness and coverage were lower for all species groups with vegetation control than without vegetation control. The effects of organic matter removal were much less than that of vegetation control. As predicted by the Intermediate Disturbance Hypothesis, repeated vegetation control resulted in declining cover and richness; however, native forest species were surprisingly resilient, maintaining as much or more cover and richness as the ruderal groups. </jats:p>","what roles do ruderals and residuals play in early forest succession and how does repeated disturbance affect them? we examined this question by monitoring plant cover and composition on a productive site for 6\xa0years after clearcutting and planting douglas-fir ( pseudotsuga menziesii (mirb.) franco). the replicated experiment included three treatments: vegetation control with five annual herbicide applications superimposed over two levels of slash removal (bole only or total tree plus most other wood) and an untreated control. three species groups were analyzed: native forest, native ruderals, and exotic ruderals. without vegetation control, the understory was rapidly invaded by exotic ruderals but was codominated by native and exotic ruderals by year 6. douglas-fir cover surpassed covers in the three species group covers at least 3\xa0years sooner with herbicide treatments than without. species richness and coverage were lower for all species groups with vegetation control than without vegetation control. the effects of organic matter removal were much less than that of vegetation control. as predicted by the intermediate disturbance hypothesis, repeated vegetation control resulted in declining cover and richness; however, native forest species were surprisingly resilient, maintaining as much or more cover and richness as the ruderal groups."
http://orkg.org/orkg/resource/R54789,Conservation of the Grassy White Box Woodlands: Relative Contributions of Size and Disturbance to Floristic Composition and Diversity of Remnants,10.1071/bt9950349,crossref,"<jats:p> Before European settlement, grassy white box woodlands were the dominant vegetation in the east of the\nwheat-sheep belt of south-eastern Australia. Tree clearing, cultivation and pasture improvement have led\nto fragmentation of this once relatively continuous ecosystem, leaving a series of remnants which\nthemselves have been modified by livestock grazing. Little-modified remnants are extremely rare. We\nexamined and compared the effects of fragmentation and disturbance on the understorey flora of\nwoodland remnants, through a survey of remnants of varying size, grazing history and tree clearing. In\naccordance with fragmentation theory, species richness generally increased with remnant size, and, for\nlittle-grazed remnants, smaller remnants were more vulnerable to weed invasion. Similarly, tree\nclearing and grazing encouraged weed invasion and reduced native species richness. Evidence for\nincreased total species richness at intermediate grazing levels, as predicted by the intermediate\ndisturbance hypothesis, was equivocal. Remnant quality was more severely affected by grazing than by\nremnant size. All little-grazed remnants had lower exotic species abundance and similar or higher native\nspecies richness than grazed remnants, despite their extremely small sizes (&lt; 6 ha). Further, small, littlegrazed\nremnants maintained the general character of the pre-European woodland understorey, while\ngrazing caused changes to the dominant species. Although generally small, the little-grazed remnants\nare the best representatives of the pre-European woodland understorey, and should be central to any\nconservation plan for the woodlands. Selected larger remnants are needed to complement these,\nhowever, to increase the total area of woodland conserved, and, because most little-grazed remnants are\ncleared, to represent the ecosystem in its original structural form. For the maintenance of native plant\ndiversity and composition in little-grazed remnants, it is critical that livestock grazing continues to be\nexcluded. For grazed remnants, maintenance of a site in its current state would allow continuation of\npast management, while restoration to a pre-European condition would require management directed\ntowards weed removal, and could take advantage of the difference noted in the predominant life-cycle\nof native (perennial) versus exotic (annual or biennial) species. </jats:p>","before european settlement, grassy white box woodlands were the dominant vegetation in the east of the\nwheat-sheep belt of south-eastern australia. tree clearing, cultivation and pasture improvement have led\nto fragmentation of this once relatively continuous ecosystem, leaving a series of remnants which\nthemselves have been modified by livestock grazing. little-modified remnants are extremely rare. we\nexamined and compared the effects of fragmentation and disturbance on the understorey flora of\nwoodland remnants, through a survey of remnants of varying size, grazing history and tree clearing. in\naccordance with fragmentation theory, species richness generally increased with remnant size, and, for\nlittle-grazed remnants, smaller remnants were more vulnerable to weed invasion. similarly, tree\nclearing and grazing encouraged weed invasion and reduced native species richness. evidence for\nincreased total species richness at intermediate grazing levels, as predicted by the intermediate\ndisturbance hypothesis, was equivocal. remnant quality was more severely affected by grazing than by\nremnant size. all little-grazed remnants had lower exotic species abundance and similar or higher native\nspecies richness than grazed remnants, despite their extremely small sizes (&lt; 6 ha). further, small, littlegrazed\nremnants maintained the general character of the pre-european woodland understorey, while\ngrazing caused changes to the dominant species. although generally small, the little-grazed remnants\nare the best representatives of the pre-european woodland understorey, and should be central to any\nconservation plan for the woodlands. selected larger remnants are needed to complement these,\nhowever, to increase the total area of woodland conserved, and, because most little-grazed remnants are\ncleared, to represent the ecosystem in its original structural form. for the maintenance of native plant\ndiversity and composition in little-grazed remnants, it is critical that livestock grazing continues to be\nexcluded. for grazed remnants, maintenance of a site in its current state would allow continuation of\npast management, while restoration to a pre-european condition would require management directed\ntowards weed removal, and could take advantage of the difference noted in the predominant life-cycle\nof native (perennial) versus exotic (annual or biennial) species."
http://orkg.org/orkg/resource/R54704,Prescribed fire effects on dalmation toadflax,10.2458/azu_jrm_v56i2_jacobs,crossref,"<jats:p>Prescribed fires are important for rangeland restoration and affect plant community composition and species interactions. Many rangeland plant communities have been, or are under the threat of noxious weed invasion, however there is little information on how fire effects weeds. Our objective was to determine the effects of prescribed rangeland fire on dalmatian toadflax [Linaria dalmatica (L.) Miller] density, cover, biomass, and seed production. These plant characteristics, as well as density, cover, and biomass of perennial grasses and forbs were measured within burned and adjacent not-burned areas on 3 Artemisia tridentata/Agropyron spicatum habitat types in Montana. Areas were burned in the spring and measured in the fall 1999. Comparisons of plant characteristics between the burned and not-burned sites were made using t-tests and non-parametric Wilcoxon Rank Sum tests. After 1 growing season, fire did not affect density or cover of dalmatian toadflax. Burning increased dalmatian toadflax bio- mass per square meter at 2 sites, and per plant biomass at all 3 sites. Seed production of dalmatian toadflax was increased by fire at all 3 sites. Fire reduced forb cover at 1 site and increased grass biomass at 2 sites. The increases in dalmatian toadflax biomass and seed production suggest that fire used to restore healthy plant communities may increase dalmatian toadflax dominance. We recommend weed management procedures, such as herbicide control and seeding desirable species, be integrated with prescribed fire where dalmatian toadflax is present in the plant community.</jats:p>","prescribed fires are important for rangeland restoration and affect plant community composition and species interactions. many rangeland plant communities have been, or are under the threat of noxious weed invasion, however there is little information on how fire effects weeds. our objective was to determine the effects of prescribed rangeland fire on dalmatian toadflax [linaria dalmatica (l.) miller] density, cover, biomass, and seed production. these plant characteristics, as well as density, cover, and biomass of perennial grasses and forbs were measured within burned and adjacent not-burned areas on 3 artemisia tridentata/agropyron spicatum habitat types in montana. areas were burned in the spring and measured in the fall 1999. comparisons of plant characteristics between the burned and not-burned sites were made using t-tests and non-parametric wilcoxon rank sum tests. after 1 growing season, fire did not affect density or cover of dalmatian toadflax. burning increased dalmatian toadflax bio- mass per square meter at 2 sites, and per plant biomass at all 3 sites. seed production of dalmatian toadflax was increased by fire at all 3 sites. fire reduced forb cover at 1 site and increased grass biomass at 2 sites. the increases in dalmatian toadflax biomass and seed production suggest that fire used to restore healthy plant communities may increase dalmatian toadflax dominance. we recommend weed management procedures, such as herbicide control and seeding desirable species, be integrated with prescribed fire where dalmatian toadflax is present in the plant community."
http://orkg.org/orkg/resource/R54236,Induced defenses in response to an invading crab predator: An explanation of historical and geographic phenotypic change,10.1073/pnas.040423397,crossref,"<jats:p>\n            The expression of defensive morphologies in prey often is correlated with predator abundance or diversity over a range of temporal and spatial scales. These patterns are assumed to reflect natural selection via differential predation on genetically determined, fixed phenotypes. Phenotypic variation, however, also can reflect within-generation developmental responses to environmental cues (phenotypic plasticity). For example, water-borne effluents from predators can induce the production of defensive morphologies in many prey taxa. This phenomenon, however, has been examined only on narrow scales. Here, we demonstrate adaptive phenotypic plasticity in prey from geographically separated populations that were reared in the presence of an introduced predator. Marine snails exposed to predatory crab effluent in the field increased shell thickness rapidly compared with controls. Induced changes were comparable to (\n            <jats:italic>i</jats:italic>\n            ) historical transitions in thickness previously attributed to selection by the invading predator and (\n            <jats:italic>ii</jats:italic>\n            ) present-day clinal variation predicted from water temperature differences. Thus, predator-induced phenotypic plasticity may explain broad-scale geographic and temporal phenotypic variation. If inducible defenses are heritable, then selection on the reaction norm may influence coevolution between predator and prey. Trade-offs may explain why inducible rather than constitutive defenses have evolved in several gastropod species.\n          </jats:p>","\n the expression of defensive morphologies in prey often is correlated with predator abundance or diversity over a range of temporal and spatial scales. these patterns are assumed to reflect natural selection via differential predation on genetically determined, fixed phenotypes. phenotypic variation, however, also can reflect within-generation developmental responses to environmental cues (phenotypic plasticity). for example, water-borne effluents from predators can induce the production of defensive morphologies in many prey taxa. this phenomenon, however, has been examined only on narrow scales. here, we demonstrate adaptive phenotypic plasticity in prey from geographically separated populations that were reared in the presence of an introduced predator. marine snails exposed to predatory crab effluent in the field increased shell thickness rapidly compared with controls. induced changes were comparable to (\n i \n ) historical transitions in thickness previously attributed to selection by the invading predator and (\n ii \n ) present-day clinal variation predicted from water temperature differences. thus, predator-induced phenotypic plasticity may explain broad-scale geographic and temporal phenotypic variation. if inducible defenses are heritable, then selection on the reaction norm may influence coevolution between predator and prey. trade-offs may explain why inducible rather than constitutive defenses have evolved in several gastropod species.\n"
http://orkg.org/orkg/resource/R54148,Developmental plasticity of shell morphology of quagga mussels from shallow and deep-water habitats of the Great Lakes ,10.1242/jeb.042549,crossref,"<jats:title>SUMMARY</jats:title>\n               <jats:p>The invasive zebra mussel (Dreissena polymorpha) has quickly colonized shallow-water habitats in the North American Great Lakes since the 1980s but the quagga mussel (Dreissena bugensis) is becoming dominant in both shallow and deep-water habitats. While quagga mussel shell morphology differs between shallow and deep habitats, functional causes and consequences of such difference are unknown. We examined whether quagga mussel shell morphology could be induced by three environmental variables through developmental plasticity. We predicted that shallow-water conditions (high temperature, food quantity, water motion) would yield a morphotype typical of wild quagga mussels from shallow habitats, while deep-water conditions (low temperature, food quantity, water motion) would yield a morphotype present in deep habitats. We tested this prediction by examining shell morphology and growth rate of quagga mussels collected from shallow and deep habitats and reared under common-garden treatments that manipulated the three variables. Shell morphology was quantified using the polar moment of inertia. Of the variables tested, temperature had the greatest effect on shell morphology. Higher temperature (∼18–20°C) yielded a morphotype typical of wild shallow mussels regardless of the levels of food quantity or water motion. In contrast, lower temperature (∼6–8°C) yielded a morphotype approaching that of wild deep mussels. If shell morphology has functional consequences in particular habitats, a plastic response might confer quagga mussels with a greater ability than zebra mussels to colonize a wider range of habitats within the Great Lakes.</jats:p>","summary \n the invasive zebra mussel (dreissena polymorpha) has quickly colonized shallow-water habitats in the north american great lakes since the 1980s but the quagga mussel (dreissena bugensis) is becoming dominant in both shallow and deep-water habitats. while quagga mussel shell morphology differs between shallow and deep habitats, functional causes and consequences of such difference are unknown. we examined whether quagga mussel shell morphology could be induced by three environmental variables through developmental plasticity. we predicted that shallow-water conditions (high temperature, food quantity, water motion) would yield a morphotype typical of wild quagga mussels from shallow habitats, while deep-water conditions (low temperature, food quantity, water motion) would yield a morphotype present in deep habitats. we tested this prediction by examining shell morphology and growth rate of quagga mussels collected from shallow and deep habitats and reared under common-garden treatments that manipulated the three variables. shell morphology was quantified using the polar moment of inertia. of the variables tested, temperature had the greatest effect on shell morphology. higher temperature (∼18–20°c) yielded a morphotype typical of wild shallow mussels regardless of the levels of food quantity or water motion. in contrast, lower temperature (∼6–8°c) yielded a morphotype approaching that of wild deep mussels. if shell morphology has functional consequences in particular habitats, a plastic response might confer quagga mussels with a greater ability than zebra mussels to colonize a wider range of habitats within the great lakes."
http://orkg.org/orkg/resource/R54162,Trade-off between morphological convergence and opportunistic diet behavior in fish hybrid zone ,10.1186/1742-9994-6-26,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>The invasive <jats:italic>Chondrostoma nasus nasus</jats:italic> has colonized part of the distribution area of the protected endemic species <jats:italic>Chondrostoma toxostoma toxostoma</jats:italic>. This hybrid zone is a complex system where multiple effects such as inter-species competition, bi-directional introgression, strong environmental pressure and so on are combined. Why do sympatric <jats:italic>Chondrostoma</jats:italic> fish present a unidirectional change in body shape? Is this the result of inter-species interactions and/or a response to environmental effects or the result of trade-offs? Studies focusing on the understanding of a trade-off between multiple parameters are still rare. Although this has previously been done for Cichlid species flock and for Darwin finches, where mouth or beak morphology were coupled to diet and genetic identification, no similar studies have been done for a fish hybrid zone in a river. We tested the correlation between morphology (body and mouth morphology), diet (stable carbon and nitrogen isotopes) and genomic combinations in different allopatric and sympatric populations for a global data set of 1330 specimens. To separate the species interaction effect from the environmental effect in sympatry, we distinguished two data sets: the first one was obtained from a highly regulated part of the river and the second was obtained from specimens coming from the less regulated part.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>The distribution of the hybrid combinations was different in the two part of the sympatric zone, whereas all the specimens presented similar overall changes in body shape and in mouth morphology. Sympatric specimens were also characterized by a larger diet behavior variance than reference populations, characteristic of an opportunistic diet. No correlation was established between the body shape (or mouth deformation) and the stable isotope signature.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusion</jats:title>\n            <jats:p>The Durance River is an untamed Mediterranean river despite the presence of numerous dams that split the river from upstream to downstream. The sympatric effect on morphology and the large diet behavior range can be explained by a tendency toward an opportunistic behavior of the sympatric specimens. Indeed, the similar response of the two species and their hybrids implied an adaptation that could be defined as an alternative trade-off that underline the importance of epigenetics mechanisms for potential success in a novel environment.</jats:p>\n          </jats:sec>","abstract \n \n background \n the invasive chondrostoma nasus nasus has colonized part of the distribution area of the protected endemic species chondrostoma toxostoma toxostoma . this hybrid zone is a complex system where multiple effects such as inter-species competition, bi-directional introgression, strong environmental pressure and so on are combined. why do sympatric chondrostoma fish present a unidirectional change in body shape? is this the result of inter-species interactions and/or a response to environmental effects or the result of trade-offs? studies focusing on the understanding of a trade-off between multiple parameters are still rare. although this has previously been done for cichlid species flock and for darwin finches, where mouth or beak morphology were coupled to diet and genetic identification, no similar studies have been done for a fish hybrid zone in a river. we tested the correlation between morphology (body and mouth morphology), diet (stable carbon and nitrogen isotopes) and genomic combinations in different allopatric and sympatric populations for a global data set of 1330 specimens. to separate the species interaction effect from the environmental effect in sympatry, we distinguished two data sets: the first one was obtained from a highly regulated part of the river and the second was obtained from specimens coming from the less regulated part. \n \n \n results \n the distribution of the hybrid combinations was different in the two part of the sympatric zone, whereas all the specimens presented similar overall changes in body shape and in mouth morphology. sympatric specimens were also characterized by a larger diet behavior variance than reference populations, characteristic of an opportunistic diet. no correlation was established between the body shape (or mouth deformation) and the stable isotope signature. \n \n \n conclusion \n the durance river is an untamed mediterranean river despite the presence of numerous dams that split the river from upstream to downstream. the sympatric effect on morphology and the large diet behavior range can be explained by a tendency toward an opportunistic behavior of the sympatric specimens. indeed, the similar response of the two species and their hybrids implied an adaptation that could be defined as an alternative trade-off that underline the importance of epigenetics mechanisms for potential success in a novel environment. \n"
http://orkg.org/orkg/resource/R54170,Life history plasticity magnifies the ecological effects of a social wasp invasion ,10.1073/pnas.0902979106,crossref,"<jats:p>\n            An unresolved question in ecology concerns why the ecological effects of invasions vary in magnitude. Many introduced species fail to interact strongly with the recipient biota, whereas others profoundly disrupt the ecosystems they invade through predation, competition, and other mechanisms. In the context of ecological impacts, research on biological invasions seldom considers phenotypic or microevolutionary changes that occur following introduction. Here, we show how plasticity in key life history traits (colony size and longevity), together with omnivory, magnifies the predatory impacts of an invasive social wasp (\n            <jats:italic>Vespula pensylvanica</jats:italic>\n            ) on a largely endemic arthropod fauna in Hawaii. Using a combination of molecular, experimental, and behavioral approaches, we demonstrate (\n            <jats:italic>i</jats:italic>\n            ) that yellowjackets consume an astonishing diversity of arthropod resources and depress prey populations in invaded Hawaiian ecosystems and (\n            <jats:italic>ii</jats:italic>\n            ) that their impact as predators in this region increases when they shift from small annual colonies to large perennial colonies. Such trait plasticity may influence invasion success and the degree of disruption that invaded ecosystems experience. Moreover, postintroduction phenotypic changes may help invaders to compensate for reductions in adaptive potential resulting from founder events and small population sizes. The dynamic nature of biological invasions necessitates a more quantitative understanding of how postintroduction changes in invader traits affect invasion processes.\n          </jats:p>","\n an unresolved question in ecology concerns why the ecological effects of invasions vary in magnitude. many introduced species fail to interact strongly with the recipient biota, whereas others profoundly disrupt the ecosystems they invade through predation, competition, and other mechanisms. in the context of ecological impacts, research on biological invasions seldom considers phenotypic or microevolutionary changes that occur following introduction. here, we show how plasticity in key life history traits (colony size and longevity), together with omnivory, magnifies the predatory impacts of an invasive social wasp (\n vespula pensylvanica \n ) on a largely endemic arthropod fauna in hawaii. using a combination of molecular, experimental, and behavioral approaches, we demonstrate (\n i \n ) that yellowjackets consume an astonishing diversity of arthropod resources and depress prey populations in invaded hawaiian ecosystems and (\n ii \n ) that their impact as predators in this region increases when they shift from small annual colonies to large perennial colonies. such trait plasticity may influence invasion success and the degree of disruption that invaded ecosystems experience. moreover, postintroduction phenotypic changes may help invaders to compensate for reductions in adaptive potential resulting from founder events and small population sizes. the dynamic nature of biological invasions necessitates a more quantitative understanding of how postintroduction changes in invader traits affect invasion processes.\n"
http://orkg.org/orkg/resource/R54176,Inducible defences as key adaptations for the successful invasion of Daphnia lumholtzi in North America?,10.1098/rspb.2008.1861,crossref,"<jats:p>The mechanisms underlying successful biological invasions often remain unclear. In the case of the tropical water flea<jats:italic>Daphnia lumholtzi</jats:italic>, which invaded North America, it has been suggested that this species possesses a high thermal tolerance, which in the course of global climate change promotes its establishment and rapid spread. However,<jats:italic>D. lumholtzi</jats:italic>has an additional remarkable feature: it is the only water flea that forms rigid head spines in response to chemicals released in the presence of fishes. These morphologically (phenotypically) plastic traits serve as an inducible defence against these predators. Here, we show in controlled mesocosm experiments that the native North American species<jats:italic>Daphnia pulicaria</jats:italic>is competitively superior to<jats:italic>D. lumholtzi</jats:italic>in the absence of predators. However, in the presence of fish predation the invasive species formed its defences and became dominant. This observation of a predator-mediated switch in dominance suggests that the inducible defence against fish predation may represent a key adaptation for the invasion success of<jats:italic>D. lumholtzi</jats:italic>.</jats:p>","the mechanisms underlying successful biological invasions often remain unclear. in the case of the tropical water flea daphnia lumholtzi , which invaded north america, it has been suggested that this species possesses a high thermal tolerance, which in the course of global climate change promotes its establishment and rapid spread. however, d. lumholtzi has an additional remarkable feature: it is the only water flea that forms rigid head spines in response to chemicals released in the presence of fishes. these morphologically (phenotypically) plastic traits serve as an inducible defence against these predators. here, we show in controlled mesocosm experiments that the native north american species daphnia pulicaria is competitively superior to d. lumholtzi in the absence of predators. however, in the presence of fish predation the invasive species formed its defences and became dominant. this observation of a predator-mediated switch in dominance suggests that the inducible defence against fish predation may represent a key adaptation for the invasion success of d. lumholtzi ."
http://orkg.org/orkg/resource/R54190,Phenotypic variability in Holcus lanatus L. in southern Chile: a strategy that enhances plant survival and pasture stability,10.1071/cp09001,crossref,"<jats:p>\n\nHolcus lanatus L. can colonise a wide range of sites within the naturalised grassland of the Humid Dominion of Chile. The objectives were to determine plant growth mechanisms and strategies that have allowed H. lanatus to colonise contrasting pastures and to determine the existence of ecotypes of H. lanatus in southern Chile. Plants of H. lanatus were collected from four geographic zones of southern Chile and established in a randomised complete block design with four replicates. Five newly emerging tillers were marked per plant and evaluated at the vegetative, pre-ear emergence, complete emerged inflorescence, end of flowering period, and mature seed stages. At each evaluation, one marked tiller was harvested per plant. The variables measured included lamina length and width, tiller height, length of the inflorescence, total number of leaves, and leaf, stem, and inflorescence mass. At each phenological stage, groups of accessions were statistically formed using cluster analysis. The grouping of accessions (cluster analysis) into statistically different groups (ANOVA and canonical variate analysis) indicated the existence of different ecotypes. The phenotypic variation within each group of the accessions suggested that each group has its own phenotypic plasticity. It is concluded that the successful colonisation by H. lanatus has resulted from diversity within the species.\n</jats:p>","\n\nholcus lanatus l. can colonise a wide range of sites within the naturalised grassland of the humid dominion of chile. the objectives were to determine plant growth mechanisms and strategies that have allowed h. lanatus to colonise contrasting pastures and to determine the existence of ecotypes of h. lanatus in southern chile. plants of h. lanatus were collected from four geographic zones of southern chile and established in a randomised complete block design with four replicates. five newly emerging tillers were marked per plant and evaluated at the vegetative, pre-ear emergence, complete emerged inflorescence, end of flowering period, and mature seed stages. at each evaluation, one marked tiller was harvested per plant. the variables measured included lamina length and width, tiller height, length of the inflorescence, total number of leaves, and leaf, stem, and inflorescence mass. at each phenological stage, groups of accessions were statistically formed using cluster analysis. the grouping of accessions (cluster analysis) into statistically different groups (anova and canonical variate analysis) indicated the existence of different ecotypes. the phenotypic variation within each group of the accessions suggested that each group has its own phenotypic plasticity. it is concluded that the successful colonisation by h. lanatus has resulted from diversity within the species.\n"
http://orkg.org/orkg/resource/R54218,Rapid evolution in response to introduced predators I: rates and patterns of morphological and life-history trait divergence,10.1186/1471-2148-7-22,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Introduced species can have profound effects on native species, communities, and ecosystems, and have caused extinctions or declines in native species globally. We examined the evolutionary response of native zooplankton populations to the introduction of non-native salmonids in alpine lakes in the Sierra Nevada of California, USA. We compared morphological and life-history traits in populations of <jats:italic>Daphnia</jats:italic> with a known history of introduced salmonids and populations that have no history of salmonid introductions.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>Our results show that <jats:italic>Daphnia</jats:italic> populations co-existing with fish have undergone rapid adaptive reductions in body size and in the timing of reproduction. Size-related traits decreased by up to 13 percent in response to introduced fish. Rates of evolutionary change are as high as 4,238 darwins (0.036 haldanes).</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusion</jats:title>\n            <jats:p>Species introductions into aquatic habitats can dramatically alter the selective environment of native species leading to a rapid evolutionary response. Knowledge of the rates and limits of adaptation is an important component of understanding the long-term effects of alterations in the species composition of communities. We discuss the evolutionary consequences of species introductions and compare the rate of evolution observed in the Sierra Nevada <jats:italic>Daphnia</jats:italic> to published estimates of evolutionary change in ecological timescales.</jats:p>\n          </jats:sec>","abstract \n \n background \n introduced species can have profound effects on native species, communities, and ecosystems, and have caused extinctions or declines in native species globally. we examined the evolutionary response of native zooplankton populations to the introduction of non-native salmonids in alpine lakes in the sierra nevada of california, usa. we compared morphological and life-history traits in populations of daphnia with a known history of introduced salmonids and populations that have no history of salmonid introductions. \n \n \n results \n our results show that daphnia populations co-existing with fish have undergone rapid adaptive reductions in body size and in the timing of reproduction. size-related traits decreased by up to 13 percent in response to introduced fish. rates of evolutionary change are as high as 4,238 darwins (0.036 haldanes). \n \n \n conclusion \n species introductions into aquatic habitats can dramatically alter the selective environment of native species leading to a rapid evolutionary response. knowledge of the rates and limits of adaptation is an important component of understanding the long-term effects of alterations in the species composition of communities. we discuss the evolutionary consequences of species introductions and compare the rate of evolution observed in the sierra nevada daphnia to published estimates of evolutionary change in ecological timescales. \n"
http://orkg.org/orkg/resource/R54224,Phenotypic plasticity of an invasive acacia versus two native Mediterranean species,10.1071/fp04197,crossref,"<jats:p>The phenotypic plasticity and the competitive ability of the invasive Acacia longifolia v. the indigenous Mediterranean dune species Halimium halimifolium and Pinus pinea were evaluated. In particular, we explored the hypothesis that phenotypic plasticity in response to biotic and abiotic factors explains the observed differences in competitiveness between invasive and native species. The seedlings’ ability to exploit different resource availabilities was examined in a two factorial experimental design of light and nutrient treatments by analysing 20 physiological and morphological traits. Competitiveness was tested using an additive experimental design in combination with 15N-labelling experiments. Light and nutrient availability had only minor effects on most physiological traits and differences between species were not significant. Plasticity in response to changes in resource availability occurred in morphological and allocation traits, revealing A. longifolia to be a species of intermediate responsiveness. The major competitive advantage of A. longifolia was its constitutively high shoot elongation rate at most resource treatments and its effective nutrient acquisition. Further, A. longifolia was found to be highly tolerant against competition from native species. In contrast to common expectations, the competition experiment indicated that A. longifolia expressed a constant allocation pattern and a phenotypic plasticity similar to that of the native species.</jats:p>","the phenotypic plasticity and the competitive ability of the invasive acacia longifolia v. the indigenous mediterranean dune species halimium halimifolium and pinus pinea were evaluated. in particular, we explored the hypothesis that phenotypic plasticity in response to biotic and abiotic factors explains the observed differences in competitiveness between invasive and native species. the seedlings’ ability to exploit different resource availabilities was examined in a two factorial experimental design of light and nutrient treatments by analysing 20 physiological and morphological traits. competitiveness was tested using an additive experimental design in combination with 15n-labelling experiments. light and nutrient availability had only minor effects on most physiological traits and differences between species were not significant. plasticity in response to changes in resource availability occurred in morphological and allocation traits, revealing a. longifolia to be a species of intermediate responsiveness. the major competitive advantage of a. longifolia was its constitutively high shoot elongation rate at most resource treatments and its effective nutrient acquisition. further, a. longifolia was found to be highly tolerant against competition from native species. in contrast to common expectations, the competition experiment indicated that a. longifolia expressed a constant allocation pattern and a phenotypic plasticity similar to that of the native species."
http://orkg.org/orkg/resource/R54054,Phenotypic Plasticity in the Invasion of Crofton Weed (Eupatorium adenophorum) in China,10.1614/ws-d-11-00198.1,crossref,"<jats:p>Phenotypic plasticity and rapid evolution are two important strategies by which invasive species adapt to a wide range of environments and consequently are closely associated with plant invasion. To test their importance in invasion success of Crofton weed, we examined the phenotypic response and genetic variation of the weed by conducting a field investigation, common garden experiments, and intersimple sequence repeat (ISSR) marker analysis on 16 populations in China. Molecular markers revealed low genetic variation among and within the sampled populations. There were significant differences in leaf area (LA), specific leaf area (SLA), and seed number (SN) among field populations, and plasticity index (PI<jats:sub>v</jats:sub>) for LA, SLA, and SN were 0.62, 0.46 and 0.85, respectively. Regression analyses revealed a significant quadratic effect of latitude of population origin on LA, SLA, and SN based on field data but not on traits in the common garden experiments (greenhouse and open air). Plants from different populations showed similar reaction norms across the two common gardens for functional traits. LA, SLA, aboveground biomass, plant height at harvest, first flowering day, and life span were higher in the greenhouse than in the open-air garden, whereas SN was lower. Growth conditions (greenhouse vs. open air) and the interactions between growth condition and population origin significantly affect plant traits. The combined evidence suggests high phenotypic plasticity but low genetically based variation for functional traits of Crofton weed in the invaded range. Therefore, we suggest that phenotypic plasticity is the primary strategy for Crofton weed as an aggressive invader that can adapt to diverse environments in China.</jats:p>","phenotypic plasticity and rapid evolution are two important strategies by which invasive species adapt to a wide range of environments and consequently are closely associated with plant invasion. to test their importance in invasion success of crofton weed, we examined the phenotypic response and genetic variation of the weed by conducting a field investigation, common garden experiments, and intersimple sequence repeat (issr) marker analysis on 16 populations in china. molecular markers revealed low genetic variation among and within the sampled populations. there were significant differences in leaf area (la), specific leaf area (sla), and seed number (sn) among field populations, and plasticity index (pi v ) for la, sla, and sn were 0.62, 0.46 and 0.85, respectively. regression analyses revealed a significant quadratic effect of latitude of population origin on la, sla, and sn based on field data but not on traits in the common garden experiments (greenhouse and open air). plants from different populations showed similar reaction norms across the two common gardens for functional traits. la, sla, aboveground biomass, plant height at harvest, first flowering day, and life span were higher in the greenhouse than in the open-air garden, whereas sn was lower. growth conditions (greenhouse vs. open air) and the interactions between growth condition and population origin significantly affect plant traits. the combined evidence suggests high phenotypic plasticity but low genetically based variation for functional traits of crofton weed in the invaded range. therefore, we suggest that phenotypic plasticity is the primary strategy for crofton weed as an aggressive invader that can adapt to diverse environments in china."
http://orkg.org/orkg/resource/R54062,Shell morphology and relative growth variability of the invasive pearl oyster Pinctada radiata in coastal Tunisia,10.1017/s0025315411001925,crossref,"""<jats:p>The variability of shell morphology and relative growth of the invasive pearl oyster <jats:italic>Pinctada radiata</jats:italic> was studied within and among ten populations from coastal Tunisia using discriminant tests. Therefore, 12 morphological characters were examined and 34 metric and weight ratios were defined. In addition to the classic morphological characters, populations were compared by the thickness of the nacreous layer. Results of Duncan's multiple comparison test showed that the most discriminative ratios were the width of nacreous layer of right valve to the inflation of shell, the hinge line length to the maximum width of shell and the nacre thickness to the maximum width of shell. The analysis of variance revealed an important inter-population morphological variability. Both multidimensional scaling analysis and the squared Mahalanobis distances (<jats:italic>D</jats:italic><jats:sup><jats:italic>2</jats:italic></jats:sup>) of metric ratios divided Tunisian <jats:italic>P. radiata</jats:italic> populations into four biogeographical groupings: the north coast (La Marsa); harbours (Hammamet, Monastir and Zarzis); the Gulf of Gabès (Sfax, Kerkennah Island, Maharès, Skhira and Djerba) and the intertidal area (Ajim). However, the Kerkennah Island population was discriminated by the squared Mahalanobis distances (<jats:italic>D</jats:italic><jats:sup><jats:italic>2</jats:italic></jats:sup>) of weight ratios in an isolated group suggesting particular trophic conditions in this area. The allometric study revealed high linear correlation between shell morphological characters and differences in allometric growth among <jats:italic>P. radiata</jats:italic> populations. Unlike the morphological discrimination, allometric differentiation shows no clear geographical distinction. This study revealed that the pearl oyster <jats:italic>P. radiata</jats:italic> exhibited considerable phenotypic plasticity related to differences of environmental and/or ecological conditions along Tunisian coasts and highlighted the discriminative character of the nacreous layer thickness parameter.</jats:p>""",""" the variability of shell morphology and relative growth of the invasive pearl oyster pinctada radiata was studied within and among ten populations from coastal tunisia using discriminant tests. therefore, 12 morphological characters were examined and 34 metric and weight ratios were defined. in addition to the classic morphological characters, populations were compared by the thickness of the nacreous layer. results of duncan's multiple comparison test showed that the most discriminative ratios were the width of nacreous layer of right valve to the inflation of shell, the hinge line length to the maximum width of shell and the nacre thickness to the maximum width of shell. the analysis of variance revealed an important inter-population morphological variability. both multidimensional scaling analysis and the squared mahalanobis distances ( d 2 ) of metric ratios divided tunisian p. radiata populations into four biogeographical groupings: the north coast (la marsa); harbours (hammamet, monastir and zarzis); the gulf of gabès (sfax, kerkennah island, maharès, skhira and djerba) and the intertidal area (ajim). however, the kerkennah island population was discriminated by the squared mahalanobis distances ( d 2 ) of weight ratios in an isolated group suggesting particular trophic conditions in this area. the allometric study revealed high linear correlation between shell morphological characters and differences in allometric growth among p. radiata populations. unlike the morphological discrimination, allometric differentiation shows no clear geographical distinction. this study revealed that the pearl oyster p. radiata exhibited considerable phenotypic plasticity related to differences of environmental and/or ecological conditions along tunisian coasts and highlighted the discriminative character of the nacreous layer thickness parameter. """
http://orkg.org/orkg/resource/R54082,"Geographically distinct Ceratophyllum demersum populations differ in growth, photosynthetic responses and phenotypic plasticity to nitrogen availability",10.1071/fp12068,crossref,"<jats:p>\n\nTwo geographically distinct populations of the submerged aquatic macrophyte Ceratophyllum demersum L. were compared after acclimation to five different nitrogen concentrations (0.005, 0.02, 0.05, 0.1 and 0.2\u2009mM N) in a common garden setup. The two populations were an apparent invasive population from New Zealand (NZ) and a noninvasive population from Denmark (DK). The populations were compared with a focus on both morphological and physiological traits. The NZ population had higher relative growth rates (RGRs) and photosynthesis rates (Pmax) (range: RGR, 0.06–0.08 per day; Pmax, 200–395\u2009µmol\u2009O2\u2009g–1 dry mass (DM) h–1) compared with the Danish population (range: RGR, 0.02–0.05 per day; Pmax, 88–169\u2009µmol O2 g–1 DM h–1). The larger, faster-growing NZ population also showed higher plasticity than the DK population in response to nitrogen in traits important for growth. Hence, the observed differences in growth behaviour between the two populations are a result of genetic differences and differences in their level of plasticity. Here, we show that two populations of the same species from similar climates but different geographical areas can differ in several ecophysiological traits after growth in a common garden setup.\n</jats:p>","\n\ntwo geographically distinct populations of the submerged aquatic macrophyte ceratophyllum demersum l. were compared after acclimation to five different nitrogen concentrations (0.005, 0.02, 0.05, 0.1 and 0.2\u2009mm n) in a common garden setup. the two populations were an apparent invasive population from new zealand (nz) and a noninvasive population from denmark (dk). the populations were compared with a focus on both morphological and physiological traits. the nz population had higher relative growth rates (rgrs) and photosynthesis rates (pmax) (range: rgr, 0.06–0.08 per day; pmax, 200–395\u2009µmol\u2009o2\u2009g–1 dry mass (dm) h–1) compared with the danish population (range: rgr, 0.02–0.05 per day; pmax, 88–169\u2009µmol o2 g–1 dm h–1). the larger, faster-growing nz population also showed higher plasticity than the dk population in response to nitrogen in traits important for growth. hence, the observed differences in growth behaviour between the two populations are a result of genetic differences and differences in their level of plasticity. here, we show that two populations of the same species from similar climates but different geographical areas can differ in several ecophysiological traits after growth in a common garden setup.\n"
http://orkg.org/orkg/resource/R53382,Exotic taxa less related to native species are more invasive,10.1073/pnas.0508073103,crossref,"<jats:p>Some species introduced into new geographical areas from their native ranges wreak ecological and economic havoc in their new environment. Although many studies have searched for either species or habitat characteristics that predict invasiveness of exotic species, the match between characteristics of the invader and those of members of the existing native community may be essential to understanding invasiveness. Here, we find that one metric, the phylogenetic relatedness of an invader to the native community, provides a predictive tool for invasiveness. Using a phylogenetic supertree of all grass species in California, we show that highly invasive grass species are, on average, significantly less related to native grasses than are introduced but noninvasive grasses. The match between the invader and the existing native community may explain why exotic pest species are not uniformly noxious in all novel habitats. Relatedness of invaders to the native biota may be one useful criterion for prioritizing management efforts of exotic species.</jats:p>","some species introduced into new geographical areas from their native ranges wreak ecological and economic havoc in their new environment. although many studies have searched for either species or habitat characteristics that predict invasiveness of exotic species, the match between characteristics of the invader and those of members of the existing native community may be essential to understanding invasiveness. here, we find that one metric, the phylogenetic relatedness of an invader to the native community, provides a predictive tool for invasiveness. using a phylogenetic supertree of all grass species in california, we show that highly invasive grass species are, on average, significantly less related to native grasses than are introduced but noninvasive grasses. the match between the invader and the existing native community may explain why exotic pest species are not uniformly noxious in all novel habitats. relatedness of invaders to the native biota may be one useful criterion for prioritizing management efforts of exotic species."
http://orkg.org/orkg/resource/R53387,Fish species introductions provide novel insights into the patterns and drivers of phylogenetic structure in freshwaters,10.1098/rspb.2013.3003,crossref,"""<jats:p>Despite long-standing interest of terrestrial ecologists, freshwater ecosystems are a fertile, yet unappreciated, testing ground for applying community phylogenetics to uncover mechanisms of species assembly. We quantify phylogenetic clustering and overdispersion of native and non-native fishes of a large river basin in the American Southwest to test for the mechanisms (environmental filtering versus competitive exclusion) and spatial scales influencing community structure. Contrary to expectations, non-native species were phylogenetically clustered and related to natural environmental conditions, whereas native species were not phylogenetically structured, likely reflecting human-related changes to the basin. The species that are most invasive (in terms of ecological impacts) tended to be the most phylogenetically divergent from natives across watersheds, but not within watersheds, supporting the hypothesis that Darwin's naturalization conundrum is driven by the spatial scale. Phylogenetic distinctiveness may facilitate non-native establishment at regional scales, but environmental filtering restricts local membership to closely related species with physiological tolerances for current environments. By contrast, native species may have been phylogenetically clustered in historical times, but species loss from contemporary populations by anthropogenic activities has likely shaped the phylogenetic signal. Our study implies that fundamental mechanisms of community assembly have changed, with fundamental consequences for the biogeography of both native and non-native species.</jats:p>""",""" despite long-standing interest of terrestrial ecologists, freshwater ecosystems are a fertile, yet unappreciated, testing ground for applying community phylogenetics to uncover mechanisms of species assembly. we quantify phylogenetic clustering and overdispersion of native and non-native fishes of a large river basin in the american southwest to test for the mechanisms (environmental filtering versus competitive exclusion) and spatial scales influencing community structure. contrary to expectations, non-native species were phylogenetically clustered and related to natural environmental conditions, whereas native species were not phylogenetically structured, likely reflecting human-related changes to the basin. the species that are most invasive (in terms of ecological impacts) tended to be the most phylogenetically divergent from natives across watersheds, but not within watersheds, supporting the hypothesis that darwin's naturalization conundrum is driven by the spatial scale. phylogenetic distinctiveness may facilitate non-native establishment at regional scales, but environmental filtering restricts local membership to closely related species with physiological tolerances for current environments. by contrast, native species may have been phylogenetically clustered in historical times, but species loss from contemporary populations by anthropogenic activities has likely shaped the phylogenetic signal. our study implies that fundamental mechanisms of community assembly have changed, with fundamental consequences for the biogeography of both native and non-native species. """
http://orkg.org/orkg/resource/R70733,Everything in moderation: ICT and reading performance of Dutch 15-year-olds,10.1186/s40536-020-0079-0,crossref,"<jats:title>Abstract</jats:title><jats:p>Previous research on the relationship between students’ home and school Information and Communication Technology (ICT) resources and academic performance has shown ambiguous results. The availability of ICT resources at school has been found to be unrelated or negatively related to academic performance, whereas the availability of ICT resources at home has been found to be both positively and negatively related to academic performance. In addition, the frequency of use of ICT is related to students’ academic achievement. This relationship has been found to be negative for ICT use at school, however, for ICT use at home the literature on the relationship with academic performance is again ambiguous. In addition to ICT availability and ICT use, students’ attitudes towards ICT have also been found to play a role in student performance. In the present study, we examine how availability of ICT resources, students’ use of those resources (at school, outside school for schoolwork, outside school for leisure), and students’ attitudes toward ICT (interest in ICT, perceived ICT competence, perceived ICT autonomy) relate to individual differences in performance on a digital assessment of reading in one comprehensive model using the Dutch PISA 2015 sample of 5183 15-year-olds (49.2% male). Student gender and students’ economic, social, and cultural status accounted for a substantial part of the variation in digitally assessed reading performance. Controlling for these relationships, results indicated that students with moderate access to ICT resources, moderate use of ICT at school or outside school for schoolwork, and moderate interest in ICT had the highest digitally assessed reading performance. In contrast, students who reported moderate competence in ICT had the lowest digitally assessed reading performance. In addition, frequent use of ICT outside school for leisure was negatively related to digitally assessed reading performance, whereas perceived autonomy was positively related. Taken together, the findings suggest that excessive access to ICT resources, excessive use of ICT, and excessive interest in ICT is associated with lower digitally assessed reading performance.</jats:p>","abstract previous research on the relationship between students’ home and school information and communication technology (ict) resources and academic performance has shown ambiguous results. the availability of ict resources at school has been found to be unrelated or negatively related to academic performance, whereas the availability of ict resources at home has been found to be both positively and negatively related to academic performance. in addition, the frequency of use of ict is related to students’ academic achievement. this relationship has been found to be negative for ict use at school, however, for ict use at home the literature on the relationship with academic performance is again ambiguous. in addition to ict availability and ict use, students’ attitudes towards ict have also been found to play a role in student performance. in the present study, we examine how availability of ict resources, students’ use of those resources (at school, outside school for schoolwork, outside school for leisure), and students’ attitudes toward ict (interest in ict, perceived ict competence, perceived ict autonomy) relate to individual differences in performance on a digital assessment of reading in one comprehensive model using the dutch pisa 2015 sample of 5183 15-year-olds (49.2% male). student gender and students’ economic, social, and cultural status accounted for a substantial part of the variation in digitally assessed reading performance. controlling for these relationships, results indicated that students with moderate access to ict resources, moderate use of ict at school or outside school for schoolwork, and moderate interest in ict had the highest digitally assessed reading performance. in contrast, students who reported moderate competence in ict had the lowest digitally assessed reading performance. in addition, frequent use of ict outside school for leisure was negatively related to digitally assessed reading performance, whereas perceived autonomy was positively related. taken together, the findings suggest that excessive access to ict resources, excessive use of ict, and excessive interest in ict is associated with lower digitally assessed reading performance."
http://orkg.org/orkg/resource/R70548,Machine-Learning-Based Laboratory Developed Test for the Diagnosis of Sepsis in High-Risk Patients,,crossref,"<jats:p>Sepsis, a dysregulated host response to infection, is a major health burden in terms of both mortality and cost. The difficulties clinicians face in diagnosing sepsis, alongside the insufficiencies of diagnostic biomarkers, motivate the present study. This work develops a machine-learning-based sepsis diagnostic for a high-risk patient group, using a geographically and institutionally diverse collection of nearly 500,000 patient health records. Using only a minimal set of clinical variables, our diagnostics outperform common severity scoring systems and sepsis biomarkers and benefit from being available immediately upon ordering.</jats:p>","sepsis, a dysregulated host response to infection, is a major health burden in terms of both mortality and cost. the difficulties clinicians face in diagnosing sepsis, alongside the insufficiencies of diagnostic biomarkers, motivate the present study. this work develops a machine-learning-based sepsis diagnostic for a high-risk patient group, using a geographically and institutionally diverse collection of nearly 500,000 patient health records. using only a minimal set of clinical variables, our diagnostics outperform common severity scoring systems and sepsis biomarkers and benefit from being available immediately upon ordering."
http://orkg.org/orkg/resource/R70560,Physiological monitoring for critically ill patients: testing a predictive model for the early detection of sepsis,,crossref,"<jats:p>• Objective To assess the predictive value for the early detection of sepsis of the physiological monitoring parameters currently recommended by the Surviving Sepsis Campaign.</jats:p><jats:p>• Methods The Project IMPACT data set was used to assess whether the physiological parameters of heart rate, mean arterial pressure, body temperature, and respiratory rate can be used to distinguish between critically ill adult patients with and without sepsis in the first 24 hours of admission to an intensive care unit.</jats:p><jats:p>• Results All predictor variables used in the analyses differed significantly between patients with sepsis and patients without sepsis. However, only 2 of the predictor variables, mean arterial pressure and high temperature, were independently associated with sepsis. In addition, the temperature mean for hypothermia was significantly lower in patients without sepsis. The odds ratio for having sepsis was 2.126 for patients with a temperature of 38°C or higher, 3.874 for patients with a mean arterial blood pressure of less than 70 mm Hg, and 4.63 times greater for patients who had both of these conditions.</jats:p><jats:p>• Conclusions The results support the use of some of the guidelines of the Surviving Sepsis Campaign. However, the lowest mean temperature was significantly less for patients without sepsis than for patients with sepsis, a finding that calls into question the clinical usefulness of using hypothermia as an early predictor of sepsis. Alone the group of variables used is not sufficient for discriminating between critically ill patients with and without sepsis.</jats:p>","• objective to assess the predictive value for the early detection of sepsis of the physiological monitoring parameters currently recommended by the surviving sepsis campaign. • methods the project impact data set was used to assess whether the physiological parameters of heart rate, mean arterial pressure, body temperature, and respiratory rate can be used to distinguish between critically ill adult patients with and without sepsis in the first 24 hours of admission to an intensive care unit. • results all predictor variables used in the analyses differed significantly between patients with sepsis and patients without sepsis. however, only 2 of the predictor variables, mean arterial pressure and high temperature, were independently associated with sepsis. in addition, the temperature mean for hypothermia was significantly lower in patients without sepsis. the odds ratio for having sepsis was 2.126 for patients with a temperature of 38°c or higher, 3.874 for patients with a mean arterial blood pressure of less than 70 mm hg, and 4.63 times greater for patients who had both of these conditions. • conclusions the results support the use of some of the guidelines of the surviving sepsis campaign. however, the lowest mean temperature was significantly less for patients without sepsis than for patients with sepsis, a finding that calls into question the clinical usefulness of using hypothermia as an early predictor of sepsis. alone the group of variables used is not sufficient for discriminating between critically ill patients with and without sepsis."
http://orkg.org/orkg/resource/R70614,Maximizing Interpretability and Cost-Effectiveness of Surgical Site Infection (SSI) Predictive Models Using Feature-Specific Regularized Logistic Regression on Preoperative Temporal Data,,crossref,"<jats:p>This study describes a novel approach to solve the surgical site infection (SSI) classification problem. Feature engineering has traditionally been one of the most important steps in solving complex classification problems, especially in cases with temporal data. The described novel approach is based on abstraction of temporal data recorded in three temporal windows. Maximum likelihood L1-norm (lasso) regularization was used in penalized logistic regression to predict the onset of surgical site infection occurrence based on available patient blood testing results up to the day of surgery. Prior knowledge of predictors (blood tests) was integrated in the modelling by introduction of penalty factors depending on blood test prices and an early stopping parameter limiting the maximum number of selected features used in predictive modelling. Finally, solutions resulting in higher interpretability and cost-effectiveness were demonstrated. Using repeated holdout cross-validation, the baseline C-reactive protein (CRP) classifier achieved a mean AUC of 0.801, whereas our best full lasso model achieved a mean AUC of 0.956. Best model testing results were achieved for full lasso model with maximum number of features limited at 20 features with an AUC of 0.967. Presented models showed the potential to not only support domain experts in their decision making but could also prove invaluable for improvement in prediction of SSI occurrence, which may even help setting new guidelines in the field of preoperative SSI prevention and surveillance.</jats:p>","this study describes a novel approach to solve the surgical site infection (ssi) classification problem. feature engineering has traditionally been one of the most important steps in solving complex classification problems, especially in cases with temporal data. the described novel approach is based on abstraction of temporal data recorded in three temporal windows. maximum likelihood l1-norm (lasso) regularization was used in penalized logistic regression to predict the onset of surgical site infection occurrence based on available patient blood testing results up to the day of surgery. prior knowledge of predictors (blood tests) was integrated in the modelling by introduction of penalty factors depending on blood test prices and an early stopping parameter limiting the maximum number of selected features used in predictive modelling. finally, solutions resulting in higher interpretability and cost-effectiveness were demonstrated. using repeated holdout cross-validation, the baseline c-reactive protein (crp) classifier achieved a mean auc of 0.801, whereas our best full lasso model achieved a mean auc of 0.956. best model testing results were achieved for full lasso model with maximum number of features limited at 20 features with an auc of 0.967. presented models showed the potential to not only support domain experts in their decision making but could also prove invaluable for improvement in prediction of ssi occurrence, which may even help setting new guidelines in the field of preoperative ssi prevention and surveillance."
http://orkg.org/orkg/resource/R54028,Jack-of-all-trades: phenotypic plasticity facilitates the invasion of an alien slug species,10.1098/rspb.2012.1564,crossref,"<jats:p>\n            Invasive alien species might benefit from phenotypic plasticity by being able to (i) maintain fitness in stressful environments (‘robust’), (ii) increase fitness in favourable environments (‘opportunistic’), or (iii) combine both abilities (‘robust and opportunistic’). Here, we applied this framework, for the first time, to an animal, the invasive slug,\n            <jats:italic>Arion lusitanicus</jats:italic>\n            , and tested (i) whether it has a more adaptive phenotypic plasticity compared with a congeneric native slug,\n            <jats:italic>Arion fuscus</jats:italic>\n            , and (ii) whether it is robust, opportunistic or both. During one year, we exposed specimens of both species to a range of temperatures along an altitudinal gradient (700–2400 m a.s.l.) and to high and low food levels, and we compared the responsiveness of two fitness traits: survival and egg production\n            <jats:italic>.</jats:italic>\n            During summer, the invasive species had a more adaptive phenotypic plasticity, and at high temperatures and low food levels, it survived better and produced more eggs than\n            <jats:italic>A. fuscus</jats:italic>\n            , representing the robust phenotype. During winter,\n            <jats:italic>A. lusitanicus</jats:italic>\n            displayed a less adaptive phenotype than\n            <jats:italic>A. fuscus</jats:italic>\n            . We show that the framework developed for plants is also very useful for a better mechanistic understanding of animal invasions. Warmer summers and milder winters might lead to an expansion of this invasive species to higher altitudes and enhance its spread in the lowlands, supporting the concern that global climate change will increase biological invasions.\n          </jats:p>","\n invasive alien species might benefit from phenotypic plasticity by being able to (i) maintain fitness in stressful environments (‘robust’), (ii) increase fitness in favourable environments (‘opportunistic’), or (iii) combine both abilities (‘robust and opportunistic’). here, we applied this framework, for the first time, to an animal, the invasive slug,\n arion lusitanicus \n , and tested (i) whether it has a more adaptive phenotypic plasticity compared with a congeneric native slug,\n arion fuscus \n , and (ii) whether it is robust, opportunistic or both. during one year, we exposed specimens of both species to a range of temperatures along an altitudinal gradient (700–2400 m a.s.l.) and to high and low food levels, and we compared the responsiveness of two fitness traits: survival and egg production\n . \n during summer, the invasive species had a more adaptive phenotypic plasticity, and at high temperatures and low food levels, it survived better and produced more eggs than\n a. fuscus \n , representing the robust phenotype. during winter,\n a. lusitanicus \n displayed a less adaptive phenotype than\n a. fuscus \n . we show that the framework developed for plants is also very useful for a better mechanistic understanding of animal invasions. warmer summers and milder winters might lead to an expansion of this invasive species to higher altitudes and enhance its spread in the lowlands, supporting the concern that global climate change will increase biological invasions.\n"
http://orkg.org/orkg/resource/R54040,Architectural strategies of Rhamnus cathartica (Rhamnaceae) in relation to canopy openness,10.1139/b2012-069,crossref,"<jats:p> While phenotypic plasticity is considered the major means that allows plant to cope with environmental heterogeneity, scant information is available on phenotypic plasticity of the whole-plant architecture in relation to ontogenic processes. We performed an architectural analysis to gain an understanding of the structural and ontogenic properties of common buckthorn ( Rhamnus cathartica L., Rhamnaceae) growing in the understory and under an open canopy. We found that ontogenic effects on growth need to be calibrated if a full description of phenotypic plasticity is to be obtained. Our analysis pointed to three levels of organization (or nested structural units) in R.\xa0cathartica. Their modulation in relation to light conditions leads to the expression of two architectural strategies that involve sets of traits known to confer competitive advantage in their respective environments. In the understory, the plant develops a tree-like form. Its strategy here is based on restricting investment in exploitation structures while promoting major vertical exploration and is probably key to species survival in the understory. Under an open canopy, the second strategy leads the plant to adopt a shrub-like shape. It develops densely branched exploitation structures and flowers abundantly and rapidly. This strategy perfectly matches its aggressive behaviour observed in full sunlight. We propose, as hypotheses, that these two light-related strategies are implicated in the ability of R.\xa0cathartica to outcompete the surrounding vegetation in a range of environmental conditions. </jats:p>","while phenotypic plasticity is considered the major means that allows plant to cope with environmental heterogeneity, scant information is available on phenotypic plasticity of the whole-plant architecture in relation to ontogenic processes. we performed an architectural analysis to gain an understanding of the structural and ontogenic properties of common buckthorn ( rhamnus cathartica l., rhamnaceae) growing in the understory and under an open canopy. we found that ontogenic effects on growth need to be calibrated if a full description of phenotypic plasticity is to be obtained. our analysis pointed to three levels of organization (or nested structural units) in r.\xa0cathartica. their modulation in relation to light conditions leads to the expression of two architectural strategies that involve sets of traits known to confer competitive advantage in their respective environments. in the understory, the plant develops a tree-like form. its strategy here is based on restricting investment in exploitation structures while promoting major vertical exploration and is probably key to species survival in the understory. under an open canopy, the second strategy leads the plant to adopt a shrub-like shape. it develops densely branched exploitation structures and flowers abundantly and rapidly. this strategy perfectly matches its aggressive behaviour observed in full sunlight. we propose, as hypotheses, that these two light-related strategies are implicated in the ability of r.\xa0cathartica to outcompete the surrounding vegetation in a range of environmental conditions."
http://orkg.org/orkg/resource/R53295,Establishment of introduced reptiles increases with the presence and richness of native congeners,10.1163/15685381-00002841,crossref,"<jats:p>Darwin proposed two contradictory hypotheses to explain the influence of congeners on the outcomes of invasion: the naturalization hypothesis, which predicts a negative relationship between the presence of congeners and invasion success, and the pre-adaptation hypothesis, which predicts a positive relationship between the presence of congeners and invasion success. Studies testing these hypotheses have shown mixed support. We tested these hypotheses using the establishment success of non-native reptiles and congener presence/absence and richness across the globe. Our results demonstrated support for the pre-adaptation hypothesis. We found that globally, both on islands and continents, establishment success was higher in the presence than in the absence of congeners and that establishment success increased with increasing congener richness. At the life form level, establishment success was higher for lizards, marginally higher for snakes, and not different for turtles in the presence of congeners; data were insufficient to test the hypotheses for crocodiles. There was no relationship between establishment success and congener richness for any life form. We suggest that we found support for the pre-adaptation hypothesis because, at the scale of our analysis, native congeners represent environmental conditions appropriate for the species rather than competition for niche space. Our results imply that areas to target for early detection of non-native reptiles are those that host closely related species.</jats:p>","darwin proposed two contradictory hypotheses to explain the influence of congeners on the outcomes of invasion: the naturalization hypothesis, which predicts a negative relationship between the presence of congeners and invasion success, and the pre-adaptation hypothesis, which predicts a positive relationship between the presence of congeners and invasion success. studies testing these hypotheses have shown mixed support. we tested these hypotheses using the establishment success of non-native reptiles and congener presence/absence and richness across the globe. our results demonstrated support for the pre-adaptation hypothesis. we found that globally, both on islands and continents, establishment success was higher in the presence than in the absence of congeners and that establishment success increased with increasing congener richness. at the life form level, establishment success was higher for lizards, marginally higher for snakes, and not different for turtles in the presence of congeners; data were insufficient to test the hypotheses for crocodiles. there was no relationship between establishment success and congener richness for any life form. we suggest that we found support for the pre-adaptation hypothesis because, at the scale of our analysis, native congeners represent environmental conditions appropriate for the species rather than competition for niche space. our results imply that areas to target for early detection of non-native reptiles are those that host closely related species."
http://orkg.org/orkg/resource/R53311,Biotic interactions experienced by a new invader: effects of its close relatives at the community scale,10.1139/b11-084,crossref,"<jats:p> The success of nonindigenous species may be influenced by biotic interactions during the initial stages of invasion. Here, we investigated whether a potential invader, Solidago virgaurea L., would experience more damage by natural enemies in communities dominated by close relatives than those without them; interactions with mutualistic mycorrhizae might partially counteract these effects. We monitored damage experienced by S.\xa0virgaurea planted into communities with native congeners and without close relatives. Community type was crossed with a vegetation removal treatment to assess the combined effects of herbivory and competition on survival. We also evaluated growth of S.\xa0virgaurea in a greenhouse experiment where seedlings were exposed to soil biota sampled from these communities and compared with sterile controls. Overall, community type did not affect levels of herbivory or plant survival. Removal of surrounding vegetation resulted in reduced damage and increased survival; these effects were largest in grass-dominated communities. Soil sterilization reduced root growth and tended to reduce shoot growth, especially when compared with plants inoculated with biota collected near congeners. Overall, our results suggest that the presence of close relatives is unlikely to make old-field communities more resistant to invasion by S.\xa0virgaurea; instead, soil biota might facilitate growth in communities dominated by close relatives. </jats:p>","the success of nonindigenous species may be influenced by biotic interactions during the initial stages of invasion. here, we investigated whether a potential invader, solidago virgaurea l., would experience more damage by natural enemies in communities dominated by close relatives than those without them; interactions with mutualistic mycorrhizae might partially counteract these effects. we monitored damage experienced by s.\xa0virgaurea planted into communities with native congeners and without close relatives. community type was crossed with a vegetation removal treatment to assess the combined effects of herbivory and competition on survival. we also evaluated growth of s.\xa0virgaurea in a greenhouse experiment where seedlings were exposed to soil biota sampled from these communities and compared with sterile controls. overall, community type did not affect levels of herbivory or plant survival. removal of surrounding vegetation resulted in reduced damage and increased survival; these effects were largest in grass-dominated communities. soil sterilization reduced root growth and tended to reduce shoot growth, especially when compared with plants inoculated with biota collected near congeners. overall, our results suggest that the presence of close relatives is unlikely to make old-field communities more resistant to invasion by s.\xa0virgaurea; instead, soil biota might facilitate growth in communities dominated by close relatives."
http://orkg.org/orkg/resource/R53345,A test of Darwin's naturalization hypothesis in the thistle tribe shows that close relatives make bad neighbors,10.1073/pnas.1309948110,crossref,"<jats:title>Significance</jats:title>\n          <jats:p>Invasive species negatively impact both natural ecosystems and human society and are notoriously difficult to control once established. Thus, identifying potentially invasive taxa and preventing their dislocation is the most efficient management method. Darwin’s naturalization hypothesis, which predicts that the less closely related to native flora species are, the more likely they are to succeed as invaders, is tested here with an unprecedentedly thorough molecular phylogenetic approach, examining &gt;100,000 phylogenies of the weed-rich thistle tribe Cardueae. Branch lengths between taxa were used as measures of evolutionary relatedness. Results show that invasive thistles are more closely related to natives than noninvasive introduced thistles, suggesting they share preadaptive traits with the natives that make them more likely to succeed as invaders.</jats:p>","significance \n invasive species negatively impact both natural ecosystems and human society and are notoriously difficult to control once established. thus, identifying potentially invasive taxa and preventing their dislocation is the most efficient management method. darwin’s naturalization hypothesis, which predicts that the less closely related to native flora species are, the more likely they are to succeed as invaders, is tested here with an unprecedentedly thorough molecular phylogenetic approach, examining &gt;100,000 phylogenies of the weed-rich thistle tribe cardueae. branch lengths between taxa were used as measures of evolutionary relatedness. results show that invasive thistles are more closely related to natives than noninvasive introduced thistles, suggesting they share preadaptive traits with the natives that make them more likely to succeed as invaders."
http://orkg.org/orkg/resource/R74330,Impact of SARS-CoV-2 on the mobility behaviour in Germany,10.1186/s12544-021-00469-3,crossref,"<jats:title>Abstract</jats:title><jats:sec>\n                <jats:title>Background</jats:title>\n                <jats:p>The COVID-19 pandemic and the measures taken to combat it led to severe constraints for various areas of life, including mobility. To study the effects of\xa0this disruptive situation on the mobility behaviour of entire subgroups, and how they shape their mobility in reaction to the special circumstances, can help\xa0to better understand, how people react to external changes.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Methodology</jats:title>\n                <jats:p>Aim of the study presented in this article was to investigate to what extent, how and in what areas mobility behaviour has changed during the outbreak of\xa0SARS-CoV-2 in Germany. In addition, a focus was put on the comparison of federal states with and without lockdown in order to investigate a possible\xa0contribution of this measure to changes in mobility. We asked respondents via an online survey about their trip purposes and trip frequency, their choice of\xa0transport mode and the reasons for choosing it in the context of the COVID-19 crisis. For the analyses presented in this paper, we used the data of 4157survey participants (2512 without lockdown, 1645 with lockdown).</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Results</jats:title>\n                <jats:p>The data confirmed a profound impact on the mobility behaviour with a shift away from public transport and increases in car usage, walking and cycling.\xa0Comparisons of federal states with and without lockdown revealed only isolated differences. It seems that, even if the lockdown had some minor effects, its\xa0role in the observed behavioural changes was minimal.</jats:p>\n              </jats:sec>","abstract \n background \n the covid-19 pandemic and the measures taken to combat it led to severe constraints for various areas of life, including mobility. to study the effects of\xa0this disruptive situation on the mobility behaviour of entire subgroups, and how they shape their mobility in reaction to the special circumstances, can help\xa0to better understand, how people react to external changes. \n \n methodology \n aim of the study presented in this article was to investigate to what extent, how and in what areas mobility behaviour has changed during the outbreak of\xa0sars-cov-2 in germany. in addition, a focus was put on the comparison of federal states with and without lockdown in order to investigate a possible\xa0contribution of this measure to changes in mobility. we asked respondents via an online survey about their trip purposes and trip frequency, their choice of\xa0transport mode and the reasons for choosing it in the context of the covid-19 crisis. for the analyses presented in this paper, we used the data of 4157survey participants (2512 without lockdown, 1645 with lockdown). \n \n results \n the data confirmed a profound impact on the mobility behaviour with a shift away from public transport and increases in car usage, walking and cycling.\xa0comparisons of federal states with and without lockdown revealed only isolated differences. it seems that, even if the lockdown had some minor effects, its\xa0role in the observed behavioural changes was minimal. \n"
http://orkg.org/orkg/resource/R68931,The New DBpedia Release Cycle: Increasing Agility and Efficiency in Knowledge Extraction Workflows,10.1007/978-3-030-59833-4_1,crossref,"<jats:title>Abstract</jats:title>\n<jats:p>Since its inception in 2007, DBpedia has been constantly releasing open data in RDF, extracted from various Wikimedia projects using a complex software system called the DBpedia Information Extraction Framework (DIEF). For the past 12 years, the software received a plethora of extensions by the community, which positively affected the size and data quality. Due to the increase in size and complexity, the release process was facing huge delays (from 12 to 17 months cycle), thus impacting the agility of the development. In this paper, we describe the new DBpedia release cycle including our innovative release workflow, which allows development teams (in particular those who publish large, open data) to implement agile, cost-efficient processes and scale up productivity. The DBpedia release workflow has been re-engineered, its new primary focus is on <jats:italic>productivity</jats:italic> and <jats:italic>agility</jats:italic>, to address the challenges of size and complexity. At the same time, <jats:italic>quality</jats:italic> is assured by implementing a comprehensive testing methodology. We run an experimental evaluation and argue that the implemented measures increase agility and allow for cost-effective quality-control and debugging and thus achieve a higher level of maintainability. As a result, DBpedia now publishes regular (i.e. monthly) releases with over 21 billion triples with minimal publishing effort\n.</jats:p>","abstract \n since its inception in 2007, dbpedia has been constantly releasing open data in rdf, extracted from various wikimedia projects using a complex software system called the dbpedia information extraction framework (dief). for the past 12 years, the software received a plethora of extensions by the community, which positively affected the size and data quality. due to the increase in size and complexity, the release process was facing huge delays (from 12 to 17 months cycle), thus impacting the agility of the development. in this paper, we describe the new dbpedia release cycle including our innovative release workflow, which allows development teams (in particular those who publish large, open data) to implement agile, cost-efficient processes and scale up productivity. the dbpedia release workflow has been re-engineered, its new primary focus is on productivity and agility , to address the challenges of size and complexity. at the same time, quality is assured by implementing a comprehensive testing methodology. we run an experimental evaluation and argue that the implemented measures increase agility and allow for cost-effective quality-control and debugging and thus achieve a higher level of maintainability. as a result, dbpedia now publishes regular (i.e. monthly) releases with over 21 billion triples with minimal publishing effort\n."
http://orkg.org/orkg/resource/R52098,Community assembly and invasion: An experimental test of neutral versus niche processes,10.1073/pnas.1033107100,crossref,"<jats:p>A species-addition experiment showed that prairie grasslands have a\n structured, nonneutral assembly process in which resident species inhibit, via\n resource consumption, the establishment and growth of species with similar\n resource use patterns and in which the success of invaders decreases as\n diversity increases. In our experiment, species in each of four functional\n guilds were introduced, as seed, into 147 prairie–grassland plots that\n previously had been established and maintained to have different compositions\n and diversities. Established species most strongly inhibited introduced\n species from their own functional guild. Introduced species attained lower\n abundances when functionally similar species were abundant and when\n established species left lower levels of resources unconsumed, which occurred\n at lower species richness. Residents of the C4 grass functional guild, the\n dominant guild in nearby native grasslands, reduced the major limiting\n resource, soil nitrate, to the lowest levels in midsummer and exhibited the\n greatest inhibitory effect on introduced species. This simple mechanism of\n greater competitive inhibition of invaders that are similar to established\n abundant species could, in theory, explain many of the patterns observed in\n plant communities.</jats:p>","a species-addition experiment showed that prairie grasslands have a\n structured, nonneutral assembly process in which resident species inhibit, via\n resource consumption, the establishment and growth of species with similar\n resource use patterns and in which the success of invaders decreases as\n diversity increases. in our experiment, species in each of four functional\n guilds were introduced, as seed, into 147 prairie–grassland plots that\n previously had been established and maintained to have different compositions\n and diversities. established species most strongly inhibited introduced\n species from their own functional guild. introduced species attained lower\n abundances when functionally similar species were abundant and when\n established species left lower levels of resources unconsumed, which occurred\n at lower species richness. residents of the c4 grass functional guild, the\n dominant guild in nearby native grasslands, reduced the major limiting\n resource, soil nitrate, to the lowest levels in midsummer and exhibited the\n greatest inhibitory effect on introduced species. this simple mechanism of\n greater competitive inhibition of invaders that are similar to established\n abundant species could, in theory, explain many of the patterns observed in\n plant communities."
http://orkg.org/orkg/resource/R52124,Resistance of Native Plant Functional Groups to Invasion by Medusahead (Taeniatherum caput-medusae),10.1614/ipsm-d-09-00056.1,crossref,"<jats:title>Abstract</jats:title><jats:p>Understanding the relative importance of various functional groups in minimizing invasion by medusahead is central to increasing the resistance of native plant communities. The objective of this study was to determine the relative importance of key functional groups within an intact Wyoming big sagebrush–bluebunch wheatgrass community type on minimizing medusahead invasion. Treatments consisted of removal of seven functional groups at each of two sites, one with shrubs and one without shrubs. Removal treatments included (1) everything, (2) shrubs, (3) perennial grasses, (4) taprooted forbs, (5) rhizomatous forbs, (6) annual forbs, and (7) mosses. A control where nothing was removed was also established. Plots were arranged in a randomized complete block with 4 replications (blocks) at each site. Functional groups were removed beginning in the spring of 2004 and maintained monthly throughout each growing season through 2009. Medusahead was seeded at a rate of 2,000 seeds m<jats:sup>−2</jats:sup> (186 seeds ft<jats:sup>−2</jats:sup>) in fall 2005. Removing perennial grasses nearly doubled medusahead density and biomass compared with any other removal treatment. The second highest density and biomass of medusahead occurred from removing rhizomatous forbs (phlox). We found perennial grasses played a relatively more significant role than other species in minimizing invasion by medusahead. We suggest that the most effective basis for establishing medusahead-resistant plant communities is to establish 2 or 3 highly productive grasses that are complementary in niche and that overlap that of the invading species.</jats:p>","abstract understanding the relative importance of various functional groups in minimizing invasion by medusahead is central to increasing the resistance of native plant communities. the objective of this study was to determine the relative importance of key functional groups within an intact wyoming big sagebrush–bluebunch wheatgrass community type on minimizing medusahead invasion. treatments consisted of removal of seven functional groups at each of two sites, one with shrubs and one without shrubs. removal treatments included (1) everything, (2) shrubs, (3) perennial grasses, (4) taprooted forbs, (5) rhizomatous forbs, (6) annual forbs, and (7) mosses. a control where nothing was removed was also established. plots were arranged in a randomized complete block with 4 replications (blocks) at each site. functional groups were removed beginning in the spring of 2004 and maintained monthly throughout each growing season through 2009. medusahead was seeded at a rate of 2,000 seeds m −2 (186 seeds ft −2 ) in fall 2005. removing perennial grasses nearly doubled medusahead density and biomass compared with any other removal treatment. the second highest density and biomass of medusahead occurred from removing rhizomatous forbs (phlox). we found perennial grasses played a relatively more significant role than other species in minimizing invasion by medusahead. we suggest that the most effective basis for establishing medusahead-resistant plant communities is to establish 2 or 3 highly productive grasses that are complementary in niche and that overlap that of the invading species."
http://orkg.org/orkg/resource/R51386,In vitro screening of a FDA approved chemical library reveals potential inhibitors of SARS-CoV-2 replication,10.1038/s41598-020-70143-6,crossref,"<jats:title>Abstract</jats:title><jats:p>A novel coronavirus, named SARS-CoV-2, emerged in 2019 in China and rapidly spread worldwide. As no approved therapeutics exists to treat COVID-19, the disease associated to SARS-Cov-2, there is an urgent need to propose molecules that could quickly enter into clinics. Repurposing of approved drugs is a strategy that can bypass the time-consuming stages of drug development. In this study, we screened the PRESTWICK CHEMICAL LIBRARY composed of 1,520 approved drugs in an infected cell-based assay. The robustness of the screen was assessed by the identification of drugs that already demonstrated in vitro antiviral effect against SARS-CoV-2. Thereby, 90 compounds were identified as positive hits from the screen and were grouped according to their chemical composition and their known therapeutic effect. Then EC50 and CC50 were determined for a subset of 15 compounds from a panel of 23 selected drugs covering the different groups. Eleven compounds such as macrolides antibiotics, proton pump inhibitors, antiarrhythmic agents or CNS drugs emerged showing antiviral potency with 2\u2009&lt;\u2009EC50\u2009≤\u200920\xa0µM. By providing new information on molecules inhibiting SARS-CoV-2 replication in vitro, this study provides information for the selection of drugs to be further validated in vivo. Disclaimer: This study corresponds to the early stages of antiviral development and the results do not support by themselves the use of the selected drugs to treat SARS-CoV-2 infection.</jats:p>","abstract a novel coronavirus, named sars-cov-2, emerged in 2019 in china and rapidly spread worldwide. as no approved therapeutics exists to treat covid-19, the disease associated to sars-cov-2, there is an urgent need to propose molecules that could quickly enter into clinics. repurposing of approved drugs is a strategy that can bypass the time-consuming stages of drug development. in this study, we screened the prestwick chemical library composed of 1,520 approved drugs in an infected cell-based assay. the robustness of the screen was assessed by the identification of drugs that already demonstrated in vitro antiviral effect against sars-cov-2. thereby, 90 compounds were identified as positive hits from the screen and were grouped according to their chemical composition and their known therapeutic effect. then ec50 and cc50 were determined for a subset of 15 compounds from a panel of 23 selected drugs covering the different groups. eleven compounds such as macrolides antibiotics, proton pump inhibitors, antiarrhythmic agents or cns drugs emerged showing antiviral potency with 2\u2009&lt;\u2009ec50\u2009≤\u200920\xa0µm. by providing new information on molecules inhibiting sars-cov-2 replication in vitro, this study provides information for the selection of drugs to be further validated in vivo. disclaimer: this study corresponds to the early stages of antiviral development and the results do not support by themselves the use of the selected drugs to treat sars-cov-2 infection."
http://orkg.org/orkg/resource/R25081,Private traits and attributes are predictable from digital records of human behavior,10.1073/pnas.1218772110,crossref,"<jats:p>We show that easily accessible digital records of behavior, Facebook Likes, can be used to automatically and accurately predict a range of highly sensitive personal attributes including: sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age, and gender. The analysis presented is based on a dataset of over 58,000 volunteers who provided their Facebook Likes, detailed demographic profiles, and the results of several psychometric tests. The proposed model uses dimensionality reduction for preprocessing the Likes data, which are then entered into logistic/linear regression to predict individual psychodemographic profiles from Likes. The model correctly discriminates between homosexual and heterosexual men in 88% of cases, African Americans and Caucasian Americans in 95% of cases, and between Democrat and Republican in 85% of cases. For the personality trait “Openness,” prediction accuracy is close to the test–retest accuracy of a standard personality test. We give examples of associations between attributes and Likes and discuss implications for online personalization and privacy.</jats:p>","we show that easily accessible digital records of behavior, facebook likes, can be used to automatically and accurately predict a range of highly sensitive personal attributes including: sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age, and gender. the analysis presented is based on a dataset of over 58,000 volunteers who provided their facebook likes, detailed demographic profiles, and the results of several psychometric tests. the proposed model uses dimensionality reduction for preprocessing the likes data, which are then entered into logistic/linear regression to predict individual psychodemographic profiles from likes. the model correctly discriminates between homosexual and heterosexual men in 88% of cases, african americans and caucasian americans in 95% of cases, and between democrat and republican in 85% of cases. for the personality trait “openness,” prediction accuracy is close to the test–retest accuracy of a standard personality test. we give examples of associations between attributes and likes and discuss implications for online personalization and privacy."
http://orkg.org/orkg/resource/R44901,Real-Time Estimation of the Risk of Death from Novel Coronavirus (COVID-19) Infection: Inference Using Exported Cases,10.3390/jcm9020523,crossref,"<jats:p>The exported cases of 2019 novel coronavirus (COVID-19) infection that were confirmed outside China provide an opportunity to estimate the cumulative incidence and confirmed case fatality risk (cCFR) in mainland China. Knowledge of the cCFR is critical to characterize the severity and understand the pandemic potential of COVID-19 in the early stage of the epidemic. Using the exponential growth rate of the incidence, the present study statistically estimated the cCFR and the basic reproduction number—the average number of secondary cases generated by a single primary case in a naïve population. We modeled epidemic growth either from a single index case with illness onset on 8 December 2019 (Scenario 1), or using the growth rate fitted along with the other parameters (Scenario 2) based on data from 20 exported cases reported by 24 January 2020. The cumulative incidence in China by 24 January was estimated at 6924 cases (95% confidence interval [CI]: 4885, 9211) and 19,289 cases (95% CI: 10,901, 30,158), respectively. The latest estimated values of the cCFR were 5.3% (95% CI: 3.5%, 7.5%) for Scenario 1 and 8.4% (95% CI: 5.3%, 12.3%) for Scenario 2. The basic reproduction number was estimated to be 2.1 (95% CI: 2.0, 2.2) and 3.2 (95% CI: 2.7, 3.7) for Scenarios 1 and 2, respectively. Based on these results, we argued that the current COVID-19 epidemic has a substantial potential for causing a pandemic. The proposed approach provides insights in early risk assessment using publicly available data.</jats:p>","the exported cases of 2019 novel coronavirus (covid-19) infection that were confirmed outside china provide an opportunity to estimate the cumulative incidence and confirmed case fatality risk (ccfr) in mainland china. knowledge of the ccfr is critical to characterize the severity and understand the pandemic potential of covid-19 in the early stage of the epidemic. using the exponential growth rate of the incidence, the present study statistically estimated the ccfr and the basic reproduction number—the average number of secondary cases generated by a single primary case in a naïve population. we modeled epidemic growth either from a single index case with illness onset on 8 december 2019 (scenario 1), or using the growth rate fitted along with the other parameters (scenario 2) based on data from 20 exported cases reported by 24 january 2020. the cumulative incidence in china by 24 january was estimated at 6924 cases (95% confidence interval [ci]: 4885, 9211) and 19,289 cases (95% ci: 10,901, 30,158), respectively. the latest estimated values of the ccfr were 5.3% (95% ci: 3.5%, 7.5%) for scenario 1 and 8.4% (95% ci: 5.3%, 12.3%) for scenario 2. the basic reproduction number was estimated to be 2.1 (95% ci: 2.0, 2.2) and 3.2 (95% ci: 2.7, 3.7) for scenarios 1 and 2, respectively. based on these results, we argued that the current covid-19 epidemic has a substantial potential for causing a pandemic. the proposed approach provides insights in early risk assessment using publicly available data."
http://orkg.org/orkg/resource/R44918,Estimation of the Transmission Risk of the 2019-nCoV and Its Implication for Public Health Interventions,10.3390/jcm9020462,crossref,"<jats:p>Since the emergence of the first cases in Wuhan, China, the novel coronavirus (2019-nCoV) infection has been quickly spreading out to other provinces and neighboring countries. Estimation of the basic reproduction number by means of mathematical modeling can be helpful for determining the potential and severity of an outbreak and providing critical information for identifying the type of disease interventions and intensity. A deterministic compartmental model was devised based on the clinical progression of the disease, epidemiological status of the individuals, and intervention measures. The estimations based on likelihood and model analysis show that the control reproduction number may be as high as 6.47 (95% CI 5.71–7.23). Sensitivity analyses show that interventions, such as intensive contact tracing followed by quarantine and isolation, can effectively reduce the control reproduction number and transmission risk, with the effect of travel restriction adopted by Wuhan on 2019-nCoV infection in Beijing being almost equivalent to increasing quarantine by a 100 thousand baseline value. It is essential to assess how the expensive, resource-intensive measures implemented by the Chinese authorities can contribute to the prevention and control of the 2019-nCoV infection, and how long they should be maintained. Under the most restrictive measures, the outbreak is expected to peak within two weeks (since 23 January 2020) with a significant low peak value. With travel restriction (no imported exposed individuals to Beijing), the number of infected individuals in seven days will decrease by 91.14% in Beijing, compared with the scenario of no travel restriction.</jats:p>","since the emergence of the first cases in wuhan, china, the novel coronavirus (2019-ncov) infection has been quickly spreading out to other provinces and neighboring countries. estimation of the basic reproduction number by means of mathematical modeling can be helpful for determining the potential and severity of an outbreak and providing critical information for identifying the type of disease interventions and intensity. a deterministic compartmental model was devised based on the clinical progression of the disease, epidemiological status of the individuals, and intervention measures. the estimations based on likelihood and model analysis show that the control reproduction number may be as high as 6.47 (95% ci 5.71–7.23). sensitivity analyses show that interventions, such as intensive contact tracing followed by quarantine and isolation, can effectively reduce the control reproduction number and transmission risk, with the effect of travel restriction adopted by wuhan on 2019-ncov infection in beijing being almost equivalent to increasing quarantine by a 100 thousand baseline value. it is essential to assess how the expensive, resource-intensive measures implemented by the chinese authorities can contribute to the prevention and control of the 2019-ncov infection, and how long they should be maintained. under the most restrictive measures, the outbreak is expected to peak within two weeks (since 23 january 2020) with a significant low peak value. with travel restriction (no imported exposed individuals to beijing), the number of infected individuals in seven days will decrease by 91.14% in beijing, compared with the scenario of no travel restriction."
http://orkg.org/orkg/resource/R25133,Complementary Audio-Visual Collision Warnings,10.1177/154193120905302315,crossref,"<jats:p> The growing number of driver assistance systems increases the demand for warnings that are intuitively comprehensible. Particularly in hazardous situations, such as a threatening collision, a driver must understand the warning immediately. For this reason, collision warnings should convey as much information as needed to interpret the situation properly and to prepare preventive actions. The present study investigated whether informing about the object and the location of an imminent crash by a multimodal warning (visual and auditory) leads to shorter reaction times and fewer collisions compared to warning signals which only inform about the object of the crash (auditory icons) or give no additional information (simple tone). Results reveal that multimodal warnings have the potential to produce a significant advantage over unimodal signals as long as their components complement each other in a way that realistically fits the situation at hand. </jats:p>","the growing number of driver assistance systems increases the demand for warnings that are intuitively comprehensible. particularly in hazardous situations, such as a threatening collision, a driver must understand the warning immediately. for this reason, collision warnings should convey as much information as needed to interpret the situation properly and to prepare preventive actions. the present study investigated whether informing about the object and the location of an imminent crash by a multimodal warning (visual and auditory) leads to shorter reaction times and fewer collisions compared to warning signals which only inform about the object of the crash (auditory icons) or give no additional information (simple tone). results reveal that multimodal warnings have the potential to produce a significant advantage over unimodal signals as long as their components complement each other in a way that realistically fits the situation at hand."
http://orkg.org/orkg/resource/R23287,A Modified Dynamic Framework for the Atmospheric Spectral Model and Its Application,10.1175/2007JAS2514.1,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>This paper describes a dynamic framework for an atmospheric general circulation spectral model in which a reference stratified atmospheric temperature and a reference surface pressure are introduced into the governing equations so as to improve the calculation of the pressure gradient force and gradients of surface pressure and temperature. The vertical profile of the reference atmospheric temperature approximately corresponds to that of the U.S. midlatitude standard atmosphere within the troposphere and stratosphere, and the reference surface pressure is a function of surface terrain geopotential and is close to the observed mean surface pressure. Prognostic variables for the temperature and surface pressure are replaced by their perturbations from the prescribed references. The numerical algorithms of the explicit time difference scheme for vorticity and the semi-implicit time difference scheme for divergence, perturbation temperature, and perturbation surface pressure equation are given in detail. The modified numerical framework is implemented in the Community Atmosphere Model version 3 (CAM3) developed at the National Center for Atmospheric Research (NCAR) to test its validation and impact on simulated climate. Both the original and the modified models are run with the same spectral resolution (T42), the same physical parameterizations, and the same boundary conditions corresponding to the observed monthly mean sea surface temperature and sea ice concentration from 1971 to 2000. This permits one to evaluate the performance of the new dynamic framework compared to the commonly used one. Results show that there is a general improvement for the simulated climate at regional and global scales, especially for temperature and wind.</jats:p>","abstract \n this paper describes a dynamic framework for an atmospheric general circulation spectral model in which a reference stratified atmospheric temperature and a reference surface pressure are introduced into the governing equations so as to improve the calculation of the pressure gradient force and gradients of surface pressure and temperature. the vertical profile of the reference atmospheric temperature approximately corresponds to that of the u.s. midlatitude standard atmosphere within the troposphere and stratosphere, and the reference surface pressure is a function of surface terrain geopotential and is close to the observed mean surface pressure. prognostic variables for the temperature and surface pressure are replaced by their perturbations from the prescribed references. the numerical algorithms of the explicit time difference scheme for vorticity and the semi-implicit time difference scheme for divergence, perturbation temperature, and perturbation surface pressure equation are given in detail. the modified numerical framework is implemented in the community atmosphere model version 3 (cam3) developed at the national center for atmospheric research (ncar) to test its validation and impact on simulated climate. both the original and the modified models are run with the same spectral resolution (t42), the same physical parameterizations, and the same boundary conditions corresponding to the observed monthly mean sea surface temperature and sea ice concentration from 1971 to 2000. this permits one to evaluate the performance of the new dynamic framework compared to the commonly used one. results show that there is a general improvement for the simulated climate at regional and global scales, especially for temperature and wind."
http://orkg.org/orkg/resource/R23338,"Present-Day Atmospheric Simulations Using GISS ModelE: Comparison to In Situ, Satellite, and Reanalysis Data",10.1175/JCLI3612.1,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>A full description of the ModelE version of the Goddard Institute for Space Studies (GISS) atmospheric general circulation model (GCM) and results are presented for present-day climate simulations (ca. 1979). This version is a complete rewrite of previous models incorporating numerous improvements in basic physics, the stratospheric circulation, and forcing fields. Notable changes include the following: the model top is now above the stratopause, the number of vertical layers has increased, a new cloud microphysical scheme is used, vegetation biophysics now incorporates a sensitivity to humidity, atmospheric turbulence is calculated over the whole column, and new land snow and lake schemes are introduced. The performance of the model using three configurations with different horizontal and vertical resolutions is compared to quality-controlled in situ data, remotely sensed and reanalysis products. Overall, significant improvements over previous models are seen, particularly in upper-atmosphere temperatures and winds, cloud heights, precipitation, and sea level pressure. Data–model comparisons continue, however, to highlight persistent problems in the marine stratocumulus regions.</jats:p>","abstract \n a full description of the modele version of the goddard institute for space studies (giss) atmospheric general circulation model (gcm) and results are presented for present-day climate simulations (ca. 1979). this version is a complete rewrite of previous models incorporating numerous improvements in basic physics, the stratospheric circulation, and forcing fields. notable changes include the following: the model top is now above the stratopause, the number of vertical layers has increased, a new cloud microphysical scheme is used, vegetation biophysics now incorporates a sensitivity to humidity, atmospheric turbulence is calculated over the whole column, and new land snow and lake schemes are introduced. the performance of the model using three configurations with different horizontal and vertical resolutions is compared to quality-controlled in situ data, remotely sensed and reanalysis products. overall, significant improvements over previous models are seen, particularly in upper-atmosphere temperatures and winds, cloud heights, precipitation, and sea level pressure. data–model comparisons continue, however, to highlight persistent problems in the marine stratocumulus regions."
http://orkg.org/orkg/resource/R23457,Evaluation of the carbon cycle components in the Norwegian Earth System Model (NorESM),10.5194/gmdd-5-3035-2012,crossref,"<jats:p>Abstract. The recently developed Norwegian Earth System Model (NorESM) is employed for simulations contributing to the CMIP5 (Coupled Model Intercomparison Project phase 5) experiments and the fifth assessment report of the Intergovernmental Panel on Climate Change (IPCC-AR5). In this manuscript, we focus on evaluating the ocean and land carbon cycle components of the NorESM, based on the control and historical simulations. Many of the observed large scale ocean biogeochemical features are reproduced satisfactorily by the NorESM. When compared to the climatological estimates from the World Ocean Atlas (WOA), the model simulated temperature, salinity, oxygen, and phosphate distributions agree reasonably well in both the surface layer and deep water structure. However, the model simulates a relatively strong overturning circulation strength that leads to noticeable model-data bias, especially within the North Atlantic Deep Water (NADW). This strong overturning circulation slightly distorts the structure of the biogeochemical tracers at depth. Advancements in simulating the oceanic mixed layer depth with respect to the previous generation model particularly improve the surface tracer distribution as well as the upper ocean biogeochemical processes, particularly in the Southern Ocean. Consequently, near surface ocean processes such as biological production and air-sea gas exchange, are in good agreement with climatological observations. NorESM reproduces the general pattern of land-vegetation gross primary productivity (GPP) when compared to the observationally-based values derived from the FLUXNET network of eddy covariance towers. Globally, the NorESM simulated annual mean GPP and terrestrial respiration are 129.8 and 106.6 Pg C yr−1, slightly larger than observed of 119.4 ± 5.9 and 96.4 ± 6.0 Pg C yr−1. The latitudinal distribution of GPP fluxes simulated by NorESM shows a GPP overestimation of 10% in the tropics and a substantial underestimation of GPP at high latitudes.</jats:p>","abstract. the recently developed norwegian earth system model (noresm) is employed for simulations contributing to the cmip5 (coupled model intercomparison project phase 5) experiments and the fifth assessment report of the intergovernmental panel on climate change (ipcc-ar5). in this manuscript, we focus on evaluating the ocean and land carbon cycle components of the noresm, based on the control and historical simulations. many of the observed large scale ocean biogeochemical features are reproduced satisfactorily by the noresm. when compared to the climatological estimates from the world ocean atlas (woa), the model simulated temperature, salinity, oxygen, and phosphate distributions agree reasonably well in both the surface layer and deep water structure. however, the model simulates a relatively strong overturning circulation strength that leads to noticeable model-data bias, especially within the north atlantic deep water (nadw). this strong overturning circulation slightly distorts the structure of the biogeochemical tracers at depth. advancements in simulating the oceanic mixed layer depth with respect to the previous generation model particularly improve the surface tracer distribution as well as the upper ocean biogeochemical processes, particularly in the southern ocean. consequently, near surface ocean processes such as biological production and air-sea gas exchange, are in good agreement with climatological observations. noresm reproduces the general pattern of land-vegetation gross primary productivity (gpp) when compared to the observationally-based values derived from the fluxnet network of eddy covariance towers. globally, the noresm simulated annual mean gpp and terrestrial respiration are 129.8 and 106.6 pg c yr−1, slightly larger than observed of 119.4 ± 5.9 and 96.4 ± 6.0 pg c yr−1. the latitudinal distribution of gpp fluxes simulated by noresm shows a gpp overestimation of 10% in the tropics and a substantial underestimation of gpp at high latitudes."
http://orkg.org/orkg/resource/R23383,"Present-Day Atmospheric Simulations Using GISS ModelE: Comparison to In Situ, Satellite, and Reanalysis Data",10.1175/JCLI3612.1,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>A full description of the ModelE version of the Goddard Institute for Space Studies (GISS) atmospheric general circulation model (GCM) and results are presented for present-day climate simulations (ca. 1979). This version is a complete rewrite of previous models incorporating numerous improvements in basic physics, the stratospheric circulation, and forcing fields. Notable changes include the following: the model top is now above the stratopause, the number of vertical layers has increased, a new cloud microphysical scheme is used, vegetation biophysics now incorporates a sensitivity to humidity, atmospheric turbulence is calculated over the whole column, and new land snow and lake schemes are introduced. The performance of the model using three configurations with different horizontal and vertical resolutions is compared to quality-controlled in situ data, remotely sensed and reanalysis products. Overall, significant improvements over previous models are seen, particularly in upper-atmosphere temperatures and winds, cloud heights, precipitation, and sea level pressure. Data–model comparisons continue, however, to highlight persistent problems in the marine stratocumulus regions.</jats:p>","abstract \n a full description of the modele version of the goddard institute for space studies (giss) atmospheric general circulation model (gcm) and results are presented for present-day climate simulations (ca. 1979). this version is a complete rewrite of previous models incorporating numerous improvements in basic physics, the stratospheric circulation, and forcing fields. notable changes include the following: the model top is now above the stratopause, the number of vertical layers has increased, a new cloud microphysical scheme is used, vegetation biophysics now incorporates a sensitivity to humidity, atmospheric turbulence is calculated over the whole column, and new land snow and lake schemes are introduced. the performance of the model using three configurations with different horizontal and vertical resolutions is compared to quality-controlled in situ data, remotely sensed and reanalysis products. overall, significant improvements over previous models are seen, particularly in upper-atmosphere temperatures and winds, cloud heights, precipitation, and sea level pressure. data–model comparisons continue, however, to highlight persistent problems in the marine stratocumulus regions."
http://orkg.org/orkg/resource/R23312,GFDL's CM2 Global Coupled Climate Models. Part I: Formulation and Simulation Characteristics,10.1175/JCLI3629.1,crossref,"""<jats:title>Abstract</jats:title>\n               <jats:p>The formulation and simulation characteristics of two new global coupled climate models developed at NOAA's Geophysical Fluid Dynamics Laboratory (GFDL) are described. The models were designed to simulate atmospheric and oceanic climate and variability from the diurnal time scale through multicentury climate change, given our computational constraints. In particular, an important goal was to use the same model for both experimental seasonal to interannual forecasting and the study of multicentury global climate change, and this goal has been achieved.</jats:p>\n               <jats:p>Two versions of the coupled model are described, called CM2.0 and CM2.1. The versions differ primarily in the dynamical core used in the atmospheric component, along with the cloud tuning and some details of the land and ocean components. For both coupled models, the resolution of the land and atmospheric components is 2° latitude × 2.5° longitude; the atmospheric model has 24 vertical levels. The ocean resolution is 1° in latitude and longitude, with meridional resolution equatorward of 30° becoming progressively finer, such that the meridional resolution is 1/3° at the equator. There are 50 vertical levels in the ocean, with 22 evenly spaced levels within the top 220 m. The ocean component has poles over North America and Eurasia to avoid polar filtering. Neither coupled model employs flux adjustments.</jats:p>\n               <jats:p>The control simulations have stable, realistic climates when integrated over multiple centuries. Both models have simulations of ENSO that are substantially improved relative to previous GFDL coupled models. The CM2.0 model has been further evaluated as an ENSO forecast model and has good skill (CM2.1 has not been evaluated as an ENSO forecast model). Generally reduced temperature and salinity biases exist in CM2.1 relative to CM2.0. These reductions are associated with 1) improved simulations of surface wind stress in CM2.1 and associated changes in oceanic gyre circulations; 2) changes in cloud tuning and the land model, both of which act to increase the net surface shortwave radiation in CM2.1, thereby reducing an overall cold bias present in CM2.0; and 3) a reduction of ocean lateral viscosity in the extratropics in CM2.1, which reduces sea ice biases in the North Atlantic.</jats:p>\n               <jats:p>Both models have been used to conduct a suite of climate change simulations for the 2007 Intergovernmental Panel on Climate Change (IPCC) assessment report and are able to simulate the main features of the observed warming of the twentieth century. The climate sensitivities of the CM2.0 and CM2.1 models are 2.9 and 3.4 K, respectively. These sensitivities are defined by coupling the atmospheric components of CM2.0 and CM2.1 to a slab ocean model and allowing the model to come into equilibrium with a doubling of atmospheric CO2. The output from a suite of integrations conducted with these models is freely available online (see http://nomads.gfdl.noaa.gov/).</jats:p>""",""" abstract \n the formulation and simulation characteristics of two new global coupled climate models developed at noaa's geophysical fluid dynamics laboratory (gfdl) are described. the models were designed to simulate atmospheric and oceanic climate and variability from the diurnal time scale through multicentury climate change, given our computational constraints. in particular, an important goal was to use the same model for both experimental seasonal to interannual forecasting and the study of multicentury global climate change, and this goal has been achieved. \n two versions of the coupled model are described, called cm2.0 and cm2.1. the versions differ primarily in the dynamical core used in the atmospheric component, along with the cloud tuning and some details of the land and ocean components. for both coupled models, the resolution of the land and atmospheric components is 2° latitude × 2.5° longitude; the atmospheric model has 24 vertical levels. the ocean resolution is 1° in latitude and longitude, with meridional resolution equatorward of 30° becoming progressively finer, such that the meridional resolution is 1/3° at the equator. there are 50 vertical levels in the ocean, with 22 evenly spaced levels within the top 220 m. the ocean component has poles over north america and eurasia to avoid polar filtering. neither coupled model employs flux adjustments. \n the control simulations have stable, realistic climates when integrated over multiple centuries. both models have simulations of enso that are substantially improved relative to previous gfdl coupled models. the cm2.0 model has been further evaluated as an enso forecast model and has good skill (cm2.1 has not been evaluated as an enso forecast model). generally reduced temperature and salinity biases exist in cm2.1 relative to cm2.0. these reductions are associated with 1) improved simulations of surface wind stress in cm2.1 and associated changes in oceanic gyre circulations; 2) changes in cloud tuning and the land model, both of which act to increase the net surface shortwave radiation in cm2.1, thereby reducing an overall cold bias present in cm2.0; and 3) a reduction of ocean lateral viscosity in the extratropics in cm2.1, which reduces sea ice biases in the north atlantic. \n both models have been used to conduct a suite of climate change simulations for the 2007 intergovernmental panel on climate change (ipcc) assessment report and are able to simulate the main features of the observed warming of the twentieth century. the climate sensitivities of the cm2.0 and cm2.1 models are 2.9 and 3.4 k, respectively. these sensitivities are defined by coupling the atmospheric components of cm2.0 and cm2.1 to a slab ocean model and allowing the model to come into equilibrium with a doubling of atmospheric co2. the output from a suite of integrations conducted with these models is freely available online (see http://nomads.gfdl.noaa.gov/). """
http://orkg.org/orkg/resource/R23326,GFDL’s ESM2 Global Coupled Climate–Carbon Earth System Models. Part I: Physical Formulation and Baseline Simulation Characteristics,10.1175/JCLI-D-11-00560.1,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>The physical climate formulation and simulation characteristics of two new global coupled carbon–climate Earth System Models, ESM2M and ESM2G, are described. These models demonstrate similar climate fidelity as the Geophysical Fluid Dynamics Laboratory’s previous Climate Model version 2.1 (CM2.1) while incorporating explicit and consistent carbon dynamics. The two models differ exclusively in the physical ocean component; ESM2M uses Modular Ocean Model version 4p1 with vertical pressure layers while ESM2G uses Generalized Ocean Layer Dynamics with a bulk mixed layer and interior isopycnal layers. Differences in the ocean mean state include the thermocline depth being relatively deep in ESM2M and relatively shallow in ESM2G compared to observations. The crucial role of ocean dynamics on climate variability is highlighted in El Niño–Southern Oscillation being overly strong in ESM2M and overly weak in ESM2G relative to observations. Thus, while ESM2G might better represent climate changes relating to total heat content variability given its lack of long-term drift, gyre circulation, and ventilation in the North Pacific, tropical Atlantic, and Indian Oceans, and depth structure in the overturning and abyssal flows, ESM2M might better represent climate changes relating to surface circulation given its superior surface temperature, salinity, and height patterns, tropical Pacific circulation and variability, and Southern Ocean dynamics. The overall assessment is that neither model is fundamentally superior to the other, and that both models achieve sufficient fidelity to allow meaningful climate and earth system modeling applications. This affords the ability to assess the role of ocean configuration on earth system interactions in the context of two state-of-the-art coupled carbon–climate models.</jats:p>","abstract \n the physical climate formulation and simulation characteristics of two new global coupled carbon–climate earth system models, esm2m and esm2g, are described. these models demonstrate similar climate fidelity as the geophysical fluid dynamics laboratory’s previous climate model version 2.1 (cm2.1) while incorporating explicit and consistent carbon dynamics. the two models differ exclusively in the physical ocean component; esm2m uses modular ocean model version 4p1 with vertical pressure layers while esm2g uses generalized ocean layer dynamics with a bulk mixed layer and interior isopycnal layers. differences in the ocean mean state include the thermocline depth being relatively deep in esm2m and relatively shallow in esm2g compared to observations. the crucial role of ocean dynamics on climate variability is highlighted in el niño–southern oscillation being overly strong in esm2m and overly weak in esm2g relative to observations. thus, while esm2g might better represent climate changes relating to total heat content variability given its lack of long-term drift, gyre circulation, and ventilation in the north pacific, tropical atlantic, and indian oceans, and depth structure in the overturning and abyssal flows, esm2m might better represent climate changes relating to surface circulation given its superior surface temperature, salinity, and height patterns, tropical pacific circulation and variability, and southern ocean dynamics. the overall assessment is that neither model is fundamentally superior to the other, and that both models achieve sufficient fidelity to allow meaningful climate and earth system modeling applications. this affords the ability to assess the role of ocean configuration on earth system interactions in the context of two state-of-the-art coupled carbon–climate models."
http://orkg.org/orkg/resource/R23353,"Present-Day Atmospheric Simulations Using GISS ModelE: Comparison to In Situ, Satellite, and Reanalysis Data",10.1175/JCLI3612.1,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>A full description of the ModelE version of the Goddard Institute for Space Studies (GISS) atmospheric general circulation model (GCM) and results are presented for present-day climate simulations (ca. 1979). This version is a complete rewrite of previous models incorporating numerous improvements in basic physics, the stratospheric circulation, and forcing fields. Notable changes include the following: the model top is now above the stratopause, the number of vertical layers has increased, a new cloud microphysical scheme is used, vegetation biophysics now incorporates a sensitivity to humidity, atmospheric turbulence is calculated over the whole column, and new land snow and lake schemes are introduced. The performance of the model using three configurations with different horizontal and vertical resolutions is compared to quality-controlled in situ data, remotely sensed and reanalysis products. Overall, significant improvements over previous models are seen, particularly in upper-atmosphere temperatures and winds, cloud heights, precipitation, and sea level pressure. Data–model comparisons continue, however, to highlight persistent problems in the marine stratocumulus regions.</jats:p>","abstract \n a full description of the modele version of the goddard institute for space studies (giss) atmospheric general circulation model (gcm) and results are presented for present-day climate simulations (ca. 1979). this version is a complete rewrite of previous models incorporating numerous improvements in basic physics, the stratospheric circulation, and forcing fields. notable changes include the following: the model top is now above the stratopause, the number of vertical layers has increased, a new cloud microphysical scheme is used, vegetation biophysics now incorporates a sensitivity to humidity, atmospheric turbulence is calculated over the whole column, and new land snow and lake schemes are introduced. the performance of the model using three configurations with different horizontal and vertical resolutions is compared to quality-controlled in situ data, remotely sensed and reanalysis products. overall, significant improvements over previous models are seen, particularly in upper-atmosphere temperatures and winds, cloud heights, precipitation, and sea level pressure. data–model comparisons continue, however, to highlight persistent problems in the marine stratocumulus regions."
http://orkg.org/orkg/resource/R23368,"Present-Day Atmospheric Simulations Using GISS ModelE: Comparison to In Situ, Satellite, and Reanalysis Data",10.1175/JCLI3612.1,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>A full description of the ModelE version of the Goddard Institute for Space Studies (GISS) atmospheric general circulation model (GCM) and results are presented for present-day climate simulations (ca. 1979). This version is a complete rewrite of previous models incorporating numerous improvements in basic physics, the stratospheric circulation, and forcing fields. Notable changes include the following: the model top is now above the stratopause, the number of vertical layers has increased, a new cloud microphysical scheme is used, vegetation biophysics now incorporates a sensitivity to humidity, atmospheric turbulence is calculated over the whole column, and new land snow and lake schemes are introduced. The performance of the model using three configurations with different horizontal and vertical resolutions is compared to quality-controlled in situ data, remotely sensed and reanalysis products. Overall, significant improvements over previous models are seen, particularly in upper-atmosphere temperatures and winds, cloud heights, precipitation, and sea level pressure. Data–model comparisons continue, however, to highlight persistent problems in the marine stratocumulus regions.</jats:p>","abstract \n a full description of the modele version of the goddard institute for space studies (giss) atmospheric general circulation model (gcm) and results are presented for present-day climate simulations (ca. 1979). this version is a complete rewrite of previous models incorporating numerous improvements in basic physics, the stratospheric circulation, and forcing fields. notable changes include the following: the model top is now above the stratopause, the number of vertical layers has increased, a new cloud microphysical scheme is used, vegetation biophysics now incorporates a sensitivity to humidity, atmospheric turbulence is calculated over the whole column, and new land snow and lake schemes are introduced. the performance of the model using three configurations with different horizontal and vertical resolutions is compared to quality-controlled in situ data, remotely sensed and reanalysis products. overall, significant improvements over previous models are seen, particularly in upper-atmosphere temperatures and winds, cloud heights, precipitation, and sea level pressure. data–model comparisons continue, however, to highlight persistent problems in the marine stratocumulus regions."
http://orkg.org/orkg/resource/R23398,Development and evaluation of an Earth-System model – HadGEM2,10.5194/gmd-4-1051-2011,crossref,"<jats:p>Abstract. We describe here the development and evaluation of an Earth system model suitable for centennial-scale climate prediction. The principal new components added to the physical climate model are the terrestrial and ocean ecosystems and gas-phase tropospheric chemistry, along with their coupled interactions.  The individual Earth system components are described briefly and the relevant interactions between the components are explained. Because the multiple interactions could lead to unstable feedbacks, we go through a careful process of model spin up to ensure that all components are stable and the interactions balanced. This spun-up configuration is evaluated against observed data for the Earth system components and is generally found to perform very satisfactorily. The reason for the evaluation phase is that the model is to be used for the core climate simulations carried out by the Met Office Hadley Centre for the Coupled Model Intercomparison Project (CMIP5), so it is essential that addition of the extra complexity does not detract substantially from its climate performance. Localised changes in some specific meteorological variables can be identified, but the impacts on the overall simulation of present day climate are slight.  This model is proving valuable both for climate predictions, and for investigating the strengths of biogeochemical feedbacks.\n                    </jats:p>","abstract. we describe here the development and evaluation of an earth system model suitable for centennial-scale climate prediction. the principal new components added to the physical climate model are the terrestrial and ocean ecosystems and gas-phase tropospheric chemistry, along with their coupled interactions. the individual earth system components are described briefly and the relevant interactions between the components are explained. because the multiple interactions could lead to unstable feedbacks, we go through a careful process of model spin up to ensure that all components are stable and the interactions balanced. this spun-up configuration is evaluated against observed data for the earth system components and is generally found to perform very satisfactorily. the reason for the evaluation phase is that the model is to be used for the core climate simulations carried out by the met office hadley centre for the coupled model intercomparison project (cmip5), so it is essential that addition of the extra complexity does not detract substantially from its climate performance. localised changes in some specific meteorological variables can be identified, but the impacts on the overall simulation of present day climate are slight. this model is proving valuable both for climate predictions, and for investigating the strengths of biogeochemical feedbacks.\n"
http://orkg.org/orkg/resource/R23443,"The Norwegian Earth System Model, NorESM1-M – Part 1: Description and basic evaluation of the physical climate",10.5194/gmd-6-687-2013,crossref,"""<jats:p>Abstract. The core version of the Norwegian Climate Center's Earth System Model, named NorESM1-M, is presented. The NorESM family of models are based on the Community Climate System Model version 4 (CCSM4) of the University Corporation for Atmospheric Research, but differs from the latter by, in particular, an isopycnic coordinate ocean model and advanced chemistry–aerosol–cloud–radiation interaction schemes. NorESM1-M has a horizontal resolution of approximately 2° for the atmosphere and land components and 1° for the ocean and ice components. NorESM is also available in a lower resolution version (NorESM1-L) and a version that includes prognostic biogeochemical cycling (NorESM1-ME). The latter two model configurations are not part of this paper. Here, a first-order assessment of the model stability, the mean model state and the internal variability based on the model experiments made available to CMIP5 are presented. Further analysis of the model performance is provided in an accompanying paper (Iversen et al., 2013), presenting the corresponding climate response and scenario projections made with NorESM1-M.\n                    </jats:p>""",""" abstract. the core version of the norwegian climate center's earth system model, named noresm1-m, is presented. the noresm family of models are based on the community climate system model version 4 (ccsm4) of the university corporation for atmospheric research, but differs from the latter by, in particular, an isopycnic coordinate ocean model and advanced chemistry–aerosol–cloud–radiation interaction schemes. noresm1-m has a horizontal resolution of approximately 2° for the atmosphere and land components and 1° for the ocean and ice components. noresm is also available in a lower resolution version (noresm1-l) and a version that includes prognostic biogeochemical cycling (noresm1-me). the latter two model configurations are not part of this paper. here, a first-order assessment of the model stability, the mean model state and the internal variability based on the model experiments made available to cmip5 are presented. further analysis of the model performance is provided in an accompanying paper (iversen et al., 2013), presenting the corresponding climate response and scenario projections made with noresm1-m.\n """
http://orkg.org/orkg/resource/R25251,Arabic senti-lexicon: Constructing publicly available language resources for Arabic sentiment analysis,10.1177/0165551516683908,crossref,"<jats:p>Sentiment analysis is held to be one of the highly dynamic recent research fields in Natural Language Processing, facilitated by the quickly growing volume of Web opinion data. Most of the approaches in this field are focused on English due to the lack of sentiment resources in other languages such as the Arabic language and its large variety of dialects. In most sentiment analysis applications, good sentiment resources play a critical role. Based on that, in this article, several publicly available sentiment analysis resources for Arabic are introduced. This article introduces the Arabic senti-lexicon, a list of 3880 positive and negative synsets annotated with their part of speech, polarity scores, dialects synsets and inflected forms. This article also presents a Multi-domain Arabic Sentiment Corpus (MASC) with a size of 8860 positive and negative reviews from different domains. In this article, an in-depth study has been conducted on five types of feature sets for exploiting effective features and investigating their effect on performance of Arabic sentiment analysis. The aim is to assess the quality of the developed language resources and to integrate different feature sets and classification algorithms to synthesise a more accurate sentiment analysis method. The Arabic senti-lexicon is used for generating feature vectors. Five well-known machine learning algorithms: naïve Bayes, k-nearest neighbours, support vector machines (SVMs), logistic linear regression and neural network are employed as base-classifiers for each of the feature sets. A wide range of comparative experiments on standard Arabic data sets were conducted, discussion is presented and conclusions are drawn. The experimental results show that the Arabic senti-lexicon is a very useful resource for Arabic sentiment analysis. Moreover, results show that classifiers which are trained on feature vectors derived from the corpus using the Arabic sentiment lexicon are more accurate than classifiers trained using the raw corpus.</jats:p>","sentiment analysis is held to be one of the highly dynamic recent research fields in natural language processing, facilitated by the quickly growing volume of web opinion data. most of the approaches in this field are focused on english due to the lack of sentiment resources in other languages such as the arabic language and its large variety of dialects. in most sentiment analysis applications, good sentiment resources play a critical role. based on that, in this article, several publicly available sentiment analysis resources for arabic are introduced. this article introduces the arabic senti-lexicon, a list of 3880 positive and negative synsets annotated with their part of speech, polarity scores, dialects synsets and inflected forms. this article also presents a multi-domain arabic sentiment corpus (masc) with a size of 8860 positive and negative reviews from different domains. in this article, an in-depth study has been conducted on five types of feature sets for exploiting effective features and investigating their effect on performance of arabic sentiment analysis. the aim is to assess the quality of the developed language resources and to integrate different feature sets and classification algorithms to synthesise a more accurate sentiment analysis method. the arabic senti-lexicon is used for generating feature vectors. five well-known machine learning algorithms: naïve bayes, k-nearest neighbours, support vector machines (svms), logistic linear regression and neural network are employed as base-classifiers for each of the feature sets. a wide range of comparative experiments on standard arabic data sets were conducted, discussion is presented and conclusions are drawn. the experimental results show that the arabic senti-lexicon is a very useful resource for arabic sentiment analysis. moreover, results show that classifiers which are trained on feature vectors derived from the corpus using the arabic sentiment lexicon are more accurate than classifiers trained using the raw corpus."
http://orkg.org/orkg/resource/R25276,Semantics-based composition-oriented discovery of Web services,10.1145/1391949.1391953,crossref,"<jats:p>Service discovery and service aggregation are two crucial issues in the emerging area of service-oriented computing (SOC). We propose a new technique for the discovery of (Web) services that accounts for the need of composing several services to satisfy a client query. The proposed algorithm makes use of OWL-S ontologies, and explicitly returns the sequence of atomic process invocations that the client must perform in order to achieve the desired result. When no full match is possible, the algorithm features a flexible matching by returning partial matches and by suggesting additional inputs that would produce a full match.</jats:p>","service discovery and service aggregation are two crucial issues in the emerging area of service-oriented computing (soc). we propose a new technique for the discovery of (web) services that accounts for the need of composing several services to satisfy a client query. the proposed algorithm makes use of owl-s ontologies, and explicitly returns the sequence of atomic process invocations that the client must perform in order to achieve the desired result. when no full match is possible, the algorithm features a flexible matching by returning partial matches and by suggesting additional inputs that would produce a full match."
http://orkg.org/orkg/resource/R25290,An abstract model of service discovery and binding,10.1007/s00165-010-0166-z,crossref,"<jats:title>Abstract</jats:title>\n          <jats:p>We propose a formal operational semantics for service discovery and binding. This semantics is based on a graph-based representation of the configuration of global computers typed by business activities. Business activities execute distributed workflows that can trigger, at run time, the discovery, ranking and selection of services to which they bind, thus reconfiguring the workflows that they execute. Discovery, ranking and selection are based on compliance with required business and interaction protocols and optimisation of quality-of-service constraints. Binding and reconfiguration are captured as algebraic operations on configuration graphs. We also discuss the methodological implications that this model framework has on software engineering using a typical travel-booking scenario. To the best of our knowledge, our approach is the first to provide a clear separation between service computation and discovery/instantiation/binding, and to offer a formal framework that is independent of the SOA middleware components that act as service registries or brokers, and the protocols through which bindings and invocations are performed.</jats:p>","abstract \n we propose a formal operational semantics for service discovery and binding. this semantics is based on a graph-based representation of the configuration of global computers typed by business activities. business activities execute distributed workflows that can trigger, at run time, the discovery, ranking and selection of services to which they bind, thus reconfiguring the workflows that they execute. discovery, ranking and selection are based on compliance with required business and interaction protocols and optimisation of quality-of-service constraints. binding and reconfiguration are captured as algebraic operations on configuration graphs. we also discuss the methodological implications that this model framework has on software engineering using a typical travel-booking scenario. to the best of our knowledge, our approach is the first to provide a clear separation between service computation and discovery/instantiation/binding, and to offer a formal framework that is independent of the soa middleware components that act as service registries or brokers, and the protocols through which bindings and invocations are performed."
http://orkg.org/orkg/resource/R25439,Reliability of Component Based systems- a Critical Survey,10.1145/2047414.2047434,crossref,"<jats:p>Software reliability is defined as the probability of the failure free operation of a software system for a specified period of time in a specified environment. Day by day software applications are growing more complex and with more emphasis on reuse. Component Based Software (CBS) applications have emerged. The focus of this paper is to provide an overview for the state of the art of Component Based Systems reliability estimation. In this paper, we discussed various approaches in terms of their scope, model, methods, technique and validation scheme. This comparison provides insight into determining the direction of future CBS reliability research.</jats:p>","software reliability is defined as the probability of the failure free operation of a software system for a specified period of time in a specified environment. day by day software applications are growing more complex and with more emphasis on reuse. component based software (cbs) applications have emerged. the focus of this paper is to provide an overview for the state of the art of component based systems reliability estimation. in this paper, we discussed various approaches in terms of their scope, model, methods, technique and validation scheme. this comparison provides insight into determining the direction of future cbs reliability research."
http://orkg.org/orkg/resource/R25497,Crowdfunding to generate crowdsourced R&D: The alternative paradigm of societal problem solving offered by second generation innovation and R&D,10.19030/iber.v13i6.8937,crossref,"<jats:p>In a global context of resource scarcity few incentives exist for firms to pursue innovations that provide social externalities if these are not inherently profitable. The purpose of this article is to present an alternative paradigm of societal problem solving entirely premised on second generation innovation processes. Further, a theoretical model of multidimensional, or three dimensional, knowledge creation is offered, together with the notion of a multiplier effect that relates to how knowledge creation can increase exponentially when knowledge is not constrained by proprietary requirements. Second generation innovation is based on probabilistic processes that utilize and maximize economies of scale in pursuit of problem solving. Two processes that contribute to the potential of second generation innovation to solve societal problems are crowdfunding and crowdsourcing. It is argued that the processes required to enable a new paradigm in societal problem solving already exist. A further model is developed based on potential synergies between crowdfunding and crowdsourced research and development. This theoretical model predicts that R&amp;D productivity can be accelerated significantly, and if applied in fields such as proteomics or medical research in general can accelerated increases in research output and therefore benefits to society.</jats:p>","in a global context of resource scarcity few incentives exist for firms to pursue innovations that provide social externalities if these are not inherently profitable. the purpose of this article is to present an alternative paradigm of societal problem solving entirely premised on second generation innovation processes. further, a theoretical model of multidimensional, or three dimensional, knowledge creation is offered, together with the notion of a multiplier effect that relates to how knowledge creation can increase exponentially when knowledge is not constrained by proprietary requirements. second generation innovation is based on probabilistic processes that utilize and maximize economies of scale in pursuit of problem solving. two processes that contribute to the potential of second generation innovation to solve societal problems are crowdfunding and crowdsourcing. it is argued that the processes required to enable a new paradigm in societal problem solving already exist. a further model is developed based on potential synergies between crowdfunding and crowdsourced research and development. this theoretical model predicts that r&amp;d productivity can be accelerated significantly, and if applied in fields such as proteomics or medical research in general can accelerated increases in research output and therefore benefits to society."
http://orkg.org/orkg/resource/R25503,Product and Pricing Decisions in Crowdfunding,10.1287/mksc.2014.0900,crossref,"<jats:p> This paper studies the optimal product and pricing decisions in a crowdfunding mechanism by which a project between a creator and many buyers will be realized only if the total funds committed by the buyers reach a specified goal. When the buyers are sufficiently heterogeneous in their product valuations, the creator should offer a line of products with different levels of product quality. Compared to the traditional situation where orders are placed and fulfilled individually, with the crowdfunding mechanism, a product line is more likely than a single product to be optimal and the quality gap between products is smaller. This paper also shows the effect of the crowdfunding mechanism on pricing dynamics over time. Together, these results underscore the substantial influence of the emerging crowdfunding mechanisms on common marketing decisions. </jats:p>","this paper studies the optimal product and pricing decisions in a crowdfunding mechanism by which a project between a creator and many buyers will be realized only if the total funds committed by the buyers reach a specified goal. when the buyers are sufficiently heterogeneous in their product valuations, the creator should offer a line of products with different levels of product quality. compared to the traditional situation where orders are placed and fulfilled individually, with the crowdfunding mechanism, a product line is more likely than a single product to be optimal and the quality gap between products is smaller. this paper also shows the effect of the crowdfunding mechanism on pricing dynamics over time. together, these results underscore the substantial influence of the emerging crowdfunding mechanisms on common marketing decisions."
http://orkg.org/orkg/resource/R25521,The backer–developer connection: Exploring crowdfunding’s influence on video game production,10.1177/1461444814558910,crossref,"<jats:p>As video game development studios increasingly turn to digital crowdfunding platforms such as Kickstarter for financing, this article explores the ways in which these processes shape production. It examines in particular the interactions that typically occur between studios and players as part of crowdfunded development, analysing the ways in which these activities inform aspects of video game design. By charting the implications of this burgeoning economic model, the article contributes to scholarship concerning video game production and intervenes within more specific discussions concerning the role of the player within development. The article’s case study, which draws from evidence of production concerning multiple Kickstarter projects, is organised into two sections. The first ascertains the degrees to which Kickstarter users can influence the details of a proposed project during a crowdfunding campaign; the second looks at how developers involve crowdfunding communities within production once funding is secured.</jats:p>","as video game development studios increasingly turn to digital crowdfunding platforms such as kickstarter for financing, this article explores the ways in which these processes shape production. it examines in particular the interactions that typically occur between studios and players as part of crowdfunded development, analysing the ways in which these activities inform aspects of video game design. by charting the implications of this burgeoning economic model, the article contributes to scholarship concerning video game production and intervenes within more specific discussions concerning the role of the player within development. the article’s case study, which draws from evidence of production concerning multiple kickstarter projects, is organised into two sections. the first ascertains the degrees to which kickstarter users can influence the details of a proposed project during a crowdfunding campaign; the second looks at how developers involve crowdfunding communities within production once funding is secured."
http://orkg.org/orkg/resource/R25527,What Goes around Comes Around? Rewards as Strategic Assets in Crowdfunding,10.1525/cmr.2016.58.2.88,crossref,"<jats:p> In crowdfunding, rewards can make or break success. Yet reward design, choice, and planning still occur based on availability rather than strategy. To address this challenge, this article provides an empirically derived crowd-funding reward toolbox offering guidance in strategically selecting rewards. Based on a large-scale analysis of successful and unsuccessful Kickstarter projects, this article classifies rewards that are currently offered along eight dimensions. It identifies emerging patterns and derives five strategic core tools and two add-on tools. Finally, it delivers exploratory insights into the relative effectiveness of different tools that can facilitate decision making and strategic planning for entrepreneurs and individuals who plan to launch a crowdfunding project and who seek ways to reward their supporters. </jats:p>","in crowdfunding, rewards can make or break success. yet reward design, choice, and planning still occur based on availability rather than strategy. to address this challenge, this article provides an empirically derived crowd-funding reward toolbox offering guidance in strategically selecting rewards. based on a large-scale analysis of successful and unsuccessful kickstarter projects, this article classifies rewards that are currently offered along eight dimensions. it identifies emerging patterns and derives five strategic core tools and two add-on tools. finally, it delivers exploratory insights into the relative effectiveness of different tools that can facilitate decision making and strategic planning for entrepreneurs and individuals who plan to launch a crowdfunding project and who seek ways to reward their supporters."
http://orkg.org/orkg/resource/R25535,Automatic prototyping in model-driven game development,10.1145/1541895.1541909,crossref,"<jats:p>Model-driven game development (MDGD) is an emerging paradigm where models become first-order elements in game development, maintenance, and evolution. In this article, we present a first approach to 2D platform game prototyping automatization through the use of model-driven engineering (MDE). Platform-independent models (PIM) define the structure and the behavior of the games and a platform-specific model (PSM) describes the game control mapping. Automatic MOFscript transformations from these models generate the software prototype code in C++. As an example, Bubble Bobble has been prototyped in a few hours following the MDGD approach. The resulting code generation represents 93% of the game prototype.</jats:p>","model-driven game development (mdgd) is an emerging paradigm where models become first-order elements in game development, maintenance, and evolution. in this article, we present a first approach to 2d platform game prototyping automatization through the use of model-driven engineering (mde). platform-independent models (pim) define the structure and the behavior of the games and a platform-specific model (psm) describes the game control mapping. automatic mofscript transformations from these models generate the software prototype code in c++. as an example, bubble bobble has been prototyped in a few hours following the mdgd approach. the resulting code generation represents 93% of the game prototype."
http://orkg.org/orkg/resource/R25783,Pd@C core–shell nanoparticles on carbon nanotubes as highly stable and selective catalysts for hydrogenation of acetylene to ethylene,10.1039/c7nr04992g,crossref,<p>Highly stable and selective Pd-based catalyst was synthesized by covering supported Pd nanoparticles with an N-doped carbon shell for acetylene hydrogenation.</p>,highly stable and selective pd-based catalyst was synthesized by covering supported pd nanoparticles with an n-doped carbon shell for acetylene hydrogenation.
http://orkg.org/orkg/resource/R25858,Beyond the use of modifiers in selective alkyne hydrogenation: silver and gold nanocatalysts in flow mode for sustainable alkene production,10.1039/c4nr02777a,crossref,<p>Supported silver and gold nanoparticles are highly stereo and chemoselective catalysts for the three-phase hydrogenation of alkynes in continuous mode.</p>,supported silver and gold nanoparticles are highly stereo and chemoselective catalysts for the three-phase hydrogenation of alkynes in continuous mode.
http://orkg.org/orkg/resource/R25876,A Pd- Cu2O nanocomposite as an effective synergistic catalyst for selective semi-hydrogenation of the terminal alkynes only,10.1039/c6cc00143b,crossref,"<p>A new type lead-free Pd–Cu<sub>2</sub>O nanocomposite catalyst shows “double” selectivities for hydrogenation of alkynes: only terminal alkynes hydrogenated and only alkenes produced, <italic>i.e.</italic> no internal alkyne is hydrogenated.</p>","a new type lead-free pd–cu 2 o nanocomposite catalyst shows “double” selectivities for hydrogenation of alkynes: only terminal alkynes hydrogenated and only alkenes produced, i.e. no internal alkyne is hydrogenated."
http://orkg.org/orkg/resource/R25892,Palladium–gold single atom alloy catalysts for liquid phase selective hydrogenation of 1-hexyne,10.1039/c7cy00794a,crossref,<p>Silica supported and unsupported PdAu single atom alloys (SAAs) were investigated for the selective hydrogenation of 1-hexyne to hexenes under mild conditions.</p>,silica supported and unsupported pdau single atom alloys (saas) were investigated for the selective hydrogenation of 1-hexyne to hexenes under mild conditions.
http://orkg.org/orkg/resource/R25906,"LEPTO dipstick, a dipstick assay for detection of Leptospira-specific immunoglobulin M antibodies in human sera.",10.1128/jcm.35.1.92-97.1997,crossref,"<jats:p>We studied a dipstick assay for the detection of Leptospira-specific immunoglobulin M (IgM) antibodies in human serum samples. A high degree of concordance was observed between the results of the dipstick assay and an IgM enzyme-linked immunosorbent assay (ELISA). Application of the dipstick assay for the detection of acute leptospirosis enabled the accurate identification, early in the disease, of a high proportion of the cases of leptospirosis. Analysis of a second serum sample is recommended, in order to determine seroconversion or increased staining intensity. All serum samples from the patients who were confirmed to be positive for leptospirosis by either a positive microscopic agglutination test or a positive culture but were found to be negative by the dipstick assay were also judged to be negative by the IgM ELISA or revealed borderline titers by the IgM ELISA. Some cross-reactivity was observed for sera from patients with diseases other than leptospirosis, and this should be taken into account in the interpretation of test results. The dipstick assay is easy to perform, can be performed quickly, and requires no electricity or special equipment, and the assay components, a dipstick and a staining reagent, can be stored for a prolonged period without a loss of reactivity, even at elevated temperatures.</jats:p>","we studied a dipstick assay for the detection of leptospira-specific immunoglobulin m (igm) antibodies in human serum samples. a high degree of concordance was observed between the results of the dipstick assay and an igm enzyme-linked immunosorbent assay (elisa). application of the dipstick assay for the detection of acute leptospirosis enabled the accurate identification, early in the disease, of a high proportion of the cases of leptospirosis. analysis of a second serum sample is recommended, in order to determine seroconversion or increased staining intensity. all serum samples from the patients who were confirmed to be positive for leptospirosis by either a positive microscopic agglutination test or a positive culture but were found to be negative by the dipstick assay were also judged to be negative by the igm elisa or revealed borderline titers by the igm elisa. some cross-reactivity was observed for sera from patients with diseases other than leptospirosis, and this should be taken into account in the interpretation of test results. the dipstick assay is easy to perform, can be performed quickly, and requires no electricity or special equipment, and the assay components, a dipstick and a staining reagent, can be stored for a prolonged period without a loss of reactivity, even at elevated temperatures."
http://orkg.org/orkg/resource/R25908,"Development, Characterization, and Diagnostic Applications of Monoclonal Antibodies against Bovine Rotavirus",10.1128/cdli.7.2.288-292.2000,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>Hybridomas secreting monoclonal antibodies (MAbs) against the Nebraska calf diarrhea strain of bovine rotavirus (BRV) were characterized. Indirect fluorescent-antibody assay, immunodot assay, and immunoprecipitation were used to select hybridomas that produced anti-BRV MAbs. Seven of the MAbs were shown by sodium dodecyl sulfate-polyacrylamide gel electrophoresis and Western blot assay to be reactive with the BRV outer capsid protein, VP7, which has a molecular mass of 37.5 kDa. None of the seven MAbs were reactive with canine rotavirus, bovine coronavirus, or uninfected Madin-Darby bovine kidney cells. Two clones, 8B4 (immunoglobulin G2a [IgG2a]) and 2B11 (IgG1), were found suitable for use in an antigen capture enzyme-linked immunosorbent assay for detecting BRV in bovine fecal samples. Both were subtype A specific (G6 subtype) but did not react with all isolates of BRV group A.</jats:p>","abstract \n hybridomas secreting monoclonal antibodies (mabs) against the nebraska calf diarrhea strain of bovine rotavirus (brv) were characterized. indirect fluorescent-antibody assay, immunodot assay, and immunoprecipitation were used to select hybridomas that produced anti-brv mabs. seven of the mabs were shown by sodium dodecyl sulfate-polyacrylamide gel electrophoresis and western blot assay to be reactive with the brv outer capsid protein, vp7, which has a molecular mass of 37.5 kda. none of the seven mabs were reactive with canine rotavirus, bovine coronavirus, or uninfected madin-darby bovine kidney cells. two clones, 8b4 (immunoglobulin g2a [igg2a]) and 2b11 (igg1), were found suitable for use in an antigen capture enzyme-linked immunosorbent assay for detecting brv in bovine fecal samples. both were subtype a specific (g6 subtype) but did not react with all isolates of brv group a."
http://orkg.org/orkg/resource/R25914,Diagnosis of Schistosomiasis by Reagent Strip Test for Detection of Circulating Cathodic Antigen,10.1128/jcm.42.12.5458-5461.2004,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            A newly developed reagent strip assay for the diagnosis of schistosomiasis based on parasite antigen detection in urine of infected individuals was evaluated. The test uses the principle of lateral flow through a nitrocellulose strip of the sample mixed with a colloidal carbon conjugate of a monoclonal antibody specific for\n            <jats:italic>Schistosoma</jats:italic>\n            circulating cathodic antigen (CCA). The strip assay to diagnose a group of highly infected schoolchildren in Mwanza, Tanzania, demonstrated a high sensitivity and association with the intensity of infection as measured both by egg counts, and by circulating anodic antigen and CCA levels determined by enzyme-linked immunosorbent assay. A specificity of ca. 90% was shown in a group of schistosome-negative schoolchildren from Tarime, Tanzania, an area where schistosomiasis is not endemic. The test is easy to perform and requires no technical equipment or special training. The stability of the strips and the conjugate in the dry format lasts for at least 3 months at ambient temperature in sealed packages, making it suitable for transport and use in areas where schistosomiasis is endemic. This assay can easily be developed to an end-user format.\n          </jats:p>","abstract \n \n a newly developed reagent strip assay for the diagnosis of schistosomiasis based on parasite antigen detection in urine of infected individuals was evaluated. the test uses the principle of lateral flow through a nitrocellulose strip of the sample mixed with a colloidal carbon conjugate of a monoclonal antibody specific for\n schistosoma \n circulating cathodic antigen (cca). the strip assay to diagnose a group of highly infected schoolchildren in mwanza, tanzania, demonstrated a high sensitivity and association with the intensity of infection as measured both by egg counts, and by circulating anodic antigen and cca levels determined by enzyme-linked immunosorbent assay. a specificity of ca. 90% was shown in a group of schistosome-negative schoolchildren from tarime, tanzania, an area where schistosomiasis is not endemic. the test is easy to perform and requires no technical equipment or special training. the stability of the strips and the conjugate in the dry format lasts for at least 3 months at ambient temperature in sealed packages, making it suitable for transport and use in areas where schistosomiasis is endemic. this assay can easily be developed to an end-user format.\n"
http://orkg.org/orkg/resource/R25925,One-step competitive immunochromatographic assay for semiquantitative determination of lipoprotein(a) in plasma,10.1093/clinchem/39.4.619,crossref,"""<jats:title>Abstract</jats:title>\n               <jats:p>Numerous studies have associated high concentrations of lipoprotein(a) [Lp(a)] with atherosclerosis. We developed a rapid, one-step competitive immunochromatographic assay to measure Lp(a) in plasma. The assay is performed on a nitrocellulose membrane strip and the result is determined by a visual readout of rust-colored colloidal selenium. The assay is based on the principle that Lp(a) in the sample will compete with Lp(a)-coated colloidal selenium for binding to the anti-Lp(a) monoclonal antibody immobilized on the assay strip in the format of four ladder bars. The number of capture bars that appear as a result of the formation of colloidal selenium color is proportional to the concentration of the Lp(a) protein in the samples. The strip assay semiquantitatively measures Lp(a) concentrations ranging from 0 to 180 mg/L of Lp(a) protein in serum, plasma, or fingerstick whole-blood samples. This assay appears very useful for quick identification of individuals with above-normal concentrations of plasma Lp(a) protein (&amp;gt; 70 mg/L), and has potential for monitoring a patient's response to treatment with Lp(a)-lowering drugs.</jats:p>""",""" abstract \n numerous studies have associated high concentrations of lipoprotein(a) [lp(a)] with atherosclerosis. we developed a rapid, one-step competitive immunochromatographic assay to measure lp(a) in plasma. the assay is performed on a nitrocellulose membrane strip and the result is determined by a visual readout of rust-colored colloidal selenium. the assay is based on the principle that lp(a) in the sample will compete with lp(a)-coated colloidal selenium for binding to the anti-lp(a) monoclonal antibody immobilized on the assay strip in the format of four ladder bars. the number of capture bars that appear as a result of the formation of colloidal selenium color is proportional to the concentration of the lp(a) protein in the samples. the strip assay semiquantitatively measures lp(a) concentrations ranging from 0 to 180 mg/l of lp(a) protein in serum, plasma, or fingerstick whole-blood samples. this assay appears very useful for quick identification of individuals with above-normal concentrations of plasma lp(a) protein (&amp;gt; 70 mg/l), and has potential for monitoring a patient's response to treatment with lp(a)-lowering drugs. """
http://orkg.org/orkg/resource/R25927,Lateral Flow Immunoassay Using Europium (III) Chelate Microparticles and Time-Resolved Fluorescence for Eosinophils and Neutrophils in Whole Blood,10.1373/clinchem.2006.074021,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Background: A simple point-of-care method for measuring leukocyte counts in a doctor’s office or emergency room could be of great importance. We developed a protocol for measuring cell count by disrupting the cell membrane and analyzing specific proteins within the cells and used it to analyze proteins from eosinophils and neutrophils.</jats:p>\n               <jats:p>Methods: Lateral immunochromatographic (ICR) assays have been developed for eosinophil protein X (EPX) and human neutrophil lipocalin (HNL) as measures of the concentration of eosinophils and neutrophils. The correlation between the lateral ICR assays and cell counting of eosinophils and neutrophils was performed manually and with an automated cell counter. RIA assays measuring the same analytes were also compared with the results from cell counting and lateral ICR assays.</jats:p>\n               <jats:p>Results: The optimized assays showed analytical detection limits below the clinical ranges of 3.36 μg/L and 2.05 μg/L for EPX and HNL, respectively. The recovery was 114.8%–122.8% for EPX and 94.5%–96.9% for HNL. The imprecision was 3%–17% CV for EPX over the whole range and 5%–16% CV for HNL. The correlation coefficients between manually counted cells and lateral ICR assays were 0.9 and 0.83 for EPX and HNL, respectively.</jats:p>\n               <jats:p>Conclusion: The numbers of eosinophils and neutrophils in small amounts of blood can be estimated in the point-of-care setting by means of fast lateral ICR assays of EPX and HNL.</jats:p>","abstract \n background: a simple point-of-care method for measuring leukocyte counts in a doctor’s office or emergency room could be of great importance. we developed a protocol for measuring cell count by disrupting the cell membrane and analyzing specific proteins within the cells and used it to analyze proteins from eosinophils and neutrophils. \n methods: lateral immunochromatographic (icr) assays have been developed for eosinophil protein x (epx) and human neutrophil lipocalin (hnl) as measures of the concentration of eosinophils and neutrophils. the correlation between the lateral icr assays and cell counting of eosinophils and neutrophils was performed manually and with an automated cell counter. ria assays measuring the same analytes were also compared with the results from cell counting and lateral icr assays. \n results: the optimized assays showed analytical detection limits below the clinical ranges of 3.36 μg/l and 2.05 μg/l for epx and hnl, respectively. the recovery was 114.8%–122.8% for epx and 94.5%–96.9% for hnl. the imprecision was 3%–17% cv for epx over the whole range and 5%–16% cv for hnl. the correlation coefficients between manually counted cells and lateral icr assays were 0.9 and 0.83 for epx and hnl, respectively. \n conclusion: the numbers of eosinophils and neutrophils in small amounts of blood can be estimated in the point-of-care setting by means of fast lateral icr assays of epx and hnl."
http://orkg.org/orkg/resource/R25949,"Rapid, Sensitive, and Specific Lateral-Flow Immunochromatographic Device To Measure Anti-Anthrax Protective Antigen Immunoglobulin G in Serum and Whole Blood",10.1128/cvi.13.5.541-546.2006,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            Evidence from animals suggests that anti-anthrax protective antigen (PA) immunoglobulin G (IgG) from vaccination with anthrax vaccine adsorbed (AVA) is protective against\n            <jats:italic>Bacillus anthracis</jats:italic>\n            infection. Measurement of anti-PA IgG in human sera can be performed using either enzyme-linked immunosorbent assay or fluorescent covalent microsphere immunoassay (ELISA) (R. E. Biagini, D. L. Sammons, J. P. Smith, B. A. MacKenzie, C. A. Striley, V. Semenova, E. Steward-Clark, K. Stamey, A. E. Freeman, C. P. Quinn, and J. E. Snawder, Clin. Diagn. Lab. Immunol. 11:50-55, 2004). Both these methods are laboratory based. We describe the development of a rapid lateral-flow immunochromatographic assay (LFIA) test kit for the measurement of anti-PA IgG in serum or whole-blood samples (30-μl samples) using colloidal gold nanoparticles as the detection reagent and an internal control. Using sera from 19 anthrax AVA vaccinees (anti-PA IgG range, 2.4 to 340 μg/ml) and 10 controls and PA-supplemented whole-blood samples, we demonstrated that the LFIA had a sensitivity of approximately 3 μg/ml anti-PA IgG in serum and ∼14 μg/ml anti-PA IgG in whole blood. Preabsorption of sera with PA yielded negative anti-PA LFIAs. The diagnostic sensitivity and specificity of the assay were 100% using ELISA-measured anti-PA IgG as the standard. This kit has utility in determining anti-PA antibody reactivity in the sera of individuals vaccinated with AVA or individuals with clinical anthrax.\n          </jats:p>","abstract \n \n evidence from animals suggests that anti-anthrax protective antigen (pa) immunoglobulin g (igg) from vaccination with anthrax vaccine adsorbed (ava) is protective against\n bacillus anthracis \n infection. measurement of anti-pa igg in human sera can be performed using either enzyme-linked immunosorbent assay or fluorescent covalent microsphere immunoassay (elisa) (r. e. biagini, d. l. sammons, j. p. smith, b. a. mackenzie, c. a. striley, v. semenova, e. steward-clark, k. stamey, a. e. freeman, c. p. quinn, and j. e. snawder, clin. diagn. lab. immunol. 11:50-55, 2004). both these methods are laboratory based. we describe the development of a rapid lateral-flow immunochromatographic assay (lfia) test kit for the measurement of anti-pa igg in serum or whole-blood samples (30-μl samples) using colloidal gold nanoparticles as the detection reagent and an internal control. using sera from 19 anthrax ava vaccinees (anti-pa igg range, 2.4 to 340 μg/ml) and 10 controls and pa-supplemented whole-blood samples, we demonstrated that the lfia had a sensitivity of approximately 3 μg/ml anti-pa igg in serum and ∼14 μg/ml anti-pa igg in whole blood. preabsorption of sera with pa yielded negative anti-pa lfias. the diagnostic sensitivity and specificity of the assay were 100% using elisa-measured anti-pa igg as the standard. this kit has utility in determining anti-pa antibody reactivity in the sera of individuals vaccinated with ava or individuals with clinical anthrax.\n"
http://orkg.org/orkg/resource/R25957,A dipstick immunoassay to rapidly measure serum oestrone sulfate concentrations in horses,10.1071/rd00062,crossref,"<jats:p>\nA dipstick, competitive immunoassay for rapidly measuring serum oestrone\nsulfate (OS) concentrations in horses was developed to distinguish mares 100\nor more days pregnant from non-pregnant animals. 6-Ketoestrone\n6-carboxymethyloxime conjugated to bovine serum albumin (oestrone CMO-BSA) was\n‘dotted’ 25 mm from the bottom edge of 45 5 mm strips of\npolyester-film-supported cellulose nitrate membrane, pore size 3 m. The strips\nwere blocked, dried and a 15 5-mm cellulose absorbent sink attached 10 mm from\nthe top of each strip. The manufactured dipsticks were stored with desiccant\nat room temperature until used. A monoclonal antibody recognizing OS was\ncoated onto uniform, blue-dyed polystyrene microspheres (mean diameter, 0.31\nm) by adsorption. After blocking, several washes and resuspension by\nsonication, the antibody-coated microspheres were stored at 4˚C. The\nconcentrations of oestrone CMO-BSA dotted onto the dipsticks and OS antibody\ncoated onto the microspheres were optimized to produce a test that allowed\nmaximum discrimination between the concentrations of OS found in serum of\nmares 100 or more days pregnant (i.e. &gt;30 ng OS mL\n–1 ) relative to those found in non-pregnant mares\n(i.e. &lt;10 ng OS mL –1 ). To perform the\ndipstick test, 30 L of carrier buffer, 10 L of OS antibody-coated microspheres\nand 10 l of OS standard or serum sample were pipetted into a microwell and\nmixed. A dipstick was placed in the solution. All the liquid migrated up the\ndipstick into the absorbent sink within 15–20 min leaving a blue dot\nwhere the OCMO-BSA had been placed. The intensity of colour of the blue dot,\nwhich correlated inversely with the concentration of OS in the standard or\nserum sample, was assessed visually and by computer image analysis. An OS\nconcentration less than 5 ng mL –1 produced a deep\nblue dot, 20 ng/ml a light blue dot and a concentration greater than 50 ng\nmL –1 a very faint blue dot, or none at all. Serum\nsamples from 42 non-pregnant mares and 40 mares over 100 days pregnant were\nanalysed by the dipstick test. All the serum samples from non-pregnant mares\nproduced dipsticks with deep blue dots that ranged in intensity from 20 to 38\ncolour intensity units, equivalent to OS concentrations less than 7 ng mL\n–1 . Sera from all the pregnant mares generated\ndipsticks with either faint blue dots or none at all (i.e. £=5\ncolour intensity units, equivalent to OS concentrations &gt;40 ng mL\n–1 ). It is concluded that this novel, rapid\ndipstick immunoassay offers a practical, alternative means of analysing serum\nOS concentrations in horses, and enables mares that are 100 or more days\npregnant to be distinguished from those that are not pregnant.</jats:p>","\na dipstick, competitive immunoassay for rapidly measuring serum oestrone\nsulfate (os) concentrations in horses was developed to distinguish mares 100\nor more days pregnant from non-pregnant animals. 6-ketoestrone\n6-carboxymethyloxime conjugated to bovine serum albumin (oestrone cmo-bsa) was\n‘dotted’ 25 mm from the bottom edge of 45 5 mm strips of\npolyester-film-supported cellulose nitrate membrane, pore size 3 m. the strips\nwere blocked, dried and a 15 5-mm cellulose absorbent sink attached 10 mm from\nthe top of each strip. the manufactured dipsticks were stored with desiccant\nat room temperature until used. a monoclonal antibody recognizing os was\ncoated onto uniform, blue-dyed polystyrene microspheres (mean diameter, 0.31\nm) by adsorption. after blocking, several washes and resuspension by\nsonication, the antibody-coated microspheres were stored at 4˚c. the\nconcentrations of oestrone cmo-bsa dotted onto the dipsticks and os antibody\ncoated onto the microspheres were optimized to produce a test that allowed\nmaximum discrimination between the concentrations of os found in serum of\nmares 100 or more days pregnant (i.e. &gt;30 ng os ml\n–1 ) relative to those found in non-pregnant mares\n(i.e. &lt;10 ng os ml –1 ). to perform the\ndipstick test, 30 l of carrier buffer, 10 l of os antibody-coated microspheres\nand 10 l of os standard or serum sample were pipetted into a microwell and\nmixed. a dipstick was placed in the solution. all the liquid migrated up the\ndipstick into the absorbent sink within 15–20 min leaving a blue dot\nwhere the ocmo-bsa had been placed. the intensity of colour of the blue dot,\nwhich correlated inversely with the concentration of os in the standard or\nserum sample, was assessed visually and by computer image analysis. an os\nconcentration less than 5 ng ml –1 produced a deep\nblue dot, 20 ng/ml a light blue dot and a concentration greater than 50 ng\nml –1 a very faint blue dot, or none at all. serum\nsamples from 42 non-pregnant mares and 40 mares over 100 days pregnant were\nanalysed by the dipstick test. all the serum samples from non-pregnant mares\nproduced dipsticks with deep blue dots that ranged in intensity from 20 to 38\ncolour intensity units, equivalent to os concentrations less than 7 ng ml\n–1 . sera from all the pregnant mares generated\ndipsticks with either faint blue dots or none at all (i.e. £=5\ncolour intensity units, equivalent to os concentrations &gt;40 ng ml\n–1 ). it is concluded that this novel, rapid\ndipstick immunoassay offers a practical, alternative means of analysing serum\nos concentrations in horses, and enables mares that are 100 or more days\npregnant to be distinguished from those that are not pregnant."
http://orkg.org/orkg/resource/R26035,Stresses in Adhesively Bonded Joints: A Closed-Form Solution,10.1177/002199838101500305,crossref,"<jats:p>In this paper the general plane strain problem of adhesively bonded struc tures which consist of two different orthotropic adherends is considered. Assuming that the thicknesses of the adherends are constant and are small in relation to the lateral dimensions of the bonded region, the adherends are treated as plates. Also, assuming that the thickness of the adhesive is small compared to that of the adherends, the thickness variation of the stresses in the adhesive layer is neglected. However, the transverse shear effects in the adherends and the in-plane normal strain in the adhesive are taken into ac count. The problem is reduced to a system of differential equations for the adhesive stresses which is solved in closed form. A single lap joint and a stif fened plate under various loading conditions are considered as examples. To verify the basic trend of the solutions obtained from the plate theory and to give some idea about the validity of the plate assumption itself, a sample pro blem is solved by using the finite element method and by treating the adherends and the adhesive as elastic continua. It is found that the plate theory used in the analysis not only predicts the correct trend for the adhesive stresses but also gives rather surprisingly accurate results. The solution is ob tained by assuming linear stress-strain relations for the adhesive. In the Ap pendix the problem is formulated by using a nonlinear material for the adhesive and by following two different approaches.</jats:p>","in this paper the general plane strain problem of adhesively bonded struc tures which consist of two different orthotropic adherends is considered. assuming that the thicknesses of the adherends are constant and are small in relation to the lateral dimensions of the bonded region, the adherends are treated as plates. also, assuming that the thickness of the adhesive is small compared to that of the adherends, the thickness variation of the stresses in the adhesive layer is neglected. however, the transverse shear effects in the adherends and the in-plane normal strain in the adhesive are taken into ac count. the problem is reduced to a system of differential equations for the adhesive stresses which is solved in closed form. a single lap joint and a stif fened plate under various loading conditions are considered as examples. to verify the basic trend of the solutions obtained from the plate theory and to give some idea about the validity of the plate assumption itself, a sample pro blem is solved by using the finite element method and by treating the adherends and the adhesive as elastic continua. it is found that the plate theory used in the analysis not only predicts the correct trend for the adhesive stresses but also gives rather surprisingly accurate results. the solution is ob tained by assuming linear stress-strain relations for the adhesive. in the ap pendix the problem is formulated by using a nonlinear material for the adhesive and by following two different approaches."
http://orkg.org/orkg/resource/R26043,Development of a full elasto-plastic adhesive joint design analysis,10.1243/03093247v274211,crossref,"<jats:p>A previous adhesive joint analysis that accommodated non-linear adhesive behaviour is extended to model the elasto-plastic response of the adherends. The resulting analysis models the joint as an adherend-adhesive sandwich capable of sustaining any combination of end load conditions, thus enabling a wide variety of adhesive joints to be modelled. The adhesive is assumed to behave as a coupled set of non-linear shear and tension springs, and the adherends as cylindrically bent plates which yield under the action of combined tension and bending. The complete problem is reduced to a set of six non-linear first-order ordinary differential equations which are solved numerically using a finite-difference method. In this way a reasonable assessment of adhesive stresses and strains can be obtained easily, without resorting to the complexity of a two-dimensional finite element solution. A comparison between the results from these two methods has been made and is presented in this paper after the outline of the analysis derivations.</jats:p>","a previous adhesive joint analysis that accommodated non-linear adhesive behaviour is extended to model the elasto-plastic response of the adherends. the resulting analysis models the joint as an adherend-adhesive sandwich capable of sustaining any combination of end load conditions, thus enabling a wide variety of adhesive joints to be modelled. the adhesive is assumed to behave as a coupled set of non-linear shear and tension springs, and the adherends as cylindrically bent plates which yield under the action of combined tension and bending. the complete problem is reduced to a set of six non-linear first-order ordinary differential equations which are solved numerically using a finite-difference method. in this way a reasonable assessment of adhesive stresses and strains can be obtained easily, without resorting to the complexity of a two-dimensional finite element solution. a comparison between the results from these two methods has been made and is presented in this paper after the outline of the analysis derivations."
http://orkg.org/orkg/resource/R26077,Perceived Importance of the Quality of the Indoor Environment in Commercial Buildings,10.1177/1420326x07080463,crossref,"<jats:p> Recognition of the importance of the quality of the indoor environment (IEQ) to health, comfort and productivity of building end users has produced increasing numbers of voluntary schemes whose assessment embraces a wide spectrum of environmental attributes. Studies which aim to derive appropriate weighting factors for these attributes through soliciting the perceived importance from experts are abundant. This article reports the findings of a study which, based on face-to-face interviews with 548 end users and 66 building professionals, processed their perceived importance of IEQ using the analytical hierarchy process (AHP). Attributes included were thermal comfort, air cleanliness, odor and noise associated with the air conditioning system of typical commercial buildings. Correlation analysis of the ranking results of the AHP weights revealed the difference in perceived importance of the attributes according to gender of the respondents. Other factors also found to have influence on the perceived importance of the IEQ were whether the respondents were professionals or other end users and the reason for them working or visiting the buildings and the duration of their stay. These all varied with psychophysical factors such as personal experiences, needs and expectations. Further work is needed to study whether the weighting factors should be derived from the perceptions of experts, end users, or a balance between the two. </jats:p>","recognition of the importance of the quality of the indoor environment (ieq) to health, comfort and productivity of building end users has produced increasing numbers of voluntary schemes whose assessment embraces a wide spectrum of environmental attributes. studies which aim to derive appropriate weighting factors for these attributes through soliciting the perceived importance from experts are abundant. this article reports the findings of a study which, based on face-to-face interviews with 548 end users and 66 building professionals, processed their perceived importance of ieq using the analytical hierarchy process (ahp). attributes included were thermal comfort, air cleanliness, odor and noise associated with the air conditioning system of typical commercial buildings. correlation analysis of the ranking results of the ahp weights revealed the difference in perceived importance of the attributes according to gender of the respondents. other factors also found to have influence on the perceived importance of the ieq were whether the respondents were professionals or other end users and the reason for them working or visiting the buildings and the duration of their stay. these all varied with psychophysical factors such as personal experiences, needs and expectations. further work is needed to study whether the weighting factors should be derived from the perceptions of experts, end users, or a balance between the two."
http://orkg.org/orkg/resource/R26156,A Combined Vehicle Routing and Inventory Allocation Problem,10.1287/opre.32.5.1019,crossref,"<jats:p> We address the combined problem of allocating a scarce resource among several locations, and planning deliveries using a fleet of vehicles. Demands are random, and holding and shortage costs must be considered in the decision along with transportation costs. We show how to extend some of the available methods for the deterministic vehicle routing problem to this case. Computational results using one such adaptation show that the algorithm is fast enough for practical work, and that substantial cost savings can be achieved with this approach. </jats:p>","we address the combined problem of allocating a scarce resource among several locations, and planning deliveries using a fleet of vehicles. demands are random, and holding and shortage costs must be considered in the decision along with transportation costs. we show how to extend some of the available methods for the deterministic vehicle routing problem to this case. computational results using one such adaptation show that the algorithm is fast enough for practical work, and that substantial cost savings can be achieved with this approach."
http://orkg.org/orkg/resource/R26167,An Allocation and Distribution Model for Perishable Products,10.1287/opre.34.1.75,crossref,"<jats:p> This paper presents an allocation model for a perishable product, distributed from a regional center to a given set of locations with random demands. We consider the combined problem of allocating the available inventory at the center while deciding how these deliveries should be performed. Two types of delivery patterns are analyzed: the first pattern assumes that all demand points receive individual deliveries; the second pattern subsumes the frequently occurring case in which deliveries are combined in multistop routes traveled by a fleet of vehicles. Computational experience is reported. </jats:p>","this paper presents an allocation model for a perishable product, distributed from a regional center to a given set of locations with random demands. we consider the combined problem of allocating the available inventory at the center while deciding how these deliveries should be performed. two types of delivery patterns are analyzed: the first pattern assumes that all demand points receive individual deliveries; the second pattern subsumes the frequently occurring case in which deliveries are combined in multistop routes traveled by a fleet of vehicles. computational experience is reported."
http://orkg.org/orkg/resource/R26173,An Integrated Inventory Allocation and Vehicle Routing Problem,10.1287/trsc.23.2.67,crossref,"<jats:p> We address the problem of distributing a limited amount of inventory among customers using a fleet of vehicles so as to maximize profit. Both the inventory allocation and the vehicle routing problems are important logistical decisions. In many practical situations, these two decisions are closely interrelated, and therefore, require a systematic approach to take into account both activities jointly. We formulate the integrated problem as a mixed integer program and develop a Lagrangian-based procedure to generate both good upper bounds and heuristic solutions. Computational results show that the procedure is able to generate solutions with small gaps between the upper and lower bounds for a wide range of cost structures. </jats:p>","we address the problem of distributing a limited amount of inventory among customers using a fleet of vehicles so as to maximize profit. both the inventory allocation and the vehicle routing problems are important logistical decisions. in many practical situations, these two decisions are closely interrelated, and therefore, require a systematic approach to take into account both activities jointly. we formulate the integrated problem as a mixed integer program and develop a lagrangian-based procedure to generate both good upper bounds and heuristic solutions. computational results show that the procedure is able to generate solutions with small gaps between the upper and lower bounds for a wide range of cost structures."
http://orkg.org/orkg/resource/R26175,Stochastic Inventory Routing: Route Design with Stockouts and Route Failures,10.1287/trsc.26.3.171,crossref,"""<jats:p> The stochastic inventory routing problem involves the distribution of a commodity such as heating oil over a long period of time to a large set of customers. The customers maintain a local inventory of the commodity which they consume at a daily rate. Their consumption varies daily and seasonally and their exact demand is known only upon the arrival of the delivery vehicle. This paper presentes a detailed analysis of this problem incorporating the stochastic nature of customers' consumptions and the possibility of route failures when the actual demand on a route exceeds the capacity of a vehicle. A number of solution procedures are compared on a large set of real life data for a period of 12 consecutive weeks. The winning strategy, though computationally more expensive, provides the best system performance and reduces (almost eliminates) the stockout phenomena. </jats:p>""",""" the stochastic inventory routing problem involves the distribution of a commodity such as heating oil over a long period of time to a large set of customers. the customers maintain a local inventory of the commodity which they consume at a daily rate. their consumption varies daily and seasonally and their exact demand is known only upon the arrival of the delivery vehicle. this paper presentes a detailed analysis of this problem incorporating the stochastic nature of customers' consumptions and the possibility of route failures when the actual demand on a route exceeds the capacity of a vehicle. a number of solution procedures are compared on a large set of real life data for a period of 12 consecutive weeks. the winning strategy, though computationally more expensive, provides the best system performance and reduces (almost eliminates) the stockout phenomena. """
http://orkg.org/orkg/resource/R26181,Dynamic allocations for multi-product distribution,10.1287/trsc.29.3.256,crossref,"<jats:p> Consider the problem of allocating multiple products by a distributor with limited capacity (truck size), who has a fixed sequence of customers (retailers) whose demands are unknown. Each time the distributor visits a customer, he gets information about the realization of the demand for this customer, but he does not yet know the demands of the following customers. The decision faced by the distributor is how much to allocate to each customer given that the penalties for not satisfying demand are not identical. In addition, we optimally solve the problem of loading the truck with the multiple products, given the limited storage capacity. This framework can also be used for the general problem of seat allocation in the airline industry. As with the truck in the distribution problem, the airplane has limited capacity. A critical decision is how to allocate the available seats between early and late reservations (sequence of customers), for the different fare classes (multiple products), where the revenues from discount (early) and regular (late) passengers are different. </jats:p>","consider the problem of allocating multiple products by a distributor with limited capacity (truck size), who has a fixed sequence of customers (retailers) whose demands are unknown. each time the distributor visits a customer, he gets information about the realization of the demand for this customer, but he does not yet know the demands of the following customers. the decision faced by the distributor is how much to allocate to each customer given that the penalties for not satisfying demand are not identical. in addition, we optimally solve the problem of loading the truck with the multiple products, given the limited storage capacity. this framework can also be used for the general problem of seat allocation in the airline industry. as with the truck in the distribution problem, the airplane has limited capacity. a critical decision is how to allocate the available seats between early and late reservations (sequence of customers), for the different fare classes (multiple products), where the revenues from discount (early) and regular (late) passengers are different."
http://orkg.org/orkg/resource/R26189,A decomposition approach to the inventory routing problem with satellite facilities,10.1287/trsc.32.2.189,crossref,"<jats:p> This paper presents a comprehensive decomposition scheme for solving the inventory routing problem in which a central supplier must restock a subset of customers on an intermittent basis. In this setting, the customer demand is not known with certainty and routing decisions taken over the short run might conflict with the long-run goal of minimizing annual operating costs. A unique aspect of the short-run subproblem is the presence of satellite facilities where vehicles can be reloaded and customer deliveries continued until the closing time is reached. Three heuristics have been developed to solve the vehicle routing problem with satellite facilities (randomized Clarke-Wright, GRASP, modified sweep). After the daily tours are derived, a parametric analysis is conducted to investigate the tradeoff between distance and annual costs. This leads to the development of the efficient frontier from which the decision maker is free to choose the most attractive alternative. The proposed procedures are tested on data sets generated from field experience with a national liquid propane distributor. </jats:p>","this paper presents a comprehensive decomposition scheme for solving the inventory routing problem in which a central supplier must restock a subset of customers on an intermittent basis. in this setting, the customer demand is not known with certainty and routing decisions taken over the short run might conflict with the long-run goal of minimizing annual operating costs. a unique aspect of the short-run subproblem is the presence of satellite facilities where vehicles can be reloaded and customer deliveries continued until the closing time is reached. three heuristics have been developed to solve the vehicle routing problem with satellite facilities (randomized clarke-wright, grasp, modified sweep). after the daily tours are derived, a parametric analysis is conducted to investigate the tradeoff between distance and annual costs. this leads to the development of the efficient frontier from which the decision maker is free to choose the most attractive alternative. the proposed procedures are tested on data sets generated from field experience with a national liquid propane distributor."
http://orkg.org/orkg/resource/R26192,Deliveries in an inventory/routing problem using stochastic dynamic programming,10.1287/trsc.35.2.192.10134,crossref,"<jats:p> An industrial gases tanker vehicle visits n customers on a tour, with a possible (n + 1)st customer added at the end. The amount of needed product at each customer is a known random process, typically a Wiener process. The objective is to adjust dynamically the amount of product provided on scene to each customer so as to minimize total expected costs, comprising costs of earliness, lateness, product shortfall, and returning to the depot nonempty. Earliness costs are computed by invocation of an annualized incremental cost argument. Amounts of product delivered to each customer are not known until the driver is on scene at the customer location, at which point the customer is either restocked to capacity or left with some residual empty capacity, the policy determined by stochastic dynamic programming. The methodology has applications beyond industrial gases. </jats:p>","an industrial gases tanker vehicle visits n customers on a tour, with a possible (n + 1)st customer added at the end. the amount of needed product at each customer is a known random process, typically a wiener process. the objective is to adjust dynamically the amount of product provided on scene to each customer so as to minimize total expected costs, comprising costs of earliness, lateness, product shortfall, and returning to the depot nonempty. earliness costs are computed by invocation of an annualized incremental cost argument. amounts of product delivered to each customer are not known until the driver is on scene at the customer location, at which point the customer is either restocked to capacity or left with some residual empty capacity, the policy determined by stochastic dynamic programming. the methodology has applications beyond industrial gases."
http://orkg.org/orkg/resource/R26196,Improving the distribution of industrial gases with an on-line computerized routing and scheduling optimizer,10.1287/inte.13.6.4,crossref,"<jats:p> For Air Products and Chemicals, Inc., inventory management of industrial gases at customer locations is integrated with vehicle scheduling and dispatching. Their advanced decision support system includes on-line data entry functions, customer usage forecasting, a time/distance network with a shortest path algorithm to compute intercustomer travel times and distances, a mathematical optimization module to produce daily delivery schedules, and an interactive schedule change interface. The optimization module uses a sophisticated Lagrangian relaxation algorithm to solve mixed integer programs with up to 800,000 variables and 200,000 constraints to near optimality. The system, first implemented in October, 1981, has been saving between 6% to 10% of operating costs. </jats:p>","for air products and chemicals, inc., inventory management of industrial gases at customer locations is integrated with vehicle scheduling and dispatching. their advanced decision support system includes on-line data entry functions, customer usage forecasting, a time/distance network with a shortest path algorithm to compute intercustomer travel times and distances, a mathematical optimization module to produce daily delivery schedules, and an interactive schedule change interface. the optimization module uses a sophisticated lagrangian relaxation algorithm to solve mixed integer programs with up to 800,000 variables and 200,000 constraints to near optimality. the system, first implemented in october, 1981, has been saving between 6% to 10% of operating costs."
http://orkg.org/orkg/resource/R26214,Decomposition of a Combined Inventory and Time Constrained Ship Routing Problem,10.1287/trsc.33.1.3,crossref,"<jats:p> In contrast to vehicle routing problems, little work has been done in ship routing and scheduling, although large benefits may be expected from improving this scheduling process. We will present a real ship planning problem, which is a combined inventory management problem and a routing problem with time windows. A fleet of ships transports a single product (ammonia) between production and consumption harbors. The quantities loaded and discharged are determined by the production rates of the harbors, possible stock levels, and the actual ship visiting the harbor. We describe the real problem and the underlying mathematical model. To decompose this model, we discuss some model adjustments. Then, the problem can be solved by a Dantzig–Wolfe decomposition approach including both ship routing subproblems and inventory management subproblems. The overall problem is solved by branch-and-bound. Our computational results indicate that the proposed method works for the real planning problem. </jats:p>","in contrast to vehicle routing problems, little work has been done in ship routing and scheduling, although large benefits may be expected from improving this scheduling process. we will present a real ship planning problem, which is a combined inventory management problem and a routing problem with time windows. a fleet of ships transports a single product (ammonia) between production and consumption harbors. the quantities loaded and discharged are determined by the production rates of the harbors, possible stock levels, and the actual ship visiting the harbor. we describe the real problem and the underlying mathematical model. to decompose this model, we discuss some model adjustments. then, the problem can be solved by a dantzig–wolfe decomposition approach including both ship routing subproblems and inventory management subproblems. the overall problem is solved by branch-and-bound. our computational results indicate that the proposed method works for the real planning problem."
http://orkg.org/orkg/resource/R26222,Deterministic Order-Up-To Level Policies in an Inventory Routing Problem,10.1287/trsc.36.1.119.573,crossref,"<jats:p> We consider a distribution problem in which a set of products has to be shipped from a supplier to several retailers in a given time horizon. Shipments from the supplier to the retailers are performed by a vehicle of given capacity and cost. Each retailer determines a minimum and a maximum level of the inventory of each product, and each must be visited before its inventory reaches the minimum level. Every time a retailer is visited, the quantity of each product delivered by the supplier is such that the maximum level of the inventory is reached at the retailer. The problem is to determine for each discrete time instant the retailers to be visited and the route of the vehicle. Various objective functions corresponding to different decision policies, and possibly to different decision makers, are considered. We present a heuristic algorithm and compare the solutions obtained with the different objective functions on a set of randomly generated problem instances. </jats:p>","we consider a distribution problem in which a set of products has to be shipped from a supplier to several retailers in a given time horizon. shipments from the supplier to the retailers are performed by a vehicle of given capacity and cost. each retailer determines a minimum and a maximum level of the inventory of each product, and each must be visited before its inventory reaches the minimum level. every time a retailer is visited, the quantity of each product delivered by the supplier is such that the maximum level of the inventory is reached at the retailer. the problem is to determine for each discrete time instant the retailers to be visited and the route of the vehicle. various objective functions corresponding to different decision policies, and possibly to different decision makers, are considered. we present a heuristic algorithm and compare the solutions obtained with the different objective functions on a set of randomly generated problem instances."
http://orkg.org/orkg/resource/R26224,A Decomposition Approach for the Inventory-Routing Problem,10.1287/trsc.1030.0054,crossref,"<jats:p> In this paper, we present a solution approach for the inventory-routing problem. The inventory-routing problem is a variation of the vehicle-routing problem that arises in situations where a vendor has the ability to make decisions about the timing and sizing of deliveries, as well as the routing, with the restriction that customers are not allowed to run out of product. We develop a two-phase approach based on decomposing the set of decisions: A delivery schedule is created first, followed by the construction of a set of delivery routes. The first phase utilizes integer programming, whereas the second phase employs routing and scheduling heuristics. Our focus is on creating a solution methodology appropriate for large-scale real-life instances. Computational experiments demonstrating the effectiveness of our approach are presented. </jats:p>","in this paper, we present a solution approach for the inventory-routing problem. the inventory-routing problem is a variation of the vehicle-routing problem that arises in situations where a vendor has the ability to make decisions about the timing and sizing of deliveries, as well as the routing, with the restriction that customers are not allowed to run out of product. we develop a two-phase approach based on decomposing the set of decisions: a delivery schedule is created first, followed by the construction of a set of delivery routes. the first phase utilizes integer programming, whereas the second phase employs routing and scheduling heuristics. our focus is on creating a solution methodology appropriate for large-scale real-life instances. computational experiments demonstrating the effectiveness of our approach are presented."
http://orkg.org/orkg/resource/R26228,A Periodic Inventory Routing Problem at a Supermarket Chain,10.1287/opre.1040.0150,crossref,"<jats:p> Albert Heijn, BV, a supermarket chain in the Netherlands, faces a vehicle routing and delivery scheduling problem once every three to six months. Given hourly demand forecasts for each store, travel times and distances, cost parameters, and various transportation constraints, the firm seeks to determine a weekly delivery schedule specifying the times when each store should be replenished from a central distribution center, and to determine the vehicle routes that service these requirements at minimum cost. We describe the development and implementation of a system to solve this problem at Albert Heijn. The system resulted in savings of 4% of distribution costs in its first year of implementation and is expected to yield 12%–20% savings as the firm expands its usage. It also has tactical and strategic advantages for the firm, such as in assessing the cost impact of various logistics and marketing decisions, in performance measurement, and in competing effectively through reduced lead time and increased frequency of replenishment. </jats:p>","albert heijn, bv, a supermarket chain in the netherlands, faces a vehicle routing and delivery scheduling problem once every three to six months. given hourly demand forecasts for each store, travel times and distances, cost parameters, and various transportation constraints, the firm seeks to determine a weekly delivery schedule specifying the times when each store should be replenished from a central distribution center, and to determine the vehicle routes that service these requirements at minimum cost. we describe the development and implementation of a system to solve this problem at albert heijn. the system resulted in savings of 4% of distribution costs in its first year of implementation and is expected to yield 12%–20% savings as the firm expands its usage. it also has tactical and strategic advantages for the firm, such as in assessing the cost impact of various logistics and marketing decisions, in performance measurement, and in competing effectively through reduced lead time and increased frequency of replenishment."
http://orkg.org/orkg/resource/R26244,A branch-and-cut algorithm for a vendor-managed inventory-routing problem,10.1287/trsc.1060.0188,crossref,"<jats:p> We consider a distribution problem in which a product has to be shipped from a supplier to several retailers over a given time horizon. Each retailer defines a maximum inventory level. The supplier monitors the inventory of each retailer and determines its replenishment policy, guaranteeing that no stockout occurs at the retailer (vendor-managed inventory policy). Every time a retailer is visited, the quantity delivered by the supplier is such that the maximum inventory level is reached (deterministic order-up-to level policy). Shipments from the supplier to the retailers are performed by a vehicle of given capacity. The problem is to determine for each discrete time instant the quantity to ship to each retailer and the vehicle route. We present a mixed-integer linear programming model and derive new additional valid inequalities used to strengthen the linear relaxation of the model. We implement a branch-and-cut algorithm to solve the model optimally. We then compare the optimal solution of the problem with the optimal solution of two problems obtained by relaxing in different ways the deterministic order-up-to level policy. Computational results are presented on a set of randomly generated problem instances. </jats:p>","we consider a distribution problem in which a product has to be shipped from a supplier to several retailers over a given time horizon. each retailer defines a maximum inventory level. the supplier monitors the inventory of each retailer and determines its replenishment policy, guaranteeing that no stockout occurs at the retailer (vendor-managed inventory policy). every time a retailer is visited, the quantity delivered by the supplier is such that the maximum inventory level is reached (deterministic order-up-to level policy). shipments from the supplier to the retailers are performed by a vehicle of given capacity. the problem is to determine for each discrete time instant the quantity to ship to each retailer and the vehicle route. we present a mixed-integer linear programming model and derive new additional valid inequalities used to strengthen the linear relaxation of the model. we implement a branch-and-cut algorithm to solve the model optimally. we then compare the optimal solution of the problem with the optimal solution of two problems obtained by relaxing in different ways the deterministic order-up-to level policy. computational results are presented on a set of randomly generated problem instances."
http://orkg.org/orkg/resource/R26248,Omya Hustadmarmor optimizes its supply chain for delivering calcium carbonate slurry to European paper manufacturers,10.1287/inte.1060.0276,crossref,"<jats:p> The Norwegian company Omya Hustadmarmor supplies calcium carbonate slurry to European paper manufacturers from a single processing plant, using chemical tank ships of various sizes to transport its products. Transportation costs are lower for large ships than for small ships, but their use increases planning complexity and creates problems in production. In 2001, the company faced overwhelming operational challenges and sought operations-research-based planning support. The CEO, Sturla Steinsvik, contacted Møre Research Molde, which conducted a project that led to the development of a decision-support system (DSS) for maritime inventory routing. The core of the DSS is an optimization model that is solved through a metaheuristic-based algorithm. The system helps planners to make stronger, faster decisions and has increased predictability and flexibility throughout the supply chain. It has saved production and transportation costs close to US$7 million a year. We project additional direct savings of nearly US$4 million a year as the company adds even larger ships to the fleet as a result of the project. In addition, the company has avoided investments of US$35 million by increasing capacity utilization. Finally, the project has had a positive environmental effect by reducing overall oil consumption by more than 10 percent. </jats:p>","the norwegian company omya hustadmarmor supplies calcium carbonate slurry to european paper manufacturers from a single processing plant, using chemical tank ships of various sizes to transport its products. transportation costs are lower for large ships than for small ships, but their use increases planning complexity and creates problems in production. in 2001, the company faced overwhelming operational challenges and sought operations-research-based planning support. the ceo, sturla steinsvik, contacted møre research molde, which conducted a project that led to the development of a decision-support system (dss) for maritime inventory routing. the core of the dss is an optimization model that is solved through a metaheuristic-based algorithm. the system helps planners to make stronger, faster decisions and has increased predictability and flexibility throughout the supply chain. it has saved production and transportation costs close to us$7 million a year. we project additional direct savings of nearly us$4 million a year as the company adds even larger ships to the fleet as a result of the project. in addition, the company has avoided investments of us$35 million by increasing capacity utilization. finally, the project has had a positive environmental effect by reducing overall oil consumption by more than 10 percent."
http://orkg.org/orkg/resource/R26269,One Warehouse Multiple Retailer Systems with Vehicle Routing Costs,10.1287/mnsc.36.1.92,crossref,"<jats:p> We consider distribution systems with a depot and many geographically dispersed retailers each of which faces external demands occurring at constant, deterministic but retailer specific rates. All stock enters the system through the depot from where it is distributed to the retailers by a fleet of capacitated vehicles combining deliveries into efficient routes. Inventories are kept at the retailers but not at the depot. </jats:p><jats:p> We wish to determine feasible replenishment strategies (i.e., inventory rules and routing patterns) minimising (infinite horizon) long-run average transportation and inventory costs. We restrict ourselves to a class of strategies in which a collection of regions (sets of retailers) is specified which cover all outlets: if an outlet belongs to several regions, a specific fraction of its sales/operations is assigned to each of these regions. Each time one of the retailers in a given region receives a delivery, this delivery is made by a vehicle who visits all other outlets in the region as well (in an efficient route). </jats:p><jats:p> We describe a class of low complexity heuristics and show under mild probabilistic assumptions that the generated solutions are asymptotically optimal (within the above class of strategies). We also show that lower and upper bounds on the system-wide costs may be computed and that these bounds are asymptotically tight under the same assumptions. A numerical study exhibits the performance of these heuristics and bounds for problems of moderate size. </jats:p>","we consider distribution systems with a depot and many geographically dispersed retailers each of which faces external demands occurring at constant, deterministic but retailer specific rates. all stock enters the system through the depot from where it is distributed to the retailers by a fleet of capacitated vehicles combining deliveries into efficient routes. inventories are kept at the retailers but not at the depot. we wish to determine feasible replenishment strategies (i.e., inventory rules and routing patterns) minimising (infinite horizon) long-run average transportation and inventory costs. we restrict ourselves to a class of strategies in which a collection of regions (sets of retailers) is specified which cover all outlets: if an outlet belongs to several regions, a specific fraction of its sales/operations is assigned to each of these regions. each time one of the retailers in a given region receives a delivery, this delivery is made by a vehicle who visits all other outlets in the region as well (in an efficient route). we describe a class of low complexity heuristics and show under mild probabilistic assumptions that the generated solutions are asymptotically optimal (within the above class of strategies). we also show that lower and upper bounds on the system-wide costs may be computed and that these bounds are asymptotically tight under the same assumptions. a numerical study exhibits the performance of these heuristics and bounds for problems of moderate size."
http://orkg.org/orkg/resource/R26272,On the Effectiveness of Direct Shipping Strategy for the One-Warehouse Multi-Retailer R-Systems,10.1287/mnsc.36.2.240,crossref,"<jats:p> We consider the problem of integrating inventory control and vehicle routing into a cost-effective strategy for a distribution system consisting of one depot and many geographically dispersed retailers. All stock enters the system through the depot and is distributed to the retailers by vehicles of limited constant capacity. We assume that each one of the retailers faces a constant, retailer specific, demand rate and that inventory is charged only at the retailers but not at the depot. We provide a lower bound on the long run average cost over all inventory-routing strategies. We use this lower bound to show that the effectiveness of direct shipping over all inventory-routing strategies is at least 94% whenever the Economic Lot Size of each of the retailers is at least 71% of vehicle capacity. The effectiveness deteriorates as the Economic Lot Sizes become smaller. These results are important because they provide useful guidelines as to when to embark into the much more difficult task of finding cost-effective routes. Additional advantages of direct shipping are lower in-transit inventory and ease of coordination. </jats:p>","we consider the problem of integrating inventory control and vehicle routing into a cost-effective strategy for a distribution system consisting of one depot and many geographically dispersed retailers. all stock enters the system through the depot and is distributed to the retailers by vehicles of limited constant capacity. we assume that each one of the retailers faces a constant, retailer specific, demand rate and that inventory is charged only at the retailers but not at the depot. we provide a lower bound on the long run average cost over all inventory-routing strategies. we use this lower bound to show that the effectiveness of direct shipping over all inventory-routing strategies is at least 94% whenever the economic lot size of each of the retailers is at least 71% of vehicle capacity. the effectiveness deteriorates as the economic lot sizes become smaller. these results are important because they provide useful guidelines as to when to embark into the much more difficult task of finding cost-effective routes. additional advantages of direct shipping are lower in-transit inventory and ease of coordination."
http://orkg.org/orkg/resource/R26274,Two-echelon distribution systems with vehicle routing costs and central inventory,10.1287/opre.41.1.37,crossref,"<jats:p> We consider distribution systems with a single depot and many retailers each of which faces external demands for a single item that occurs at a specific deterministic demand rate. All stock enters the systems through the depot where it can be stored and then picked up and distributed to the retailers by a fleet of vehicles, combining deliveries into efficient routes. We extend earlier methods for obtaining low complexity lower bounds and heuristics for systems without central stock. We show under mild probabilistic assumptions that the generated solutions and bounds come asymptotically within a few percentage points of optimality (within the considered class of strategies). A numerical study exhibits the performance of these heuristics and bounds for problems of moderate size. </jats:p>","we consider distribution systems with a single depot and many retailers each of which faces external demands for a single item that occurs at a specific deterministic demand rate. all stock enters the systems through the depot where it can be stored and then picked up and distributed to the retailers by a fleet of vehicles, combining deliveries into efficient routes. we extend earlier methods for obtaining low complexity lower bounds and heuristics for systems without central stock. we show under mild probabilistic assumptions that the generated solutions and bounds come asymptotically within a few percentage points of optimality (within the considered class of strategies). a numerical study exhibits the performance of these heuristics and bounds for problems of moderate size."
http://orkg.org/orkg/resource/R26279,A Markov Decision Model and Decomposition Heuristic for Dynamic Vehicle Dispatching,10.1287/opre.41.1.77,crossref,"""<jats:p> We describe a dynamic and stochastic vehicle dispatching problem called the delivery dispatching problem. This problem is modeled as a Markov decision process. Because exact solution of this model is impractical, we adopt a heuristic approach for handling the problem. The heuristic is based in part on a decomposition of the problem by customer, where customer subproblems generate penalty functions that are applied in a master dispatching problem. We describe how to compute bounds on the algorithm's performance, and apply it to several examples with good results. </jats:p>""",""" we describe a dynamic and stochastic vehicle dispatching problem called the delivery dispatching problem. this problem is modeled as a markov decision process. because exact solution of this model is impractical, we adopt a heuristic approach for handling the problem. the heuristic is based in part on a decomposition of the problem by customer, where customer subproblems generate penalty functions that are applied in a master dispatching problem. we describe how to compute bounds on the algorithm's performance, and apply it to several examples with good results. """
http://orkg.org/orkg/resource/R26284,Minimizing Transportation and Inventory Costs for Several Products on a Single Link,10.1287/opre.42.5.879,crossref,"<jats:p> This paper deals with the problem of determining the frequencies at which several products have to be shipped on a common link to minimize the sum of transportation and inventory costs. A set of feasible shipping frequencies is given. Transportation costs are supposed to be proportional to the number of journeys performed by vehicles of a given capacity. Vehicles may or may not be supposed to carry out completely all materials available, and products assigned to different frequencies may or may not share the same truck. Integer and mixed integer linear programming models are formulated for each of the resulting four situations, and their properties are investigated. In particular, we show that allowing products to be split among several shipping frequencies makes trucks traveling at high frequencies to be filled up completely. In this situation, trucks may always be loaded with products shipped at the same frequency. </jats:p>","this paper deals with the problem of determining the frequencies at which several products have to be shipped on a common link to minimize the sum of transportation and inventory costs. a set of feasible shipping frequencies is given. transportation costs are supposed to be proportional to the number of journeys performed by vehicles of a given capacity. vehicles may or may not be supposed to carry out completely all materials available, and products assigned to different frequencies may or may not share the same truck. integer and mixed integer linear programming models are formulated for each of the resulting four situations, and their properties are investigated. in particular, we show that allowing products to be split among several shipping frequencies makes trucks traveling at high frequencies to be filled up completely. in this situation, trucks may always be loaded with products shipped at the same frequency."
http://orkg.org/orkg/resource/R26288,A Location Based Heuristic for General Routing Problems,10.1287/opre.43.4.649,crossref,"<jats:p> We present a general framework for modeling routing problems based on formulating them as a traditional location problem called the capacitated concentrator location problem. We apply this framework to two classical routing problems: the capacitated vehicle routing problem and the inventory routing problem. In the former case, the heuristic is proven to be asymptotically optimal for any distribution of customer demands and locations. Computational experiments show that the heuristic performs well for both problems and, in most cases, outperforms all published heuristics on a set of standard test problems. </jats:p>","we present a general framework for modeling routing problems based on formulating them as a traditional location problem called the capacitated concentrator location problem. we apply this framework to two classical routing problems: the capacitated vehicle routing problem and the inventory routing problem. in the former case, the heuristic is proven to be asymptotically optimal for any distribution of customer demands and locations. computational experiments show that the heuristic performs well for both problems and, in most cases, outperforms all published heuristics on a set of standard test problems."
http://orkg.org/orkg/resource/R26295,Heuristics for a One-Warehouse Multiretailer Distribution Problem with Performance Bounds,10.1287/opre.45.1.102,crossref,"<jats:p> We investigate the one warehouse multiretailer distribution problem with traveling salesman tour vehicle routing costs. We model the system in the framework of the more general production/distribution system with arbitrary non-negative monotone joint order costs. We develop polynomial time heuristics whose policy costs are provably close to the cost of an optimal policy. In particular, we show that given a submodular function which is close to the true order cost then we can find a power-of-two policy whose cost is only moderately greater than the cost of an optimal policy. Since such submodular approximations exist for traveling salesman tour vehicle routing costs we present a detailed description of heuristics for the one warehouse multiretailer distribution problem. </jats:p><jats:p> We formulate a nonpolynomial dynamic program that computes optimal power-of-two policies for the one warehouse multiretailer system assuming only that the order costs are non-negative monotone. </jats:p><jats:p> Finally, we perform computational tests which compare our heuristics to optimal power of two policies for problems of up to sixteen retailers. We also perform computational tests on larger problems; these tests give us insight into what policies one should employ. </jats:p>","we investigate the one warehouse multiretailer distribution problem with traveling salesman tour vehicle routing costs. we model the system in the framework of the more general production/distribution system with arbitrary non-negative monotone joint order costs. we develop polynomial time heuristics whose policy costs are provably close to the cost of an optimal policy. in particular, we show that given a submodular function which is close to the true order cost then we can find a power-of-two policy whose cost is only moderately greater than the cost of an optimal policy. since such submodular approximations exist for traveling salesman tour vehicle routing costs we present a detailed description of heuristics for the one warehouse multiretailer distribution problem. we formulate a nonpolynomial dynamic program that computes optimal power-of-two policies for the one warehouse multiretailer system assuming only that the order costs are non-negative monotone. finally, we perform computational tests which compare our heuristics to optimal power of two policies for problems of up to sixteen retailers. we also perform computational tests on larger problems; these tests give us insight into what policies one should employ."
http://orkg.org/orkg/resource/R26297,Fully Loaded Direct Shipping Strategy in One Warehouse/NRetailer Systems without Central Inventories,10.1287/trsc.31.2.193,crossref,"<jats:p> In this paper, we consider one warehouse/multiple retailer systems with transportation costs. The planning horizon is infinite and the warehouse keeps no central inventory. It is shown that the fully loaded direct shipping strategy is optimal among all possible shipping/allocation strategies if the truck capacity is smaller than a certain quantity, and a bound is provided for the general case. </jats:p>","in this paper, we consider one warehouse/multiple retailer systems with transportation costs. the planning horizon is infinite and the warehouse keeps no central inventory. it is shown that the fully loaded direct shipping strategy is optimal among all possible shipping/allocation strategies if the truck capacity is smaller than a certain quantity, and a bound is provided for the general case."
http://orkg.org/orkg/resource/R26300,Integrating Routing and Inventory Decisions in One-Warehouse Multiretailer Multiproduct Distribution Systems,10.1287/mnsc.43.3.294,crossref,"<jats:p> We consider distribution systems with a central warehouse and many retailers that stock a number of different products. Deterministic demand occurs at the retailers for each product. The warehouse acts as a break-bulk center and does not keep any inventory. The products are delivered from the warehouse to the retailers by vehicles that combine the deliveries to several retailers into efficient vehicle routes. The objective is to determine replenishment policies that specify the delivery quantities and the vehicle routes used for the delivery, so as to minimize the long-run average inventory and transportation costs. A new heuristic that develops a stationary nested joint replenishment policy for the problem is presented in this paper. Unlike existing methods, the proposed heuristic is capable of solving problems involving distribution systems with multiple products. Results of a computational study on randomly generated single-product problems are also presented. </jats:p>","we consider distribution systems with a central warehouse and many retailers that stock a number of different products. deterministic demand occurs at the retailers for each product. the warehouse acts as a break-bulk center and does not keep any inventory. the products are delivered from the warehouse to the retailers by vehicles that combine the deliveries to several retailers into efficient vehicle routes. the objective is to determine replenishment policies that specify the delivery quantities and the vehicle routes used for the delivery, so as to minimize the long-run average inventory and transportation costs. a new heuristic that develops a stationary nested joint replenishment policy for the problem is presented in this paper. unlike existing methods, the proposed heuristic is capable of solving problems involving distribution systems with multiple products. results of a computational study on randomly generated single-product problems are also presented."
http://orkg.org/orkg/resource/R26302,Probabilistic Analyses and Algorithms for Three-Level Distribution Systems,10.1287/mnsc.44.11.1562,crossref,"<jats:p> We consider the problem of integrating inventory control and vehicle routing into a cost-effective strategy for a distribution system consisting of a single outside vendor, a fixed number of warehouses and many geographically dispersed retailers. Each retailer faces a constant, retailer specific, demand rate and inventory holding cost is charged at the retailers and the warehouses. We show that, in an effective strategy which minimizes the asymptotic long run average cost, each warehouse receives fully loaded trucks from the vendor but never holds inventory. That is, each warehouse serves only as a coordinator of the frequency, time and sizes of deliveries to the retailers. This insight is used to construct an inventory control policy and vehicle routing strategy for multi-echelon distribution systems. Computational results are also reported. </jats:p>","we consider the problem of integrating inventory control and vehicle routing into a cost-effective strategy for a distribution system consisting of a single outside vendor, a fixed number of warehouses and many geographically dispersed retailers. each retailer faces a constant, retailer specific, demand rate and inventory holding cost is charged at the retailers and the warehouses. we show that, in an effective strategy which minimizes the asymptotic long run average cost, each warehouse receives fully loaded trucks from the vendor but never holds inventory. that is, each warehouse serves only as a coordinator of the frequency, time and sizes of deliveries to the retailers. this insight is used to construct an inventory control policy and vehicle routing strategy for multi-echelon distribution systems. computational results are also reported."
http://orkg.org/orkg/resource/R26311,Heavy Traffic Analysis of the Dynamic Stochastic Inventory-Routing Problem,10.1287/trsc.33.4.361,crossref,"<jats:p> We analyze three queueing control problems that model a dynamic stochastic distribution system, where a single capacitated vehicle serves a finite number of retailers in a make-to-stock fashion. The objective in each of these vehicle routing and inventory problems is to minimize the long run average inventory (holding and backordering) and transportation cost. In all three problems, the controller dynamically specifies whether a vehicle at the warehouse should idle or embark with a full load. In the first problem, the vehicle must travel along a prespecified (TSP) tour of all retailers, and the controller dynamically decides how many units to deliver to each retailer. In the second problem, the vehicle delivers an entire load to one retailer (direct shipping) and the controller decides which retailer to visit next. The third problem allows the additional dynamic choice between the TSP and direct shipping options. Motivated by existing heavy traffic limit theorems, we make a time scale decomposition assumption that allows us to approximate these queueing control problems by diffusion control problems, which are explicitly solved in the fixed route problems, and numerically solved in the dynamic routing case. Simulation experiments confirm that the heavy traffic approximations are quite accurate over a broad range of problem parameters. Our results lead to some new observations about the behavior of this complex system. </jats:p>","we analyze three queueing control problems that model a dynamic stochastic distribution system, where a single capacitated vehicle serves a finite number of retailers in a make-to-stock fashion. the objective in each of these vehicle routing and inventory problems is to minimize the long run average inventory (holding and backordering) and transportation cost. in all three problems, the controller dynamically specifies whether a vehicle at the warehouse should idle or embark with a full load. in the first problem, the vehicle must travel along a prespecified (tsp) tour of all retailers, and the controller dynamically decides how many units to deliver to each retailer. in the second problem, the vehicle delivers an entire load to one retailer (direct shipping) and the controller decides which retailer to visit next. the third problem allows the additional dynamic choice between the tsp and direct shipping options. motivated by existing heavy traffic limit theorems, we make a time scale decomposition assumption that allows us to approximate these queueing control problems by diffusion control problems, which are explicitly solved in the fixed route problems, and numerically solved in the dynamic routing case. simulation experiments confirm that the heavy traffic approximations are quite accurate over a broad range of problem parameters. our results lead to some new observations about the behavior of this complex system."
http://orkg.org/orkg/resource/R26313,The Stochastic Inventory Routing Problem with Direct Deliveries,10.1287/trsc.36.1.94.574,crossref,"""<jats:p> Vendor managed inventory replenishment is a business practice in which vendors monitor their customers' inventories, and decide when and how much inventory should be replenished. The inventory routing problem addresses the coordination of inventory management and transportation. The ability to solve the inventory routing problem contributes to the realization of the potential savings in inventory and transportation costs brought about by vendor managed inventory replenishment. The inventory routing problem is hard, especially if a large number of customers is involved. We formulate the inventory routing problem as a Markov decision process, and we propose approximation methods to find good solutions with reasonable computational effort. Computational results are presented for the inventory routing problem with direct deliveries. </jats:p>""",""" vendor managed inventory replenishment is a business practice in which vendors monitor their customers' inventories, and decide when and how much inventory should be replenished. the inventory routing problem addresses the coordination of inventory management and transportation. the ability to solve the inventory routing problem contributes to the realization of the potential savings in inventory and transportation costs brought about by vendor managed inventory replenishment. the inventory routing problem is hard, especially if a large number of customers is involved. we formulate the inventory routing problem as a markov decision process, and we propose approximation methods to find good solutions with reasonable computational effort. computational results are presented for the inventory routing problem with direct deliveries. """
http://orkg.org/orkg/resource/R26317,Price-Directed Replenishment of Subsets: Methodology and Its Application to Inventory Routing,10.1287/msom.5.4.348.24884,crossref,"<jats:p> The idea of price-directed control is to use an operating policy that exploits optimal dual prices from a mathematical programming relaxation of the underlying control problem. We apply it to the problem of replenishing inventory to subsets of products/locations, such as in the distribution of industrial gases, so as to minimize long-run time average replenishment costs. Given a marginal value for each product/location, whenever there is a stockout the dispatcher compares the total value of each feasible replenishment with its cost, and chooses one that maximizes the surplus. We derive this operating policy using a linear functional approximation to the optimal value function of a semi-Markov decision process on continuous spaces. This approximation also leads to a math program whose optimal dual prices yield values and whose optimal objective value gives a lower bound on system performance. We use duality theory to show that optimal prices satisfy several structural properties and can be interpreted as estimates of lowest achievable marginal costs. On real-world instances, the price-directed policy achieves superior, near optimal performance as compared with other approaches. </jats:p>","the idea of price-directed control is to use an operating policy that exploits optimal dual prices from a mathematical programming relaxation of the underlying control problem. we apply it to the problem of replenishing inventory to subsets of products/locations, such as in the distribution of industrial gases, so as to minimize long-run time average replenishment costs. given a marginal value for each product/location, whenever there is a stockout the dispatcher compares the total value of each feasible replenishment with its cost, and chooses one that maximizes the surplus. we derive this operating policy using a linear functional approximation to the optimal value function of a semi-markov decision process on continuous spaces. this approximation also leads to a math program whose optimal dual prices yield values and whose optimal objective value gives a lower bound on system performance. we use duality theory to show that optimal prices satisfy several structural properties and can be interpreted as estimates of lowest achievable marginal costs. on real-world instances, the price-directed policy achieves superior, near optimal performance as compared with other approaches."
http://orkg.org/orkg/resource/R26319,A Price-Directed Approach to Stochastic Inventory/Routing,10.1287/opre.1040.0114,crossref,"<jats:p> We consider a new approach to stochastic inventory/routing that approximates the future costs of current actions using optimal dual prices of a linear program. We obtain two such linear programs by formulating the control problem as a Markov decision process and then replacing the optimal value function with the sum of single-customer inventory value functions. The resulting approximation yields statewise lower bounds on optimal infinite-horizon discounted costs. We present a linear program that takes into account inventory dynamics and economics in allocating transportation costs for stochastic inventory routing. On test instances we find that these allocations do not introduce any error in the value function approximations relative to the best approximations that can be achieved without them. Also, unlike other approaches, we do not restrict the set of allowable vehicle itineraries in any way. Instead, we develop an efficient algorithm to both generate and eliminate itineraries during solution of the linear programs and control policy. In simulation experiments, the price-directed policy outperforms other policies from the literature. </jats:p>","we consider a new approach to stochastic inventory/routing that approximates the future costs of current actions using optimal dual prices of a linear program. we obtain two such linear programs by formulating the control problem as a markov decision process and then replacing the optimal value function with the sum of single-customer inventory value functions. the resulting approximation yields statewise lower bounds on optimal infinite-horizon discounted costs. we present a linear program that takes into account inventory dynamics and economics in allocating transportation costs for stochastic inventory routing. on test instances we find that these allocations do not introduce any error in the value function approximations relative to the best approximations that can be achieved without them. also, unlike other approaches, we do not restrict the set of allowable vehicle itineraries in any way. instead, we develop an efficient algorithm to both generate and eliminate itineraries during solution of the linear programs and control policy. in simulation experiments, the price-directed policy outperforms other policies from the literature."
http://orkg.org/orkg/resource/R26321,Dynamic Programming Approximations for a Stochastic Inventory Routing Problem,10.1287/trsc.1030.0041,crossref,"<jats:p> This work is motivated by the need to solve the inventory routing problem when implementing a business practice called vendor managed inventory replenishment (VMI). With VMI, vendors monitor their customers′ inventories and decide when and how much inventory should be replenished at each customer. The inventory routing problem attempts to coordinate inventory replenishment and transportation in such a way that the cost is minimized over the long run. We formulate a Markov decision process model of the stochastic inventory routing problem and propose approximation methods to find good solutions with reasonable computational effort. We indicate how the proposed approach can be used for other Markov decision processes involving the control of multiple resources. </jats:p>","this work is motivated by the need to solve the inventory routing problem when implementing a business practice called vendor managed inventory replenishment (vmi). with vmi, vendors monitor their customers′ inventories and decide when and how much inventory should be replenished at each customer. the inventory routing problem attempts to coordinate inventory replenishment and transportation in such a way that the cost is minimized over the long run. we formulate a markov decision process model of the stochastic inventory routing problem and propose approximation methods to find good solutions with reasonable computational effort. we indicate how the proposed approach can be used for other markov decision processes involving the control of multiple resources."
http://orkg.org/orkg/resource/R26330,On the Interactions Between Routing and Inventory-Management Policies in a One-WarehouseN-Retailer Distribution System,10.1287/msom.1060.0111,crossref,"<jats:p> This paper examines the interactions between routing and inventory-management decisions in a two-level supply chain consisting of a cross-docking warehouse and N retailers. Retailer demand is normally distributed and independent across retailers and over time. Travel times are fixed between pairs of system sites. Every m time periods, system inventory is replenished at the warehouse, whereupon an uncapacitated vehicle departs on a route that visits each retailer once and only once, allocating all of its inventory based on the status of inventory at the retailers who have not yet received allocations. The retailers experience newsvendor-type inventory-holding and backorder-penalty costs each period; the vehicle experiences in-transit inventory-holding costs each period. Our goal is to determine a combined system inventory-replenishment, routing, and inventory-allocation policy that minimizes the total expected cost/period of the system over an infinite time horizon. Our analysis begins by examining the determination of the optimal static route, i.e., the best route if the vehicle must travel the same route every replenishment-allocation cycle. Here we demonstrate that the optimal static route is not the shortest-total-distance (TSP) route, but depends on the variance of customer demands, and, if in-transit inventory-holding costs are charged, also on mean customer demands. We then examine dynamic-routing policies, i.e., policies that can change the route from one system-replenishment-allocation cycle to another, based on the status of the retailers’ inventories. Here we argue that in the absence of transportation-related cost, the optimal dynamic-routing policy should be viewed as balancing management’s ability to respond to system uncertainties (by changing routes) against system uncertainties that are induced by changing routes. We then examine the performance of a change-revert heuristic policy. Although its routing decisions are not fully dynamic, but determined and fixed for a given cycle at the time of each system replenishment, simulation tests with N = 2 and N = 6 retailers indicate that its use can substantially reduce system inventory-related costs even if most of the time the chosen route is the optimal static route. </jats:p>","this paper examines the interactions between routing and inventory-management decisions in a two-level supply chain consisting of a cross-docking warehouse and n retailers. retailer demand is normally distributed and independent across retailers and over time. travel times are fixed between pairs of system sites. every m time periods, system inventory is replenished at the warehouse, whereupon an uncapacitated vehicle departs on a route that visits each retailer once and only once, allocating all of its inventory based on the status of inventory at the retailers who have not yet received allocations. the retailers experience newsvendor-type inventory-holding and backorder-penalty costs each period; the vehicle experiences in-transit inventory-holding costs each period. our goal is to determine a combined system inventory-replenishment, routing, and inventory-allocation policy that minimizes the total expected cost/period of the system over an infinite time horizon. our analysis begins by examining the determination of the optimal static route, i.e., the best route if the vehicle must travel the same route every replenishment-allocation cycle. here we demonstrate that the optimal static route is not the shortest-total-distance (tsp) route, but depends on the variance of customer demands, and, if in-transit inventory-holding costs are charged, also on mean customer demands. we then examine dynamic-routing policies, i.e., policies that can change the route from one system-replenishment-allocation cycle to another, based on the status of the retailers’ inventories. here we argue that in the absence of transportation-related cost, the optimal dynamic-routing policy should be viewed as balancing management’s ability to respond to system uncertainties (by changing routes) against system uncertainties that are induced by changing routes. we then examine the performance of a change-revert heuristic policy. although its routing decisions are not fully dynamic, but determined and fixed for a given cycle at the time of each system replenishment, simulation tests with n = 2 and n = 6 retailers indicate that its use can substantially reduce system inventory-related costs even if most of the time the chosen route is the optimal static route."
http://orkg.org/orkg/resource/R26333,An Efficient Heuristic Algorithm for a Two-Echelon Joint Inventory and Routing Problem,10.1287/trsc.1060.0160,crossref,"<jats:p> With an increasing emphasis on coordination in the supply chain, the inventory and distribution decisions, which in most part had been dealt with independently of each other, need to be considered jointly. This research considers a two-echelon distribution system consisting of one warehouse and N retailers that face external demand at a constant rate. Inventories are kept at retailers as well as at the warehouse. The products are delivered to the retailers by a fleet of vehicles with limited capacity. We develop an efficient heuristic procedure that finds a reorder interval for the warehouse, the replenishment quantities (and associated reorder interval) for each retailer, and the delivery routes so as to minimize the long-run average inventory and transportation costs. </jats:p>","with an increasing emphasis on coordination in the supply chain, the inventory and distribution decisions, which in most part had been dealt with independently of each other, need to be considered jointly. this research considers a two-echelon distribution system consisting of one warehouse and n retailers that face external demand at a constant rate. inventories are kept at retailers as well as at the warehouse. the products are delivered to the retailers by a fleet of vehicles with limited capacity. we develop an efficient heuristic procedure that finds a reorder interval for the warehouse, the replenishment quantities (and associated reorder interval) for each retailer, and the delivery routes so as to minimize the long-run average inventory and transportation costs."
http://orkg.org/orkg/resource/R26336,"The storage constrained, inbound inventory routing problem",10.1108/09600030710763396,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>This paper aims to describe the storage constrained, inbound inventory routeing problem and presents bounds and heuristics for solutions to this problem. It also seeks to analyze various characteristics of this problem by comparing the solutions generated by the two proposed heuristics with each other and with the lower bound solutions.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>The proposed heuristics use a sequential decomposition strategy for generating solutions for this problem. These heuristics are evaluated on a set of problem instances which are based on an actual application in the automotive manufacturing industry.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The storage space clearly has a significant effect on both the routeing and inventory decisions, and there are complex and interesting interactions between the problem factors and performance measures.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>Facility design decisions for the storage of inbound materials should carefully consider the impact of storage space on transportation and logistics costs.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This problem occurs in a number of different industrial applications while most of the existing literature addresses outbound distribution. Other papers that address similar problems do not consider all of the practical constraints in the problem or do not adequately benchmark and analyze their proposed solutions.</jats:p></jats:sec>","purpose this paper aims to describe the storage constrained, inbound inventory routeing problem and presents bounds and heuristics for solutions to this problem. it also seeks to analyze various characteristics of this problem by comparing the solutions generated by the two proposed heuristics with each other and with the lower bound solutions. design/methodology/approach the proposed heuristics use a sequential decomposition strategy for generating solutions for this problem. these heuristics are evaluated on a set of problem instances which are based on an actual application in the automotive manufacturing industry. findings the storage space clearly has a significant effect on both the routeing and inventory decisions, and there are complex and interesting interactions between the problem factors and performance measures. practical implications facility design decisions for the storage of inbound materials should carefully consider the impact of storage space on transportation and logistics costs. originality/value this problem occurs in a number of different industrial applications while most of the existing literature addresses outbound distribution. other papers that address similar problems do not consider all of the practical constraints in the problem or do not adequately benchmark and analyze their proposed solutions."
http://orkg.org/orkg/resource/R26343,Scenario Tree-Based Heuristics for Stochastic Inventory-Routing Problems,10.1287/ijoc.1080.0291,crossref,"<jats:p> In vendor-managed inventory replenishment, the vendor decides when to make deliveries to customers, how much to deliver, and how to combine shipments using the available vehicles. This gives rise to the inventory-routing problem in which the goal is to coordinate inventory replenishment and transportation to minimize costs. The problem tackled in this paper is the stochastic inventory-routing problem, where stochastic demands are specified through general discrete distributions. The problem is formulated as a discounted infinite-horizon Markov decision problem. Heuristics based on finite scenario trees are developed. Computational results confirm the efficiency of these heuristics. </jats:p>","in vendor-managed inventory replenishment, the vendor decides when to make deliveries to customers, how much to deliver, and how to combine shipments using the available vehicles. this gives rise to the inventory-routing problem in which the goal is to coordinate inventory replenishment and transportation to minimize costs. the problem tackled in this paper is the stochastic inventory-routing problem, where stochastic demands are specified through general discrete distributions. the problem is formulated as a discounted infinite-horizon markov decision problem. heuristics based on finite scenario trees are developed. computational results confirm the efficiency of these heuristics."
http://orkg.org/orkg/resource/R26359,Reducing Logistics Costs at General Motors,10.1287/inte.17.1.26,crossref,"""<jats:p> Automobile and truck production at General Motors involves shipping a broad variety of materials, parts, and components from 20,000 supplier plants to over 160 GM plants. To help reduce logistics costs at GM, the decision tool TRANSPART was developed. In its initial application for GM's Delco Electronics Division, TRANSPART identified a 26 percent logistics cost savings opportunity ($2.9 million per year). Today, TRANSPART II—a commercial version of the tool—is being used in more than 40 GM plants. </jats:p>""",""" automobile and truck production at general motors involves shipping a broad variety of materials, parts, and components from 20,000 supplier plants to over 160 gm plants. to help reduce logistics costs at gm, the decision tool transpart was developed. in its initial application for gm's delco electronics division, transpart identified a 26 percent logistics cost savings opportunity ($2.9 million per year). today, transpart ii—a commercial version of the tool—is being used in more than 40 gm plants. """
http://orkg.org/orkg/resource/R26393,Reaction mechanism of chitosanase from Streptomyces sp. N174,10.1042/bj3110377,crossref,"<jats:p>Chitosanase was produced by the strain of Streptomyces lividans TK24 bearing the csn gene from Streptomyces sp. N174, and purified by S-Sepharose and Bio-Gel A column chromatography. Partially (25-35%) N-acetylated chitosan was digested by the purified chitosanase, and structures of the products were analysed by NMR spectroscopy. The chitosanase produced heterooligosaccharides consisting of D-GlcN and GlcNAc in addition to glucosamine oligosaccharides [(GlcN)n, n = 1, 2 and 3]. The reducing- and non-reducing-end residues of the heterooligosaccharide products were GlcNAc and GlcN respectively, indicating that the chitosanase can split the GlcNAc-GlcN linkage in addition to that of GlcN-GlcN. Time-dependent 1H-NMR spectra showing hydrolysis of (GlcN)6 by the chitosanase were obtained in order to determine the anomeric form of the reaction products. The chitosanase was found to produce only the alpha-form; therefore it is an inverting enzyme. Separation and quantification of (GlcN)n was achieved by HPLC, and the time course of the reaction catalysed by the chitosanase was studied using (GlcN)n (n = 4, 5 and 6) as the substrate. The chitosanase hydrolysed (GlcN)6 in an endo-splitting manner producing (GlcN)2, (GlcN)3 and (GlcN)4, and did not catalyse transglycosylation. Product distribution was (GlcN)3 &amp;gt;&amp;gt; (GlcN)2 &amp;gt; (GlcN)4. Cleavage to (GlcN)3 + (GlcN)3 predominated over that to (GlcN)2 + (GlcN)4. Time courses showed a decrease in rate of substrate degradation from (GlcN)6 to (GlcN)5 to (GlcN)4. It is most likely that the substrate-binding cleft of the chitosanase can accommodate at least six GlcN residues, and that the cleavage point is located at the midpoint of the binding cleft.</jats:p>","chitosanase was produced by the strain of streptomyces lividans tk24 bearing the csn gene from streptomyces sp. n174, and purified by s-sepharose and bio-gel a column chromatography. partially (25-35%) n-acetylated chitosan was digested by the purified chitosanase, and structures of the products were analysed by nmr spectroscopy. the chitosanase produced heterooligosaccharides consisting of d-glcn and glcnac in addition to glucosamine oligosaccharides [(glcn)n, n = 1, 2 and 3]. the reducing- and non-reducing-end residues of the heterooligosaccharide products were glcnac and glcn respectively, indicating that the chitosanase can split the glcnac-glcn linkage in addition to that of glcn-glcn. time-dependent 1h-nmr spectra showing hydrolysis of (glcn)6 by the chitosanase were obtained in order to determine the anomeric form of the reaction products. the chitosanase was found to produce only the alpha-form; therefore it is an inverting enzyme. separation and quantification of (glcn)n was achieved by hplc, and the time course of the reaction catalysed by the chitosanase was studied using (glcn)n (n = 4, 5 and 6) as the substrate. the chitosanase hydrolysed (glcn)6 in an endo-splitting manner producing (glcn)2, (glcn)3 and (glcn)4, and did not catalyse transglycosylation. product distribution was (glcn)3 &amp;gt;&amp;gt; (glcn)2 &amp;gt; (glcn)4. cleavage to (glcn)3 + (glcn)3 predominated over that to (glcn)2 + (glcn)4. time courses showed a decrease in rate of substrate degradation from (glcn)6 to (glcn)5 to (glcn)4. it is most likely that the substrate-binding cleft of the chitosanase can accommodate at least six glcn residues, and that the cleavage point is located at the midpoint of the binding cleft."
http://orkg.org/orkg/resource/R26399,"Production of Two Chitosanases from a Chitosan-Assimilating Bacterium, Acinetobacter sp. Strain CHB101.",10.1128/aem.61.2.438-442.1995,crossref,"<jats:p>A bacterial strain capable of utilizing chitosan as a sole carbon source was isolated from soil and was identified as a member of the genus Acinetobacter. This strain, designated CHB101, produced extracellular chitosan-degrading enzymes in the absence of chitosan. The chitosan-degrading activity in the culture fluid increased when cultures reached the early stationary phase, although the level of activity was low in the exponential growth phase. Two chitosanases, chitosanases I and II, which had molecular weights of 37,000 and 30,000, respectively, were purified from the culture fluid. Chitosanase I exhibited substrate specificity for chitosan that had a low degree of acetylation (10 to 30%), while chitosanase II degraded colloidal chitin and glycol chitin, as well as chitosan that had a degree of acetylation of 30%. Rapid decreases in the viscosities of chitosan solutions suggested that both chitosanases catalyzed an endo type of cleavage reaction; however, chitosan oligomers (molecules smaller than pentamers) were not produced after a prolonged reaction.</jats:p>","a bacterial strain capable of utilizing chitosan as a sole carbon source was isolated from soil and was identified as a member of the genus acinetobacter. this strain, designated chb101, produced extracellular chitosan-degrading enzymes in the absence of chitosan. the chitosan-degrading activity in the culture fluid increased when cultures reached the early stationary phase, although the level of activity was low in the exponential growth phase. two chitosanases, chitosanases i and ii, which had molecular weights of 37,000 and 30,000, respectively, were purified from the culture fluid. chitosanase i exhibited substrate specificity for chitosan that had a low degree of acetylation (10 to 30%), while chitosanase ii degraded colloidal chitin and glycol chitin, as well as chitosan that had a degree of acetylation of 30%. rapid decreases in the viscosities of chitosan solutions suggested that both chitosanases catalyzed an endo type of cleavage reaction; however, chitosan oligomers (molecules smaller than pentamers) were not produced after a prolonged reaction."
http://orkg.org/orkg/resource/R26405,"Purification, Characterization, and Gene Analysis of a Chitosanase (ChoA) from Matsuebacter chitosanotabidus3001",10.1128/jb.181.21.6642-6649.1999,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            The extracellular chitosanase (34,000\n            <jats:italic>M</jats:italic>\n            <jats:sub>r</jats:sub>\n            ) produced by a novel gram-negative bacterium\n            <jats:italic>Matsuebacter chitosanotabidus</jats:italic>\n            3001 was purified. The optimal pH of this chitosanase was 4.0, and the optimal temperature was between 30 and 40°C. The purified chitosanase was most active on 90% deacetylated colloidal chitosan and glycol chitosan, both of which were hydrolyzed in an endosplitting manner, but this did not hydrolyze chitin, cellulose, or their derivatives. Among potential inhibitors, the purified chitosanase was only inhibited by Ag\n            <jats:sup>+</jats:sup>\n            . Internal amino acid sequences of the purified chitosanase were obtained. A PCR fragment corresponding to one of these amino acid sequences was then used to screen a genomic library for the entire\n            <jats:italic>choA</jats:italic>\n            gene encoding chitosanase. Sequencing of the\n            <jats:italic>choA</jats:italic>\n            gene revealed an open reading frame encoding a 391-amino-acid protein. The N-terminal amino acid sequence had an excretion signal, but the sequence did not show any significant homology to other proteins, including known chitosanases. The 80-amino-acid excretion signal of ChoA fused to green fluorescent protein was functional in\n            <jats:italic>Escherichia coli</jats:italic>\n            . Taken together, these results suggest that we have identified a novel, previously unreported chitosanase.\n          </jats:p>","abstract \n \n the extracellular chitosanase (34,000\n m \n r \n ) produced by a novel gram-negative bacterium\n matsuebacter chitosanotabidus \n 3001 was purified. the optimal ph of this chitosanase was 4.0, and the optimal temperature was between 30 and 40°c. the purified chitosanase was most active on 90% deacetylated colloidal chitosan and glycol chitosan, both of which were hydrolyzed in an endosplitting manner, but this did not hydrolyze chitin, cellulose, or their derivatives. among potential inhibitors, the purified chitosanase was only inhibited by ag\n + \n . internal amino acid sequences of the purified chitosanase were obtained. a pcr fragment corresponding to one of these amino acid sequences was then used to screen a genomic library for the entire\n choa \n gene encoding chitosanase. sequencing of the\n choa \n gene revealed an open reading frame encoding a 391-amino-acid protein. the n-terminal amino acid sequence had an excretion signal, but the sequence did not show any significant homology to other proteins, including known chitosanases. the 80-amino-acid excretion signal of choa fused to green fluorescent protein was functional in\n escherichia coli \n . taken together, these results suggest that we have identified a novel, previously unreported chitosanase.\n"
http://orkg.org/orkg/resource/R26468,Anti-ulcerogenic effect of chitin and chitosan on mucosal antioxidant defence system in HCl-ethanol-induced ulcer in rats,10.1211/0022357023079,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>The anti-ulcerogenic effect of chitin and chitosan against ulcer induced by HCl-ethanol in male Wistar rats was studied. Levels of acid output, pepsin, protein, lipid peroxides and reduced glutathione and the activity of glutathione peroxidase (GPx), glutathione-S-transferase (GST), catalase (CAT) and superoxide dismutase (SOD) were determined in the gastric mucosa of normal and experimental groups of rats. A significant increase in volume and acidity of the gastric juice was observed in the ulcer-induced group of rats. Peptic activity was significantly decreased as compared with that of normal controls. In the rats pre-treated with chitin and chitosan 2% along with feed, the volume and acid output and peptic activity of gastric mucosa were maintained at near normal levels. The level of lipid peroxidation was significantly higher in the ulcerated mucosa when compared with that of normal controls. This was paralleled by a decline in the level of reduced glutathione and in the activity of antioxidant enzymes like GPx, GST, CAT and SOD in the gastric mucosa of ulcer-induced rats. Also, the levels of mucosal proteins and glycoprotein components were significantly depleted in ulcerated mucosa. The pre-treatment with chitin and chitosan was found to exert a significant anti-ulcer effect by preventing all the HCl-ethanol-induced ulcerogenic effects in experimental rats.</jats:p>","abstract \n the anti-ulcerogenic effect of chitin and chitosan against ulcer induced by hcl-ethanol in male wistar rats was studied. levels of acid output, pepsin, protein, lipid peroxides and reduced glutathione and the activity of glutathione peroxidase (gpx), glutathione-s-transferase (gst), catalase (cat) and superoxide dismutase (sod) were determined in the gastric mucosa of normal and experimental groups of rats. a significant increase in volume and acidity of the gastric juice was observed in the ulcer-induced group of rats. peptic activity was significantly decreased as compared with that of normal controls. in the rats pre-treated with chitin and chitosan 2% along with feed, the volume and acid output and peptic activity of gastric mucosa were maintained at near normal levels. the level of lipid peroxidation was significantly higher in the ulcerated mucosa when compared with that of normal controls. this was paralleled by a decline in the level of reduced glutathione and in the activity of antioxidant enzymes like gpx, gst, cat and sod in the gastric mucosa of ulcer-induced rats. also, the levels of mucosal proteins and glycoprotein components were significantly depleted in ulcerated mucosa. the pre-treatment with chitin and chitosan was found to exert a significant anti-ulcer effect by preventing all the hcl-ethanol-induced ulcerogenic effects in experimental rats."
http://orkg.org/orkg/resource/R26524,Antimicrobial actions of degraded and native chitosan against spoilage organisms in laboratory media and foods,10.1128/aem.66.1.80-86.2000,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>The objective of this study was to determine whether chitosan (poly-β-1,4-glucosamine) and hydrolysates of chitosan can be used as novel preservatives in foods. Chitosan was hydrolyzed by using oxidative-reductive degradation, crude papaya latex, and lysozyme. Mild hydrolysis of chitosan resulted in improved microbial inactivation in saline and greater inhibition of growth of several spoilage yeasts in laboratory media, but highly degraded products of chitosan exhibited no antimicrobial activity. In pasteurized apple-elderflower juice stored at 7°C, addition of 0.3 g of chitosan per liter eliminated yeasts entirely for the duration of the experiment (13 days), while the total counts and the lactic acid bacterial counts increased at a slower rate than they increased in the control. Addition of 0.3 or 1.0 g of chitosan per kg had no effect on the microbial flora of houmous, a chickpea dip; in the presence of 5.0 g of chitosan per kg, bacterial growth but not yeast growth was substantially reduced compared with growth in control dip stored at 7°C for 6 days. Improved antimicrobial potency of chitosan hydrolysates like that observed in the saline and laboratory medium experiments was not observed in juice and dip experiments. We concluded that native chitosan has potential for use as a preservative in certain types of food but that the increase in antimicrobial activity that occurs following partial hydrolysis is too small to justify the extra processing involved.</jats:p>","abstract \n the objective of this study was to determine whether chitosan (poly-β-1,4-glucosamine) and hydrolysates of chitosan can be used as novel preservatives in foods. chitosan was hydrolyzed by using oxidative-reductive degradation, crude papaya latex, and lysozyme. mild hydrolysis of chitosan resulted in improved microbial inactivation in saline and greater inhibition of growth of several spoilage yeasts in laboratory media, but highly degraded products of chitosan exhibited no antimicrobial activity. in pasteurized apple-elderflower juice stored at 7°c, addition of 0.3 g of chitosan per liter eliminated yeasts entirely for the duration of the experiment (13 days), while the total counts and the lactic acid bacterial counts increased at a slower rate than they increased in the control. addition of 0.3 or 1.0 g of chitosan per kg had no effect on the microbial flora of houmous, a chickpea dip; in the presence of 5.0 g of chitosan per kg, bacterial growth but not yeast growth was substantially reduced compared with growth in control dip stored at 7°c for 6 days. improved antimicrobial potency of chitosan hydrolysates like that observed in the saline and laboratory medium experiments was not observed in juice and dip experiments. we concluded that native chitosan has potential for use as a preservative in certain types of food but that the increase in antimicrobial activity that occurs following partial hydrolysis is too small to justify the extra processing involved."
http://orkg.org/orkg/resource/R26527,Antimicrobial Edible Films and Coatings,10.4315/0362-028x-67.4.833,crossref,"<jats:p>Increasing consumer demand for microbiologicallysafer foods, greater convenience,smaller packages, and longer product shelf life is forcing the industry to develop new food-processing,cooking, handling, and packaging strategies. Nonfluid ready-to-eat foods are frequently exposed to postprocess surface contamination, leading to a reduction in shelf life. The food industry has at its disposal a wide range of nonedible polypropylene- and polyethylene-based packaging materials and various biodegradable protein- and polysaccharide-based edible films that can potentially serve as packaging materials. Research on the use of edible films as packaging materials continues because of the potential for these films to enhance food quality, food safety, and product shelf life. Besides acting as a barrier against mass diffusion (moisture, gases, and volatiles), edible films can serve as carriers for a wide range of food additives, including flavoring agents, antioxidants, vitamins, and colorants. When antimicrobial agents such as benzoic acid, sorbic acid, propionic acid, lactic acid, nisin, and lysozyme have been incorporated into edible films, such films retarded surface growth of bacteria, yeasts, and molds on a wide range of products, including meats and cheeses. Various antimicrobial edible films have been developed to minimize growth of spoilage and pathogenic microorganisms, including Listeria monocytogenes, which may contaminate the surface of cooked ready-to-eat foods after processing. Here, we review the various types of protein-based (wheat gluten, collagen, corn zein, soy, casein, and whey protein), polysaccharide-based (cellulose, chitosan, alginate, starch, pectin, and dextrin), and lipid-based (waxes, acylglycerols, and fatty acids) edible films and a wide range of antimicrobial agents that have been or could potentially be incorporated into such films during manufacture to enhance the safety and shelf life of ready-to-eat foods.</jats:p>","increasing consumer demand for microbiologicallysafer foods, greater convenience,smaller packages, and longer product shelf life is forcing the industry to develop new food-processing,cooking, handling, and packaging strategies. nonfluid ready-to-eat foods are frequently exposed to postprocess surface contamination, leading to a reduction in shelf life. the food industry has at its disposal a wide range of nonedible polypropylene- and polyethylene-based packaging materials and various biodegradable protein- and polysaccharide-based edible films that can potentially serve as packaging materials. research on the use of edible films as packaging materials continues because of the potential for these films to enhance food quality, food safety, and product shelf life. besides acting as a barrier against mass diffusion (moisture, gases, and volatiles), edible films can serve as carriers for a wide range of food additives, including flavoring agents, antioxidants, vitamins, and colorants. when antimicrobial agents such as benzoic acid, sorbic acid, propionic acid, lactic acid, nisin, and lysozyme have been incorporated into edible films, such films retarded surface growth of bacteria, yeasts, and molds on a wide range of products, including meats and cheeses. various antimicrobial edible films have been developed to minimize growth of spoilage and pathogenic microorganisms, including listeria monocytogenes, which may contaminate the surface of cooked ready-to-eat foods after processing. here, we review the various types of protein-based (wheat gluten, collagen, corn zein, soy, casein, and whey protein), polysaccharide-based (cellulose, chitosan, alginate, starch, pectin, and dextrin), and lipid-based (waxes, acylglycerols, and fatty acids) edible films and a wide range of antimicrobial agents that have been or could potentially be incorporated into such films during manufacture to enhance the safety and shelf life of ready-to-eat foods."
http://orkg.org/orkg/resource/R26640,An energy-efficient unequal clustering mechanism for wireless sensor networks,10.4028/www.scientific.net/amr.186.225,crossref,"<jats:p>Unequal clustering mechanism, in combination with inter-cluster multihop routing, provides a new effective way to balance the energy dissipation among nodes and prolong the lifetime of wireless sensor networks. In this paper, a distributed energy-efficient unequal clustering mechanism (DEEUC) is proposed and evaluated. By a time based competitive clustering algorithm, DEEUC partitions all nodes into clusters of unequal size, in which the clusters closer to the base station have smaller size. The cluster heads of these clusters can preserve some more energy for the inter-cluster relay traffic and the “hot-spots” problem can be avoided. For inter-cluster communication, DEEUC adopts an energy-aware multihop routing to reduce the energy consumption of the cluster heads. Simulation results demonstrate that the protocol can efficiently decrease the dead speed of the nodes and prolong the network lifetime</jats:p>","unequal clustering mechanism, in combination with inter-cluster multihop routing, provides a new effective way to balance the energy dissipation among nodes and prolong the lifetime of wireless sensor networks. in this paper, a distributed energy-efficient unequal clustering mechanism (deeuc) is proposed and evaluated. by a time based competitive clustering algorithm, deeuc partitions all nodes into clusters of unequal size, in which the clusters closer to the base station have smaller size. the cluster heads of these clusters can preserve some more energy for the inter-cluster relay traffic and the “hot-spots” problem can be avoided. for inter-cluster communication, deeuc adopts an energy-aware multihop routing to reduce the energy consumption of the cluster heads. simulation results demonstrate that the protocol can efficiently decrease the dead speed of the nodes and prolong the network lifetime"
http://orkg.org/orkg/resource/R26649,An Adaptive Data Dissemination Strategy for Wireless Sensor Networks,10.1080/15501320601067725,crossref,"""<jats:p> Future large-scale sensor networks may comprise thousands of wirelessly connected sensor nodes that could provide an unimaginable opportunity to interact with physical phenomena in real time. However, the nodes are typically highly resource-constrained. Since the communication task is a significant power consumer, various attempts have been made to introduce energy-awareness at different levels within the communication stack. Clustering is one such attempt to control energy dissipation for sensor data dissemination in a multihop fashion. The Time-Controlled Clustering Algorithm (TCCA) is proposed to realize a network-wide energy reduction. A realistic energy dissipation model is derived probabilistically to quantify the sensor network's energy consumption using the proposed clustering algorithm. A discrete-event simulator is developed to verify the mathematical model and to further investigate TCCA in other scenarios. The simulator is also extended to include the rest of the communication stack to allow a comprehensive evaluation of the proposed algorithm. </jats:p>""",""" future large-scale sensor networks may comprise thousands of wirelessly connected sensor nodes that could provide an unimaginable opportunity to interact with physical phenomena in real time. however, the nodes are typically highly resource-constrained. since the communication task is a significant power consumer, various attempts have been made to introduce energy-awareness at different levels within the communication stack. clustering is one such attempt to control energy dissipation for sensor data dissemination in a multihop fashion. the time-controlled clustering algorithm (tcca) is proposed to realize a network-wide energy reduction. a realistic energy dissipation model is derived probabilistically to quantify the sensor network's energy consumption using the proposed clustering algorithm. a discrete-event simulator is developed to verify the mathematical model and to further investigate tcca in other scenarios. the simulator is also extended to include the rest of the communication stack to allow a comprehensive evaluation of the proposed algorithm. """
http://orkg.org/orkg/resource/R26754,An Energy-Aware Distributed Unequal Clustering Protocol for Wireless Sensor Networks,10.1155/2011/202145,crossref,"<jats:p> Due to the imbalance of energy consumption of nodes in wireless sensor networks (WSNs), some local nodes die prematurely, which causes the network partitions and then shortens the lifetime of the network. The phenomenon is called “hot spot” or “energy hole” problem. For this problem, an energy-aware distributed unequal clustering protocol (EADUC) in multihop heterogeneous WSNs is proposed. Compared with the previous protocols, the cluster heads obtained by EADUC can achieve balanced energy, good distribution, and seamless coverage for all the nodes. Moreover, the complexity of time and control message is low. Simulation experiments show that EADUC can prolong the lifetime of the network significantly. </jats:p>","due to the imbalance of energy consumption of nodes in wireless sensor networks (wsns), some local nodes die prematurely, which causes the network partitions and then shortens the lifetime of the network. the phenomenon is called “hot spot” or “energy hole” problem. for this problem, an energy-aware distributed unequal clustering protocol (eaduc) in multihop heterogeneous wsns is proposed. compared with the previous protocols, the cluster heads obtained by eaduc can achieve balanced energy, good distribution, and seamless coverage for all the nodes. moreover, the complexity of time and control message is low. simulation experiments show that eaduc can prolong the lifetime of the network significantly."
http://orkg.org/orkg/resource/R26819,The Heterogeneous Vehicle-Routing Game,10.1287/trsc.1030.0035,crossref,"<jats:p> In this paper, we study a cost-allocation problem that arises in a distribution-planning situation at the Logistics Department at Norsk Hydro Olje AB, Stockholm, Sweden. We consider the routes from one depot during one day. The total distribution cost for these routes is to be divided among the customers that are visited. This cost-allocation problem is formulated as a vehicle-routing game (VRG), allowing the use of vehicles with different capacities. Cost-allocation methods based on different concepts from cooperative game theory, such as the core and the nucleolus, are discussed. A procedure that can be used to investigate whether the core is empty or not is presented, as well as a procedure to compute the nucleolus. Computational results for the Norsk Hydro case are presented and discussed. </jats:p>","in this paper, we study a cost-allocation problem that arises in a distribution-planning situation at the logistics department at norsk hydro olje ab, stockholm, sweden. we consider the routes from one depot during one day. the total distribution cost for these routes is to be divided among the customers that are visited. this cost-allocation problem is formulated as a vehicle-routing game (vrg), allowing the use of vehicles with different capacities. cost-allocation methods based on different concepts from cooperative game theory, such as the core and the nucleolus, are discussed. a procedure that can be used to investigate whether the core is empty or not is presented, as well as a procedure to compute the nucleolus. computational results for the norsk hydro case are presented and discussed."
http://orkg.org/orkg/resource/R26828,An Integrated Model and Solution Approach for Fleet Sizing with Heterogeneous Assets,10.1287/trsc.1030.0050,crossref,"<jats:p> This paper addresses a fleet-sizing problem in the context of the truck-rental industry. Specifically, trucks that vary in capacity and age are utilized over space and time to meet customer demand. Operational decisions (including demand allocation and empty truck repositioning) and tactical decisions (including asset procurements and sales) are explicitly examined in a linear programming model to determine the optimal fleet size and mix. The method uses a time-space network, common to fleet-management problems, but also includes capital cost decisions, wherein assets of different ages carry different costs, as is common to replacement analysis problems. A two-phase solution approach is developed to solve large-scale instances of the problem. Phase I allocates customer demand among assets through Benders decomposition with a demand-shifting algorithm assuring feasibility in each subproblem. Phase II uses the initial bounds and dual variables from Phase I and further improves the solution convergence without increasing computer memory requirements through the use of Lagrangian relaxation. Computational studies are presented to show the effectiveness of the approach for solving large problems within reasonable solution gaps. </jats:p>","this paper addresses a fleet-sizing problem in the context of the truck-rental industry. specifically, trucks that vary in capacity and age are utilized over space and time to meet customer demand. operational decisions (including demand allocation and empty truck repositioning) and tactical decisions (including asset procurements and sales) are explicitly examined in a linear programming model to determine the optimal fleet size and mix. the method uses a time-space network, common to fleet-management problems, but also includes capital cost decisions, wherein assets of different ages carry different costs, as is common to replacement analysis problems. a two-phase solution approach is developed to solve large-scale instances of the problem. phase i allocates customer demand among assets through benders decomposition with a demand-shifting algorithm assuring feasibility in each subproblem. phase ii uses the initial bounds and dual variables from phase i and further improves the solution convergence without increasing computer memory requirements through the use of lagrangian relaxation. computational studies are presented to show the effectiveness of the approach for solving large problems within reasonable solution gaps."
http://orkg.org/orkg/resource/R26883,Lagrangian Relaxation Methods for Solving the Minimum Fleet Size Multiple Traveling Salesman Problem with Time Windows,10.1287/mnsc.34.8.1005,crossref,"<jats:p> We consider the problem of finding the minimum number of vehicles required to visit once a set of nodes subject to time window constraints, for a homogeneous fleet of vehicles located at a common depot. This problem can be formulated as a network flow problem with additional time constraints. </jats:p><jats:p> The paper presents an optimal solution approach using the augmented Lagrangian method. Two Lagrangian relaxations are studied. In the first one, the time constraints are relaxed producing network subproblems which are easy to solve, but the bound obtained is weak. In the second relaxation, constraints requiring that each node be visited are relaxed producing shortest path subproblems with time window constraints and integrality conditions. The bound produced is always excellent. </jats:p><jats:p> Numerical results for several actual school busing problems with up to 223 nodes are discussed. Comparisons with a set partitioning formulation solved by column generation are given. </jats:p>","we consider the problem of finding the minimum number of vehicles required to visit once a set of nodes subject to time window constraints, for a homogeneous fleet of vehicles located at a common depot. this problem can be formulated as a network flow problem with additional time constraints. the paper presents an optimal solution approach using the augmented lagrangian method. two lagrangian relaxations are studied. in the first one, the time constraints are relaxed producing network subproblems which are easy to solve, but the bound obtained is weak. in the second relaxation, constraints requiring that each node be visited are relaxed producing shortest path subproblems with time window constraints and integrality conditions. the bound produced is always excellent. numerical results for several actual school busing problems with up to 223 nodes are discussed. comparisons with a set partitioning formulation solved by column generation are given."
http://orkg.org/orkg/resource/R26893,Minimum Vehicle Fleet Size Under Time-Window Constraints at a Container Terminal,10.1287/trsc.1030.0063,crossref,"<jats:p> Products can be transported in containers from one port to another. At a container terminal these containers are transshipped from one mode of transportation to another. Cranes remove containers from a ship and put them at a certain time (i.e., release time) into a buffer area with limited capacity. A vehicle lifts a container from the buffer area before the buffer area is full (i.e., in due time) and transports the container from the buffer area to the storage area. At the storage area the container is placed in another buffer area. The advantage of using these buffer areas is the resultant decoupling of the unloading and transportation processes. We study the case in which each container has a time window [release time, due time] in which the transportation should start. </jats:p><jats:p> The objective is to minimize the vehicle fleet size such that the transportation of each container starts within its time window. No literature has been found studying this relevant problem. We have developed an integer linear programming model to solve the problem of determining vehicle requirements under time-window constraints. We use simulation to validate the estimates of the vehicle fleet size by the analytical model. We test the ability of the model under various conditions. From these numerical experiments we conclude that the results of the analytical model are close to the results of the simulation model. Furthermore, we conclude that the analytical model performs well in the context of a container terminal. </jats:p>","products can be transported in containers from one port to another. at a container terminal these containers are transshipped from one mode of transportation to another. cranes remove containers from a ship and put them at a certain time (i.e., release time) into a buffer area with limited capacity. a vehicle lifts a container from the buffer area before the buffer area is full (i.e., in due time) and transports the container from the buffer area to the storage area. at the storage area the container is placed in another buffer area. the advantage of using these buffer areas is the resultant decoupling of the unloading and transportation processes. we study the case in which each container has a time window [release time, due time] in which the transportation should start. the objective is to minimize the vehicle fleet size such that the transportation of each container starts within its time window. no literature has been found studying this relevant problem. we have developed an integer linear programming model to solve the problem of determining vehicle requirements under time-window constraints. we use simulation to validate the estimates of the vehicle fleet size by the analytical model. we test the ability of the model under various conditions. from these numerical experiments we conclude that the results of the analytical model are close to the results of the simulation model. furthermore, we conclude that the analytical model performs well in the context of a container terminal."
http://orkg.org/orkg/resource/R26906,Heuristic Approaches for the Fleet Size and Mix Vehicle Routing Problem with Time Windows,10.1287/trsc.1070.0190,crossref,"<jats:p> The fleet size and mix vehicle routing problem with time windows (FSMVRPTW) is the problem of determining, at the same time, the composition and the routing of a fleet of heterogeneous vehicles aimed to serve a given set of customers. The routing problem requires us to design a set of minimum-cost routes originating and terminating at a central depot and serving customers with known demands, within given time windows. This paper develops a constructive insertion heuristic and a metaheuristic algorithm for FSMVRPTW. Extensive computational experiments on benchmark instances show that the proposed method is robust and efficient, and outperforms the previously published results. </jats:p>","the fleet size and mix vehicle routing problem with time windows (fsmvrptw) is the problem of determining, at the same time, the composition and the routing of a fleet of heterogeneous vehicles aimed to serve a given set of customers. the routing problem requires us to design a set of minimum-cost routes originating and terminating at a central depot and serving customers with known demands, within given time windows. this paper develops a constructive insertion heuristic and a metaheuristic algorithm for fsmvrptw. extensive computational experiments on benchmark instances show that the proposed method is robust and efficient, and outperforms the previously published results."
http://orkg.org/orkg/resource/R26910,An Effective Multirestart Deterministic Annealing Metaheuristic for the Fleet Size and Mix Vehicle-Routing Problem with Time Windows,10.1287/trsc.1070.0217,crossref,"<jats:p> This paper presents a new deterministic annealing metaheuristic for the fleet size and mix vehicle-routing problem with time windows. The objective is to service, at minimal total cost, a set of customers within their time windows by a heterogeneous capacitated vehicle fleet. First, we motivate and define the problem. We then give a mathematical formulation of the most studied variant in the literature in the form of a mixed-integer linear program. We also suggest an industrially relevant, alternative definition that leads to a linear mixed-integer formulation. The suggested metaheuristic solution method solves both problem variants and comprises three phases. In Phase 1, high-quality initial solutions are generated by means of a savings-based heuristic that combines diversification strategies with learning mechanisms. In Phase 2, an attempt is made to reduce the number of routes in the initial solution with a new local search procedure. In Phase 3, the solution from Phase 2 is further improved by a set of four local search operators that are embedded in a deterministic annealing framework to guide the improvement process. Some new implementation strategies are also suggested for efficient time window feasibility checks. Extensive computational experiments on the 168 benchmark instances have shown that the suggested method outperforms the previously published results and found 167 best-known solutions. Experimental results are also given for the new problem variant. </jats:p>","this paper presents a new deterministic annealing metaheuristic for the fleet size and mix vehicle-routing problem with time windows. the objective is to service, at minimal total cost, a set of customers within their time windows by a heterogeneous capacitated vehicle fleet. first, we motivate and define the problem. we then give a mathematical formulation of the most studied variant in the literature in the form of a mixed-integer linear program. we also suggest an industrially relevant, alternative definition that leads to a linear mixed-integer formulation. the suggested metaheuristic solution method solves both problem variants and comprises three phases. in phase 1, high-quality initial solutions are generated by means of a savings-based heuristic that combines diversification strategies with learning mechanisms. in phase 2, an attempt is made to reduce the number of routes in the initial solution with a new local search procedure. in phase 3, the solution from phase 2 is further improved by a set of four local search operators that are embedded in a deterministic annealing framework to guide the improvement process. some new implementation strategies are also suggested for efficient time window feasibility checks. extensive computational experiments on the 168 benchmark instances have shown that the suggested method outperforms the previously published results and found 167 best-known solutions. experimental results are also given for the new problem variant."
http://orkg.org/orkg/resource/R26929,A Decision Support System for Fleet Management: A Linear Programming Approach,10.1287/inte.12.3.1,crossref,"<jats:p> This paper describes a successful implementation of a decision support system that is used by the fleet management division at North American Van Lines to plan fleet configuration. At the heart of the system is a large linear programming (LP) model that helps management decide what type of tractors to sell to owner/operators or to trade in each week. The system is used to answer a wide variety of “What if” questions, many of which have significant financial impact. </jats:p>","this paper describes a successful implementation of a decision support system that is used by the fleet management division at north american van lines to plan fleet configuration. at the heart of the system is a large linear programming (lp) model that helps management decide what type of tractors to sell to owner/operators or to trade in each week. the system is used to answer a wide variety of “what if” questions, many of which have significant financial impact."
http://orkg.org/orkg/resource/R26966,Fleet Size and Mix Optimization for Paratransit Services,10.3141/1884-05,crossref,"<jats:p> Most paratransit agencies use a mix of different types of vehicles ranging from small sedans to large converted vans as a cost-effective way to meet the diverse travel needs and seating requirements of their clients. Currently, decisions on what types of vehicles and how many vehicles to use are mostly made by service managers on an ad hoc basis without much systematic analysis and optimization. The objective of this research is to address the underlying fleet size and mix problem and to develop a practical procedure that can be used to determine the optimal fleet mix for a given application. A real-life example illustrates the relationship between the performance of a paratransit service system and the size of its service vehicles. A heuristic procedure identifies the optimal fleet mix that maximizes the operating efficiency of a service system. A set of recommendations is offered for future research; the most important is the need to incorporate a life-cycle cost framework into the paratransit service planning process. </jats:p>","most paratransit agencies use a mix of different types of vehicles ranging from small sedans to large converted vans as a cost-effective way to meet the diverse travel needs and seating requirements of their clients. currently, decisions on what types of vehicles and how many vehicles to use are mostly made by service managers on an ad hoc basis without much systematic analysis and optimization. the objective of this research is to address the underlying fleet size and mix problem and to develop a practical procedure that can be used to determine the optimal fleet mix for a given application. a real-life example illustrates the relationship between the performance of a paratransit service system and the size of its service vehicles. a heuristic procedure identifies the optimal fleet mix that maximizes the operating efficiency of a service system. a set of recommendations is offered for future research; the most important is the need to incorporate a life-cycle cost framework into the paratransit service planning process."
http://orkg.org/orkg/resource/R26980,Coca-Cola Enterprises Optimizes Vehicle Routes for Efficient Product Delivery,10.1287/inte.1070.0331,crossref,"""<jats:p> In 2004 and 2005, Coca-Cola Enterprises (CCE)—the world's largest bottler and distributor of Coca-Cola products—implemented ORTEC's vehicle-routing software. Today, over 300 CCE dispatchers use this software daily to plan the routes of approximately 10,000 trucks. In addition to handling nonstandard constraints, the implementation is notable for its progressive transition from the prior business practice. CCE has realized an annual cost saving of $45 million and major improvements in customer service. This approach has been so successful that Coca-Cola has extended it beyond CCE to other Coca-Cola bottling companies and beer distributors. </jats:p>""",""" in 2004 and 2005, coca-cola enterprises (cce)—the world's largest bottler and distributor of coca-cola products—implemented ortec's vehicle-routing software. today, over 300 cce dispatchers use this software daily to plan the routes of approximately 10,000 trucks. in addition to handling nonstandard constraints, the implementation is notable for its progressive transition from the prior business practice. cce has realized an annual cost saving of $45 million and major improvements in customer service. this approach has been so successful that coca-cola has extended it beyond cce to other coca-cola bottling companies and beer distributors. """
http://orkg.org/orkg/resource/R26984,Transporting Sludge to the 106-Mile Site: An Inventory/Routing Model for Fleet Sizing and Logistics System Design,10.1287/trsc.22.3.186,crossref,"<jats:p> This paper develops a model that is being used by the City of New York to design a new logistics system to transport municipal sewage sludge from city-operated wastewater treatment plants to a new ocean dumping site 106 miles offshore. The model provides an integrative framework for considering such strategic planning issues as fleet sizing, choice of vessel size, sizing local inventory holding capacities, and analyzing system behavior with and without transshipment. A unique feature of the model is that plant visitation frequencies are determined naturally by the characteristics of the problem (vessel size, inventory holding capacities, statistics of sludge production, proximity of other plants), rather than stated as exogeneous constraints. The formulation should be useful in a more general class of depot-to-customer distribution systems, including the distribution of industrial gases. The paper concludes with a description of additional research that is required in refining both the assumptions and the mechanisms of execution of the model. </jats:p>","this paper develops a model that is being used by the city of new york to design a new logistics system to transport municipal sewage sludge from city-operated wastewater treatment plants to a new ocean dumping site 106 miles offshore. the model provides an integrative framework for considering such strategic planning issues as fleet sizing, choice of vessel size, sizing local inventory holding capacities, and analyzing system behavior with and without transshipment. a unique feature of the model is that plant visitation frequencies are determined naturally by the characteristics of the problem (vessel size, inventory holding capacities, statistics of sludge production, proximity of other plants), rather than stated as exogeneous constraints. the formulation should be useful in a more general class of depot-to-customer distribution systems, including the distribution of industrial gases. the paper concludes with a description of additional research that is required in refining both the assumptions and the mechanisms of execution of the model."
http://orkg.org/orkg/resource/R26998,Modeling the Increased Complexity of New York City's Refuse Marine Transport System,10.1287/trsc.31.3.272,crossref,"""<jats:p> The New York City Department of Sanitation operates the world's largest refuse marine transport system. Waste trucks unload their cargo at land-based transfer stations where refuse is placed in barges and then towed by tugboats to the Fresh Kills Landfill in Staten Island. In the early 1980s, the city commissioned the development of a computer-based model for use in fleet sizing and operations planning. As a result of the complexities introduced by environmental regulation and technological innovation, the marine transport system operations changed and the existing model became obsolete. Based on the success achieved with the first model in 1993, the city commissioned the development of a new model. In this paper, we present a PC-based model developed to meet the increased complexity of the system. Analysis performed for validation and calibration of the model demonstrates that it tracks well the operations of the real system. We illustrate through a detailed design exercise how to use the model to configure the system in a way that meets the requirements of the refuse marine transport system. </jats:p>""",""" the new york city department of sanitation operates the world's largest refuse marine transport system. waste trucks unload their cargo at land-based transfer stations where refuse is placed in barges and then towed by tugboats to the fresh kills landfill in staten island. in the early 1980s, the city commissioned the development of a computer-based model for use in fleet sizing and operations planning. as a result of the complexities introduced by environmental regulation and technological innovation, the marine transport system operations changed and the existing model became obsolete. based on the success achieved with the first model in 1993, the city commissioned the development of a new model. in this paper, we present a pc-based model developed to meet the increased complexity of the system. analysis performed for validation and calibration of the model demonstrates that it tracks well the operations of the real system. we illustrate through a detailed design exercise how to use the model to configure the system in a way that meets the requirements of the refuse marine transport system. """
http://orkg.org/orkg/resource/R27027,Ship Routing and Scheduling: Status and Perspectives,10.1287/trsc.1030.0036,crossref,"<jats:p> The objective of this paper is to review the current status of ship routing and scheduling. We focus on literature published during the last decade. Because routing and scheduling problems are closely related to many other fleet planning problems, we have divided this review into several parts. We start at the strategic fleet planning level and discuss the design of fleets and sea transport systems. We continue with the tactical and operational fleet planning level and consider problems that comprise various ship routing and scheduling aspects. Here, we separately discuss the different modes of operations: industrial, tramp, and liner shipping. Finally, we take a glimpse at naval applications and other related problems that do not naturally fall into these categories. The paper also presents some perspectives regarding future developments and use of optimization-based decision-support systems for ship routing and scheduling. Several of the trends indicate both accelerating needs for and benefits from such systems and, hopefully, this paper will stimulate further research in this area. </jats:p>","the objective of this paper is to review the current status of ship routing and scheduling. we focus on literature published during the last decade. because routing and scheduling problems are closely related to many other fleet planning problems, we have divided this review into several parts. we start at the strategic fleet planning level and discuss the design of fleets and sea transport systems. we continue with the tactical and operational fleet planning level and consider problems that comprise various ship routing and scheduling aspects. here, we separately discuss the different modes of operations: industrial, tramp, and liner shipping. finally, we take a glimpse at naval applications and other related problems that do not naturally fall into these categories. the paper also presents some perspectives regarding future developments and use of optimization-based decision-support systems for ship routing and scheduling. several of the trends indicate both accelerating needs for and benefits from such systems and, hopefully, this paper will stimulate further research in this area."
http://orkg.org/orkg/resource/R27043,Smart cities ranking: an effective instrument for the positioning of the cities?,10.5821/ace.v4i12.2483,crossref,"<jats:p>Due to different reasons cities are increasingly challenged to improve their competitiveness.&#x0D;\nDifferent strategic efforts are discussed in planning sciences, new approaches and instruments&#x0D;\nare elaborated and applied, steering the positioning of cities in a competitive urban world. As&#x0D;\none specific consequence city rankings have experienced a remarkable boom. However, there&#x0D;\nis some evidence that public attention of city rankings is mainly concentrated simply on the&#x0D;\nranks themselves totally neglecting its meaning as an instrument for strategic planning.&#x0D;\nIn order to elaborate this potential meaning of rankings the paper gives an overview of different&#x0D;\ntypes and introduces an own approach called ‘Smart City ranking’. Based on this ranking&#x0D;\napproach and corresponding experiences of different cities reacting on its dissemination in the&#x0D;\nsecond part the paper shows how this approach can be used as an effective instrument&#x0D;\ndetecting strengths and weaknesses and improving a city’s competitiveness through relevant&#x0D;\nstrategic efforts.</jats:p>\n          <jats:p>Per diferents motius, les ciutats es troben cada vegada en major mesura front el repte de millorar la seva competitivitat. Diferents esforços estratègics són discutits en les ciències del planejament, nous enfocaments i instruments són elaborats i posats en pràctica, guiant el posicionament de les ciutats en un competitiu món urbà. Una conseqüència específica d’això és que la categorització de ciutats ha experimentat un augment notable. No obstant, hi ha certs indicis de que l’atenció pública respecte de la categorització de ciutats es centra principalment en les últimes posicions, oblidant completament la seva significança com instrument de planificació estratègica.&#x0D;\n&#x0D;\nAmb la finalitat de desenvolupar aquesta potencial significança de les categoritzacions, l’article fa un repàs de diferents tipus i presenta una aproximació pròpia anomenada “Smart City Ranking”. Basat en aquest enfocament a la categorització i les corresponents experiències de la reacció de diferents ciutats respecte a la seva difusió, en una segona part l’article mostra com aquesta aproximació pot ser utilitzada com instrument eficaç, reconeixent fortaleses i debilitats i millorant la competitivitat de les ciutats a través d’esforços estratègics adequats.</jats:p>\n          <jats:p>Por diferentes motivos, las ciudades se encuentran cada vez en mayor medida ante el reto de mejorar su competitividad. Distintos esfuerzos estratégicos son discutidos en las ciencias del planeamiento, nuevos enfoques e instrumentos son elaborados y puestos en práctica, guiando el posicionamiento de las ciudades en un mundo urbano competitivo. Una consecuencia específica de ello es que la categorización de ciudades ha experimentado un  auge notable. Sin embargo, hay ciertos indicios de que la atención sobre la jerarquización de ciudades se centra principalmente en las últimas posiciones, olvidando completamente su significancia como instrumento de planificación estratégica.&#x0D;\n&#x0D;\nCon el fin de desarrollar esta potencial significancia este  artículo hace un repaso de distintos tipos y presenta una aproximación propia llamada “Smart City Ranking”. Basado en este enfoque a la categorización y las correspondientes experiencias de la reacción de diferentes ciudades en cuanto a su difusión, en una segunda parte el artículo muestra como esta aproximación puede ser utilizada como un instrumento eficaz, reconociendo fortalezas y debilidades y mejorando la competitividad de las ciudades a través de esfuerzos estratégicos adecuados.</jats:p>","due to different reasons cities are increasingly challenged to improve their competitiveness.&#x0d;\ndifferent strategic efforts are discussed in planning sciences, new approaches and instruments&#x0d;\nare elaborated and applied, steering the positioning of cities in a competitive urban world. as&#x0d;\none specific consequence city rankings have experienced a remarkable boom. however, there&#x0d;\nis some evidence that public attention of city rankings is mainly concentrated simply on the&#x0d;\nranks themselves totally neglecting its meaning as an instrument for strategic planning.&#x0d;\nin order to elaborate this potential meaning of rankings the paper gives an overview of different&#x0d;\ntypes and introduces an own approach called ‘smart city ranking’. based on this ranking&#x0d;\napproach and corresponding experiences of different cities reacting on its dissemination in the&#x0d;\nsecond part the paper shows how this approach can be used as an effective instrument&#x0d;\ndetecting strengths and weaknesses and improving a city’s competitiveness through relevant&#x0d;\nstrategic efforts. \n per diferents motius, les ciutats es troben cada vegada en major mesura front el repte de millorar la seva competitivitat. diferents esforços estratègics són discutits en les ciències del planejament, nous enfocaments i instruments són elaborats i posats en pràctica, guiant el posicionament de les ciutats en un competitiu món urbà. una conseqüència específica d’això és que la categorització de ciutats ha experimentat un augment notable. no obstant, hi ha certs indicis de que l’atenció pública respecte de la categorització de ciutats es centra principalment en les últimes posicions, oblidant completament la seva significança com instrument de planificació estratègica.&#x0d;\n&#x0d;\namb la finalitat de desenvolupar aquesta potencial significança de les categoritzacions, l’article fa un repàs de diferents tipus i presenta una aproximació pròpia anomenada “smart city ranking”. basat en aquest enfocament a la categorització i les corresponents experiències de la reacció de diferents ciutats respecte a la seva difusió, en una segona part l’article mostra com aquesta aproximació pot ser utilitzada com instrument eficaç, reconeixent fortaleses i debilitats i millorant la competitivitat de les ciutats a través d’esforços estratègics adequats. \n por diferentes motivos, las ciudades se encuentran cada vez en mayor medida ante el reto de mejorar su competitividad. distintos esfuerzos estratégicos son discutidos en las ciencias del planeamiento, nuevos enfoques e instrumentos son elaborados y puestos en práctica, guiando el posicionamiento de las ciudades en un mundo urbano competitivo. una consecuencia específica de ello es que la categorización de ciudades ha experimentado un auge notable. sin embargo, hay ciertos indicios de que la atención sobre la jerarquización de ciudades se centra principalmente en las últimas posiciones, olvidando completamente su significancia como instrumento de planificación estratégica.&#x0d;\n&#x0d;\ncon el fin de desarrollar esta potencial significancia este artículo hace un repaso de distintos tipos y presenta una aproximación propia llamada “smart city ranking”. basado en este enfoque a la categorización y las correspondientes experiencias de la reacción de diferentes ciudades en cuanto a su difusión, en una segunda parte el artículo muestra como esta aproximación puede ser utilizada como un instrumento eficaz, reconociendo fortalezas y debilidades y mejorando la competitividad de las ciudades a través de esfuerzos estratégicos adecuados."
http://orkg.org/orkg/resource/R27113,Kinetics of acetylcholinesterase immobilized on polyethylene tubing,10.1139/o79-156,crossref,"<jats:p> Acetylcholinesterase was covalently attached to the inner surface of polyethylene tubing. Initial oxidation generated surface carboxylic groups which, on reaction with thionyl chloride, produced acid chloride groups; these were caused to react with excess ethylenediamine. The amine groups on the surface were linked to glutaraldehyde, and acetylcholinesterase was then attached to the surface. Various kinetic tests showed the catalysis of the hydrolysis of acetylthiocholine iodide to be diffusion controlled. The apparent Michaelis constants were strongly dependent on flow rate and were much larger than the value for the free enzyme. Rate measurements over the temperature range 6–42 °C showed changes in activation energies consistent with diffusion control. </jats:p>","acetylcholinesterase was covalently attached to the inner surface of polyethylene tubing. initial oxidation generated surface carboxylic groups which, on reaction with thionyl chloride, produced acid chloride groups; these were caused to react with excess ethylenediamine. the amine groups on the surface were linked to glutaraldehyde, and acetylcholinesterase was then attached to the surface. various kinetic tests showed the catalysis of the hydrolysis of acetylthiocholine iodide to be diffusion controlled. the apparent michaelis constants were strongly dependent on flow rate and were much larger than the value for the free enzyme. rate measurements over the temperature range 6–42 °c showed changes in activation energies consistent with diffusion control."
http://orkg.org/orkg/resource/R27230,Exchange Rate Volatility and Trade Flows of the U.K. in 1990s,10.1177/223386590500800109,crossref,"<jats:p> This paper examines the impact of exchange rate volatility on trade flows in the U.K. over the period 1990–2000. According to the conventional approach, exchange rate volatility clamps down trade volumes. This paper, however, identifies the existence of a positive relationship between exchange rate volatility and imports in the U.K. in the 1990s by using a bivariate GARCH-in-mean model. It highlights a possible emergence of a polarized version with conventional proposition that ERV works as an impediment factor on trade flows. </jats:p>","this paper examines the impact of exchange rate volatility on trade flows in the u.k. over the period 1990–2000. according to the conventional approach, exchange rate volatility clamps down trade volumes. this paper, however, identifies the existence of a positive relationship between exchange rate volatility and imports in the u.k. in the 1990s by using a bivariate garch-in-mean model. it highlights a possible emergence of a polarized version with conventional proposition that erv works as an impediment factor on trade flows."
http://orkg.org/orkg/resource/R27259,Cyberbotics ltd. webots professional mobile robot simulation,10.5772/5618,crossref,"<jats:p> Cyberbotics Ltd. develops Webots<jats:sup>™</jats:sup>, a mobile robotics simulation software that provides you with a rapid prototyping environment for modelling, programming and simulating mobile robots. The provided robot libraries enable you to transfer your control programs to several commercially available real mobile robots. Webots<jats:sup>™</jats:sup> lets you define and modify a complete mobile robotics setup, even several different robots sharing the same environment. For each object, you can define a number of properties, such as shape, color, texture, mass, friction, etc. You can equip each robot with a large number of available sensors and actuators. You can program these robots using your favorite development environment, simulate them and optionally transfer the resulting programs onto your real robots. Webots<jats:sup>™</jats:sup> has been developed in collaboration with the Swiss Federal Institute of Technology in Lausanne, thoroughly tested, well documented and continuously maintained for over 7 years. It is now the main commercial product available from Cyberbotics Ltd. </jats:p>","cyberbotics ltd. develops webots ™ , a mobile robotics simulation software that provides you with a rapid prototyping environment for modelling, programming and simulating mobile robots. the provided robot libraries enable you to transfer your control programs to several commercially available real mobile robots. webots ™ lets you define and modify a complete mobile robotics setup, even several different robots sharing the same environment. for each object, you can define a number of properties, such as shape, color, texture, mass, friction, etc. you can equip each robot with a large number of available sensors and actuators. you can program these robots using your favorite development environment, simulate them and optionally transfer the resulting programs onto your real robots. webots ™ has been developed in collaboration with the swiss federal institute of technology in lausanne, thoroughly tested, well documented and continuously maintained for over 7 years. it is now the main commercial product available from cyberbotics ltd."
http://orkg.org/orkg/resource/R27374,Relaxation of residual stresses induced by turning and shot peening on steels,10.1243/030932404323042704,crossref,"<jats:p> Experiments on the relaxation of residual stresses in steels by fatigue loading are described. This question is of interest because it is well known that compressive residual stresses are often induced by special surface treatments (such as shot peening) to improve the fatigue life of metal parts; however, if cyclic relaxation occurs, the beneficial effects can, in part, vanish during service. Two hardened and tempered steels of grade C45 and 39NiCrMo3 were used in the tests. For both materials, different specimens were given two surface treatments: simple turning without successive surface treatment, inducing on the surface a moderate tensile residual stress state, and shot peening, inducing high residual compressive stresses. The specimens were submitted to constant-amplitude tension-compression fatigue loading, and the surface residual stresses were measured after 0, 1, 10 cycles and more. Results show that relaxation occurs from the very first cycle; the amount of residual stress relaxation depends on many parameters and on the type of steel. The results are in agreement with data obtained by other researchers. </jats:p>","experiments on the relaxation of residual stresses in steels by fatigue loading are described. this question is of interest because it is well known that compressive residual stresses are often induced by special surface treatments (such as shot peening) to improve the fatigue life of metal parts; however, if cyclic relaxation occurs, the beneficial effects can, in part, vanish during service. two hardened and tempered steels of grade c45 and 39nicrmo3 were used in the tests. for both materials, different specimens were given two surface treatments: simple turning without successive surface treatment, inducing on the surface a moderate tensile residual stress state, and shot peening, inducing high residual compressive stresses. the specimens were submitted to constant-amplitude tension-compression fatigue loading, and the surface residual stresses were measured after 0, 1, 10 cycles and more. results show that relaxation occurs from the very first cycle; the amount of residual stress relaxation depends on many parameters and on the type of steel. the results are in agreement with data obtained by other researchers."
http://orkg.org/orkg/resource/R27407,Experimental Study of Turbulent Flow Heat Transfer and Pressure Drop in a Plate Heat Exchanger With Chevron Plates,10.1115/1.2825923,crossref,"<jats:p>Experimental heat transfer and isothermal pressure drop data for single-phase water flows in a plate heat exchanger (PHE) with chevron plates are presented. In a single-pass U-type counterflow PHE, three different chevron plate arrangements are considered: two symmetric plate arrangements with β = 30 deg/30 deg and 60 deg/60 deg, and one mixed-plate arrangement with β = 30 deg/60 deg. For water (2 &lt; Pr &lt; 6) flow rates in the 600 &lt; Re &lt; 104 regime, data for Nu and f are presented. The results show significant effects of both the chevron angle β and surface area enlargement factor φ. As β increases, and compared to a flat-plate pack, up to two to five times higher Nu are obtained; the concomitant f, however, are 13 to 44 times higher. Increasing φ also has a similar, though smaller effect. Based on experimental data for Re a 7000 and 30 deg ≤ β ≤ 60 deg, predictive correlations of the form Nu = C1,(β) D1(φ) Rep1(β)Pr1/3(μ/μw)0.14 and f = C2(β) D2(φ) Rep2(β) are devised. Finally, at constant pumping power, and depending upon Re, β, and φ, the heat transfer is found to be enhanced by up to 2.8 times that in an equivalent flat-plate channel.</jats:p>","experimental heat transfer and isothermal pressure drop data for single-phase water flows in a plate heat exchanger (phe) with chevron plates are presented. in a single-pass u-type counterflow phe, three different chevron plate arrangements are considered: two symmetric plate arrangements with β = 30 deg/30 deg and 60 deg/60 deg, and one mixed-plate arrangement with β = 30 deg/60 deg. for water (2 &lt; pr &lt; 6) flow rates in the 600 &lt; re &lt; 104 regime, data for nu and f are presented. the results show significant effects of both the chevron angle β and surface area enlargement factor φ. as β increases, and compared to a flat-plate pack, up to two to five times higher nu are obtained; the concomitant f, however, are 13 to 44 times higher. increasing φ also has a similar, though smaller effect. based on experimental data for re a 7000 and 30 deg ≤ β ≤ 60 deg, predictive correlations of the form nu = c1,(β) d1(φ) rep1(β)pr1/3(μ/μw)0.14 and f = c2(β) d2(φ) rep2(β) are devised. finally, at constant pumping power, and depending upon re, β, and φ, the heat transfer is found to be enhanced by up to 2.8 times that in an equivalent flat-plate channel."
http://orkg.org/orkg/resource/R27527,The relationship between energy consumption and economic growth in Pakistan,10.15421/2019_98,crossref,"<jats:p>Energy is substantial for economic development. This study aims to unveil the causal relationship and long-term association between economic growth and energy consumption in Pakistan. The Granger-Causality test finds that; natural gas consumption, electricity consumption and coal consumption have uni-directional causal relationship with economic growth as (GC, EC and CC→GDP), however, GDP growth rate, natural gas consumption and coal consumption unilaterally Granger causes Inflation (GDP, GC and CC→CPI) and lastly coal consumption→natural gas consumption (GC), Electricity consumption (EC)→GC. The ARDL estimations delineate natural gas consumption and oil consumption having a positive and negative association with GDP growth rate may have significant long term impacts respectively on the the economic growth of Pakistan.</jats:p>","energy is substantial for economic development. this study aims to unveil the causal relationship and long-term association between economic growth and energy consumption in pakistan. the granger-causality test finds that; natural gas consumption, electricity consumption and coal consumption have uni-directional causal relationship with economic growth as (gc, ec and cc→gdp), however, gdp growth rate, natural gas consumption and coal consumption unilaterally granger causes inflation (gdp, gc and cc→cpi) and lastly coal consumption→natural gas consumption (gc), electricity consumption (ec)→gc. the ardl estimations delineate natural gas consumption and oil consumption having a positive and negative association with gdp growth rate may have significant long term impacts respectively on the the economic growth of pakistan."
http://orkg.org/orkg/resource/R27682,"Electricity consumption, income, foreign direct investment, and population in Malaysia: new evidence from multivariate framework analysis",10.1108/01443580910973583,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>This study attempts to re‐investigate the electricity consumption function for Malaysia through the cointegration and causality analyses over the period 1970 to 2005.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>The study employed the bounds‐testing procedure for cointegration to examine the potential long‐run relationship, while an autoregressive distributed lag model is used to derive the short‐ and long‐run coefficients. The Granger causality test is applied to determine the causality direction between electricity consumption and its determinants.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>New evidence is found in this study: first, electricity consumption, income, foreign direct investment, and population in Malaysia are cointegrated. Second, the influx of foreign direct investment and population growth are positively related to electricity consumption in Malaysia and the Granger causality evidence indicates that electricity consumption, income, and foreign direct investment are of bilateral causality.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>The estimated multivariate electricity consumption function for Malaysia implies that Malaysia is an energy‐dependent country; thus energy‐saving policies may have an inverse effect on current and also future economic development in Malaysia.</jats:p></jats:sec>","purpose this study attempts to re‐investigate the electricity consumption function for malaysia through the cointegration and causality analyses over the period 1970 to 2005. design/methodology/approach the study employed the bounds‐testing procedure for cointegration to examine the potential long‐run relationship, while an autoregressive distributed lag model is used to derive the short‐ and long‐run coefficients. the granger causality test is applied to determine the causality direction between electricity consumption and its determinants. findings new evidence is found in this study: first, electricity consumption, income, foreign direct investment, and population in malaysia are cointegrated. second, the influx of foreign direct investment and population growth are positively related to electricity consumption in malaysia and the granger causality evidence indicates that electricity consumption, income, and foreign direct investment are of bilateral causality. originality/value the estimated multivariate electricity consumption function for malaysia implies that malaysia is an energy‐dependent country; thus energy‐saving policies may have an inverse effect on current and also future economic development in malaysia."
http://orkg.org/orkg/resource/R27690,Co-integration and causality relationship between energy consumption and economic growth: further empirical evidence for Nigeria,10.3846/jbem.202010.05,crossref,"<jats:p>The Paper re - examined co‐integration and causality relationship between energy consumption and economic growth for Nigeria using data covering the period 1970 to 2005. Unlike previous related study for Nigeria, different proxies of energy consumption (electricity demand, domestic crude oil consumption and gas utilization) were used for the estimation. It also included government activities proxied by health expenditure and monetary policy proxied by broad money supply though; emphasis was on energy consumption. Using the Johansen co‐integration technique, it was found that there existed a long run relationship among the series. It was also found that all the variables used for the study were I(1). Furthermore, unidirectional causality was established between electricity consumption and economic growth, domestic crude oil production and economic growth as well as between gas utilization and economic growth in Nigeria. While causality runs from electricity consumption to economic growth as well as from gas utilization to economic growth, it was found that causality runs from economic growth to domestic crude oil production. Therefore, conservation policy regarding electricity consumption and gas utilization would harm economic growth in Nigeria while energy conservation policy as regards domestic crude oil consumption would not. Santrauka Tyrinejamas energijos suvartojimo ir ekonominio augimo tarpusavio ryšys bei priežastingumas Ni‐gerijoje, remiantis 1970–2005 m. statistiniais duomenimis. Naujai, lyginant su ankstesniais Nigerijos tyrimais, parenkami energijos vartojimo matavimo būdai (elektros energijos paklausa, vietines naftos žaliavos suvartojimas, duju utilizavimas). Straipsnyje atsižvelgiama i socialine ir monetarine valstybes politika, kurios atspindi valstybes gerove. Pritaikius Johansen tarpusavio priklausomybes metodabuvo gauta, kad tarp visu energijos vartojima atspindinčiu rodikliu ir ekonominio augimo yra netiesioginis priežastinis ryšys. Manoma, kad elektros bei dujunaudojimo apribojimas stabdytu Nigerijos ekonomini augima, o naftos žaliavos vartojimo masto mažinimas nepaveiktu tolesnes šalies pletros.</jats:p>","the paper re - examined co‐integration and causality relationship between energy consumption and economic growth for nigeria using data covering the period 1970 to 2005. unlike previous related study for nigeria, different proxies of energy consumption (electricity demand, domestic crude oil consumption and gas utilization) were used for the estimation. it also included government activities proxied by health expenditure and monetary policy proxied by broad money supply though; emphasis was on energy consumption. using the johansen co‐integration technique, it was found that there existed a long run relationship among the series. it was also found that all the variables used for the study were i(1). furthermore, unidirectional causality was established between electricity consumption and economic growth, domestic crude oil production and economic growth as well as between gas utilization and economic growth in nigeria. while causality runs from electricity consumption to economic growth as well as from gas utilization to economic growth, it was found that causality runs from economic growth to domestic crude oil production. therefore, conservation policy regarding electricity consumption and gas utilization would harm economic growth in nigeria while energy conservation policy as regards domestic crude oil consumption would not. santrauka tyrinejamas energijos suvartojimo ir ekonominio augimo tarpusavio ryšys bei priežastingumas ni‐gerijoje, remiantis 1970–2005 m. statistiniais duomenimis. naujai, lyginant su ankstesniais nigerijos tyrimais, parenkami energijos vartojimo matavimo būdai (elektros energijos paklausa, vietines naftos žaliavos suvartojimas, duju utilizavimas). straipsnyje atsižvelgiama i socialine ir monetarine valstybes politika, kurios atspindi valstybes gerove. pritaikius johansen tarpusavio priklausomybes metodabuvo gauta, kad tarp visu energijos vartojima atspindinčiu rodikliu ir ekonominio augimo yra netiesioginis priežastinis ryšys. manoma, kad elektros bei dujunaudojimo apribojimas stabdytu nigerijos ekonomini augima, o naftos žaliavos vartojimo masto mažinimas nepaveiktu tolesnes šalies pletros."
http://orkg.org/orkg/resource/R27735,A video game improves behavioral outcomes in adolescents and young adults with cancer: A randomized trial,10.1542/peds.2007-3134,crossref,"<jats:p>OBJECTIVE. Suboptimal adherence to self-administered medications is a common problem. The purpose of this study was to determine the effectiveness of a video-game intervention for improving adherence and other behavioral outcomes for adolescents and young adults with malignancies including acute leukemia, lymphoma, and soft-tissue sarcoma.</jats:p>\n               <jats:p>METHODS. A randomized trial with baseline and 1- and 3-month assessments was conducted from 2004 to 2005 at 34 medical centers in the United States, Canada, and Australia. A total of 375 male and female patients who were 13 to 29 years old, had an initial or relapse diagnosis of a malignancy, and currently undergoing treatment and expected to continue treatment for at least 4 months from baseline assessment were randomly assigned to the intervention or control group. The intervention was a video game that addressed issues of cancer treatment and care for teenagers and young adults. Outcome measures included adherence, self-efficacy, knowledge, control, stress, and quality of life. For patients who were prescribed prophylactic antibiotics, adherence to trimethoprim-sulfamethoxazole was tracked by electronic pill-monitoring devices (n = 200). Adherence to 6-mercaptopurine was assessed through serum metabolite assays (n = 54).</jats:p>\n               <jats:p>RESULTS. Adherence to trimethoprim-sulfamethoxazole and 6-mercaptopurine was greater in the intervention group. Self-efficacy and knowledge also increased in the intervention group compared with the control group. The intervention did not affect self-report measures of adherence, stress, control, or quality of life.</jats:p>\n               <jats:p>CONCLUSIONS. The video-game intervention significantly improved treatment adherence and indicators of cancer-related self-efficacy and knowledge in adolescents and young adults who were undergoing cancer therapy. The findings support current efforts to develop effective video-game interventions for education and training in health care.</jats:p>","objective. suboptimal adherence to self-administered medications is a common problem. the purpose of this study was to determine the effectiveness of a video-game intervention for improving adherence and other behavioral outcomes for adolescents and young adults with malignancies including acute leukemia, lymphoma, and soft-tissue sarcoma. \n methods. a randomized trial with baseline and 1- and 3-month assessments was conducted from 2004 to 2005 at 34 medical centers in the united states, canada, and australia. a total of 375 male and female patients who were 13 to 29 years old, had an initial or relapse diagnosis of a malignancy, and currently undergoing treatment and expected to continue treatment for at least 4 months from baseline assessment were randomly assigned to the intervention or control group. the intervention was a video game that addressed issues of cancer treatment and care for teenagers and young adults. outcome measures included adherence, self-efficacy, knowledge, control, stress, and quality of life. for patients who were prescribed prophylactic antibiotics, adherence to trimethoprim-sulfamethoxazole was tracked by electronic pill-monitoring devices (n = 200). adherence to 6-mercaptopurine was assessed through serum metabolite assays (n = 54). \n results. adherence to trimethoprim-sulfamethoxazole and 6-mercaptopurine was greater in the intervention group. self-efficacy and knowledge also increased in the intervention group compared with the control group. the intervention did not affect self-report measures of adherence, stress, control, or quality of life. \n conclusions. the video-game intervention significantly improved treatment adherence and indicators of cancer-related self-efficacy and knowledge in adolescents and young adults who were undergoing cancer therapy. the findings support current efforts to develop effective video-game interventions for education and training in health care."
http://orkg.org/orkg/resource/R27767,"What Do Students Learn When Collaboratively Using A Computer Game in the Study of Historical Disease Epidemics, and Why?",10.1177/1555412011431361,crossref,"<jats:p> The use of computer games and virtual environments has been shown to engage and motivate students and can provide opportunities to visualize the historical period and make sense of complex visual information. This article presents the results of a study in which university students were asked to collaboratively solve inquiry-based problems related to historical disease epidemics using game-based learning. A multimethod approach to the data collection was used. Initial results indicated that students attended to visual information with more specificity than text-based information when using a virtual environment. Models of student’s decision-making processes when interacting with the world confirmed that students were making decisions related to these visual elements, and not the inquiry process. Building on theories from the learning sciences, such as learning from animations/visualizations and computer-supported collaborative learning, in this article, the authors begin to answer the question of why students learned what they did about historical disease epidemics. </jats:p>","the use of computer games and virtual environments has been shown to engage and motivate students and can provide opportunities to visualize the historical period and make sense of complex visual information. this article presents the results of a study in which university students were asked to collaboratively solve inquiry-based problems related to historical disease epidemics using game-based learning. a multimethod approach to the data collection was used. initial results indicated that students attended to visual information with more specificity than text-based information when using a virtual environment. models of student’s decision-making processes when interacting with the world confirmed that students were making decisions related to these visual elements, and not the inquiry process. building on theories from the learning sciences, such as learning from animations/visualizations and computer-supported collaborative learning, in this article, the authors begin to answer the question of why students learned what they did about historical disease epidemics."
http://orkg.org/orkg/resource/R27829,Surgical experience correlates with performance on a virtual reality simulator for shoulder arthroscopy,10.1177/0363546506296521,crossref,"<jats:sec><jats:title>Background</jats:title><jats:p> The traditional process of surgical education is being increasingly challenged by economic constraints and concerns about patient safety. Sophisticated computer-based devices have become available to simulate the surgical experience in a protected environment. As with any new educational tool, these devices have generated controversy about the validity of the training experience. </jats:p></jats:sec><jats:sec><jats:title>Hypothesis</jats:title><jats:p> Performance on a virtual reality simulator correlates with actual surgical experience. </jats:p></jats:sec><jats:sec><jats:title>Study Design</jats:title><jats:p> Controlled laboratory study. </jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p> Forty-three test subjects of various experience levels in shoulder arthroscopy were tested on an arthroscopy simulator according to a standardized protocol. Subjects were evaluated for time to completion, distance traveled with the tip of the simulated probe compared with a computer-determined optimal distance, average probe velocity, and number of probe collisions with the tissues. </jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p> Subjects were grouped according to prior experience with shoulder arthroscopy. Comparing the least experienced with most experienced groups, the average time to completion decreased by 62% from 128.8 seconds to 49.2 seconds; path length and hook collisions were more than halved from 8.2 to 3.8 and 34.1 to 16.8, respectively; and average probe velocity more than doubled from 0.18 to 0.4 cm/second. There were no significant differences for any parameter tested between subjects with video game experience compared to those without. </jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p> The study demonstrated a close and statistically significant correlation between simulator results and surgical experience, thus confirming the hypothesis. Conversely, experience with video games was not associated with improved simulator performance. This indicates that the skill set tested may be similar to the one developed in the operating room, thus suggesting its use as a potential tool for future evaluation of surgical trainees. </jats:p></jats:sec><jats:sec><jats:title>Clinical Relevance</jats:title><jats:p> The results have implications for the future of orthopaedic surgical training programs, the majority of which have not embraced virtual reality technology for physician education. </jats:p></jats:sec>","background the traditional process of surgical education is being increasingly challenged by economic constraints and concerns about patient safety. sophisticated computer-based devices have become available to simulate the surgical experience in a protected environment. as with any new educational tool, these devices have generated controversy about the validity of the training experience. hypothesis performance on a virtual reality simulator correlates with actual surgical experience. study design controlled laboratory study. methods forty-three test subjects of various experience levels in shoulder arthroscopy were tested on an arthroscopy simulator according to a standardized protocol. subjects were evaluated for time to completion, distance traveled with the tip of the simulated probe compared with a computer-determined optimal distance, average probe velocity, and number of probe collisions with the tissues. results subjects were grouped according to prior experience with shoulder arthroscopy. comparing the least experienced with most experienced groups, the average time to completion decreased by 62% from 128.8 seconds to 49.2 seconds; path length and hook collisions were more than halved from 8.2 to 3.8 and 34.1 to 16.8, respectively; and average probe velocity more than doubled from 0.18 to 0.4 cm/second. there were no significant differences for any parameter tested between subjects with video game experience compared to those without. conclusions the study demonstrated a close and statistically significant correlation between simulator results and surgical experience, thus confirming the hypothesis. conversely, experience with video games was not associated with improved simulator performance. this indicates that the skill set tested may be similar to the one developed in the operating room, thus suggesting its use as a potential tool for future evaluation of surgical trainees. clinical relevance the results have implications for the future of orthopaedic surgical training programs, the majority of which have not embraced virtual reality technology for physician education."
http://orkg.org/orkg/resource/R27831,Individual Skill Progression on a Virtual Reality Simulator for Shoulder Arthroscopy A 3-Year Follow-up Study,10.1177/0363546508314406,crossref,"<jats:sec><jats:title>Background</jats:title><jats:p> Previous studies have demonstrated a correlation between surgical experience and performance on a virtual reality arthroscopy simulator but only provided single time point evaluations. Additional longitudinal studies are necessary to confirm the validity of virtual reality simulation before these teaching aids can be more fully recommended for surgical education. </jats:p></jats:sec><jats:sec><jats:title>Hypothesis</jats:title><jats:p> Subjects will show improved performance on simulator retesting several years after an initial baseline evaluation, commensurate with their advanced surgical experience. </jats:p></jats:sec><jats:sec><jats:title>Study Design</jats:title><jats:p> Controlled laboratory study. </jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p> After gaining further arthroscopic experience, 10 orthopaedic residents underwent retesting 3 years after initial evaluation on a Procedicus virtual reality arthroscopy simulator. Using a paired t test, simulator parameters were compared in each subject before and after additional arthroscopic experience. Subjects were evaluated for time to completion, number of probe collisions with the tissues, average probe velocity, and distance traveled with the tip of the simulated probe compared to an optimal computer-determined distance. In addition, to evaluate consistency of simulator performance, results were compared to historical controls of equal experience. </jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p> Subjects improved significantly ( P &lt; .02 for all) in the 4 simulator parameters: completion time (−51 %), probe collisions (−29%), average velocity (+122%), and distance traveled (−;32%). With the exception of probe velocity, there were no significant differences between the performance of this group and that of a historical group with equal experience, indicating that groups with similar arthroscopic experience consistently demonstrate equivalent scores on the simulator. </jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p> Subjects significantly improved their performance on simulator retesting 3 years after initial evaluation. Additionally, across independent groups with equivalent surgical experience, similar performance can be expected on simulator parameters; thus it may eventually be possible to establish simulator benchmarks to indicate likely arthroscopic skill. </jats:p></jats:sec><jats:sec><jats:title>Clinical Relevance</jats:title><jats:p> These results further validate the use of surgical simulation as an important tool for the evaluation of surgical skills. </jats:p></jats:sec>","background previous studies have demonstrated a correlation between surgical experience and performance on a virtual reality arthroscopy simulator but only provided single time point evaluations. additional longitudinal studies are necessary to confirm the validity of virtual reality simulation before these teaching aids can be more fully recommended for surgical education. hypothesis subjects will show improved performance on simulator retesting several years after an initial baseline evaluation, commensurate with their advanced surgical experience. study design controlled laboratory study. methods after gaining further arthroscopic experience, 10 orthopaedic residents underwent retesting 3 years after initial evaluation on a procedicus virtual reality arthroscopy simulator. using a paired t test, simulator parameters were compared in each subject before and after additional arthroscopic experience. subjects were evaluated for time to completion, number of probe collisions with the tissues, average probe velocity, and distance traveled with the tip of the simulated probe compared to an optimal computer-determined distance. in addition, to evaluate consistency of simulator performance, results were compared to historical controls of equal experience. results subjects improved significantly ( p &lt; .02 for all) in the 4 simulator parameters: completion time (−51 %), probe collisions (−29%), average velocity (+122%), and distance traveled (−;32%). with the exception of probe velocity, there were no significant differences between the performance of this group and that of a historical group with equal experience, indicating that groups with similar arthroscopic experience consistently demonstrate equivalent scores on the simulator. conclusion subjects significantly improved their performance on simulator retesting 3 years after initial evaluation. additionally, across independent groups with equivalent surgical experience, similar performance can be expected on simulator parameters; thus it may eventually be possible to establish simulator benchmarks to indicate likely arthroscopic skill. clinical relevance these results further validate the use of surgical simulation as an important tool for the evaluation of surgical skills."
http://orkg.org/orkg/resource/R28008,Matching Cost Filtering for Dense Stereo Correspondence,10.1155/2013/654139,crossref,"<jats:p>Dense stereo correspondence enabling reconstruction of depth information in a scene is of great importance in the field of computer vision. Recently, some local solutions based on matching cost filtering with an edge-preserving filter have been proved to be capable of achieving more accuracy than global approaches. Unfortunately, the computational complexity of these algorithms is quadratically related to the window size used to aggregate the matching costs. The recent trend has been to pursue higher accuracy with greater efficiency in execution. Therefore, this paper proposes a new cost-aggregation module to compute the matching responses for all the image pixels at a set of sampling points generated by a hierarchical clustering algorithm. The complexity of this implementation is linear both in the number of image pixels and the number of clusters. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art local methods in terms of both accuracy and speed. Moreover, performance tests indicate that parameters such as the height of the hierarchical binary tree and the spatial and range standard deviations have a significant influence on time consumption and the accuracy of disparity maps.</jats:p>","dense stereo correspondence enabling reconstruction of depth information in a scene is of great importance in the field of computer vision. recently, some local solutions based on matching cost filtering with an edge-preserving filter have been proved to be capable of achieving more accuracy than global approaches. unfortunately, the computational complexity of these algorithms is quadratically related to the window size used to aggregate the matching costs. the recent trend has been to pursue higher accuracy with greater efficiency in execution. therefore, this paper proposes a new cost-aggregation module to compute the matching responses for all the image pixels at a set of sampling points generated by a hierarchical clustering algorithm. the complexity of this implementation is linear both in the number of image pixels and the number of clusters. experimental results demonstrate that the proposed algorithm outperforms state-of-the-art local methods in terms of both accuracy and speed. moreover, performance tests indicate that parameters such as the height of the hierarchical binary tree and the spatial and range standard deviations have a significant influence on time consumption and the accuracy of disparity maps."
http://orkg.org/orkg/resource/R28037,"A performance and energy comparison of convolution on GPUs, FPGAs, and multicore processors",10.1145/2400682.2400684,crossref,"<jats:p>Recent architectural trends have focused on increased parallelism via multicore processors and increased heterogeneity via accelerator devices (e.g., graphics-processing units, field-programmable gate arrays). Although these architectures have significant performance and energy potential, application designers face many device-specific challenges when choosing an appropriate accelerator or when customizing an algorithm for an accelerator. To help address this problem, in this article we thoroughly evaluate convolution, one of the most common operations in digital-signal processing, on multicores, graphics-processing units, and field-programmable gate arrays. Whereas many previous application studies evaluate a specific usage of an application, this article assists designers with design space exploration for numerous use cases by analyzing effects of different input sizes, different algorithms, and different devices, while also determining Pareto-optimal trade-offs between performance and energy.</jats:p>","recent architectural trends have focused on increased parallelism via multicore processors and increased heterogeneity via accelerator devices (e.g., graphics-processing units, field-programmable gate arrays). although these architectures have significant performance and energy potential, application designers face many device-specific challenges when choosing an appropriate accelerator or when customizing an algorithm for an accelerator. to help address this problem, in this article we thoroughly evaluate convolution, one of the most common operations in digital-signal processing, on multicores, graphics-processing units, and field-programmable gate arrays. whereas many previous application studies evaluate a specific usage of an application, this article assists designers with design space exploration for numerous use cases by analyzing effects of different input sizes, different algorithms, and different devices, while also determining pareto-optimal trade-offs between performance and energy."
http://orkg.org/orkg/resource/R28059,Fast and Accurate Stereo Vision System on FPGA,10.1145/2567659,crossref,"<jats:p>In this article, we present a fast and high quality stereo matching algorithm on FPGA using cost aggregation (CA) and fast locally consistent (FLC) dense stereo. In many software programs, global matching algorithms are used in order to obtain accurate disparity maps. Although their error rates are considerably low, their processing speeds are far from that required for real-time processing because of their complex processing sequences. In order to realize real-time processing, many hardware systems have been proposed to date. They have achieved considerably high processing speeds; however, their error rates are not as good as those of software programs, because simple local matching algorithms have been widely used in those systems. In our system, sophisticated local matching algorithms (CA and FLC) that are suitable for FPGA implementation are used to achieve low error rate while maintaining the high processing speed. We evaluate the performance of our circuit on Xilinx Vertex-6 FPGAs. Its error rate is comparable to that of top-level software algorithms, and its processing speed is nearly 2 clock cycles per pixel, which reaches 507.9 fps for 640 480 pixel images.</jats:p>","in this article, we present a fast and high quality stereo matching algorithm on fpga using cost aggregation (ca) and fast locally consistent (flc) dense stereo. in many software programs, global matching algorithms are used in order to obtain accurate disparity maps. although their error rates are considerably low, their processing speeds are far from that required for real-time processing because of their complex processing sequences. in order to realize real-time processing, many hardware systems have been proposed to date. they have achieved considerably high processing speeds; however, their error rates are not as good as those of software programs, because simple local matching algorithms have been widely used in those systems. in our system, sophisticated local matching algorithms (ca and flc) that are suitable for fpga implementation are used to achieve low error rate while maintaining the high processing speed. we evaluate the performance of our circuit on xilinx vertex-6 fpgas. its error rate is comparable to that of top-level software algorithms, and its processing speed is nearly 2 clock cycles per pixel, which reaches 507.9 fps for 640 480 pixel images."
http://orkg.org/orkg/resource/R28067,Hardware-Efficient Design of Real-Time Profile Shape Matching Stereo Vision Algorithm on FPGA,10.1155/2014/945926,crossref,"<jats:p>A variety of platforms, such as micro-unmanned vehicles, are limited in the amount of computational hardware they can support due to weight and power constraints. An efficient stereo vision algorithm implemented on an FPGA would be able to minimize payload and power consumption in microunmanned vehicles, while providing 3D information and still leaving computational resources available for other processing tasks. This work presents a hardware design of the efficient profile shape matching stereo vision algorithm. Hardware resource usage is presented for the targeted micro-UV platform, Helio-copter, that uses the Xilinx Virtex 4 FX60 FPGA. Less than a fifth of the resources on this FGPA were used to produce dense disparity maps for image sizes up to 450 × 375, with the ability to scale up easily by increasing BRAM usage. A comparison is given of accuracy, speed performance, and resource usage of a census transform-based stereo vision FPGA implementation by Jin et al. Results show that the profile shape matching algorithm is an efficient real-time stereo vision algorithm for hardware implementation for resource limited systems such as microunmanned vehicles.</jats:p>","a variety of platforms, such as micro-unmanned vehicles, are limited in the amount of computational hardware they can support due to weight and power constraints. an efficient stereo vision algorithm implemented on an fpga would be able to minimize payload and power consumption in microunmanned vehicles, while providing 3d information and still leaving computational resources available for other processing tasks. this work presents a hardware design of the efficient profile shape matching stereo vision algorithm. hardware resource usage is presented for the targeted micro-uv platform, helio-copter, that uses the xilinx virtex 4 fx60 fpga. less than a fifth of the resources on this fgpa were used to produce dense disparity maps for image sizes up to 450 × 375, with the ability to scale up easily by increasing bram usage. a comparison is given of accuracy, speed performance, and resource usage of a census transform-based stereo vision fpga implementation by jin et al. results show that the profile shape matching algorithm is an efficient real-time stereo vision algorithm for hardware implementation for resource limited systems such as microunmanned vehicles."
http://orkg.org/orkg/resource/R28135,Duodenal gangliocytic paraganglioma showing lymph node metastasis: A rare case report,10.1186/1746-1596-5-27,crossref,"<jats:title>Abstract</jats:title>\n          <jats:p>We describe a case of duodenal gangliocytic paraganglioma showing lymph node metastasis. A 61-year-old Japanese man underwent pylorus preserving pancreaticoduodenectomy to remove a tumor at the papilla of Vater. The section of the tumor extending from the mucosa to submucosa of the duodenum was sharply demarcated, solid, and white-yellowish. Neither necrosis nor hemorrhage was present. Histological examination confirmed the immunohistochemical identification of three components comprising epithelioid cells, spindle-shaped cells, and ganglion-like cells. Epithelioid cells showed positive reactivity for synaptophysin, somatostatin, and CD56. In contrast, spindle-shaped cells showed positive reactivity for S-100 protein, but not for synaptophysin, somatostatin or CD56. Furthermore, we found lymph node metastasis despite lack of bcl-2 and p53 expression. In addition to the rarity of the tumor, we are describing here the present case suggests the malignant potency of the tumor despite lack of acceptable prognostic indicators for neuroendocrine tumor.</jats:p>","abstract \n we describe a case of duodenal gangliocytic paraganglioma showing lymph node metastasis. a 61-year-old japanese man underwent pylorus preserving pancreaticoduodenectomy to remove a tumor at the papilla of vater. the section of the tumor extending from the mucosa to submucosa of the duodenum was sharply demarcated, solid, and white-yellowish. neither necrosis nor hemorrhage was present. histological examination confirmed the immunohistochemical identification of three components comprising epithelioid cells, spindle-shaped cells, and ganglion-like cells. epithelioid cells showed positive reactivity for synaptophysin, somatostatin, and cd56. in contrast, spindle-shaped cells showed positive reactivity for s-100 protein, but not for synaptophysin, somatostatin or cd56. furthermore, we found lymph node metastasis despite lack of bcl-2 and p53 expression. in addition to the rarity of the tumor, we are describing here the present case suggests the malignant potency of the tumor despite lack of acceptable prognostic indicators for neuroendocrine tumor."
http://orkg.org/orkg/resource/R28193,A Two-Stage Stochastic Network Model and Solution Methods for the Dynamic Empty Container Allocation Problem,10.1287/trsc.32.2.142,crossref,"""<jats:p> Containerized liner trades have been growing steadily since the globalization of world economies intensified in the early 1990s. However, these trades are typically imbalanced in terms of the numbers of inbound and outbound containers. As a result, the relocation of empty containers has become one of the major problems faced by liner operators. In this paper, we consider the dynamic empty container allocation problem where we need to reposition empty containers and to determine the number of leased containers needed to meet customers' demand over time. We formulate this problem as a two-stage stochastic network: in stage one, the parameters such as supplies, demands, and ship capacities for empty containers are deterministic; whereas in stage two, these parameters are random variables. We need to make decisions in stage one such that the total of the stage one cost and the expected stage two cost is minimized. By taking advantage of the network structure, we show how a stochastic quasi-gradient method and a stochastic hybrid approximation procedure can be applied to solve the problem. In addition, we propose some new variations of these methods that seem to work faster in practice. We conduct numerical tests to evaluate the value of the two-stage stochastic model over a rolling horizon environment and to investigate the behavior of the solution methods with different implementations. </jats:p>""",""" containerized liner trades have been growing steadily since the globalization of world economies intensified in the early 1990s. however, these trades are typically imbalanced in terms of the numbers of inbound and outbound containers. as a result, the relocation of empty containers has become one of the major problems faced by liner operators. in this paper, we consider the dynamic empty container allocation problem where we need to reposition empty containers and to determine the number of leased containers needed to meet customers' demand over time. we formulate this problem as a two-stage stochastic network: in stage one, the parameters such as supplies, demands, and ship capacities for empty containers are deterministic; whereas in stage two, these parameters are random variables. we need to make decisions in stage one such that the total of the stage one cost and the expected stage two cost is minimized. by taking advantage of the network structure, we show how a stochastic quasi-gradient method and a stochastic hybrid approximation procedure can be applied to solve the problem. in addition, we propose some new variations of these methods that seem to work faster in practice. we conduct numerical tests to evaluate the value of the two-stage stochastic model over a rolling horizon environment and to investigate the behavior of the solution methods with different implementations. """
http://orkg.org/orkg/resource/R28307,Schedule Design and Container Routing in Liner Shipping,10.3141/2222-04,crossref,"<jats:p> A liner shipping company seeks to provide liner services with shorter transit time compared with the benchmark of market-level transit time because of the ever-increasing competition. When the itineraries of its liner service routes are determined, the liner shipping company designs the schedules of the liner routes such that the wait time at transshipment ports is minimized. As a result of transshipment, multiple paths are available for delivering containers from the origin port to the destination port. Therefore, the medium-term (3 to 6 months) schedule design problem and the operational-level container-routing problem must be investigated simultaneously. The schedule design and container-routing problems were formulated by minimization of the sum of the total transshipment cost and penalty cost associated with longer transit time than the market-level transit time, minus the bonus for shorter transit time. The formulation is nonlinear, noncontinuous, and nonconvex. A genetic local search approach was developed to find good solutions to the problem. The proposed solution method was applied to optimize the Asia–Europe–Oceania liner shipping services of a global liner company. </jats:p>","a liner shipping company seeks to provide liner services with shorter transit time compared with the benchmark of market-level transit time because of the ever-increasing competition. when the itineraries of its liner service routes are determined, the liner shipping company designs the schedules of the liner routes such that the wait time at transshipment ports is minimized. as a result of transshipment, multiple paths are available for delivering containers from the origin port to the destination port. therefore, the medium-term (3 to 6 months) schedule design problem and the operational-level container-routing problem must be investigated simultaneously. the schedule design and container-routing problems were formulated by minimization of the sum of the total transshipment cost and penalty cost associated with longer transit time than the market-level transit time, minus the bonus for shorter transit time. the formulation is nonlinear, noncontinuous, and nonconvex. a genetic local search approach was developed to find good solutions to the problem. the proposed solution method was applied to optimize the asia–europe–oceania liner shipping services of a global liner company."
http://orkg.org/orkg/resource/R28344,Designing container shipping network under changing demand and freight rates,10.3846/transport.2010.07,crossref,"<jats:p>This paper focuses on the optimization of container shipping network and its operations under changing cargo demand and freight rates. The problem is formulated as a mixed integer non-linear programming problem (MINP) with an objective of maximizing the average unit ship-slot profit at three stages using analytical methodology. The issues such as empty container repositioning, ship-slot allocating, ship sizing, and container configuration are simultaneously considered based on a series of the matrices of demand for a year. To solve the model, a bi-level genetic algorithm based method is proposed. Finally, numerical experiments are provided to illustrate the validity of the proposed model and algorithms. The obtained results show that the suggested model can provide a more realistic solution to the issues on the basis of changing demand and freight rates and arrange a more effective approach to the optimization of container shipping network structures and operations than does the model based on the average demand.</jats:p>","this paper focuses on the optimization of container shipping network and its operations under changing cargo demand and freight rates. the problem is formulated as a mixed integer non-linear programming problem (minp) with an objective of maximizing the average unit ship-slot profit at three stages using analytical methodology. the issues such as empty container repositioning, ship-slot allocating, ship sizing, and container configuration are simultaneously considered based on a series of the matrices of demand for a year. to solve the model, a bi-level genetic algorithm based method is proposed. finally, numerical experiments are provided to illustrate the validity of the proposed model and algorithms. the obtained results show that the suggested model can provide a more realistic solution to the issues on the basis of changing demand and freight rates and arrange a more effective approach to the optimization of container shipping network structures and operations than does the model based on the average demand."
http://orkg.org/orkg/resource/R28356,A model and solution algorithm for optimal routing of a time-chartered containership,10.1287/trsc.22.2.83,crossref,"<jats:p> We formulate a mathematical programming model for optimally routing a chartered container ship. Our model helps in evaluating whether a container ship should be chartered or not. The model calculates the optimal sequence of port calls, the number of containers transported between port pairs, and the number of trips the ship makes in the chartered period. A specialized algorithm is developed to solve the integer network subprograms which determine the sequence of port calls. Our algorithm, which solves an integer program optimally, is quite efficient. Comparison of computational results with a Lagrangean Relaxation method and an embedded dynamic program are also presented. </jats:p>","we formulate a mathematical programming model for optimally routing a chartered container ship. our model helps in evaluating whether a container ship should be chartered or not. the model calculates the optimal sequence of port calls, the number of containers transported between port pairs, and the number of trips the ship makes in the chartered period. a specialized algorithm is developed to solve the integer network subprograms which determine the sequence of port calls. our algorithm, which solves an integer program optimally, is quite efficient. comparison of computational results with a lagrangean relaxation method and an embedded dynamic program are also presented."
http://orkg.org/orkg/resource/R28372,Ship Scheduling and Network Design for Cargo Routing in Liner Shipping,10.1287/trsc.1070.0205,crossref,"""<jats:p> Acommon problem faced by carriers in liner shipping is the design of their service network. Given a set of demands to be transported and a set of ports, a carrier wants to design service routes for its ships as efficiently as possible, using the underlying facilities. Furthermore, the profitability of the service routes designed depends on the paths chosen to ship the cargo. We present an integrated model, a mixed-integer linear program, to solve the ship-scheduling and the cargo-routing problems, simultaneously. The proposed model incorporates relevant constraints, such as the weekly frequency constraint on the operated routes, and emerging trends, such as the transshipment of cargo between two or more service routes. To solve the mixed-integer program, we propose algorithms that exploit the separability of the problem. More specifically, a greedy heuristic, a column generation-based algorithm, and a two-phase Benders decomposition-based algorithm are developed, and their computational efficiency in terms of the solution quality and the computational time taken is discussed. An efficient iterative search algorithm is proposed to generate schedules for ships. Computational experiments are performed on randomly generated instances simulating real life with up to 20 ports and 100 ships. Our results indicate high percentage utilization of ships' capacities and a significant number of transshipments in the final solution. </jats:p>""",""" acommon problem faced by carriers in liner shipping is the design of their service network. given a set of demands to be transported and a set of ports, a carrier wants to design service routes for its ships as efficiently as possible, using the underlying facilities. furthermore, the profitability of the service routes designed depends on the paths chosen to ship the cargo. we present an integrated model, a mixed-integer linear program, to solve the ship-scheduling and the cargo-routing problems, simultaneously. the proposed model incorporates relevant constraints, such as the weekly frequency constraint on the operated routes, and emerging trends, such as the transshipment of cargo between two or more service routes. to solve the mixed-integer program, we propose algorithms that exploit the separability of the problem. more specifically, a greedy heuristic, a column generation-based algorithm, and a two-phase benders decomposition-based algorithm are developed, and their computational efficiency in terms of the solution quality and the computational time taken is discussed. an efficient iterative search algorithm is proposed to generate schedules for ships. computational experiments are performed on randomly generated instances simulating real life with up to 20 ports and 100 ships. our results indicate high percentage utilization of ships' capacities and a significant number of transshipments in the final solution. """
http://orkg.org/orkg/resource/R28375,Network Design and Allocation Mechanisms for Carrier Alliances in Liner Shipping,10.1287/opre.1100.0848,crossref,"<jats:p> Many real-world systems operate in a decentralized manner, where individual operators interact with varying degrees of cooperation and self motive. In this paper, we study transportation networks that operate as an alliance among different carriers. In particular, we study alliance formation among carriers in liner shipping. We address tactical problems such as the design of large-scale networks (that result from integrating the service networks of different carriers in an alliance) and operational problems such as the allocation of limited capacity on a transportation network among the carriers in the alliance. We utilize concepts from mathematical programming and game theory and design a mechanism to guide the carriers in an alliance to pursue an optimal collaborative strategy. The mechanism provides side payments to the carriers, as an added incentive, to motivate them to act in the best interest of the alliance while maximizing their own profits. Our computational results suggest that the mechanism can be used to help carriers form sustainable alliances. </jats:p>","many real-world systems operate in a decentralized manner, where individual operators interact with varying degrees of cooperation and self motive. in this paper, we study transportation networks that operate as an alliance among different carriers. in particular, we study alliance formation among carriers in liner shipping. we address tactical problems such as the design of large-scale networks (that result from integrating the service networks of different carriers in an alliance) and operational problems such as the allocation of limited capacity on a transportation network among the carriers in the alliance. we utilize concepts from mathematical programming and game theory and design a mechanism to guide the carriers in an alliance to pursue an optimal collaborative strategy. the mechanism provides side payments to the carriers, as an added incentive, to motivate them to act in the best interest of the alliance while maximizing their own profits. our computational results suggest that the mechanism can be used to help carriers form sustainable alliances."
http://orkg.org/orkg/resource/R28383,A Base Integer Programming Model and Benchmark Suite for Liner-Shipping Network Design,10.1287/trsc.2013.0471,crossref,"<jats:p> The liner-shipping network design problem is to create a set of nonsimple cyclic sailing routes for a designated fleet of container vessels that jointly transports multiple commodities. The objective is to maximize the revenue of cargo transport while minimizing the costs of operation. The potential for making cost-effective and energy-efficient liner-shipping networks using operations research (OR) is huge and neglected. The implementation of logistic planning tools based upon OR has enhanced performance of airlines, railways, and general transportation companies, but within the field of liner shipping, applications of OR are scarce. We believe that access to domain knowledge and data is a barrier for researchers to approach the important liner-shipping network design problem. The purpose of the benchmark suite and the paper at hand is to provide easy access to the domain and the data sources of liner shipping for OR researchers in general. We describe and analyze the liner-shipping domain applied to network design and present a rich integer programming model based on services that constitute the fixed schedule of a liner shipping company. We prove the liner-shipping network design problem to be strongly NP-hard. A benchmark suite of data instances to reflect the business structure of a global liner shipping network is presented. The design of the benchmark suite is discussed in relation to industry standards, business rules, and mathematical programming. The data are based on real-life data from the largest global liner-shipping company, Maersk Line, and supplemented by data from several industry and public stakeholders. Computational results yielding the first best known solutions for six of the seven benchmark instances is provided using a heuristic combining tabu search and heuristic column generation. </jats:p>","the liner-shipping network design problem is to create a set of nonsimple cyclic sailing routes for a designated fleet of container vessels that jointly transports multiple commodities. the objective is to maximize the revenue of cargo transport while minimizing the costs of operation. the potential for making cost-effective and energy-efficient liner-shipping networks using operations research (or) is huge and neglected. the implementation of logistic planning tools based upon or has enhanced performance of airlines, railways, and general transportation companies, but within the field of liner shipping, applications of or are scarce. we believe that access to domain knowledge and data is a barrier for researchers to approach the important liner-shipping network design problem. the purpose of the benchmark suite and the paper at hand is to provide easy access to the domain and the data sources of liner shipping for or researchers in general. we describe and analyze the liner-shipping domain applied to network design and present a rich integer programming model based on services that constitute the fixed schedule of a liner shipping company. we prove the liner-shipping network design problem to be strongly np-hard. a benchmark suite of data instances to reflect the business structure of a global liner shipping network is presented. the design of the benchmark suite is discussed in relation to industry standards, business rules, and mathematical programming. the data are based on real-life data from the largest global liner-shipping company, maersk line, and supplemented by data from several industry and public stakeholders. computational results yielding the first best known solutions for six of the seven benchmark instances is provided using a heuristic combining tabu search and heuristic column generation."
http://orkg.org/orkg/resource/R28609,Undifferentiated embryonal sarcoma of the liver mimicking acute appendicitis. Case report and review of the literature,10.1186/1477-7819-4-9,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Undifferentiated embryonal sarcoma (UES) of liver is a rare malignant neoplasm, which affects mostly the pediatric population accounting for 13% of pediatric hepatic malignancies, a few cases has been reported in adults.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Case presentation</jats:title>\n            <jats:p>We report a case of undifferentiated embryonal sarcoma of the liver in a 20-year-old Caucasian male. The patient was referred to us for further investigation after a laparotomy in a district hospital for spontaneous abdominal hemorrhage, which was due to a liver mass. After a through evaluation with computed tomography scan and magnetic resonance imaging of the liver and taking into consideration the previous history of the patient, it was decided to surgically explore the patient. Resection of I–IV and VIII hepatic lobe. Patient developed disseminated intravascular coagulation one day after the surgery and died the next day.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusion</jats:title>\n            <jats:p>It is a rare, highly malignant hepatic neoplasm, affecting almost exclusively the pediatric population. The prognosis is poor but recent evidence has shown that long-term survival is possible after complete surgical resection with or without postoperative chemotherapy.</jats:p>\n          </jats:sec>","abstract \n \n background \n undifferentiated embryonal sarcoma (ues) of liver is a rare malignant neoplasm, which affects mostly the pediatric population accounting for 13% of pediatric hepatic malignancies, a few cases has been reported in adults. \n \n \n case presentation \n we report a case of undifferentiated embryonal sarcoma of the liver in a 20-year-old caucasian male. the patient was referred to us for further investigation after a laparotomy in a district hospital for spontaneous abdominal hemorrhage, which was due to a liver mass. after a through evaluation with computed tomography scan and magnetic resonance imaging of the liver and taking into consideration the previous history of the patient, it was decided to surgically explore the patient. resection of i–iv and viii hepatic lobe. patient developed disseminated intravascular coagulation one day after the surgery and died the next day. \n \n \n conclusion \n it is a rare, highly malignant hepatic neoplasm, affecting almost exclusively the pediatric population. the prognosis is poor but recent evidence has shown that long-term survival is possible after complete surgical resection with or without postoperative chemotherapy. \n"
http://orkg.org/orkg/resource/R29149,A comprehensive literature review of the ERP research field over a Decade,10.1108/17410391011061780,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is first, to develop a methodological framework for conducting a comprehensive literature review on an empirical phenomenon based on a vast amount of papers published. Second, to use this framework to gain an understanding of the current state of the enterprise resource planning (ERP) research field, and third, based on the literature review, to develop a conceptual framework identifying areas of concern with regard to ERP systems.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>Abstracts from 885 peer‐reviewed journal publications from 2000 to 2009 have been analysed according to journal, authors and year of publication, and further categorised into research discipline, research topic and methods used, using the structured methodological framework.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The body of academic knowledge about ERP systems has reached a certain maturity and several different research disciplines have contributed to the field from different points of view using different methods, showing that the ERP research field is very much an interdisciplinary field. It demonstrates that the number of ERP publications has decreased, and it indicates that the academic interest in ERP is driven by an interest in an empirical phenomenon rather than that ERP is a new research discipline. Different research topics of interest are identified and used in developing a conceptual framework for “areas of concern” regarding ERP systems. Finally the usefulness of the framework is confirmed by analysing one specific aspect of ERP research; business process reengineering (BPR) to establish which theories different authors and journals have used in their efforts to explore BPR and ERP.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>The findings of the literature study, the structured methodological framework for comprehensive literature review and the conceptual framework identifying different areas of concern are believed to be useful for other researchers in their effort to obtain an overview of the evolution of the ERP research field and in positioning their own ERP research.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>The paper provides guidance for researchers with insight into what has been published, where to publish ERP‐related research and how to study it, and in positioning their own interest in ERP systems in the interdisciplinary research field. Access to the EndNote database containing bibliographical data of more than 880 papers can be used in future research and literature analysis. For managers, the conceptual framework can be useful in increasing their understanding of the complexity and areas of concern with regard to the ERP system.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>The paper presents a structured methodological framework for analysing a vast amount of academic publications with an interest in an empirical phenomenon, demonstration of how academic interdisciplinary interest in ERP has evolved over time and reached a certain amount of maturity and a conceptual framework of areas of concern with regard to ERP systems.</jats:p></jats:sec>","purpose the purpose of this paper is first, to develop a methodological framework for conducting a comprehensive literature review on an empirical phenomenon based on a vast amount of papers published. second, to use this framework to gain an understanding of the current state of the enterprise resource planning (erp) research field, and third, based on the literature review, to develop a conceptual framework identifying areas of concern with regard to erp systems. design/methodology/approach abstracts from 885 peer‐reviewed journal publications from 2000 to 2009 have been analysed according to journal, authors and year of publication, and further categorised into research discipline, research topic and methods used, using the structured methodological framework. findings the body of academic knowledge about erp systems has reached a certain maturity and several different research disciplines have contributed to the field from different points of view using different methods, showing that the erp research field is very much an interdisciplinary field. it demonstrates that the number of erp publications has decreased, and it indicates that the academic interest in erp is driven by an interest in an empirical phenomenon rather than that erp is a new research discipline. different research topics of interest are identified and used in developing a conceptual framework for “areas of concern” regarding erp systems. finally the usefulness of the framework is confirmed by analysing one specific aspect of erp research; business process reengineering (bpr) to establish which theories different authors and journals have used in their efforts to explore bpr and erp. research limitations/implications the findings of the literature study, the structured methodological framework for comprehensive literature review and the conceptual framework identifying different areas of concern are believed to be useful for other researchers in their effort to obtain an overview of the evolution of the erp research field and in positioning their own erp research. practical implications the paper provides guidance for researchers with insight into what has been published, where to publish erp‐related research and how to study it, and in positioning their own interest in erp systems in the interdisciplinary research field. access to the endnote database containing bibliographical data of more than 880 papers can be used in future research and literature analysis. for managers, the conceptual framework can be useful in increasing their understanding of the complexity and areas of concern with regard to the erp system. originality/value the paper presents a structured methodological framework for analysing a vast amount of academic publications with an interest in an empirical phenomenon, demonstration of how academic interdisciplinary interest in erp has evolved over time and reached a certain amount of maturity and a conceptual framework of areas of concern with regard to erp systems."
http://orkg.org/orkg/resource/R29159,Planning for ERP systems: analysis and future trend,10.1108/14637150110406768,crossref,"<jats:p>The successful implementation of various enterprise resource planning (ERP) systems has provoked considerable interest over the last few years. Management has recently been enticed to look toward these new information technologies and philosophies of manufacturing for the key to survival or competitive edges. Although there is no shortage of glowing reports on the success of ERP installations, many companies have tossed millions of dollars in this direction with little to show for it. Since many of the ERP failures today can be attributed to inadequate planning prior to installation, we choose to analyze several critical planning issues including needs assessment and choosing a right ERP system, matching business process with the ERP system, understanding the organizational requirements, and economic and strategic justification. In addition, this study also identifies new windows of opportunity as well as challenges facing companies today as enterprise systems continue to evolve and expand.</jats:p>","the successful implementation of various enterprise resource planning (erp) systems has provoked considerable interest over the last few years. management has recently been enticed to look toward these new information technologies and philosophies of manufacturing for the key to survival or competitive edges. although there is no shortage of glowing reports on the success of erp installations, many companies have tossed millions of dollars in this direction with little to show for it. since many of the erp failures today can be attributed to inadequate planning prior to installation, we choose to analyze several critical planning issues including needs assessment and choosing a right erp system, matching business process with the erp system, understanding the organizational requirements, and economic and strategic justification. in addition, this study also identifies new windows of opportunity as well as challenges facing companies today as enterprise systems continue to evolve and expand."
http://orkg.org/orkg/resource/R29161,Enterprise resource planning (ERP) systems: a research agenda,10.1108/02635570210421354,crossref,"<jats:p>The continuing development of enterprise resource planning (ERP) systems has been considered by many researchers and practitioners as one of the major IT innovations in this decade. ERP solutions seek to integrate and streamline business processes and their associated information and work flows. What makes this technology more appealing to organizations is increasing capability to integrate with the most advanced electronic and mobile commerce technologies. However, as is the case with any new IT field, research in the ERP area is still lacking and the gap in the ERP literature is huge. Attempts to fill this gap by proposing a novel taxonomy for ERP research. Also presents the current status with some major themes of ERP research relating to ERP adoption, technical aspects of ERP and ERP in IS curricula. The discussion presented on these issues should be of value to researchers and practitioners. Future research work will continue to survey other major areas presented in the taxonomy framework.</jats:p>","the continuing development of enterprise resource planning (erp) systems has been considered by many researchers and practitioners as one of the major it innovations in this decade. erp solutions seek to integrate and streamline business processes and their associated information and work flows. what makes this technology more appealing to organizations is increasing capability to integrate with the most advanced electronic and mobile commerce technologies. however, as is the case with any new it field, research in the erp area is still lacking and the gap in the erp literature is huge. attempts to fill this gap by proposing a novel taxonomy for erp research. also presents the current status with some major themes of erp research relating to erp adoption, technical aspects of erp and erp in is curricula. the discussion presented on these issues should be of value to researchers and practitioners. future research work will continue to survey other major areas presented in the taxonomy framework."
http://orkg.org/orkg/resource/R29176,A Review of ERP Research: A Future Agenda for Accounting Information Systems,10.2308/jis.2011.25.1.37,crossref,"<jats:p>ABSTRACT: ERP systems are typically the largest, most complex, and most demanding information systems implemented by firms, representing a major departure from the individual and departmental information systems prevalent in the past. Firms and individuals are extensively impacted, and many problematic issues remain to be researched. ERP and related integrated technologies are a transformative force on the accounting profession. As the nature of business evolves, accounting expertise is being called on to make broader contributions such as reporting on nonfinancial measures, auditing information systems, implementing management controls within information systems, and providing management consulting services. This review of ERP research is drawn from an extensive examination of the breadth of ERP-related literature without constraints as to a narrow timeframe or limited journal list, although particular attention is directed to the leading journals in information systems and accounting information systems. Early research consisted of descriptive studies of firms implementing ERP systems. Then researchers started to address other research questions about the factors that lead to successful implementations: the need for change management and expanded forms of user education, whether the financial benefit outweighed the cost, and whether the issues are different depending on organizational type and cultural factors. This research encouraged the development of several major ERP research areas: (1) critical success factors, (2) the organizational impact, and (3) the economic impact of ERP systems. We use this taxonomy to establish (1) what we know, (2) what we need, and (3) where we are going in ERP research. The objective of this review is to synthesize the extant ERP research reported without regard to publication domain and make this readily available to accounting researchers. We organize key ERP research by topics of interest in accounting, and map ERP topics onto existing accounting information systems research areas. An emphasis is placed on topics important to accounting, including (but not limited to) the risk management and auditing of ERP systems, regulatory issues, the internal and external economic impacts of ERP systems, extensions needed in ERP systems for XBRL, for interorganizational support, and for the design of management control systems.</jats:p><jats:p>See Supplemental Material.</jats:p>","abstract: erp systems are typically the largest, most complex, and most demanding information systems implemented by firms, representing a major departure from the individual and departmental information systems prevalent in the past. firms and individuals are extensively impacted, and many problematic issues remain to be researched. erp and related integrated technologies are a transformative force on the accounting profession. as the nature of business evolves, accounting expertise is being called on to make broader contributions such as reporting on nonfinancial measures, auditing information systems, implementing management controls within information systems, and providing management consulting services. this review of erp research is drawn from an extensive examination of the breadth of erp-related literature without constraints as to a narrow timeframe or limited journal list, although particular attention is directed to the leading journals in information systems and accounting information systems. early research consisted of descriptive studies of firms implementing erp systems. then researchers started to address other research questions about the factors that lead to successful implementations: the need for change management and expanded forms of user education, whether the financial benefit outweighed the cost, and whether the issues are different depending on organizational type and cultural factors. this research encouraged the development of several major erp research areas: (1) critical success factors, (2) the organizational impact, and (3) the economic impact of erp systems. we use this taxonomy to establish (1) what we know, (2) what we need, and (3) where we are going in erp research. the objective of this review is to synthesize the extant erp research reported without regard to publication domain and make this readily available to accounting researchers. we organize key erp research by topics of interest in accounting, and map erp topics onto existing accounting information systems research areas. an emphasis is placed on topics important to accounting, including (but not limited to) the risk management and auditing of erp systems, regulatory issues, the internal and external economic impacts of erp systems, extensions needed in erp systems for xbrl, for interorganizational support, and for the design of management control systems. see supplemental material."
http://orkg.org/orkg/resource/R29191,Critical factors for successful implementation of enterprise systems,10.1108/14637150110392782,crossref,"<jats:p>Enterprise resource planning (ERP) systems have emerged as the core of successful information management and the enterprise backbone of organizations. The difficulties of ERP implementations have been widely cited in the literature but research on the critical factors for initial and ongoing ERP implementation success is rare and fragmented. Through a comprehensive review of the literature, 11 factors were found to be critical to ERP implementation success – ERP teamwork and composition; change management program and culture; top management support; business plan and vision; business process reengineering with minimum customization; project management; monitoring and evaluation of performance; effective communication; software development, testing and troubleshooting; project champion; appropriate business and IT legacy systems. The classification of these factors into the respective phases (chartering, project, shakedown, onward and upward) in Markus and Tanis’ ERP life cycle model is presented and the importance of each factor is discussed.</jats:p>","enterprise resource planning (erp) systems have emerged as the core of successful information management and the enterprise backbone of organizations. the difficulties of erp implementations have been widely cited in the literature but research on the critical factors for initial and ongoing erp implementation success is rare and fragmented. through a comprehensive review of the literature, 11 factors were found to be critical to erp implementation success – erp teamwork and composition; change management program and culture; top management support; business plan and vision; business process reengineering with minimum customization; project management; monitoring and evaluation of performance; effective communication; software development, testing and troubleshooting; project champion; appropriate business and it legacy systems. the classification of these factors into the respective phases (chartering, project, shakedown, onward and upward) in markus and tanis’ erp life cycle model is presented and the importance of each factor is discussed."
http://orkg.org/orkg/resource/R29198,ERP implementation: a compilation and analysis of critical success factors,10.1108/14637150710752272,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>To explore the current literature base of critical success factors (CSFs) of ERP implementations, prepare a compilation, and identify any gaps that might exist.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>Hundreds of journals were searched using key terms identified in a preliminary literature review. Successive rounds of article abstract reviews resulted in 45 articles being selected for the compilation. CSF constructs were then identified using content analysis methodology and an inductive coding technique. A subsequent critical analysis identified gaps in the literature base.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The most significant finding is the lack of research that has focused on the identification of CSFs from the perspectives of key stakeholders. Additionally, there appears to be much variance with respect to what exactly is encompassed by change management, one of the most widely cited CSFs, and little detail of specific implementation tactics.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>There is a need to focus future research efforts on the study of CSFs as they apply to the perspectives of key stakeholders and to ensure that this stakeholder approach is also comprehensive in its coverage of CSFs. As well, there is need to conduct more in‐depth research into the concept of change management. One key limitation of this research is the occurrence of duplication in the frequency analysis of the success factors. This is attributed to secondary research being the main methodology for a large number of the articles cited.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This research provides a comprehensive compilation of all previously identified ERP implementation success factors, through a clearly structured methodological approach.</jats:p></jats:sec>","purpose to explore the current literature base of critical success factors (csfs) of erp implementations, prepare a compilation, and identify any gaps that might exist. design/methodology/approach hundreds of journals were searched using key terms identified in a preliminary literature review. successive rounds of article abstract reviews resulted in 45 articles being selected for the compilation. csf constructs were then identified using content analysis methodology and an inductive coding technique. a subsequent critical analysis identified gaps in the literature base. findings the most significant finding is the lack of research that has focused on the identification of csfs from the perspectives of key stakeholders. additionally, there appears to be much variance with respect to what exactly is encompassed by change management, one of the most widely cited csfs, and little detail of specific implementation tactics. research limitations/implications there is a need to focus future research efforts on the study of csfs as they apply to the perspectives of key stakeholders and to ensure that this stakeholder approach is also comprehensive in its coverage of csfs. as well, there is need to conduct more in‐depth research into the concept of change management. one key limitation of this research is the occurrence of duplication in the frequency analysis of the success factors. this is attributed to secondary research being the main methodology for a large number of the articles cited. originality/value this research provides a comprehensive compilation of all previously identified erp implementation success factors, through a clearly structured methodological approach."
http://orkg.org/orkg/resource/R29217,The Core Critical Success Factors in Implementation of Enterprise Resource Planning Systems,10.4018/jeis.2010040105,crossref,"<p>The Implementation of Enterprise Resource Planning (ERP) systems require huge investments while ineffective implementations of such projects are commonly observed. A considerable number of these projects have been reported to fail or take longer than it was initially planned, while previous studies show that the aim of rapid implementation of such projects has not been successful and the failure of the fundamental goals in these projects have imposed huge amounts of costs on investors. Some of the major consequences are the reduction in demand for such products and the introduction of further skepticism to the managers and investors of ERP systems. In this regard, it is important to understand the factors determining success or failure of ERP implementation. The aim of this paper is to study the critical success factors (CSFs) in implementing ERP systems and to develop a conceptual model which can serve as a basis for ERP project managers. These critical success factors that are called “core critical success factors” are extracted from 62 published papers using the content analysis and the entropy method. The proposed conceptual model has been verified in the context of five multinational companies.</p>","the implementation of enterprise resource planning (erp) systems require huge investments while ineffective implementations of such projects are commonly observed. a considerable number of these projects have been reported to fail or take longer than it was initially planned, while previous studies show that the aim of rapid implementation of such projects has not been successful and the failure of the fundamental goals in these projects have imposed huge amounts of costs on investors. some of the major consequences are the reduction in demand for such products and the introduction of further skepticism to the managers and investors of erp systems. in this regard, it is important to understand the factors determining success or failure of erp implementation. the aim of this paper is to study the critical success factors (csfs) in implementing erp systems and to develop a conceptual model which can serve as a basis for erp project managers. these critical success factors that are called “core critical success factors” are extracted from 62 published papers using the content analysis and the entropy method. the proposed conceptual model has been verified in the context of five multinational companies."
http://orkg.org/orkg/resource/R29238,Critical success factors in enterprise resource planning systems,10.1145/2501654.2501669,crossref,"<jats:p>Organizations perceive ERP as a vital tool for organizational competition as it integrates dispersed organizational systems and enables flawless transactions and production. This review examines studies investigating Critical Success Factors (CSFs) in implementing Enterprise Resource Planning (ERP) systems. Keywords relating to the theme of this study were defined and used to search known Web engines and journal databases for studies on both implementing ERP systems per se and integrating ERP systems with other well- known systems (e.g., SCM, CRM) whose importance to business organizations and academia is acknowledged to work in a complementary fashion. A total of 341 articles were reviewed to address three main goals. This study structures previous research by presenting a comprehensive taxonomy of CSFs in the area of ERP. Second, it maps studies, identified through an exhaustive and comprehensive literature review, to different dimensions and facets of ERP system implementation. Third, it presents studies investigating CSFs in terms of a specific ERP lifecycle phase and across the entire ERP life cycle. This study not only reviews articles in which an ERP system is the sole or primary field of research, but also articles that refer to an integration of ERP systems and other popular systems (e.g., SCM, CRM). Finally it provides a comprehensive bibliography of the articles published during this period that can serve as a guide for future research.</jats:p>","organizations perceive erp as a vital tool for organizational competition as it integrates dispersed organizational systems and enables flawless transactions and production. this review examines studies investigating critical success factors (csfs) in implementing enterprise resource planning (erp) systems. keywords relating to the theme of this study were defined and used to search known web engines and journal databases for studies on both implementing erp systems per se and integrating erp systems with other well- known systems (e.g., scm, crm) whose importance to business organizations and academia is acknowledged to work in a complementary fashion. a total of 341 articles were reviewed to address three main goals. this study structures previous research by presenting a comprehensive taxonomy of csfs in the area of erp. second, it maps studies, identified through an exhaustive and comprehensive literature review, to different dimensions and facets of erp system implementation. third, it presents studies investigating csfs in terms of a specific erp lifecycle phase and across the entire erp life cycle. this study not only reviews articles in which an erp system is the sole or primary field of research, but also articles that refer to an integration of erp systems and other popular systems (e.g., scm, crm). finally it provides a comprehensive bibliography of the articles published during this period that can serve as a guide for future research."
http://orkg.org/orkg/resource/R29248,ERP systems and open source: an initial review and some implications for SMEs,10.1108/17410390810911230,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to further build up the knowledge about reasons for small and mid‐sized enterprises (SMEs) to adopt open source enterprise resource planning (ERP) systems.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>The paper presents and analyses findings in articles about proprietary ERPs and open source ERPs. In addition, a limited investigation of the distribution channel SourceForge for open source is made.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The cost perspective seems to receive a high attention regarding adoption of open source ERPs. This can be questioned and the main conclusion is that costs seem to have a secondary role in adoption or non adoption of open source ERPs.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>The paper is mainly a conceptual paper written from a literature review. The ambition is to search support for the findings by doing more research in the area.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>The findings presented are of interest both for developers of proprietary ERPs as well as SMEs since it is shown that there are definitely reasons other than costs involved when deciding on proprietary ERPs or open source ERPs.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>It can be argued that there is a lack of research conducted and published about why SMEs choose open source ERPs instead of proprietary ERPs. This paper identifies the gap and suggests future research directions about this subject.</jats:p></jats:sec>","purpose the purpose of this paper is to further build up the knowledge about reasons for small and mid‐sized enterprises (smes) to adopt open source enterprise resource planning (erp) systems. design/methodology/approach the paper presents and analyses findings in articles about proprietary erps and open source erps. in addition, a limited investigation of the distribution channel sourceforge for open source is made. findings the cost perspective seems to receive a high attention regarding adoption of open source erps. this can be questioned and the main conclusion is that costs seem to have a secondary role in adoption or non adoption of open source erps. research limitations/implications the paper is mainly a conceptual paper written from a literature review. the ambition is to search support for the findings by doing more research in the area. practical implications the findings presented are of interest both for developers of proprietary erps as well as smes since it is shown that there are definitely reasons other than costs involved when deciding on proprietary erps or open source erps. originality/value it can be argued that there is a lack of research conducted and published about why smes choose open source erps instead of proprietary erps. this paper identifies the gap and suggests future research directions about this subject."
http://orkg.org/orkg/resource/R29304,Deconstructing information packages: organizational and behavioural implications of ERP systems,10.1108/09593840410522152,crossref,"<jats:p>Argues that the organizational involvement of large scale information technology packages, such as those known as enterprise resource planning (ERP), has important implications that go far beyond the acknowledged effects of keeping the organizational operations accountable and integrated across functions and production sites. Claims that ERP packages are predicated on an understanding of human agency as a procedural affair and of organizations as an extended series of functional or cross‐functional transactions. Accordingly, the massive introduction of ERP packages to organizations is bound to have serious implications that precisely recount the procedural forms by which such packages instrument organizational operations and fashion organizational roles. The conception of human agency and organizational operations in procedural terms may seem reasonable yet it recounts a very specific and, in a sense, limited understanding of humans and organizations. The distinctive status of framing human agency and organizations in procedural terms becomes evident in its juxtaposition with other forms of human action like improvisation, exploration or playing. These latter forms of human involvement stand out against the serial fragmentation underlying procedural action. They imply acting on the world on loose premises that trade off a variety of forms of knowledge and courses of action in attempts to explore and discover alternative ways of coping with reality.</jats:p>","argues that the organizational involvement of large scale information technology packages, such as those known as enterprise resource planning (erp), has important implications that go far beyond the acknowledged effects of keeping the organizational operations accountable and integrated across functions and production sites. claims that erp packages are predicated on an understanding of human agency as a procedural affair and of organizations as an extended series of functional or cross‐functional transactions. accordingly, the massive introduction of erp packages to organizations is bound to have serious implications that precisely recount the procedural forms by which such packages instrument organizational operations and fashion organizational roles. the conception of human agency and organizational operations in procedural terms may seem reasonable yet it recounts a very specific and, in a sense, limited understanding of humans and organizations. the distinctive status of framing human agency and organizations in procedural terms becomes evident in its juxtaposition with other forms of human action like improvisation, exploration or playing. these latter forms of human involvement stand out against the serial fragmentation underlying procedural action. they imply acting on the world on loose premises that trade off a variety of forms of knowledge and courses of action in attempts to explore and discover alternative ways of coping with reality."
http://orkg.org/orkg/resource/R29306,Developing a cultural perspective on ERP,10.1108/14637150510591138,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>To develop an analytical framework through which the organizational cultural dimension of enterprise resource planning (ERP) implementations can be analyzed.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>This paper is primarily based on a review of the literature.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>ERP is an enterprise system that offers, to a certain extent, standard business solutions. This standardization is reinforced by two processes: ERP systems are generally implemented by intermediary IT organizations, mediating between the development of ERP‐standard software packages and specific business domains of application; and ERP systems integrate complex networks of production divisions, suppliers and customers.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>In this paper, ERP itself is presented as problematic, laying heavy burdens on organizations – ERP is a demanding technology. While in some cases recognizing the mutual shaping of technology and organization, research into ERP mainly addresses the economic‐technological rationality of ERP (i.e. matters of effectiveness and efficiency). We want to supplement and complement this perspective with a cultural approach. How do individuals in organizations define and experience ERP‐standards? How and to what extent are management and working positions redefined in the process of developing and implementing ERP? In the paper, we highlight three perspectives from which ERP systems can be experienced, defined and analyzed. These perspectives are specified as the “constitution” of ERP, ERP as a “condition” of organizations, and the (unintended) “consequences” of ERP.</jats:p></jats:sec>","purpose to develop an analytical framework through which the organizational cultural dimension of enterprise resource planning (erp) implementations can be analyzed. design/methodology/approach this paper is primarily based on a review of the literature. findings erp is an enterprise system that offers, to a certain extent, standard business solutions. this standardization is reinforced by two processes: erp systems are generally implemented by intermediary it organizations, mediating between the development of erp‐standard software packages and specific business domains of application; and erp systems integrate complex networks of production divisions, suppliers and customers. originality/value in this paper, erp itself is presented as problematic, laying heavy burdens on organizations – erp is a demanding technology. while in some cases recognizing the mutual shaping of technology and organization, research into erp mainly addresses the economic‐technological rationality of erp (i.e. matters of effectiveness and efficiency). we want to supplement and complement this perspective with a cultural approach. how do individuals in organizations define and experience erp‐standards? how and to what extent are management and working positions redefined in the process of developing and implementing erp? in the paper, we highlight three perspectives from which erp systems can be experienced, defined and analyzed. these perspectives are specified as the “constitution” of erp, erp as a “condition” of organizations, and the (unintended) “consequences” of erp."
http://orkg.org/orkg/resource/R29312,Extended-enterprise systems’ impact on enterprise risk management”,10.1108/17410390610636904,crossref,"""<jats:p><jats:bold><jats:bold>Purpose</jats:bold></jats:bold> – This article aims to focus on raising awareness of the limitations of traditional “enterprise‐centric” views of enterprise risk management that ignore the risks that are inherited from key business and supply chain partners. In essence, enterprise systems implementations have allowed organizations to couple their operations more tightly with other business partners, particularly in the area of supply chain management, and in the process enterprise systems applications are redefining the boundaries of the entity in terms of risk management concerns and the scope of financial audits. <jats:bold><jats:bold>Design/methodology/approach</jats:bold></jats:bold> – The prior literature that has begun to explore aspects of assessing key risk components in these relationships is reviewed with an eye to highlighting the limitations of what is understood about risk in interorganizational relationships. This analysis of the prior research establishes the basis for the logical formation of a framework for future enterprise risk management research in the area of e‐commerce relationships. <jats:bold><jats:bold>Findings</jats:bold></jats:bold> – Conclusions focus on the overall framework of risks that should be considered when interorganizational relationships are critical to an enterprise's operations and advocate an “extended‐enterprise” view of enterprise risk management. <jats:bold><jats:bold>Research limitations/implications</jats:bold></jats:bold> – The framework introduced in this paper provides guidance for future research in the area of interorganizational systems control and risk assessment. <jats:bold><jats:bold>Practical implications</jats:bold></jats:bold> – The framework further highlights areas of risk that auditors and corporate risk managers should consider in assessing the risk inherited through interorganizational relationships. <jats:bold><jats:bold>Originality/value</jats:bold></jats:bold> – The paper highlights the need to shift from an enterprise‐centric view of risk management to an extended‐enterprise risk management view.</jats:p>""",""" purpose – this article aims to focus on raising awareness of the limitations of traditional “enterprise‐centric” views of enterprise risk management that ignore the risks that are inherited from key business and supply chain partners. in essence, enterprise systems implementations have allowed organizations to couple their operations more tightly with other business partners, particularly in the area of supply chain management, and in the process enterprise systems applications are redefining the boundaries of the entity in terms of risk management concerns and the scope of financial audits. design/methodology/approach – the prior literature that has begun to explore aspects of assessing key risk components in these relationships is reviewed with an eye to highlighting the limitations of what is understood about risk in interorganizational relationships. this analysis of the prior research establishes the basis for the logical formation of a framework for future enterprise risk management research in the area of e‐commerce relationships. findings – conclusions focus on the overall framework of risks that should be considered when interorganizational relationships are critical to an enterprise's operations and advocate an “extended‐enterprise” view of enterprise risk management. research limitations/implications – the framework introduced in this paper provides guidance for future research in the area of interorganizational systems control and risk assessment. practical implications – the framework further highlights areas of risk that auditors and corporate risk managers should consider in assessing the risk inherited through interorganizational relationships. originality/value – the paper highlights the need to shift from an enterprise‐centric view of risk management to an extended‐enterprise risk management view. """
http://orkg.org/orkg/resource/R29380,The environmental Kuznets curve: an empirical analysis,10.1017/s1355770x97000211,crossref,"<jats:p>This paper examines the relationship between per capita income and a wide range of environmental indicators using cross-country panel sets. The manner in which this has been done overcomes several of the weaknesses asscociated with the estimation of environmental Kuznets curves (EKCs). outlined by Stern <jats:italic>et al.</jats:italic> (1996). Results suggest that meaningful EKCs exist only for local air pollutants whilst indicators with a more global, or indirect, impact either increase monotonically with income or else have predicted turning points at high per capita income levels with large standard errors – unless they have been subjected to a multilateral policy initiative. Two other findings are also made: that concentration of local pollutants in urban areas peak at a lower per capita income level than total emissions per capita; and that transport-generated local air pollutants peak at a higher per capita income level than total emissions per capita. Given these findings, suggestions are made regarding the necessary future direction of environmental policy.</jats:p>","this paper examines the relationship between per capita income and a wide range of environmental indicators using cross-country panel sets. the manner in which this has been done overcomes several of the weaknesses asscociated with the estimation of environmental kuznets curves (ekcs). outlined by stern et al. (1996). results suggest that meaningful ekcs exist only for local air pollutants whilst indicators with a more global, or indirect, impact either increase monotonically with income or else have predicted turning points at high per capita income levels with large standard errors – unless they have been subjected to a multilateral policy initiative. two other findings are also made: that concentration of local pollutants in urban areas peak at a lower per capita income level than total emissions per capita; and that transport-generated local air pollutants peak at a higher per capita income level than total emissions per capita. given these findings, suggestions are made regarding the necessary future direction of environmental policy."
http://orkg.org/orkg/resource/R29384,Are environmental Kuznets curves misleading us? The case of CO2 emissions,10.1017/s1355770x97000247,crossref,"""<jats:p>Environmental Kuznets curve (EKC) analysis links changes in environmental quality to national economic growth. The reduced form models, however, do not provide insight into the underlying processes that generate these changes. We compare EKC models to structural transition models of per capita CO<jats:sub>2</jats:sub> emissions and per capita GDP, and find that, for the 16 countries which have undergone such a transition, the initiation of the transition correlates not with income levels but with historic events related to the oil price shocks of the 1970s and the policies that followed them. In contrast to previous EKC studies of CO<jats:sub>2</jats:sub> the transition away from positive emissions elasticities for these 16 countries is found to occur as a sudden, discontinuous transition rather than as a gradual change. We also demonstrate that the third order polynomial 'N' dependence of emissions on income is the result of data aggregation. We conclude that neither the 'U'- nor the 'N'-shaped relationship between CO<jats:sub>2</jats:sub> emissions and income provide a reliable indication of <jats:italic>future</jats:italic> behaviour. </jats:p>""",""" environmental kuznets curve (ekc) analysis links changes in environmental quality to national economic growth. the reduced form models, however, do not provide insight into the underlying processes that generate these changes. we compare ekc models to structural transition models of per capita co 2 emissions and per capita gdp, and find that, for the 16 countries which have undergone such a transition, the initiation of the transition correlates not with income levels but with historic events related to the oil price shocks of the 1970s and the policies that followed them. in contrast to previous ekc studies of co 2 the transition away from positive emissions elasticities for these 16 countries is found to occur as a sudden, discontinuous transition rather than as a gradual change. we also demonstrate that the third order polynomial 'n' dependence of emissions on income is the result of data aggregation. we conclude that neither the 'u'- nor the 'n'-shaped relationship between co 2 emissions and income provide a reliable indication of future behaviour. """
http://orkg.org/orkg/resource/R29487,An Environmental Kuznets Curve Analysis of U.S. State-Level Carbon Dioxide Emissions,10.1177/1070496504273514,crossref,"<jats:p> Most environmental Kuznets curve (EKC) theories do not apply to carbon dioxide (CO<jats:sub>2</jats:sub> )—an unregulated, invisible, odorless gas with no direct human health effects. This analysis addresses the hypothesis that the income-CO<jats:sub>2</jats:sub> relationship reflects changes in the composition of an economy as it develops and the associated role of trade in an emissions-intensive good (e.g., electricity). To test this hypothesis, I use a novel data set of 1960 to 1999 state-level CO<jats:sub>2</jats:sub> emissions to estimate pretrade (production-based) CO<jats:sub>2</jats:sub> EKCs and posttrade (consumption-based) CO<jats:sub>2</jats:sub> EKCs. Based on the first EKC analysis of CO<jats:sub>2</jats:sub> emissions in the United States, I find that consumption-based EKCs peak at significantly higher incomes than production-based EKCs, suggesting that emissions-intensive trade drives, at least in part, the income-emissions relationship. I have also investigated the robustness of the estimated income-CO<jats:sub>2</jats:sub> relationship through a variety of specifications. Estimated EKCs appear to vary by state, and the estimated income-emissions relationships could be spurious for some states with nonstationary income and emissions data. Finally, I find that cold winters, warm summers, and historic coal endowments are positively associated with states’ CO<jats:sub>2</jats:sub> emissions. </jats:p>","most environmental kuznets curve (ekc) theories do not apply to carbon dioxide (co 2 )—an unregulated, invisible, odorless gas with no direct human health effects. this analysis addresses the hypothesis that the income-co 2 relationship reflects changes in the composition of an economy as it develops and the associated role of trade in an emissions-intensive good (e.g., electricity). to test this hypothesis, i use a novel data set of 1960 to 1999 state-level co 2 emissions to estimate pretrade (production-based) co 2 ekcs and posttrade (consumption-based) co 2 ekcs. based on the first ekc analysis of co 2 emissions in the united states, i find that consumption-based ekcs peak at significantly higher incomes than production-based ekcs, suggesting that emissions-intensive trade drives, at least in part, the income-emissions relationship. i have also investigated the robustness of the estimated income-co 2 relationship through a variety of specifications. estimated ekcs appear to vary by state, and the estimated income-emissions relationships could be spurious for some states with nonstationary income and emissions data. finally, i find that cold winters, warm summers, and historic coal endowments are positively associated with states’ co 2 emissions."
http://orkg.org/orkg/resource/R29537,"Corruption, trade openness, and environmental quality: a panel data analysis of selected South Asian countries",10.30541/v46i4iipp.673-688,crossref,"<jats:p>The second half of the twentieth century emerged with two\n      important concepts of the economic world. In the start of the second\n      half, economists, developmentalists, etc., introduced the idea of\n      “development”, while; latter it was replaced by a more meaningful and\n      attractive term “sustainable development”. Sustainable development is\n      defined as “balancing the fulfillment of human needs with the protection\n      of the natural environment so that these needs can be met not only in\n      the present, but also in the indefinite future” [Wikipedia (2007)]. Or\n      “Sustainable development means that pattern of development that permits\n      future generations to live at least as well as the current generation”\n      [Todaro and Smith (2005)], eighth edition]. The field of sustainable\n      development can be conceptually broken into four constituent parts:\n      environmental sustainability, economic sustainability, social\n      sustainability and political sustainability. Although, the word\n      sustainable development is very vast and deep, but the main emphasis of\n      our study will be on environmental sustainability.</jats:p>","the second half of the twentieth century emerged with two\n important concepts of the economic world. in the start of the second\n half, economists, developmentalists, etc., introduced the idea of\n “development”, while; latter it was replaced by a more meaningful and\n attractive term “sustainable development”. sustainable development is\n defined as “balancing the fulfillment of human needs with the protection\n of the natural environment so that these needs can be met not only in\n the present, but also in the indefinite future” [wikipedia (2007)]. or\n “sustainable development means that pattern of development that permits\n future generations to live at least as well as the current generation”\n [todaro and smith (2005)], eighth edition]. the field of sustainable\n development can be conceptually broken into four constituent parts:\n environmental sustainability, economic sustainability, social\n sustainability and political sustainability. although, the word\n sustainable development is very vast and deep, but the main emphasis of\n our study will be on environmental sustainability."
http://orkg.org/orkg/resource/R29543,Beyond the Environmental Kuznets Curve: a comparative study of SO2 and CO2 emissions between Japan and China,10.1017/s1355770x07003592,crossref,"<jats:p>This study is the first systematic attempt to test statistically the contrasting hypotheses on the emission of SO<jats:sub>2</jats:sub> and CO<jats:sub>2</jats:sub>, and energy consumption in Japan and China for the last few decades. We postulate the hypotheses that local governments have incentives to internalize the local external diseconomies caused by SO<jats:sub>2</jats:sub> emissions, but not the global external diseconomies caused by CO<jats:sub>2</jats:sub> emissions. To substantiate our hypotheses, we decompose emissions of SO<jats:sub>2</jats:sub> and CO<jats:sub>2</jats:sub> into two factors: the emission factor (i.e. emission per energy use) and energy consumption. The results show that the prefectures where past energy consumption was high tend to reduce the emission factor of SO<jats:sub>2</jats:sub> significantly in Japan, while we do not find such a tendency in China. There is also evidence that neither per capita income nor past energy consumption affects the CO<jats:sub>2</jats:sub> emission factor and energy consumption significantly in both Japan and China, implying that an individual country has few incentives to reduce CO<jats:sub>2</jats:sub> emissions.</jats:p>","this study is the first systematic attempt to test statistically the contrasting hypotheses on the emission of so 2 and co 2 , and energy consumption in japan and china for the last few decades. we postulate the hypotheses that local governments have incentives to internalize the local external diseconomies caused by so 2 emissions, but not the global external diseconomies caused by co 2 emissions. to substantiate our hypotheses, we decompose emissions of so 2 and co 2 into two factors: the emission factor (i.e. emission per energy use) and energy consumption. the results show that the prefectures where past energy consumption was high tend to reduce the emission factor of so 2 significantly in japan, while we do not find such a tendency in china. there is also evidence that neither per capita income nor past energy consumption affects the co 2 emission factor and energy consumption significantly in both japan and china, implying that an individual country has few incentives to reduce co 2 emissions."
http://orkg.org/orkg/resource/R29676,"Environmental Kuznets curves, carbon emissions, and public choice",10.1017/s1355770x10000124,crossref,"<jats:title>ABSTRACT</jats:title><jats:p>Concern about global climate change has elicited responses from governments around the world. These responses began with the 1997 Kyoto Protocol and have continued with other negotiations, including the 2009 Copenhagen Summit. These negotiations raised important questions about whether countries will reduce greenhouse gas emissions and, if so, how the burden of emissions reductions will be shared. To investigate these questions, we utilize environmental Kuznets curves for carbon emissions for the G8 plus five main developing countries. Our findings raise doubts about the feasibility of reducing global carbon emissions and shed light on the different positions taken by countries on the distribution of emissions reductions.</jats:p>","abstract concern about global climate change has elicited responses from governments around the world. these responses began with the 1997 kyoto protocol and have continued with other negotiations, including the 2009 copenhagen summit. these negotiations raised important questions about whether countries will reduce greenhouse gas emissions and, if so, how the burden of emissions reductions will be shared. to investigate these questions, we utilize environmental kuznets curves for carbon emissions for the g8 plus five main developing countries. our findings raise doubts about the feasibility of reducing global carbon emissions and shed light on the different positions taken by countries on the distribution of emissions reductions."
http://orkg.org/orkg/resource/R29816,Environmental Kuznets curve and growth source in Iran,10.2298/pan1205609a,crossref,"<jats:p>Recent empirical research has examined the relationship between certain\n   indicators of environmental degradation and income, concluding that in some\n   cases an inverted U-shaped relationship, which has been called an\n   environmental Kuznets curve (EKC), exists between these variables. The source\n   of growth explanation is important for two reasons. First, it demonstrates\n   how the pollution consequences of growth depend on the source of growth.\n   Therefore, the analogy drawn by some in the environmental community between\n   the damaging effects of economic development and those of liberalized trade\n   is, at best, incomplete. Second, the source of growth explanation\n   demonstrates that a strong policy response to income gains is not necessary\n   for pollution to fall with growth. The aim of this paper investigates the\n   role of differences source of growth in environmental quality of Iran. The\n   results show the two growth resources in Iran cause, in the early stages, CO2\n   emission decreases until turning point but beyond this level of income per\n   capita, economic growth leads to environmental degradation. I find a U\n   relationship between environmental degradation (CO2 emission) and economic\n   growth in Iran.</jats:p>","recent empirical research has examined the relationship between certain\n indicators of environmental degradation and income, concluding that in some\n cases an inverted u-shaped relationship, which has been called an\n environmental kuznets curve (ekc), exists between these variables. the source\n of growth explanation is important for two reasons. first, it demonstrates\n how the pollution consequences of growth depend on the source of growth.\n therefore, the analogy drawn by some in the environmental community between\n the damaging effects of economic development and those of liberalized trade\n is, at best, incomplete. second, the source of growth explanation\n demonstrates that a strong policy response to income gains is not necessary\n for pollution to fall with growth. the aim of this paper investigates the\n role of differences source of growth in environmental quality of iran. the\n results show the two growth resources in iran cause, in the early stages, co2\n emission decreases until turning point but beyond this level of income per\n capita, economic growth leads to environmental degradation. i find a u\n relationship between environmental degradation (co2 emission) and economic\n growth in iran."
http://orkg.org/orkg/resource/R29843,"An econometric study of carbon dioxide (CO2) emissions, energy consumption, and economic growth of Pakistan",10.1108/17506221211282019,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to examine the relationship among environmental pollution, economic growth and energy consumption per capita in the case of Pakistan. The per capital carbon dioxide (CO<jats:sub>2</jats:sub>) emission is used as the environmental indicator, the commercial energy use per capita as the energy consumption indicator, and the per capita gross domestic product (GDP) as the economic indicator.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>The investigation is made on the basis of the environmental Kuznets curve (EKC), using time series data from 1971 to 2006, by applying different econometric tools like ADF Unit Root Johansen Co‐integration VECM and Granger causality tests.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The Granger causality test shows that there is a long term relationship between these three indicators, with bidirectional causality between per capita CO<jats:sub>2</jats:sub> emission and per capita energy consumption. A monotonically increasing curve between GDP and CO<jats:sub>2</jats:sub> emission has been found for the sample period, rejecting the EKC relationship, implying that as per capita GDP increases a linear increase will be observed in per capita CO<jats:sub>2</jats:sub> emission.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>Future research should replace the economic growth variable, i.e. GDP by industrial growth variable because industrial sector is major contributor of pollution by emitting CO<jats:sub>2</jats:sub>.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>The empirical findings will help the policy makers of Pakistan in understanding the severity of the CO<jats:sub>2</jats:sub> emissions issue and in developing new standards and monitoring networks for reducing CO<jats:sub>2</jats:sub> emissions.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>Energy consumption is the major cause of environmental pollution in Pakistan but no substantial work has been done in this regard with reference to Pakistan.</jats:p></jats:sec>","purpose the purpose of this paper is to examine the relationship among environmental pollution, economic growth and energy consumption per capita in the case of pakistan. the per capital carbon dioxide (co 2 ) emission is used as the environmental indicator, the commercial energy use per capita as the energy consumption indicator, and the per capita gross domestic product (gdp) as the economic indicator. design/methodology/approach the investigation is made on the basis of the environmental kuznets curve (ekc), using time series data from 1971 to 2006, by applying different econometric tools like adf unit root johansen co‐integration vecm and granger causality tests. findings the granger causality test shows that there is a long term relationship between these three indicators, with bidirectional causality between per capita co 2 emission and per capita energy consumption. a monotonically increasing curve between gdp and co 2 emission has been found for the sample period, rejecting the ekc relationship, implying that as per capita gdp increases a linear increase will be observed in per capita co 2 emission. research limitations/implications future research should replace the economic growth variable, i.e. gdp by industrial growth variable because industrial sector is major contributor of pollution by emitting co 2 . practical implications the empirical findings will help the policy makers of pakistan in understanding the severity of the co 2 emissions issue and in developing new standards and monitoring networks for reducing co 2 emissions. originality/value energy consumption is the major cause of environmental pollution in pakistan but no substantial work has been done in this regard with reference to pakistan."
http://orkg.org/orkg/resource/R30042,GREENHOUSE GASES EMISSIONS AND ECONOMIC GROWTH – EVIDENCE SUBSTANTIATING THE PRESENCE OF ENVIRONMENTAL KUZNETS CURVE IN THE EU,10.3846/20294913.2014.881434,crossref,"<jats:p>The paper considers the relationship between greenhouse gas emissions (GHG) as the main variable of climate change and gross domestic product (GDP), using the environmental Kuznets curve (EKC) technique. At early stages of economic growth, EKC indicates the increase of pollution related to the growing use of resources. However, when a certain level of income per capita is reached, the trend reverses and at a higher stage of development, further economic growth leads to improvement of the environment. According to the researchers, this implies that the environmental impact indicator is an inverted U-shaped function of income per capita. In this paper, the cubic equation is used to empirically check the validity of the EKC relationship for European countries. The analysis is based on the survey of EU-27, Norway and Switzerland in the period of 1995–2010. The data is taken from the Eurostat database. To gain some insights into the environmental trends in each country, the article highlights the specific relationship in the country based on the level of its development. The similarities between individual countries are analysed in order to identify their basic common features.</jats:p>","the paper considers the relationship between greenhouse gas emissions (ghg) as the main variable of climate change and gross domestic product (gdp), using the environmental kuznets curve (ekc) technique. at early stages of economic growth, ekc indicates the increase of pollution related to the growing use of resources. however, when a certain level of income per capita is reached, the trend reverses and at a higher stage of development, further economic growth leads to improvement of the environment. according to the researchers, this implies that the environmental impact indicator is an inverted u-shaped function of income per capita. in this paper, the cubic equation is used to empirically check the validity of the ekc relationship for european countries. the analysis is based on the survey of eu-27, norway and switzerland in the period of 1995–2010. the data is taken from the eurostat database. to gain some insights into the environmental trends in each country, the article highlights the specific relationship in the country based on the level of its development. the similarities between individual countries are analysed in order to identify their basic common features."
http://orkg.org/orkg/resource/R30175,Emissions and trade in Southeast and East Asian countries: a panel co-integration analysis,10.1108/ijccsm-11-2013-0131,crossref,"<jats:sec>\n               <jats:title content-type=""abstract-heading"">Purpose</jats:title>\n               <jats:p> – The purpose of this paper is to analyse the implication of trade on carbon emissions in a panel of eight highly trading Southeast and East Asian countries, namely, China, Indonesia, South Korea, Malaysia, Hong Kong, The Philippines, Singapore and Thailand. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title>\n               <jats:p> – The analysis relies on the standard quadratic environmental Kuznets curve (EKC) extended to include energy consumption and international trade. A battery of panel unit root and co-integration tests is applied to establish the variables’ stochastic properties and their long-run relations. Then, the specified EKC is estimated using the panel dynamic ordinary least square (OLS) estimation technique. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Findings</jats:title>\n               <jats:p> – The panel co-integration statistics verifies the validity of the extended EKC for the countries under study. Estimation of the long-run EKC via the dynamic OLS estimation method reveals the environmentally degrading effects of trade in these countries, especially in ASEAN and plus South Korea and Hong Kong. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Practical implications</jats:title>\n               <jats:p> – These countries are heavily dependent on trade for their development processes, and as such, their impacts on CO<jats:sub>2</jats:sub> emissions would be highly relevant for assessing their trade policies, along the line of the gain-from-trade hypothesis, the race-to-the-bottom hypothesis and the pollution-safe-haven hypothesis. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Originality/value</jats:title>\n               <jats:p> – The analysis adds to existing literature by focusing on the highly trading nations of Southeast and East Asian countries. The results suggest that reassessment of trade policies in these countries is much needed and it must go beyond the sole pursuit of economic development via trade.</jats:p>\n            </jats:sec>","\n purpose \n – the purpose of this paper is to analyse the implication of trade on carbon emissions in a panel of eight highly trading southeast and east asian countries, namely, china, indonesia, south korea, malaysia, hong kong, the philippines, singapore and thailand. \n \n \n design/methodology/approach \n – the analysis relies on the standard quadratic environmental kuznets curve (ekc) extended to include energy consumption and international trade. a battery of panel unit root and co-integration tests is applied to establish the variables’ stochastic properties and their long-run relations. then, the specified ekc is estimated using the panel dynamic ordinary least square (ols) estimation technique. \n \n \n findings \n – the panel co-integration statistics verifies the validity of the extended ekc for the countries under study. estimation of the long-run ekc via the dynamic ols estimation method reveals the environmentally degrading effects of trade in these countries, especially in asean and plus south korea and hong kong. \n \n \n practical implications \n – these countries are heavily dependent on trade for their development processes, and as such, their impacts on co 2 emissions would be highly relevant for assessing their trade policies, along the line of the gain-from-trade hypothesis, the race-to-the-bottom hypothesis and the pollution-safe-haven hypothesis. \n \n \n originality/value \n – the analysis adds to existing literature by focusing on the highly trading nations of southeast and east asian countries. the results suggest that reassessment of trade policies in these countries is much needed and it must go beyond the sole pursuit of economic development via trade. \n"
http://orkg.org/orkg/resource/R30486,NEAT-o-Games,10.1145/1371216.1371224,crossref,"<jats:p>This article describes research that aims to encourage physical activity through a novel pervasive gaming paradigm. Data from a wearable accelerometer are logged wirelessly to a cell phone and control the animation of an avatar that represents the player in a virtual race game with other players over the cellular network. Winners are declared every day and players with an excess of activity points can spend some to get hints in mental games of the suite, like Sudoku. The racing game runs in the background throughout the day and every little move counts. As the gaming platform is embedded in the daily routine of players, it may act as a strong behavioral modifier and increase everyday physical activity other than volitional sporting exercise. Such physical activity (e.g., taking the stairs), is termed NEAT and was shown to play a major role in obesity prevention and intervention. A pilot experiment demonstrates that players are engaged in NEAT-o-Games and become more physically active while having a good dosage of fun.</jats:p>","this article describes research that aims to encourage physical activity through a novel pervasive gaming paradigm. data from a wearable accelerometer are logged wirelessly to a cell phone and control the animation of an avatar that represents the player in a virtual race game with other players over the cellular network. winners are declared every day and players with an excess of activity points can spend some to get hints in mental games of the suite, like sudoku. the racing game runs in the background throughout the day and every little move counts. as the gaming platform is embedded in the daily routine of players, it may act as a strong behavioral modifier and increase everyday physical activity other than volitional sporting exercise. such physical activity (e.g., taking the stairs), is termed neat and was shown to play a major role in obesity prevention and intervention. a pilot experiment demonstrates that players are engaged in neat-o-games and become more physically active while having a good dosage of fun."
http://orkg.org/orkg/resource/R30586,PRECISE EYE AND MOUTH LOCALIZATION,10.1142/s0218001409007259,crossref,"<jats:p> The literature on the topic has shown a strong correlation between the degree of precision of face localization and the face recognition performance. Hence, there is a need for precise facial feature detectors, as well as objective measures for their evaluation and comparison. </jats:p><jats:p> In this paper, we will present significant improvements to a previous method for precise eye center localization, by integrating a module for mouth localization. The technique is based on Support Vector Machines trained on optimally chosen Haar wavelet coefficients. The method has been tested on several public databases; the results are reported and compared according to a standard error measure. The tests show that the algorithm achieves high precision of localization. </jats:p>","the literature on the topic has shown a strong correlation between the degree of precision of face localization and the face recognition performance. hence, there is a need for precise facial feature detectors, as well as objective measures for their evaluation and comparison. in this paper, we will present significant improvements to a previous method for precise eye center localization, by integrating a module for mouth localization. the technique is based on support vector machines trained on optimally chosen haar wavelet coefficients. the method has been tested on several public databases; the results are reported and compared according to a standard error measure. the tests show that the algorithm achieves high precision of localization."
http://orkg.org/orkg/resource/R30666,Caries trends 1996-2002 among 6- and 12-year-old children and erosive wear prevalence among 12-year-old children in The Hague,10.1159/000081650,crossref,"<jats:p>In 2002 a dental survey amongst 6- and 12-year-old schoolchildren (n = 832) in The Hague was carried out. The caries findings were compared with findings from earlier studies in The Hague. Caries prevalence (% of caries-free children) and caries experience (mean dmfs scores) among 6-year-old children had not changed significantly in the period 1996–2002. However, a significant increase of caries-free 12-year-old children of low socio-economic status was found in the period 1996–2002. The proportions of caries-free 12-year-old Dutch, Turkish and Moroccan children of low socio-economic status were 88, 69 and 78%, respectively, in 2002. The average DMFT score of 12-year-olds reached a minimum of 0.2. In 2002, 24% of the 12-year-olds exhibited signs of erosion, indicating that the presence of erosive wear was high among youngsters in The Hague.</jats:p>","in 2002 a dental survey amongst 6- and 12-year-old schoolchildren (n = 832) in the hague was carried out. the caries findings were compared with findings from earlier studies in the hague. caries prevalence (% of caries-free children) and caries experience (mean dmfs scores) among 6-year-old children had not changed significantly in the period 1996–2002. however, a significant increase of caries-free 12-year-old children of low socio-economic status was found in the period 1996–2002. the proportions of caries-free 12-year-old dutch, turkish and moroccan children of low socio-economic status were 88, 69 and 78%, respectively, in 2002. the average dmft score of 12-year-olds reached a minimum of 0.2. in 2002, 24% of the 12-year-olds exhibited signs of erosion, indicating that the presence of erosive wear was high among youngsters in the hague."
http://orkg.org/orkg/resource/R30693,"The oral health of children with clefts of the lip, palate, or both",10.1597/1545-1569_2001_038_0525_tohocw_2.0.co_2,crossref,"""<jats:sec><jats:title>Objective:</jats:title><jats:p> The purpose of this study was to assess the prevalence of dental caries, developmental defects of enamel, and related factors in children with clefts. </jats:p></jats:sec><jats:sec><jats:title>Design:</jats:title><jats:p> This cross-sectional prevalence study used standard dental indices for assessment. </jats:p></jats:sec><jats:sec><jats:title>Setting:</jats:title><jats:p> Children underwent a dental examination under standard conditions of seating and lighting in the outpatient department of a dental hospital as part of an ongoing audit to monitor clinical outcomes. </jats:p></jats:sec><jats:sec><jats:title>Participants:</jats:title><jats:p> Ninety-one children aged 4, 8, and 12 years were included in the study. </jats:p></jats:sec><jats:sec><jats:title>Outcome Measurements</jats:title><jats:p> Dental caries were assessed by use of the decayed, missing, and filled index for primary teeth (dmft); Decayed, Missing, and Filled index for permanent teeth (DMFT) according to the criteria as used in the national survey of children's dental health in the United Kingdom (O'Brien, 1994). Developmental defects were assessed using the modified Developmental Defects of Enamel Index (Clarkson and O'Mullane, 1989). Dental erosion was assessed using the criteria derived for the national survey of children's dental health (O'Brien, 1994). </jats:p></jats:sec><jats:sec><jats:title>Results:</jats:title><jats:p> Caries prevalence increased with age; 63% of patients at 4 years and 34% at 12 years were caries free. The mean dmft for the 4-year-olds was 1.3 with a mean DMFT for the 12-year-olds of 1.8. All the 4-year-olds had evidence of erosion of enamel in the primary teeth (incisors and first molars) and 56% of the 12-year-olds had erosion of permanent teeth (incisors and first permanent molars). Developmental defects of enamel became more prevalent with age, with at least one opacity in 56% of 4-year-olds and 100% of 12-year-olds. Hypoplasia was not found in the primary dentition but affected permanent teeth in 38% of 8-year-olds and 23% of the 12-year-olds. </jats:p></jats:sec><jats:sec><jats:title>Conclusion:</jats:title><jats:p> This study has shown that dental disease is prevalent in these patients. These assessments not only provide a baseline on oral health parameters in young people with clefts but underline the need for a more aggressive approach to prevention of oral disease to optimize clinical outcome. </jats:p></jats:sec>""",""" objective: the purpose of this study was to assess the prevalence of dental caries, developmental defects of enamel, and related factors in children with clefts. design: this cross-sectional prevalence study used standard dental indices for assessment. setting: children underwent a dental examination under standard conditions of seating and lighting in the outpatient department of a dental hospital as part of an ongoing audit to monitor clinical outcomes. participants: ninety-one children aged 4, 8, and 12 years were included in the study. outcome measurements dental caries were assessed by use of the decayed, missing, and filled index for primary teeth (dmft); decayed, missing, and filled index for permanent teeth (dmft) according to the criteria as used in the national survey of children's dental health in the united kingdom (o'brien, 1994). developmental defects were assessed using the modified developmental defects of enamel index (clarkson and o'mullane, 1989). dental erosion was assessed using the criteria derived for the national survey of children's dental health (o'brien, 1994). results: caries prevalence increased with age; 63% of patients at 4 years and 34% at 12 years were caries free. the mean dmft for the 4-year-olds was 1.3 with a mean dmft for the 12-year-olds of 1.8. all the 4-year-olds had evidence of erosion of enamel in the primary teeth (incisors and first molars) and 56% of the 12-year-olds had erosion of permanent teeth (incisors and first permanent molars). developmental defects of enamel became more prevalent with age, with at least one opacity in 56% of 4-year-olds and 100% of 12-year-olds. hypoplasia was not found in the primary dentition but affected permanent teeth in 38% of 8-year-olds and 23% of the 12-year-olds. conclusion: this study has shown that dental disease is prevalent in these patients. these assessments not only provide a baseline on oral health parameters in young people with clefts but underline the need for a more aggressive approach to prevention of oral disease to optimize clinical outcome. """
http://orkg.org/orkg/resource/R30726,Relationship between sports drinks and dental erosion in 304 university athletes in Columbus,10.1159/000063927,crossref,"<jats:p>Acidic soft drinks, including sports drinks, have been implicated in dental erosion with limited supporting data in scarce erosion studies worldwide. The purpose of this study was to determine the prevalence of dental erosion in a sample of athletes at a large Midwestern state university in the USA, and to evaluate whether regular consumption of sports drinks was associated with dental erosion. A cross-sectional, observational study was done using a convenience sample of 304 athletes, selected irrespective of sports drinks usage. The Lussi Index was used in a blinded clinical examination to grade the frequency and severity of erosion of all tooth surfaces excluding third molars and incisal surfaces of anterior teeth. A self-administered questionnaire was used to gather details on sports drink usage, lifestyle, health problems, dietary and oral health habits. Intraoral color slides were taken of all teeth with erosion. Sports drinks usage was found in 91.8% athletes and the total prevalence of erosion was 36.5%. Nonparametric tests and stepwise regression analysis using history variables showed no association between dental erosion and the use of sports drinks, quantity and frequency of consumption, years of usage and nonsport usage of sports drinks. The most significant predictor of erosion was found to be not belonging to the African race (p &lt; 0.0001). The results of this study reveal no relationship between consumption of sports drinks and dental erosion.</jats:p>","acidic soft drinks, including sports drinks, have been implicated in dental erosion with limited supporting data in scarce erosion studies worldwide. the purpose of this study was to determine the prevalence of dental erosion in a sample of athletes at a large midwestern state university in the usa, and to evaluate whether regular consumption of sports drinks was associated with dental erosion. a cross-sectional, observational study was done using a convenience sample of 304 athletes, selected irrespective of sports drinks usage. the lussi index was used in a blinded clinical examination to grade the frequency and severity of erosion of all tooth surfaces excluding third molars and incisal surfaces of anterior teeth. a self-administered questionnaire was used to gather details on sports drink usage, lifestyle, health problems, dietary and oral health habits. intraoral color slides were taken of all teeth with erosion. sports drinks usage was found in 91.8% athletes and the total prevalence of erosion was 36.5%. nonparametric tests and stepwise regression analysis using history variables showed no association between dental erosion and the use of sports drinks, quantity and frequency of consumption, years of usage and nonsport usage of sports drinks. the most significant predictor of erosion was found to be not belonging to the african race (p &lt; 0.0001). the results of this study reveal no relationship between consumption of sports drinks and dental erosion."
http://orkg.org/orkg/resource/R30767,A two‐stage procurement model for humanitarian relief supply chains,10.1108/20426741111188329,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to discuss and to help address the need for quantitative models to support and improve procurement in the context of humanitarian relief efforts.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>This research presents a two‐stage stochastic decision model with recourse for procurement in humanitarian relief supply chains, and compares its effectiveness on an illustrative example with respect to a standard solution approach.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>Results show the ability of the new model to capture and model both the procurement process and the uncertainty inherent in a disaster relief situation, in support of more efficient and effective procurement plans.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>The research focus is on sudden onset disasters and it does not differentiate between local and international suppliers. A number of extensions of the base model could be implemented, however, so as to address the specific needs of a given organization and their procurement process.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>Despite the prevalence of procurement expenditures in humanitarian efforts, procurement in humanitarian contexts is a topic that previously has only been discussed in a qualitative manner in the literature. This work provides practitioners with a new approach to quantitatively assess and improve their procurement decision processes.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This study adds to the existing literature by demonstrating the applicability and effectiveness of an analytic modeling technique based on uncertainty, such as stochastic programming with recourse, in the context of humanitarian relief procurement activities.</jats:p></jats:sec>","purpose the purpose of this paper is to discuss and to help address the need for quantitative models to support and improve procurement in the context of humanitarian relief efforts. design/methodology/approach this research presents a two‐stage stochastic decision model with recourse for procurement in humanitarian relief supply chains, and compares its effectiveness on an illustrative example with respect to a standard solution approach. findings results show the ability of the new model to capture and model both the procurement process and the uncertainty inherent in a disaster relief situation, in support of more efficient and effective procurement plans. research limitations/implications the research focus is on sudden onset disasters and it does not differentiate between local and international suppliers. a number of extensions of the base model could be implemented, however, so as to address the specific needs of a given organization and their procurement process. practical implications despite the prevalence of procurement expenditures in humanitarian efforts, procurement in humanitarian contexts is a topic that previously has only been discussed in a qualitative manner in the literature. this work provides practitioners with a new approach to quantitatively assess and improve their procurement decision processes. originality/value this study adds to the existing literature by demonstrating the applicability and effectiveness of an analytic modeling technique based on uncertainty, such as stochastic programming with recourse, in the context of humanitarian relief procurement activities."
http://orkg.org/orkg/resource/R30811,Two-Stage Multiobjective Optimization for Emergency Supplies Allocation Problem under Integrated Uncertainty,10.1155/2016/2823835,crossref,"<jats:p>This paper proposes a new two-stage optimization method for emergency supplies allocation problem with multisupplier, multiaffected area, multirelief, and multivehicle. The triplet of supply, demand, and the availability of path is unknown prior to the extraordinary event and is descriptive with fuzzy random variable. Considering the fairness, timeliness, and economical efficiency, a multiobjective expected value model is built for facility location, vehicle routing, and supply allocation decisions. The goals of proposed model aim to minimize the proportion of demand nonsatisfied and response time of emergency reliefs and the total cost of the whole process. When the demand and the availability of path are discrete, the expected values in the objective functions are converted into their equivalent forms. When the supply amount is continuous, the equilibrium chance in the constraint is transformed to its equivalent one. To overcome the computational difficulty caused by multiple objectives, a goal programming model is formulated to obtain a compromise solution. Finally, an example is presented to illustrate the validity of the proposed model and the effectiveness of the solution method.</jats:p>","this paper proposes a new two-stage optimization method for emergency supplies allocation problem with multisupplier, multiaffected area, multirelief, and multivehicle. the triplet of supply, demand, and the availability of path is unknown prior to the extraordinary event and is descriptive with fuzzy random variable. considering the fairness, timeliness, and economical efficiency, a multiobjective expected value model is built for facility location, vehicle routing, and supply allocation decisions. the goals of proposed model aim to minimize the proportion of demand nonsatisfied and response time of emergency reliefs and the total cost of the whole process. when the demand and the availability of path are discrete, the expected values in the objective functions are converted into their equivalent forms. when the supply amount is continuous, the equilibrium chance in the constraint is transformed to its equivalent one. to overcome the computational difficulty caused by multiple objectives, a goal programming model is formulated to obtain a compromise solution. finally, an example is presented to illustrate the validity of the proposed model and the effectiveness of the solution method."
http://orkg.org/orkg/resource/R31217,When Fast-Growing Economies Slow Down: International Evidence and Implications for China,10.1162/asep_a_00118,crossref,"<jats:p> Using international data starting in 1957, we construct a sample of cases where fast-growing economies slow down. The evidence suggests that rapidly growing economies slow down significantly, in the sense that the growth rate downshifts by at least 2 percentage points, when their per capita incomes reach around US$ 17,000 in year-2005 constant international prices, a level that China should achieve by or soon after 2015. Among our more provocative findings is that growth slowdowns are more likely in countries that maintain undervalued real exchange rates. </jats:p>","using international data starting in 1957, we construct a sample of cases where fast-growing economies slow down. the evidence suggests that rapidly growing economies slow down significantly, in the sense that the growth rate downshifts by at least 2 percentage points, when their per capita incomes reach around us$ 17,000 in year-2005 constant international prices, a level that china should achieve by or soon after 2015. among our more provocative findings is that growth slowdowns are more likely in countries that maintain undervalued real exchange rates."
http://orkg.org/orkg/resource/R31241,Middle-Income Traps: A Conceptual and Empirical Survey,10.1142/s1793993315500131,crossref,"<jats:p>The term ""middle-income trap"" has entered common parlance in the development policy community, despite the lack of a precise definition. This paper discusses in more detail the definitional issues associated with the term. It also provides evidence on whether the growth performance of middle-income countries (MICs) has been different from other income categories, including historical transition phases in the inter-country distribution of income. A transition matrix analysis and an exploration of cross-country growth patterns provide little support for the existence of a middle-income trap.</jats:p>","the term ""middle-income trap"" has entered common parlance in the development policy community, despite the lack of a precise definition. this paper discusses in more detail the definitional issues associated with the term. it also provides evidence on whether the growth performance of middle-income countries (mics) has been different from other income categories, including historical transition phases in the inter-country distribution of income. a transition matrix analysis and an exploration of cross-country growth patterns provide little support for the existence of a middle-income trap."
http://orkg.org/orkg/resource/R32108,An enhanced MPS solution for FMS using GAs,10.1108/eum0000000005712,crossref,<jats:p>Presents the master production scheduling (MPS) problem of a flexible manufacturing system (FMS). Earliness/tardiness production scheduling and planning (ETPSP) is one of the solutions used to integrate MRP and JIT effectively. Previous researches on ETPSP have only applied to problems of single/parallel machines with earliness and tardiness penalties on a common due date and capacity. Proposes a revised ETPSP for the purpose of developing an MPS that can fit into the FMS environment where a multiple machine capacity on multi‐processes and batch sizes is included. Outlines the application of an enhanced ETPSP method using a genetic algorithm (GA) solution to solve a multi‐product FMS production problem. Shows that the use of the improved ETPSP model can represent a real life FMS environment and that a solution can be effectively and efficiently obtained using the GA approach.</jats:p>,presents the master production scheduling (mps) problem of a flexible manufacturing system (fms). earliness/tardiness production scheduling and planning (etpsp) is one of the solutions used to integrate mrp and jit effectively. previous researches on etpsp have only applied to problems of single/parallel machines with earliness and tardiness penalties on a common due date and capacity. proposes a revised etpsp for the purpose of developing an mps that can fit into the fms environment where a multiple machine capacity on multi‐processes and batch sizes is included. outlines the application of an enhanced etpsp method using a genetic algorithm (ga) solution to solve a multi‐product fms production problem. shows that the use of the improved etpsp model can represent a real life fms environment and that a solution can be effectively and efficiently obtained using the ga approach.
http://orkg.org/orkg/resource/R32277,APPLICATION OF ESSENTIAL OIL OF ARTEMISIA HERBA ALBA AS GREEN CORROSION INHIBITOR FOR STEEL IN 0.5 M H2SO4,10.1142/s0218625x09012287,crossref,<jats:p> Essential oil from Artemisia herba alba (Art) was hydrodistilled and tested as corrosion inhibitor of steel in 0.5 M H <jats:sub>2</jats:sub> SO <jats:sub>4</jats:sub> using weight loss measurements and electrochemical polarization methods. Results gathered show that this natural oil reduced the corrosion rate by the cathodic action. Its inhibition efficiency attains the maximum (74%) at 1 g/L. The inhibition efficiency of Arm oil increases with the rise of temperature. The adsorption isotherm of natural product on the steel has been determined. A. herba alba essential oil was obtained by hydrodistillation and its chemical composition oil was investigated by capillary GC and GC/MS. The major components were chrysanthenone (30.6%) and camphor (24.4%). </jats:p>,essential oil from artemisia herba alba (art) was hydrodistilled and tested as corrosion inhibitor of steel in 0.5 m h 2 so 4 using weight loss measurements and electrochemical polarization methods. results gathered show that this natural oil reduced the corrosion rate by the cathodic action. its inhibition efficiency attains the maximum (74%) at 1 g/l. the inhibition efficiency of arm oil increases with the rise of temperature. the adsorption isotherm of natural product on the steel has been determined. a. herba alba essential oil was obtained by hydrodistillation and its chemical composition oil was investigated by capillary gc and gc/ms. the major components were chrysanthenone (30.6%) and camphor (24.4%).
http://orkg.org/orkg/resource/R32286,Chemical variability of Artemisia herba-alba Asso essential oils from East Morocco,10.2478/s11696-010-0051-5,crossref,"<jats:title>Abstract</jats:title><jats:p>Chemical compositions of 16 Artemisia herba-alba oil samples harvested in eight East Moroccan locations were investigated by GC and GC/MS. Chemical variability of the A. herba-alba oils is also discussed using statistical analysis. Detailed analysis of the essential oils led to the identification of 52 components amounting to 80.5–98.6 % of the total oil. The investigated chemical compositions showed significant qualitative and quantitative differences. According to their major components (camphor, chrysanthenone, and α- and β-thujone), three main groups of essential oils were found. This study also found regional specificity of the major components.</jats:p>","abstract chemical compositions of 16 artemisia herba-alba oil samples harvested in eight east moroccan locations were investigated by gc and gc/ms. chemical variability of the a. herba-alba oils is also discussed using statistical analysis. detailed analysis of the essential oils led to the identification of 52 components amounting to 80.5–98.6 % of the total oil. the investigated chemical compositions showed significant qualitative and quantitative differences. according to their major components (camphor, chrysanthenone, and α- and β-thujone), three main groups of essential oils were found. this study also found regional specificity of the major components."
http://orkg.org/orkg/resource/R32377,IMPACT OF SEASON AND HARVEST FREQUENCY ON BIOMASS AND ESSENTIAL OIL YIELDS OF ARTEMISIA HERBA-ALBA CULTIVATED IN SOUTHERN TUNISIA,10.1017/s0014479709990445,crossref,"<jats:title>SUMMARY</jats:title><jats:p><jats:italic>Artemisia herba-alba</jats:italic> Asso has been successfully cultivated in the Tunisian arid zone. However, information regarding the effects of the harvest frequency on its biomass and essential oil yields is very limited. In this study, the effects of three different frequencies of harvesting the upper half of the <jats:italic>A. herba-alba</jats:italic> plant tuft were compared. The harvest treatments were: harvesting the same individual plants at the flowering stage annually; harvesting the same individual plants at the full vegetative growth stage annually and harvesting the same individual plants every six months. Statistical analyses indicated that all properties studied were affected by the harvest frequency. Essential oil yield, depended both on the dry biomass and its essential oil content, and was significantly higher from plants harvested annually at the flowering stage than the other two treatments. The composition of the β- and α-thujone-rich oils did not vary throughout the experimental period.</jats:p>","summary artemisia herba-alba asso has been successfully cultivated in the tunisian arid zone. however, information regarding the effects of the harvest frequency on its biomass and essential oil yields is very limited. in this study, the effects of three different frequencies of harvesting the upper half of the a. herba-alba plant tuft were compared. the harvest treatments were: harvesting the same individual plants at the flowering stage annually; harvesting the same individual plants at the full vegetative growth stage annually and harvesting the same individual plants every six months. statistical analyses indicated that all properties studied were affected by the harvest frequency. essential oil yield, depended both on the dry biomass and its essential oil content, and was significantly higher from plants harvested annually at the flowering stage than the other two treatments. the composition of the β- and α-thujone-rich oils did not vary throughout the experimental period."
http://orkg.org/orkg/resource/R32665,A novel method of ship detection from spaceborne optical image based on spatial pyramid matching,10.4028/www.scientific.net/AMM.190-191.1099,crossref,"<jats:p>In this paper we propose an automatic ship detection method in High Resolution optical satellite images based on neighbor context information. First, a pre-detection of targets gives us candidates. For each candidate, we choose an extended region called candidate with neighborhood which comprises candidate and its neighbor area. Second, the patches of candidate with neighborhood are got by a regular grid, and their SIFT(Scale Invariant Feature Transform) features are extracted. Then the SIFT features of training images are clustered with the K-means algorithm to form a codebook of the patches. We quantize the patches of candidate with neighborhood according to this codebook and get the visual word representation. Finally by applying spatial pyramid matching, the candidates are classified with SVM (support vector machine). Experiment results are given for a set of images show that our method has got predominant performance.</jats:p>","in this paper we propose an automatic ship detection method in high resolution optical satellite images based on neighbor context information. first, a pre-detection of targets gives us candidates. for each candidate, we choose an extended region called candidate with neighborhood which comprises candidate and its neighbor area. second, the patches of candidate with neighborhood are got by a regular grid, and their sift(scale invariant feature transform) features are extracted. then the sift features of training images are clustered with the k-means algorithm to form a codebook of the patches. we quantize the patches of candidate with neighborhood according to this codebook and get the visual word representation. finally by applying spatial pyramid matching, the candidates are classified with svm (support vector machine). experiment results are given for a set of images show that our method has got predominant performance."
http://orkg.org/orkg/resource/R32694,NEAR REAL-TIME AUTOMATIC MARINE VESSEL DETECTION ON OPTICAL SATELLITE IMAGES,10.5194/isprsarchives-xl-1-w1-233-2013,crossref,"<jats:p>Abstract. Vessel monitoring and surveillance is important for maritime safety and security, environment protection and border control. Ship monitoring systems based on Synthetic-aperture Radar (SAR) satellite images are operational. On SAR images the ships made of metal with sharp edges appear as bright dots and edges, therefore they can be well distinguished from the water. Since the radar is independent from the sun light and can acquire images also by cloudy weather and rain, it provides a reliable service. Vessel detection from spaceborne optical images (VDSOI) can extend the SAR based systems by providing more frequent revisit times and overcoming some drawbacks of the SAR images (e.g. lower spatial resolution, difficult human interpretation). Optical satellite images (OSI) can have a higher spatial resolution thus enabling the detection of smaller vessels and enhancing the vessel type classification. The human interpretation of an optical image is also easier than as of SAR image. In this paper I present a rapid automatic vessel detection method which uses pattern recognition methods, originally developed in the computer vision field. In the first step I train a binary classifier from image samples of vessels and background. The classifier uses simple features which can be calculated very fast. For the detection the classifier is slided along the image in various directions and scales. The detector has a cascade structure which rejects most of the background in the early stages which leads to faster execution. The detections are grouped together to avoid multiple detections. Finally the position, size(i.e. length and width) and heading of the vessels is extracted from the contours of the vessel. The presented method is parallelized, thus it runs fast (in minutes for 16000 × 16000 pixels image) on a multicore computer, enabling near real-time applications, e.g. one hour from image acquisition to end user.\n                    </jats:p>","abstract. vessel monitoring and surveillance is important for maritime safety and security, environment protection and border control. ship monitoring systems based on synthetic-aperture radar (sar) satellite images are operational. on sar images the ships made of metal with sharp edges appear as bright dots and edges, therefore they can be well distinguished from the water. since the radar is independent from the sun light and can acquire images also by cloudy weather and rain, it provides a reliable service. vessel detection from spaceborne optical images (vdsoi) can extend the sar based systems by providing more frequent revisit times and overcoming some drawbacks of the sar images (e.g. lower spatial resolution, difficult human interpretation). optical satellite images (osi) can have a higher spatial resolution thus enabling the detection of smaller vessels and enhancing the vessel type classification. the human interpretation of an optical image is also easier than as of sar image. in this paper i present a rapid automatic vessel detection method which uses pattern recognition methods, originally developed in the computer vision field. in the first step i train a binary classifier from image samples of vessels and background. the classifier uses simple features which can be calculated very fast. for the detection the classifier is slided along the image in various directions and scales. the detector has a cascade structure which rejects most of the background in the early stages which leads to faster execution. the detections are grouped together to avoid multiple detections. finally the position, size(i.e. length and width) and heading of the vessels is extracted from the contours of the vessel. the presented method is parallelized, thus it runs fast (in minutes for 16000 × 16000 pixels image) on a multicore computer, enabling near real-time applications, e.g. one hour from image acquisition to end user.\n"
http://orkg.org/orkg/resource/R32733,A remote sensing ship recognition method based on co-training model,10.4028/www.scientific.net/AMM.713-715.2077,crossref,"<jats:p>Aiming at detecting sea targets efficiently, an approach using optical remote sensing data based on co-training model is proposed. Firstly, using size, texture, shape, moment invariants features and ratio codes, feature extraction is realized. Secondly, based on rough set theory, the common discernibility degree is used to select valid recognition features automatically. Finally, a co-training model for classification is introduced. Firstly, two diverse ruducts are generated, and then the model employs them to train two base classifiers on labeled dada, and makes two base classifiers teach each other on unlabeled data to boot their performance iteratively. Experimental results show the proposed approach can get better performance than K-Nearest Neighbor (KNN), Support Vector Machines (SVM), traditional hierarchical discriminant regression (HDR).</jats:p>","aiming at detecting sea targets efficiently, an approach using optical remote sensing data based on co-training model is proposed. firstly, using size, texture, shape, moment invariants features and ratio codes, feature extraction is realized. secondly, based on rough set theory, the common discernibility degree is used to select valid recognition features automatically. finally, a co-training model for classification is introduced. firstly, two diverse ruducts are generated, and then the model employs them to train two base classifiers on labeled dada, and makes two base classifiers teach each other on unlabeled data to boot their performance iteratively. experimental results show the proposed approach can get better performance than k-nearest neighbor (knn), support vector machines (svm), traditional hierarchical discriminant regression (hdr)."
http://orkg.org/orkg/resource/R32760,Combined use of optical imaging satellite data and electronic intelligence satellite data for large scale ship group surveillance,10.1017/s0373463314000654,crossref,"<jats:p>We propose a novel framework for large-scale maritime ship group surveillance using spaceborne optical imaging satellite data and Electronic Intelligence (ELINT) satellite data. Considering that the size of a ship is usually less than the distance between different ships for large-scale maritime surveillance, we treat each ship as a mass point and ship groups are modelled as point sets. Motivated by the observation that ship groups performing tactical or strategic operations often have a stable topology and their attributes remain unchanged, we combine both topological features and attributive features within the framework of Dempster-Shafer (D-S) theory for coherent ship group analysis. Our method has been tested using different sets of simulated data and recorded data. Experimental results demonstrate our method is robust and efficient for large-scale maritime surveillance.</jats:p>","we propose a novel framework for large-scale maritime ship group surveillance using spaceborne optical imaging satellite data and electronic intelligence (elint) satellite data. considering that the size of a ship is usually less than the distance between different ships for large-scale maritime surveillance, we treat each ship as a mass point and ship groups are modelled as point sets. motivated by the observation that ship groups performing tactical or strategic operations often have a stable topology and their attributes remain unchanged, we combine both topological features and attributive features within the framework of dempster-shafer (d-s) theory for coherent ship group analysis. our method has been tested using different sets of simulated data and recorded data. experimental results demonstrate our method is robust and efficient for large-scale maritime surveillance."
http://orkg.org/orkg/resource/R32786,SENTINEL-1/2 DATA FOR SHIP TRAFFIC MONITORING ON THE DANUBE RIVER,10.5194/isprs-archives-xli-b8-37-2016,crossref,"<jats:p>Abstract. After a long period of drought, the water level of the Danube River has significantly dropped especially on the Romanian sector, in July-August 2015. Danube reached the lowest water level recorded in the last 12 years, causing the blockage of the ships in the sector located close to Zimnicea Harbour. The rising sand banks in the navigable channel congested the commercial traffic for a few days with more than 100 ships involved. The monitoring of the decreasing water level and the traffic jam was performed based on Sentinel-1 and Sentinel-2 free data provided by the European Space Agency and the European Commission within the Copernicus Programme. Specific processing methods (calibration, speckle filtering, geocoding, change detection, image classification, principal component analysis, etc.) were applied in order to generate useful products that the responsible authorities could benefit from. The Sentinel data yielded good results for water mask extraction and ships detection. The analysis continued after the closure of the crisis situation when the water reached the nominal level again. The results indicate that Sentinel data can be successfully used for ship traffic monitoring, building the foundation of future endeavours for a durable monitoring of the Danube River.\n                    </jats:p>","abstract. after a long period of drought, the water level of the danube river has significantly dropped especially on the romanian sector, in july-august 2015. danube reached the lowest water level recorded in the last 12 years, causing the blockage of the ships in the sector located close to zimnicea harbour. the rising sand banks in the navigable channel congested the commercial traffic for a few days with more than 100 ships involved. the monitoring of the decreasing water level and the traffic jam was performed based on sentinel-1 and sentinel-2 free data provided by the european space agency and the european commission within the copernicus programme. specific processing methods (calibration, speckle filtering, geocoding, change detection, image classification, principal component analysis, etc.) were applied in order to generate useful products that the responsible authorities could benefit from. the sentinel data yielded good results for water mask extraction and ships detection. the analysis continued after the closure of the crisis situation when the water reached the nominal level again. the results indicate that sentinel data can be successfully used for ship traffic monitoring, building the foundation of future endeavours for a durable monitoring of the danube river.\n"
http://orkg.org/orkg/resource/R32791,A NOVEL SHIP DETECTION METHOD FOR LARGE-SCALE OPTICAL SATELLITE IMAGES BASED ON VISUAL LBP FEATURE AND VISUAL ATTENTION MODEL,10.5194/isprs-archives-xli-b3-917-2016,crossref,"<jats:p>Abstract. Reliably ship detection in optical satellite images has a wide application in both military and civil fields. However, this problem is very difficult in complex backgrounds, such as waves, clouds, and small islands. Aiming at these issues, this paper explores an automatic and robust model for ship detection in large-scale optical satellite images, which relies on detecting statistical signatures of ship targets, in terms of biologically-inspired visual features. This model first selects salient candidate regions across large-scale images by using a mechanism based on biologically-inspired visual features, combined with visual attention model with local binary pattern (CVLBP). Different from traditional studies, the proposed algorithm is high-speed and helpful to focus on the suspected ship areas avoiding the separation step of land and sea. Largearea images are cut into small image chips and analyzed in two complementary ways: Sparse saliency using visual attention model and detail signatures using LBP features, thus accordant with sparseness of ship distribution on images. Then these features are employed to classify each chip as containing ship targets or not, using a support vector machine (SVM). After getting the suspicious areas, there are still some false alarms such as microwaves and small ribbon clouds, thus simple shape and texture analysis are adopted to distinguish between ships and nonships in suspicious areas. Experimental results show the proposed method is insensitive to waves, clouds, illumination and ship size.\n                    </jats:p>","abstract. reliably ship detection in optical satellite images has a wide application in both military and civil fields. however, this problem is very difficult in complex backgrounds, such as waves, clouds, and small islands. aiming at these issues, this paper explores an automatic and robust model for ship detection in large-scale optical satellite images, which relies on detecting statistical signatures of ship targets, in terms of biologically-inspired visual features. this model first selects salient candidate regions across large-scale images by using a mechanism based on biologically-inspired visual features, combined with visual attention model with local binary pattern (cvlbp). different from traditional studies, the proposed algorithm is high-speed and helpful to focus on the suspected ship areas avoiding the separation step of land and sea. largearea images are cut into small image chips and analyzed in two complementary ways: sparse saliency using visual attention model and detail signatures using lbp features, thus accordant with sparseness of ship distribution on images. then these features are employed to classify each chip as containing ship targets or not, using a support vector machine (svm). after getting the suspicious areas, there are still some false alarms such as microwaves and small ribbon clouds, thus simple shape and texture analysis are adopted to distinguish between ships and nonships in suspicious areas. experimental results show the proposed method is insensitive to waves, clouds, illumination and ship size.\n"
http://orkg.org/orkg/resource/R32858,S-CNN-BASED SHIP DETECTION FROM HIGH-RESOLUTION REMOTE SENSING IMAGES,10.5194/isprs-archives-xli-b7-423-2016,crossref,"<jats:p>Abstract. Reliable ship detection plays an important role in both military and civil fields. However, it makes the task difficult with high-resolution remote sensing images with complex background and various types of ships with different poses, shapes and scales. Related works mostly used gray and shape features to detect ships, which obtain results with poor robustness and efficiency. To detect ships more automatically and robustly, we propose a novel ship detection method based on the convolutional neural networks (CNNs), called SCNN, fed with specifically designed proposals extracted from the ship model combined with an improved saliency detection method. Firstly we creatively propose two ship models, the “V” ship head model and the “||” ship body one, to localize the ship proposals from the line segments extracted from a test image. Next, for offshore ships with relatively small sizes, which cannot be efficiently picked out by the ship models due to the lack of reliable line segments, we propose an improved saliency detection method to find these proposals. Therefore, these two kinds of ship proposals are fed to the trained CNN for robust and efficient detection. Experimental results on a large amount of representative remote sensing images with different kinds of ships with varied poses, shapes and scales demonstrate the efficiency and robustness of our proposed S-CNN-Based ship detector.\n                    </jats:p>","abstract. reliable ship detection plays an important role in both military and civil fields. however, it makes the task difficult with high-resolution remote sensing images with complex background and various types of ships with different poses, shapes and scales. related works mostly used gray and shape features to detect ships, which obtain results with poor robustness and efficiency. to detect ships more automatically and robustly, we propose a novel ship detection method based on the convolutional neural networks (cnns), called scnn, fed with specifically designed proposals extracted from the ship model combined with an improved saliency detection method. firstly we creatively propose two ship models, the “v” ship head model and the “||” ship body one, to localize the ship proposals from the line segments extracted from a test image. next, for offshore ships with relatively small sizes, which cannot be efficiently picked out by the ship models due to the lack of reliable line segments, we propose an improved saliency detection method to find these proposals. therefore, these two kinds of ship proposals are fed to the trained cnn for robust and efficient detection. experimental results on a large amount of representative remote sensing images with different kinds of ships with varied poses, shapes and scales demonstrate the efficiency and robustness of our proposed s-cnn-based ship detector.\n"
http://orkg.org/orkg/resource/R32875,Determinants of web site information by Spanish city councils,10.1108/14684520810865976,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this research is to analyse the web sites of large Spanish city councils with the objective of assessing the extent of information disseminated on the internet and determining what factors are affecting the observed levels of information disclosure.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>The study takes as its reference point the existing literature on the examination of the quality of web sites, in particular the provisions of the Web Quality Model (WQM) and the importance of content as a key variable in determining web site quality. In order to quantify the information on city council web sites, a Disclosure Index has been designed which takes into account the content, navigability and presentation of the web sites. In order to contrast which variables determine the information provided on the web sites, our investigation bases itself on the studies about voluntary disclosure in the public sector, and six lineal regressions models have been performed.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The empirical evidence obtained reveals low disclosure levels among Spanish city council web sites. In spite of this, almost 50 per cent of the city councils have reached the “approved” level and of these, around a quarter obtained good marks. Our results show that disclosure levels depend on political competition, public media visibility and the access to technology and educational levels of the citizens.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>The strategy of communication on the internet by local Spanish authorities is limited in general to an ornamental web presence but one that does not respond efficiently to the requirements of the digital society. During the coming years, local Spanish politicians will have to strive to take advantage of the opportunities that the internet offers to increase both the relational and informational capacity of municipal web sites as well as the digital information transparency of their public management.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>The internet is a potent channel of communication that is modifying the way in which people access and relate to information and each other. The public sector is not unaware of these changes and is incorporating itself gradually into the new network society. This study systematises the analysis of local administration web sites, showing the lack of digital transparency, and orients politicians in the direction to follow in order to introduce improvements in their electronic relationships with the public.</jats:p></jats:sec>","purpose the purpose of this research is to analyse the web sites of large spanish city councils with the objective of assessing the extent of information disseminated on the internet and determining what factors are affecting the observed levels of information disclosure. design/methodology/approach the study takes as its reference point the existing literature on the examination of the quality of web sites, in particular the provisions of the web quality model (wqm) and the importance of content as a key variable in determining web site quality. in order to quantify the information on city council web sites, a disclosure index has been designed which takes into account the content, navigability and presentation of the web sites. in order to contrast which variables determine the information provided on the web sites, our investigation bases itself on the studies about voluntary disclosure in the public sector, and six lineal regressions models have been performed. findings the empirical evidence obtained reveals low disclosure levels among spanish city council web sites. in spite of this, almost 50 per cent of the city councils have reached the “approved” level and of these, around a quarter obtained good marks. our results show that disclosure levels depend on political competition, public media visibility and the access to technology and educational levels of the citizens. practical implications the strategy of communication on the internet by local spanish authorities is limited in general to an ornamental web presence but one that does not respond efficiently to the requirements of the digital society. during the coming years, local spanish politicians will have to strive to take advantage of the opportunities that the internet offers to increase both the relational and informational capacity of municipal web sites as well as the digital information transparency of their public management. originality/value the internet is a potent channel of communication that is modifying the way in which people access and relate to information and each other. the public sector is not unaware of these changes and is incorporating itself gradually into the new network society. this study systematises the analysis of local administration web sites, showing the lack of digital transparency, and orients politicians in the direction to follow in order to introduce improvements in their electronic relationships with the public."
http://orkg.org/orkg/resource/R32877,Communicating performance: the extent and effectiveness of performance reporting by U.S. colleges and universities,10.1108/jpbafm-20-02-2008-b006,crossref,"<jats:p>Performance measures have long been a topic of interest in higher education although no consensus on the best way to measure performance has been achieved. This paper examines the extent and effectiveness of service efforts and accomplishment reporting by public and not-for-profit U.S. colleges and universities using survey data provided by the National Association of College and University Business Officers. Effectiveness is evaluated using the Government Accounting Standards Board (GASB) suggested criteria. Regression analysis suggests an association between the extent of disclosure and size, leverage, level of education provided, and regional accreditation agency. Private institutions rate themselves as more effective communicators. Effectiveness of communication is also associated with the extent of disclosure, level of education provided and accreditation region.</jats:p>","performance measures have long been a topic of interest in higher education although no consensus on the best way to measure performance has been achieved. this paper examines the extent and effectiveness of service efforts and accomplishment reporting by public and not-for-profit u.s. colleges and universities using survey data provided by the national association of college and university business officers. effectiveness is evaluated using the government accounting standards board (gasb) suggested criteria. regression analysis suggests an association between the extent of disclosure and size, leverage, level of education provided, and regional accreditation agency. private institutions rate themselves as more effective communicators. effectiveness of communication is also associated with the extent of disclosure, level of education provided and accreditation region."
http://orkg.org/orkg/resource/R32881,E-Government and Public Financial Reporting: The Case of Spanish Regional Governments,10.1177/0275074006293193,crossref,"""<jats:p> Technology has changed the way public organizations relate to the public. Government's use of the Internet and other associated technologies, known as e-government, could become the instrument that makes regular timely information on public finances more forthcoming. New technologies can improve government responsiveness and empower individual citizens. By making government financial information available, the public could continuously assess a government agency through everyday interaction. The financial accountability of government and its response to public demands for information and services are thus a contribution to government openness. It is therefore relevant to determine whether public organizations are also becoming more aware of the importance of placing financial information on their Web sites to help in decision-making processes. This article focuses on the e-democracy process, specifically the transparency of government information, by analyzing governmental financial disclosures on the Web as a tool for the public to assess its financial accountability. To this end, an empirical study was carried out on regional governments in Spain. </jats:p>""",""" technology has changed the way public organizations relate to the public. government's use of the internet and other associated technologies, known as e-government, could become the instrument that makes regular timely information on public finances more forthcoming. new technologies can improve government responsiveness and empower individual citizens. by making government financial information available, the public could continuously assess a government agency through everyday interaction. the financial accountability of government and its response to public demands for information and services are thus a contribution to government openness. it is therefore relevant to determine whether public organizations are also becoming more aware of the importance of placing financial information on their web sites to help in decision-making processes. this article focuses on the e-democracy process, specifically the transparency of government information, by analyzing governmental financial disclosures on the web as a tool for the public to assess its financial accountability. to this end, an empirical study was carried out on regional governments in spain. """
http://orkg.org/orkg/resource/R32883,Cultural contexts and governmental digital reporting,10.1177/0020852306064614,crossref,"<jats:p> The way in which public sector entities disseminate information publicly is affected by the degree of transparency adopted, and the construction and management of websites are increasingly essential elements of modern public administration. Nonetheless, differences in this process exist among governments worldwide, probably due to different contextual factors. This article examines and discusses the approach of Anglo-Saxon, South American and Continental European central governments to the use of the Web as a means of making financial disclosures. To measure the disclosure of governmental financial information on the Internet, an index has been defined, taking into consideration the data considered to be relevant for a potential user, gathering the data visiting their websites. The results show that the way different countries use the Web for financial disclosure is deeply rooted in and follows from their administrative culture. In conclusion, the Continental European and South American governments should improve their digital reporting. </jats:p>","the way in which public sector entities disseminate information publicly is affected by the degree of transparency adopted, and the construction and management of websites are increasingly essential elements of modern public administration. nonetheless, differences in this process exist among governments worldwide, probably due to different contextual factors. this article examines and discusses the approach of anglo-saxon, south american and continental european central governments to the use of the web as a means of making financial disclosures. to measure the disclosure of governmental financial information on the internet, an index has been defined, taking into consideration the data considered to be relevant for a potential user, gathering the data visiting their websites. the results show that the way different countries use the web for financial disclosure is deeply rooted in and follows from their administrative culture. in conclusion, the continental european and south american governments should improve their digital reporting."
http://orkg.org/orkg/resource/R32938,Factors influencing the outcome of intestinal anastomosis,10.1177/000313481107700929,crossref,"<jats:p> Anastomotic leak (AL) is one of the most serious complications after gastrointestinal surgery. All patients aged 16 years or older who underwent a surgery with single intestinal anastomosis at Morristown Medical Center from January 2006 to June 2008 were entered into a prospective database. To compare the rate of AL, patients were divided into the following surgery-related groups: 1) stapled versus hand-sewn, 2) small bowel versus large bowel, 3) right versus left colon, 4) emergent versus elective, 5) laparoscopic versus converted (laparoscopic to open) versus open, 6) inflammatory bowel disease versus non inflammatory bowel disease, and 7) diverticulitis versus nondiverticulitis. We also looked for surgical site infection, estimated intraoperative blood loss, blood transfusion, comorbidities, preoperative chemotherapy, radiation, and anticoagulation treatment. The overall rate of AL was 3.8 per cent. Mortality rate was higher among patients with ALs (13.3%) versus patients with no AL (1.7%). Open surgery had greater risk of AL than laparoscopic operations. Surgical site infection and intraoperative blood transfusions were also associated with significantly higher rates of AL. Operations involving the left colon had greater risk of AL when compared with those of the right colon, sigmoid, and rectum. Prior chemotherapy, anticoagulation, and intraoperative blood loss all increased the AL rates. In conclusion, we identified several significant risk factors for ALs. This knowledge should help us better understand and prevent this serious complication, which has significant morbidity and mortality rates. </jats:p>","anastomotic leak (al) is one of the most serious complications after gastrointestinal surgery. all patients aged 16 years or older who underwent a surgery with single intestinal anastomosis at morristown medical center from january 2006 to june 2008 were entered into a prospective database. to compare the rate of al, patients were divided into the following surgery-related groups: 1) stapled versus hand-sewn, 2) small bowel versus large bowel, 3) right versus left colon, 4) emergent versus elective, 5) laparoscopic versus converted (laparoscopic to open) versus open, 6) inflammatory bowel disease versus non inflammatory bowel disease, and 7) diverticulitis versus nondiverticulitis. we also looked for surgical site infection, estimated intraoperative blood loss, blood transfusion, comorbidities, preoperative chemotherapy, radiation, and anticoagulation treatment. the overall rate of al was 3.8 per cent. mortality rate was higher among patients with als (13.3%) versus patients with no al (1.7%). open surgery had greater risk of al than laparoscopic operations. surgical site infection and intraoperative blood transfusions were also associated with significantly higher rates of al. operations involving the left colon had greater risk of al when compared with those of the right colon, sigmoid, and rectum. prior chemotherapy, anticoagulation, and intraoperative blood loss all increased the al rates. in conclusion, we identified several significant risk factors for als. this knowledge should help us better understand and prevent this serious complication, which has significant morbidity and mortality rates."
http://orkg.org/orkg/resource/R32955,"Topical insulin in wound healing: a randomized, double-blind, placebo-controlled trial",10.12968/jowc.1999.8.10.26217,crossref,"<jats:p> Two studies were carried out to assess the relative roles of insulin and zinc in the acceleration of wound healing. In the first study, six diabetic and five non-diabetic human volunteers had two uniform cuts created, one on each forearm. One forearm wound was treated with topical regular insulin (Iletin-II) and the other with normal saline four times a day until healed. Treatment was double-blind and forearms were assigned randomly. The wounds treated with insulin healed 2.4 ± 0.8 days faster than the wounds treated with saline (P &lt; 0.001 by paired t-test). Zinc is used to crystallise insulin. When wounds are treated with insulin, they are therefore also being treated with zinc. If insulin accelerates wound healing, it is not clear if the increase in the rate of healing would be due to insulin (a known growth factor), the zinc it contains, or a combination of the two. The second study used a randomised, double-blind, placebo-controlled design to compare the efficacy of insulin with that of a solution containing the same amount of zinc in accelerating the healing of standardised wounds in rats and humans. Although these pilot investigations did not have the power to define the relative roles of insulin and zinc with accuracy, the results suggest that zinc does play a role in the wound healing process. It is concluded that topical insulin accelerates wound healing in humans. More importantly, however, this study describes a method of creating uniform wounds in humans acceptable to an institutional review board, thus solving one of the major impediments to the scientific evaluation of human wound healing. </jats:p>","two studies were carried out to assess the relative roles of insulin and zinc in the acceleration of wound healing. in the first study, six diabetic and five non-diabetic human volunteers had two uniform cuts created, one on each forearm. one forearm wound was treated with topical regular insulin (iletin-ii) and the other with normal saline four times a day until healed. treatment was double-blind and forearms were assigned randomly. the wounds treated with insulin healed 2.4 ± 0.8 days faster than the wounds treated with saline (p &lt; 0.001 by paired t-test). zinc is used to crystallise insulin. when wounds are treated with insulin, they are therefore also being treated with zinc. if insulin accelerates wound healing, it is not clear if the increase in the rate of healing would be due to insulin (a known growth factor), the zinc it contains, or a combination of the two. the second study used a randomised, double-blind, placebo-controlled design to compare the efficacy of insulin with that of a solution containing the same amount of zinc in accelerating the healing of standardised wounds in rats and humans. although these pilot investigations did not have the power to define the relative roles of insulin and zinc with accuracy, the results suggest that zinc does play a role in the wound healing process. it is concluded that topical insulin accelerates wound healing in humans. more importantly, however, this study describes a method of creating uniform wounds in humans acceptable to an institutional review board, thus solving one of the major impediments to the scientific evaluation of human wound healing."
http://orkg.org/orkg/resource/R32969,Cyto- genetic studies at diagnosis in polycythemia vera: clinical and JAK2V617F allele burden correlates,10.1182/blood.v110.11.2545.2545,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Background: Previous cytogenetic studies in polycythemia vera (PV) have included a relatively small number of patients (“n” ranging 10–64). In the current study (n=137), we describe cytogenetic findings at presentation and examine their relationship to clinical and laboratory features, including bone marrow JAK2V617F allele burden.</jats:p>\n               <jats:p>Methods: The study consisted of a consecutive group of patients with PV who fulfilled the World Health Organization (WHO) diagnostic criteria and in whom bone marrow biopsy and cytogenetic studies were performed at diagnosis.</jats:p>\n               <jats:p>Results I: cytogenetic details At diagnosis: A total of 137 patients (median age, 64 years; 49% females) were studied at diagnosis and had adequate metaphases for interpretation. Cytogenetics were normal in 117 patients (85%) and displayed either a sole -Y abnormality in 5 patients (7% of the male patients), and other chromosomal abnormalities in 15 (11%). The latter included trisomy 8 in five patients, trisomy 9 in three patients, two patients each with del(13q), del(20q), and abnormalities of chromosome 1, and one patient each with del(3)(p13p21), dup(13)(q12q14), and del(11)(q21). At follow-up: Repeat cytogenetic studies while still in the chronic phase of the disease were performed in 19 patients at a median of 60 months (range, 8–198) from diagnosis. Of these, 4 had aquired new cytogenetic clones including 3 with normal cytogenetics at time of initial PV diagnosis. The new abnormalities included del(20q), del(5q), del(1p), chromosome 1 abnormality, and inv(3)(q21q26.2). At time of disease transformation: Leukemic transformation was documented in 3 patients of whom cytogenetic information at the time was available in 2 patients; both patients had normal results at time of initial PV diagnosis and complex cytogenetic abnormalities at time of leukemic transformation. In contrast, among 6 patients with available cytogenetic information at time of fibrotic transformation, the results were unchanged from those obtained at time of diagnosis in 5 patients. ii) Correlation between cytogenetics at diagnosis and JAK2V617F allele burden: Allele-specific, quantitative PCR analysis for JAK2V617F was performed in 71 patients using genomic DNA from archived bone marrow obtained at the time of the initial cytogenetic studies. JAK2V617F mutation was detected in 64 of the 71 (90%) patients; median mutant allele burden was 16% (range 3–80%) without significant difference among the different cytogenetic groups: normal vs. –Y vs. other cytogenetic abnormalities (p=0.72). iii) Clinical correlates and prognostic relevance of cytogenetic findings at diagnosis: Among several parameters studied for significant correlations with cytogenetic findings at diagnosis, an association was evident only for age (p=0.02); all –Y abnormalities (n=5) as well as 13 of the 15 (87%) other cytogenetic abnormalities occurred in patients ≥ 60 years of age. Stated another way, the incidence of abnormal cytogenetics (other than -Y) was 4% for patients younger than age 60 years and 15% otherwise. The presence of abnormal cytogenetics at diagnosis had no significant impact on either overall or leukemia-free survival.</jats:p>\n               <jats:p>Conclusions: Abnormal cytogenetic findings at diagnosis are infrequent in PV, especially in patients below age 60 years. Furthermore, their clinical relevance is limited and there is not significant correlation with bone marrow JAK2V617F allele burden.</jats:p>","abstract \n background: previous cytogenetic studies in polycythemia vera (pv) have included a relatively small number of patients (“n” ranging 10–64). in the current study (n=137), we describe cytogenetic findings at presentation and examine their relationship to clinical and laboratory features, including bone marrow jak2v617f allele burden. \n methods: the study consisted of a consecutive group of patients with pv who fulfilled the world health organization (who) diagnostic criteria and in whom bone marrow biopsy and cytogenetic studies were performed at diagnosis. \n results i: cytogenetic details at diagnosis: a total of 137 patients (median age, 64 years; 49% females) were studied at diagnosis and had adequate metaphases for interpretation. cytogenetics were normal in 117 patients (85%) and displayed either a sole -y abnormality in 5 patients (7% of the male patients), and other chromosomal abnormalities in 15 (11%). the latter included trisomy 8 in five patients, trisomy 9 in three patients, two patients each with del(13q), del(20q), and abnormalities of chromosome 1, and one patient each with del(3)(p13p21), dup(13)(q12q14), and del(11)(q21). at follow-up: repeat cytogenetic studies while still in the chronic phase of the disease were performed in 19 patients at a median of 60 months (range, 8–198) from diagnosis. of these, 4 had aquired new cytogenetic clones including 3 with normal cytogenetics at time of initial pv diagnosis. the new abnormalities included del(20q), del(5q), del(1p), chromosome 1 abnormality, and inv(3)(q21q26.2). at time of disease transformation: leukemic transformation was documented in 3 patients of whom cytogenetic information at the time was available in 2 patients; both patients had normal results at time of initial pv diagnosis and complex cytogenetic abnormalities at time of leukemic transformation. in contrast, among 6 patients with available cytogenetic information at time of fibrotic transformation, the results were unchanged from those obtained at time of diagnosis in 5 patients. ii) correlation between cytogenetics at diagnosis and jak2v617f allele burden: allele-specific, quantitative pcr analysis for jak2v617f was performed in 71 patients using genomic dna from archived bone marrow obtained at the time of the initial cytogenetic studies. jak2v617f mutation was detected in 64 of the 71 (90%) patients; median mutant allele burden was 16% (range 3–80%) without significant difference among the different cytogenetic groups: normal vs. –y vs. other cytogenetic abnormalities (p=0.72). iii) clinical correlates and prognostic relevance of cytogenetic findings at diagnosis: among several parameters studied for significant correlations with cytogenetic findings at diagnosis, an association was evident only for age (p=0.02); all –y abnormalities (n=5) as well as 13 of the 15 (87%) other cytogenetic abnormalities occurred in patients ≥ 60 years of age. stated another way, the incidence of abnormal cytogenetics (other than -y) was 4% for patients younger than age 60 years and 15% otherwise. the presence of abnormal cytogenetics at diagnosis had no significant impact on either overall or leukemia-free survival. \n conclusions: abnormal cytogenetic findings at diagnosis are infrequent in pv, especially in patients below age 60 years. furthermore, their clinical relevance is limited and there is not significant correlation with bone marrow jak2v617f allele burden."
http://orkg.org/orkg/resource/R32984,"The importance of diagnostic cytogenetics on outcome in AML: analysis of 1,612 patients entered into the MRC AML 10 trial",10.1182/blood.v92.7.2322,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Cytogenetics is considered one of the most valuable prognostic determinants in acute myeloid leukemia (AML). However, many studies on which this assertion is based were limited by relatively small sample sizes or varying treatment approach, leading to conflicting data regarding the prognostic implications of specific cytogenetic abnormalities. The Medical Research Council (MRC) AML 10 trial, which included children and adults up to 55 years of age, not only affords the opportunity to determine the independent prognostic significance of pretreatment cytogenetics in the context of large patient groups receiving comparable therapy, but also to address their impact on the outcome of subsequent transplantation procedures performed in first complete remission (CR). On the basis of response to induction treatment, relapse risk, and overall survival, three prognostic groups could be defined by cytogenetic abnormalities detected at presentation in comparison with the outcome of patients with normal karyotype. AML associated with t(8;21), t(15;17) or inv(16) predicted a relatively favorable outcome. Whereas in patients lacking these favorable changes, the presence of a complex karyotype, −5, del(5q), −7, or abnormalities of 3q defined a group with relatively poor prognosis. The remaining group of patients including those with 11q23 abnormalities, +8, +21, +22, del(9q), del(7q) or other miscellaneous structural or numerical defects not encompassed by the favorable or adverse risk groups were found to have an intermediate prognosis. The presence of additional cytogenetic abnormalities did not modify the outcome of patients with favorable cytogenetics. Subgroup analysis demonstrated that the three cytogenetically defined prognostic groups retained their predictive value in the context of secondary as well as de novo AML, within the pediatric age group and furthermore were found to be a key determinant of outcome from autologous or allogeneic bone marrow transplantation (BMT) in first CR. This study highlights the importance of diagnostic cytogenetics as an independent prognostic factor in AML, providing the framework for a stratified treatment approach of this disease, which has been adopted in the current MRC AML 12 trial.</jats:p>","abstract \n cytogenetics is considered one of the most valuable prognostic determinants in acute myeloid leukemia (aml). however, many studies on which this assertion is based were limited by relatively small sample sizes or varying treatment approach, leading to conflicting data regarding the prognostic implications of specific cytogenetic abnormalities. the medical research council (mrc) aml 10 trial, which included children and adults up to 55 years of age, not only affords the opportunity to determine the independent prognostic significance of pretreatment cytogenetics in the context of large patient groups receiving comparable therapy, but also to address their impact on the outcome of subsequent transplantation procedures performed in first complete remission (cr). on the basis of response to induction treatment, relapse risk, and overall survival, three prognostic groups could be defined by cytogenetic abnormalities detected at presentation in comparison with the outcome of patients with normal karyotype. aml associated with t(8;21), t(15;17) or inv(16) predicted a relatively favorable outcome. whereas in patients lacking these favorable changes, the presence of a complex karyotype, −5, del(5q), −7, or abnormalities of 3q defined a group with relatively poor prognosis. the remaining group of patients including those with 11q23 abnormalities, +8, +21, +22, del(9q), del(7q) or other miscellaneous structural or numerical defects not encompassed by the favorable or adverse risk groups were found to have an intermediate prognosis. the presence of additional cytogenetic abnormalities did not modify the outcome of patients with favorable cytogenetics. subgroup analysis demonstrated that the three cytogenetically defined prognostic groups retained their predictive value in the context of secondary as well as de novo aml, within the pediatric age group and furthermore were found to be a key determinant of outcome from autologous or allogeneic bone marrow transplantation (bmt) in first cr. this study highlights the importance of diagnostic cytogenetics as an independent prognostic factor in aml, providing the framework for a stratified treatment approach of this disease, which has been adopted in the current mrc aml 12 trial."
http://orkg.org/orkg/resource/R32987,New insights into the prognostic impact of the karyotype in MDS and correlation with subtypes: evidence from a core dataset of 2124 patients,10.1182/blood-2007-03-082404,crossref,"<jats:p>We have generated a large, unique database that includes morphologic, clinical, cytogenetic, and follow-up data from 2124 patients with myelodysplastic syndromes (MDSs) at 4 institutions in Austria and 4 in Germany. Cytogenetic analyses were successfully performed in 2072 (97.6%) patients, revealing clonal abnormalities in 1084 (52.3%) patients. Numeric and structural chromosomal abnormalities were documented for each patient and subdivided further according to the number of additional abnormalities. Thus, 684 different cytogenetic categories were identified. The impact of the karyotype on the natural course of the disease was studied in 1286 patients treated with supportive care only. Median survival was 53.4 months for patients with normal karyotypes (n = 612) and 8.7 months for those with complex anomalies (n = 166). A total of 13 rare abnormalities were identified with good (+1/+1q, t(1q), t(7q), del(9q), del(12p), chromosome 15 anomalies, t(17q), monosomy 21, trisomy 21, and −X), intermediate (del(11q), chromosome 19 anomalies), or poor (t(5q)) prognostic impact, respectively. The prognostic relevance of additional abnormalities varied considerably depending on the chromosomes affected. For all World Health Organization (WHO) and French-American-British (FAB) classification system subtypes, the karyotype provided additional prognostic information. Our analyses offer new insights into the prognostic significance of rare chromosomal abnormalities and specific karyotypic combinations in MDS.</jats:p>","we have generated a large, unique database that includes morphologic, clinical, cytogenetic, and follow-up data from 2124 patients with myelodysplastic syndromes (mdss) at 4 institutions in austria and 4 in germany. cytogenetic analyses were successfully performed in 2072 (97.6%) patients, revealing clonal abnormalities in 1084 (52.3%) patients. numeric and structural chromosomal abnormalities were documented for each patient and subdivided further according to the number of additional abnormalities. thus, 684 different cytogenetic categories were identified. the impact of the karyotype on the natural course of the disease was studied in 1286 patients treated with supportive care only. median survival was 53.4 months for patients with normal karyotypes (n = 612) and 8.7 months for those with complex anomalies (n = 166). a total of 13 rare abnormalities were identified with good (+1/+1q, t(1q), t(7q), del(9q), del(12p), chromosome 15 anomalies, t(17q), monosomy 21, trisomy 21, and −x), intermediate (del(11q), chromosome 19 anomalies), or poor (t(5q)) prognostic impact, respectively. the prognostic relevance of additional abnormalities varied considerably depending on the chromosomes affected. for all world health organization (who) and french-american-british (fab) classification system subtypes, the karyotype provided additional prognostic information. our analyses offer new insights into the prognostic significance of rare chromosomal abnormalities and specific karyotypic combinations in mds."
http://orkg.org/orkg/resource/R32990,Chromosomal abnormalities in Philadelphia chromosome negative metaphases appearing during imatinib mesylate therapy in patients with newly diagnosed chronic myeloid leukemia in chronic phase,10.1182/blood-2007-01-070045,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>The development of chromosomal abnormalities (CAs) in the Philadelphia chromosome (Ph)–negative metaphases during imatinib (IM) therapy in patients with newly diagnosed chronic myecloid leukemia (CML) has been reported only anecdotally. We assessed the frequency and significance of this phenomenon among 258 patients with newly diagnosed CML in chronic phase receiving IM. After a median follow-up of 37 months, 21 (9%) patients developed 23 CAs in Ph-negative cells; excluding −Y, this incidence was 5%. Sixteen (70%) of all CAs were observed in 2 or more metaphases. The median time from start of IM to the appearance of CAs was 18 months. The most common CAs were −Y and + 8 in 9 and 3 patients, respectively. CAs were less frequent in young patients (P = .02) and those treated with high-dose IM (P = .03). In all but 3 patients, CAs were transient and disappeared after a median of 5 months. One patient developed acute myeloid leukemia (associated with − 7). At last follow-up, 3 patients died from transplantation-related complications, myocardial infarction, and progressive disease and 2 lost cytogenetic response. CAs occur in Ph-negative cells in a small percentage of patients with newly diagnosed CML treated with IM. In rare instances, these could reflect the emergence of a new malignant clone.</jats:p>","abstract \n the development of chromosomal abnormalities (cas) in the philadelphia chromosome (ph)–negative metaphases during imatinib (im) therapy in patients with newly diagnosed chronic myecloid leukemia (cml) has been reported only anecdotally. we assessed the frequency and significance of this phenomenon among 258 patients with newly diagnosed cml in chronic phase receiving im. after a median follow-up of 37 months, 21 (9%) patients developed 23 cas in ph-negative cells; excluding −y, this incidence was 5%. sixteen (70%) of all cas were observed in 2 or more metaphases. the median time from start of im to the appearance of cas was 18 months. the most common cas were −y and + 8 in 9 and 3 patients, respectively. cas were less frequent in young patients (p = .02) and those treated with high-dose im (p = .03). in all but 3 patients, cas were transient and disappeared after a median of 5 months. one patient developed acute myeloid leukemia (associated with − 7). at last follow-up, 3 patients died from transplantation-related complications, myocardial infarction, and progressive disease and 2 lost cytogenetic response. cas occur in ph-negative cells in a small percentage of patients with newly diagnosed cml treated with im. in rare instances, these could reflect the emergence of a new malignant clone."
http://orkg.org/orkg/resource/R32995,"Chromosomal abnormalities in untreated patients with non-Hodgkin’s lymphoma: associations with histology, clinical characteristics, and treatment outcome. The Nebraska Lymphoma Study Group",10.1182/blood.v75.9.1841.1841,crossref,"""<jats:title>Abstract</jats:title>\n               <jats:p>We describe the chromosomal abnormalities found in 104 previously untreated patients with non-Hodgkin's lymphoma (NHL) and the correlations of these abnormalities with disease characteristics. The cytogenetic method used was a 24- to 48-hour culture, followed by G- banding. Several significant associations were discovered. A trisomy 3 was correlated with high-grade NHL. In the patients with an immunoblastic NHL, an abnormal chromosome no. 3 or 6 was found significantly more frequently. As previously described, a t(14;18) was significantly correlated with a follicular growth pattern. Abnormalities on chromosome no. 17 were correlated with a diffuse histology and a shorter survival. A shorter survival was also correlated with a +5, +6, +18, all abnormalities on chromosome no. 5, or involvement of breakpoint 14q11–12. In a multivariate analysis, these chromosomal abnormalities appeared to be independent prognostic factors and correlated with survival more strongly than any traditional prognostic variable. Patients with a t(11;14)(q13;q32) had an elevated lactate dehydrogenase (LDH). Skin infiltration was correlated with abnormalities on 2p. Abnormalities involving breakpoints 6q11–16 were correlated with B symptoms. Patients with abnormalities involving breakpoints 3q21–25 and 13q21–24 had more frequent bulky disease. The correlations of certain clinical findings with specific chromosomal abnormalities might help unveil the pathogenetic mechanisms of NHL and tailor treatment regimens.</jats:p>""",""" abstract \n we describe the chromosomal abnormalities found in 104 previously untreated patients with non-hodgkin's lymphoma (nhl) and the correlations of these abnormalities with disease characteristics. the cytogenetic method used was a 24- to 48-hour culture, followed by g- banding. several significant associations were discovered. a trisomy 3 was correlated with high-grade nhl. in the patients with an immunoblastic nhl, an abnormal chromosome no. 3 or 6 was found significantly more frequently. as previously described, a t(14;18) was significantly correlated with a follicular growth pattern. abnormalities on chromosome no. 17 were correlated with a diffuse histology and a shorter survival. a shorter survival was also correlated with a +5, +6, +18, all abnormalities on chromosome no. 5, or involvement of breakpoint 14q11–12. in a multivariate analysis, these chromosomal abnormalities appeared to be independent prognostic factors and correlated with survival more strongly than any traditional prognostic variable. patients with a t(11;14)(q13;q32) had an elevated lactate dehydrogenase (ldh). skin infiltration was correlated with abnormalities on 2p. abnormalities involving breakpoints 6q11–16 were correlated with b symptoms. patients with abnormalities involving breakpoints 3q21–25 and 13q21–24 had more frequent bulky disease. the correlations of certain clinical findings with specific chromosomal abnormalities might help unveil the pathogenetic mechanisms of nhl and tailor treatment regimens. """
http://orkg.org/orkg/resource/R32998,Cytogenetic studies in untreated Hodgkin's disease,10.1182/blood.V77.6.1298.1298,crossref,"""<jats:title>Abstract</jats:title>\n               <jats:p>Very little data have been published on cytogenetic abnormalities in Hodgkin's disease (HD) and their correlation with clinicopathologic features are scanty. We have performed chromosomal analysis of lymph nodes from 60 previously untreated HD patients and obtained analyzable metaphases in 49 patients (82%). Chromosomal abnormalities were found in 33 patients (55%) but only 31 karyotypes could be, at least partially, described. Twenty-nine cases showed numerical abnormalities that involved all chromosomes with the exception of chromosomes 13 and Y, which were gained less frequently and lost more frequently than other chromosomes. Structural abnormalities were found in 30 cases, involving all chromosomes except Y. Chromosomal regions 12p11–13, 13p11– 13, 3q26–28, 6q15–16, and 7q31–35 were rearranged in more than 20% of the analyzable cases. No correlation was found between cytogenetic findings and initial characteristics. When compared with diffuse B-cell lymphomas, defects in regions 2p25 (P less than .01), 12p11–13 (P less than .01), 13p11–13 (P less than .01), 14p11 (P less than .01), 15p11– 13 (P less than .02), and 20q12–13 (P less than .05) were more frequent in HD. When compared with T-cell lymphomas, only defects in regions 12p12–13 (P less than .01) and 13p11–13 (P less than .01) were more frequent in HD. Failure to obtain analyzable metaphases was correlated with stage IV of the disease (P less than .05) and with a poor survival (P less than .01), but cytogenetic results showed no other correlation with clinical outcome. We conclude that molecular studies in HD should be focused on the short arms of chromosomes 12 and 13. Determination of the clinical significance of cytogenetic findings will require a larger number of patients and a longer follow-up period.</jats:p>""",""" abstract \n very little data have been published on cytogenetic abnormalities in hodgkin's disease (hd) and their correlation with clinicopathologic features are scanty. we have performed chromosomal analysis of lymph nodes from 60 previously untreated hd patients and obtained analyzable metaphases in 49 patients (82%). chromosomal abnormalities were found in 33 patients (55%) but only 31 karyotypes could be, at least partially, described. twenty-nine cases showed numerical abnormalities that involved all chromosomes with the exception of chromosomes 13 and y, which were gained less frequently and lost more frequently than other chromosomes. structural abnormalities were found in 30 cases, involving all chromosomes except y. chromosomal regions 12p11–13, 13p11– 13, 3q26–28, 6q15–16, and 7q31–35 were rearranged in more than 20% of the analyzable cases. no correlation was found between cytogenetic findings and initial characteristics. when compared with diffuse b-cell lymphomas, defects in regions 2p25 (p less than .01), 12p11–13 (p less than .01), 13p11–13 (p less than .01), 14p11 (p less than .01), 15p11– 13 (p less than .02), and 20q12–13 (p less than .05) were more frequent in hd. when compared with t-cell lymphomas, only defects in regions 12p12–13 (p less than .01) and 13p11–13 (p less than .01) were more frequent in hd. failure to obtain analyzable metaphases was correlated with stage iv of the disease (p less than .05) and with a poor survival (p less than .01), but cytogenetic results showed no other correlation with clinical outcome. we conclude that molecular studies in hd should be focused on the short arms of chromosomes 12 and 13. determination of the clinical significance of cytogenetic findings will require a larger number of patients and a longer follow-up period. """
http://orkg.org/orkg/resource/R33001,Prognos- tic and biologic significance of chromosomal imbalances assessed by comparative genomic hybridization in multiple myeloma,10.1182/blood-2004-04-1319,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Cytogenetic abnormalities, evaluated either by karyotype or by fluorescence in situ hybridization (FISH), are considered the most important prognostic factor in multiple myeloma (MM). However, there is no information about the prognostic impact of genomic changes detected by comparative genomic hybridization (CGH). We have analyzed the frequency and prognostic impact of genetic changes as detected by CGH and evaluated the relationship between these chromosomal imbalances and IGH translocation, analyzed by FISH, in 74 patients with newly diagnosed MM. Genomic changes were identified in 51 (69%) of the 74 MM patients. The most recurrent abnormalities among the cases with genomic changes were gains on chromosome regions 1q (45%), 5q (24%), 9q (24%), 11q (22%), 15q (22%), 3q (16%), and 7q (14%), while losses mainly involved chromosomes 13 (39%), 16q (18%), 6q (10%), and 8p (10%). Remarkably, the 6 patients with gains on 11q had IGH translocations. Multivariate analysis selected chromosomal losses, 11q gains, age, and type of treatment (conventional chemotherapy vs autologous transplantation) as independent parameters for predicting survival. Genomic losses retained the prognostic value irrespective of treatment approach. According to these results, losses of chromosomal material evaluated by CGH represent a powerful prognostic factor in MM patients. (Blood. 2004;104:2661-2666)</jats:p>","abstract \n cytogenetic abnormalities, evaluated either by karyotype or by fluorescence in situ hybridization (fish), are considered the most important prognostic factor in multiple myeloma (mm). however, there is no information about the prognostic impact of genomic changes detected by comparative genomic hybridization (cgh). we have analyzed the frequency and prognostic impact of genetic changes as detected by cgh and evaluated the relationship between these chromosomal imbalances and igh translocation, analyzed by fish, in 74 patients with newly diagnosed mm. genomic changes were identified in 51 (69%) of the 74 mm patients. the most recurrent abnormalities among the cases with genomic changes were gains on chromosome regions 1q (45%), 5q (24%), 9q (24%), 11q (22%), 15q (22%), 3q (16%), and 7q (14%), while losses mainly involved chromosomes 13 (39%), 16q (18%), 6q (10%), and 8p (10%). remarkably, the 6 patients with gains on 11q had igh translocations. multivariate analysis selected chromosomal losses, 11q gains, age, and type of treatment (conventional chemotherapy vs autologous transplantation) as independent parameters for predicting survival. genomic losses retained the prognostic value irrespective of treatment approach. according to these results, losses of chromosomal material evaluated by cgh represent a powerful prognostic factor in mm patients. (blood. 2004;104:2661-2666)"
http://orkg.org/orkg/resource/R33031,A prospective long-term cytogenetic study in polycythemia vera in relation to treatment and clinical course,10.1182/blood.v72.2.386.386,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>This paper reports the results of cytogenetic studies in a consecutive series of 64 patients with polycythemia vera, 57 of whom could be followed prospectively. The median length of the cytogenetic observation time was 93 months (range, 24 to 224 months) after diagnosis. Clonal chromosome abnormalities were observed initially in 11 patients (17%) and later during the course of the disease in another 20 patients. An abnormal karyotype was found in 71% to 80% of the patients who were examined after the development of myeloid metaplasia, myelofibrosis, or leukemia. Patients treated with myelosuppressive agents showed a significantly greater risk of chromosome abnormalities developing than did patients who had been phlebotomized. Acute leukemia developed in eight patients, all of whom had been treated with myelosuppressive agents. A chromosome abnormality preceded the leukemia in only two of the patients. The initial presence of an abnormal karyotype did not predict a greater risk of development of leukemia. No consistent relationship was demonstrated between the occurrence of chromosome abnormalities and the development of myeloid metaplasia and/or myelofibrosis, which was observed in 42% of the patients. The chromosome abnormalities followed a nonrandom pattern, and those most frequently observed were trisomies for 1 q, 8, 9, or 9p and deletion of 20q. Deletions seem to be common and were found in 14 patients.</jats:p>","abstract \n this paper reports the results of cytogenetic studies in a consecutive series of 64 patients with polycythemia vera, 57 of whom could be followed prospectively. the median length of the cytogenetic observation time was 93 months (range, 24 to 224 months) after diagnosis. clonal chromosome abnormalities were observed initially in 11 patients (17%) and later during the course of the disease in another 20 patients. an abnormal karyotype was found in 71% to 80% of the patients who were examined after the development of myeloid metaplasia, myelofibrosis, or leukemia. patients treated with myelosuppressive agents showed a significantly greater risk of chromosome abnormalities developing than did patients who had been phlebotomized. acute leukemia developed in eight patients, all of whom had been treated with myelosuppressive agents. a chromosome abnormality preceded the leukemia in only two of the patients. the initial presence of an abnormal karyotype did not predict a greater risk of development of leukemia. no consistent relationship was demonstrated between the occurrence of chromosome abnormalities and the development of myeloid metaplasia and/or myelofibrosis, which was observed in 42% of the patients. the chromosome abnormalities followed a nonrandom pattern, and those most frequently observed were trisomies for 1 q, 8, 9, or 9p and deletion of 20q. deletions seem to be common and were found in 14 patients."
http://orkg.org/orkg/resource/R33033,Cytogenetic studies and their prognostic significance in agnogenic myeloid metaplasia: a report on 47 cases,10.1182/blood.v72.3.855.855,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Cytogenetic analysis was performed in 47 newly diagnosed patients with agnogenic myeloid metaplasia (AMM); 32 had a normal karyotype (68%, group I), whereas 15 had clonal abnormalities (32%, group II). The most frequent abnormal findings were a 20q- deletion in six cases (either alone or within complex anomalies), interstitial 13q- deletion in three cases (and monosomy 13 in one case), and acquired trisomy 21 or 21p+ in three cases. Four cases exhibited complex aberrations involving several chromosomes, sometimes with a mosaicism. In two patients with an initial abnormal karyotype, further cytogenetic analysis during the disease course showed the appearance of additional clonal anomalies, and particularly of a probable Philadelphia (Ph1) variant in one case. Treatment was essentially supportive. Survival was significantly shorter in group II (median, 30 months) compared with group I (median, not reached at 6 years; P = .015). In univariate analysis, other parameters significantly associated with a poor prognosis (P less than .05) were higher age, anemia, and increased percentage of circulating blasts. However, in a multivariate analysis, only cytogenetic abnormalities and age retained their independent prognostic value.</jats:p>","abstract \n cytogenetic analysis was performed in 47 newly diagnosed patients with agnogenic myeloid metaplasia (amm); 32 had a normal karyotype (68%, group i), whereas 15 had clonal abnormalities (32%, group ii). the most frequent abnormal findings were a 20q- deletion in six cases (either alone or within complex anomalies), interstitial 13q- deletion in three cases (and monosomy 13 in one case), and acquired trisomy 21 or 21p+ in three cases. four cases exhibited complex aberrations involving several chromosomes, sometimes with a mosaicism. in two patients with an initial abnormal karyotype, further cytogenetic analysis during the disease course showed the appearance of additional clonal anomalies, and particularly of a probable philadelphia (ph1) variant in one case. treatment was essentially supportive. survival was significantly shorter in group ii (median, 30 months) compared with group i (median, not reached at 6 years; p = .015). in univariate analysis, other parameters significantly associated with a poor prognosis (p less than .05) were higher age, anemia, and increased percentage of circulating blasts. however, in a multivariate analysis, only cytogenetic abnormalities and age retained their independent prognostic value."
http://orkg.org/orkg/resource/R33041,Prognostic factors in agnogenic myeloid metaplasia: a report on 195 cases with a new scoring system,10.1182/blood.v88.3.1013.bloodjournal8831013,crossref,"<jats:p>We studied the survival of 195 patients with agnogenic myeloid metaplasia (AMM) diagnosed between 1962 and 1992 in an attempt to stratify patients into risk groups. Median survival was 42 months. Adverse prognostic factors for survival were age &gt; 60 years, hepatomegaly, weight loss, low hemoglobin level (Hb), low or very high leukocyte count (WBC), high percentage of circulating blasts, male sex, and low platelet count. A new scoring system based on two adverse prognostic factors, namely Hb &lt; 10 g/dL and WBC &lt; 4 or &gt; 30 x 10(3)/L, was able to separate patients in three groups with low (0 factor), intermediate (1 factor), and high (2 factors) risks, associated with a median survival of 93, 26, and 13 months, respectively. An abnormal karyotype (32 cases of 94 tested patients) was associated with a short survival, especially in the low-risk group (median survival of 50 v 112 months in patients with normal karyotype). The prognostic factors for acute conversion were WBC &gt; 30 x 10(3)/L and abnormal karyotype. Thus, hemoglobin level and leukocyte count provide a simple prognostic model for survival in AMM, and the adverse prognostic value of abnormal karyotype may be related to a higher rate of acute conversion.</jats:p>","we studied the survival of 195 patients with agnogenic myeloid metaplasia (amm) diagnosed between 1962 and 1992 in an attempt to stratify patients into risk groups. median survival was 42 months. adverse prognostic factors for survival were age &gt; 60 years, hepatomegaly, weight loss, low hemoglobin level (hb), low or very high leukocyte count (wbc), high percentage of circulating blasts, male sex, and low platelet count. a new scoring system based on two adverse prognostic factors, namely hb &lt; 10 g/dl and wbc &lt; 4 or &gt; 30 x 10(3)/l, was able to separate patients in three groups with low (0 factor), intermediate (1 factor), and high (2 factors) risks, associated with a median survival of 93, 26, and 13 months, respectively. an abnormal karyotype (32 cases of 94 tested patients) was associated with a short survival, especially in the low-risk group (median survival of 50 v 112 months in patients with normal karyotype). the prognostic factors for acute conversion were wbc &gt; 30 x 10(3)/l and abnormal karyotype. thus, hemoglobin level and leukocyte count provide a simple prognostic model for survival in amm, and the adverse prognostic value of abnormal karyotype may be related to a higher rate of acute conversion."
http://orkg.org/orkg/resource/R33064,Acute myeloid leukemia and myelodysplastic syndromes following essential thrombocythemia treated with hydroxyurea: high proportion of cases with 17p deletion,10.1182/blood.v91.2.616.616_616_622,crossref,"<jats:p>Treatment with alkylating agents or radiophosphorous (32P) has been shown to carry a certain leukemogenic risk in myeloproliferative disorders (MPDs), including essential thrombocytemia (ET). The leukemogenic risk associated to treatment with hydroxyurea in ET, on the other hand, is generally considered to be relatively low. Between 1970 and 1991, we diagnosed ET in 357 patients, who were monitored until 1996. One or several therapeutic agents had been admistered to 326 patients, including hydroxyurea (HU) in 251 (as only treatment in 201), pipobroman in 43, busulfan in 41, and32P in 40. With a median follow-up duration of 98 months, 17 patients (4.5%) had progressed to acute myeloid leukemia (AML; six cases) or myelodysplastic syndrome (MDS; 11 cases). Fourteen of these patients had received HU, as sole treatment in seven cases, and preceded or followed by other treatment in seven cases, mainly pipobroman (five cases). The remaining three leukemic progressions occurred in patients treated with 32P (two cases) and busulfan (one case). The incidence of AML and MDS after treatment, using 32P alone and 32P with other agents, busulfan alone and with other agents, HU alone and with others agents, and pipobroman alone and with other agents was 7% and 9%, 3% and 17%, 3.5% and 14%, and 0% and 16%, respectively. Thirteen of 17 patients who progressed to AML or MDS had successful cytogenetic analysis. Seven of them had rearrangements of chromosome 17 (unbalanced translocation, partial or complete deletion, isochromosome 17q) that resulted in 17p deletion. They also had a typical form of dysgranulopoiesis combining pseudo Pelger Hüet hypolobulation and vacuoles in neutrophils, and p53 mutation, as previously described in AML and MDS with 17p deletion. Those seven patients had all received HU, as the only therapeutic agent in three, and followed by pipobroman in three. The three patients who had received no HU and progressed to AML or MDS had no 17p deletion. A review of the literature found cytogenetic analysis in 35 cases of AML and MDS occurring after ET, 11 of whom had been treated with HU alone. Five of 35 patients had rearrangements that resulted in 17p deletion. Four of them had been treated with HU alone. These results show that treatment with HU alone is associated with a leukemic risk of approximately 3.5%. A high proportion of AML and MDS occurring in ET treated with HU (alone or possibly followed by pipobroman) have morphologic, cytogenetic, and molecular characteristics of the 17p− syndrome. These findings suggest that widespread and prolonged use of HU in ET may have to be reconsidered in some situations, such as asymptomatic ET.</jats:p>","treatment with alkylating agents or radiophosphorous (32p) has been shown to carry a certain leukemogenic risk in myeloproliferative disorders (mpds), including essential thrombocytemia (et). the leukemogenic risk associated to treatment with hydroxyurea in et, on the other hand, is generally considered to be relatively low. between 1970 and 1991, we diagnosed et in 357 patients, who were monitored until 1996. one or several therapeutic agents had been admistered to 326 patients, including hydroxyurea (hu) in 251 (as only treatment in 201), pipobroman in 43, busulfan in 41, and32p in 40. with a median follow-up duration of 98 months, 17 patients (4.5%) had progressed to acute myeloid leukemia (aml; six cases) or myelodysplastic syndrome (mds; 11 cases). fourteen of these patients had received hu, as sole treatment in seven cases, and preceded or followed by other treatment in seven cases, mainly pipobroman (five cases). the remaining three leukemic progressions occurred in patients treated with 32p (two cases) and busulfan (one case). the incidence of aml and mds after treatment, using 32p alone and 32p with other agents, busulfan alone and with other agents, hu alone and with others agents, and pipobroman alone and with other agents was 7% and 9%, 3% and 17%, 3.5% and 14%, and 0% and 16%, respectively. thirteen of 17 patients who progressed to aml or mds had successful cytogenetic analysis. seven of them had rearrangements of chromosome 17 (unbalanced translocation, partial or complete deletion, isochromosome 17q) that resulted in 17p deletion. they also had a typical form of dysgranulopoiesis combining pseudo pelger hüet hypolobulation and vacuoles in neutrophils, and p53 mutation, as previously described in aml and mds with 17p deletion. those seven patients had all received hu, as the only therapeutic agent in three, and followed by pipobroman in three. the three patients who had received no hu and progressed to aml or mds had no 17p deletion. a review of the literature found cytogenetic analysis in 35 cases of aml and mds occurring after et, 11 of whom had been treated with hu alone. five of 35 patients had rearrangements that resulted in 17p deletion. four of them had been treated with hu alone. these results show that treatment with hu alone is associated with a leukemic risk of approximately 3.5%. a high proportion of aml and mds occurring in et treated with hu (alone or possibly followed by pipobroman) have morphologic, cytogenetic, and molecular characteristics of the 17p− syndrome. these findings suggest that widespread and prolonged use of hu in et may have to be reconsidered in some situations, such as asymptomatic et."
http://orkg.org/orkg/resource/R33088,The role of cytogenetic abnormalities as a prognostic marker in primary myelofibrosis: applicability at the time of diagnosis and later during disease course,10.1182/blood-2008-09-178541,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Although cytogenetic abnormalities are important prognostic factors in myeloid malignancies, they are not included in current prognostic scores for primary myelofibrosis (PMF). To determine their relevance in PMF, we retrospectively examined the impact of cytogenetic abnormalities and karyotypic evolution on the outcome of 256 patients. Baseline cytogenetic status impacted significantly on survival: patients with favorable abnormalities (sole deletions in 13q or 20q, or trisomy 9 ± one other abnormality) had survivals similar to those with normal diploid karyotypes (median, 63 and 46 months, respectively), whereas patients with unfavorable abnormalities (rearrangement of chromosome 5 or 7, or ≥ 3 abnormalities) had a poor median survival of 15 months. Patients with abnormalities of chromosome 17 had a median survival of only 5 months. A model containing karyotypic abnormalities, hemoglobin, platelet count, and performance status effectively risk-stratified patients at initial evaluation. Among 73 patients assessable for clonal evolution during stable chronic phase, those who developed unfavorable or chromosome 17 abnormalities had median survivals of 18 and 9 months, respectively, suggesting the potential role of cytogenetics as a risk factor applicable at any time in the disease course. Dynamic prognostic significance of cytogenetic abnormalities in PMF should be further prospectively evaluated.</jats:p>","abstract \n although cytogenetic abnormalities are important prognostic factors in myeloid malignancies, they are not included in current prognostic scores for primary myelofibrosis (pmf). to determine their relevance in pmf, we retrospectively examined the impact of cytogenetic abnormalities and karyotypic evolution on the outcome of 256 patients. baseline cytogenetic status impacted significantly on survival: patients with favorable abnormalities (sole deletions in 13q or 20q, or trisomy 9 ± one other abnormality) had survivals similar to those with normal diploid karyotypes (median, 63 and 46 months, respectively), whereas patients with unfavorable abnormalities (rearrangement of chromosome 5 or 7, or ≥ 3 abnormalities) had a poor median survival of 15 months. patients with abnormalities of chromosome 17 had a median survival of only 5 months. a model containing karyotypic abnormalities, hemoglobin, platelet count, and performance status effectively risk-stratified patients at initial evaluation. among 73 patients assessable for clonal evolution during stable chronic phase, those who developed unfavorable or chromosome 17 abnormalities had median survivals of 18 and 9 months, respectively, suggesting the potential role of cytogenetics as a risk factor applicable at any time in the disease course. dynamic prognostic significance of cytogenetic abnormalities in pmf should be further prospectively evaluated."
http://orkg.org/orkg/resource/R33102,The integrated logistics management system: a framework and case study,10.1108/09600039510093249,crossref,"<jats:p>Presents a framework for distribution companies to establish and\nimprove their logistics systems continuously. Recently, much attention\nhas been given to automation in services, the use of new information\ntechnology and the integration of the supply chain. Discusses these\nareas, which have great potential to increase logistics productivity and\nprovide customers with high level service. The exploration of each area\nis enriched with Taiwanese logistics management practices and\nexperiences. Includes a case study of one prominent food processor and\nretailer in Taiwan in order to demonstrate the pragmatic operations of\nthe integrated logistics management system. Also, a survey of 45\nTaiwanese retailers was conducted to investigate the extent of logistics\nmanagement in Taiwan. Concludes by suggesting how distribution companies\ncan overcome noticeable logistics management barriers, build store\nautomation systems, and follow the key steps to logistics success.</jats:p>","presents a framework for distribution companies to establish and\nimprove their logistics systems continuously. recently, much attention\nhas been given to automation in services, the use of new information\ntechnology and the integration of the supply chain. discusses these\nareas, which have great potential to increase logistics productivity and\nprovide customers with high level service. the exploration of each area\nis enriched with taiwanese logistics management practices and\nexperiences. includes a case study of one prominent food processor and\nretailer in taiwan in order to demonstrate the pragmatic operations of\nthe integrated logistics management system. also, a survey of 45\ntaiwanese retailers was conducted to investigate the extent of logistics\nmanagement in taiwan. concludes by suggesting how distribution companies\ncan overcome noticeable logistics management barriers, build store\nautomation systems, and follow the key steps to logistics success."
http://orkg.org/orkg/resource/R33118,The elements of a successful logistics partnership,10.1108/09600039610115045,crossref,"<jats:p>Describes the elements of a successful logistics partnership. Looks at what can cause failure and questions whether the benefits of a logistics partnership are worth the effort required. Concludes that strategic alliances are increasingly becoming a matter of survival, not merely a matter of competitive advantage. Refers to the example of the long‐term relationship between Kimberly‐Clark Corporation and Interamerican group’s Tricor Warehousing, Inc.</jats:p>","describes the elements of a successful logistics partnership. looks at what can cause failure and questions whether the benefits of a logistics partnership are worth the effort required. concludes that strategic alliances are increasingly becoming a matter of survival, not merely a matter of competitive advantage. refers to the example of the long‐term relationship between kimberly‐clark corporation and interamerican group’s tricor warehousing, inc."
http://orkg.org/orkg/resource/R33127,Outsourcing of logistics functions: a literature survey,10.1108/09600039810221667,crossref,"<jats:p>Recent times have witnessed a heightened global interest in outsourcing of logistics functions. This is indicated by the volume of writings on the subject in various scholarly journals, trade publications and popular magazines. However, efforts to organize them in an integrated body of knowledge appear to be very limited. Keeping this in view, this paper makes an attempt to develop a comprehensive literature on outsourcing based on more than 100 published articles, papers and books on the subject.</jats:p>","recent times have witnessed a heightened global interest in outsourcing of logistics functions. this is indicated by the volume of writings on the subject in various scholarly journals, trade publications and popular magazines. however, efforts to organize them in an integrated body of knowledge appear to be very limited. keeping this in view, this paper makes an attempt to develop a comprehensive literature on outsourcing based on more than 100 published articles, papers and books on the subject."
http://orkg.org/orkg/resource/R33136,Success factors in the fresh produce supply chain: insights from the UK,10.1108/13598549910279567,crossref,"<jats:p>Presents recent evidence of supply chain developments in the UK fresh produce industry, based on interviews with chief executives from some of the country’s most successful suppliers. A number of success factors were evident, to varying degrees, in all of the companies interviewed. These included: continuous investment (despite increasingly tight margins), good staff (to drive the process of innovation and develop good trading relationships with key customers), volume growth (to fund the necessary investments and provide a degree of confidence in the future), improvement of measurement and control of costs (in the pursuit of further gains in efficiency), and innovation (not just the product offer but also the level of service and the way of doing business with key customers).</jats:p>","presents recent evidence of supply chain developments in the uk fresh produce industry, based on interviews with chief executives from some of the country’s most successful suppliers. a number of success factors were evident, to varying degrees, in all of the companies interviewed. these included: continuous investment (despite increasingly tight margins), good staff (to drive the process of innovation and develop good trading relationships with key customers), volume growth (to fund the necessary investments and provide a degree of confidence in the future), improvement of measurement and control of costs (in the pursuit of further gains in efficiency), and innovation (not just the product offer but also the level of service and the way of doing business with key customers)."
http://orkg.org/orkg/resource/R33163,Critical success factors in agile supply chain management ‐ An empirical study,10.1108/09600030110394923,crossref,"<jats:p>This paper analyses results from a survey of 962 Australian manufacturing companies in order to identify some of the factors critical for successful agile organizations in managing their supply chains. Analysis of the survey results provided some interesting insights into factors differentiating “more agile” organizations from “less agile” organizations. “More agile” companies from this study can be characterized as more customer focused, and applying a combination of “soft” and “hard” methodologies in order to meet changing customer requirements. They also see the involvement of suppliers in this process as being crucial to their ability to attain high levels of customer satisfaction. The “less agile” group, on the other hand, can be characterized as more internally focused with a bias toward internal operational outcomes. They saw no link between any of the independent variables and innovation, and appear to see technology as more closely linked to the promotion of these operational outcomes than to customer satisfaction. The role of suppliers for this group is to support productivity and process improvement rather than to promote customer satisfaction.</jats:p>","this paper analyses results from a survey of 962 australian manufacturing companies in order to identify some of the factors critical for successful agile organizations in managing their supply chains. analysis of the survey results provided some interesting insights into factors differentiating “more agile” organizations from “less agile” organizations. “more agile” companies from this study can be characterized as more customer focused, and applying a combination of “soft” and “hard” methodologies in order to meet changing customer requirements. they also see the involvement of suppliers in this process as being crucial to their ability to attain high levels of customer satisfaction. the “less agile” group, on the other hand, can be characterized as more internally focused with a bias toward internal operational outcomes. they saw no link between any of the independent variables and innovation, and appear to see technology as more closely linked to the promotion of these operational outcomes than to customer satisfaction. the role of suppliers for this group is to support productivity and process improvement rather than to promote customer satisfaction."
http://orkg.org/orkg/resource/R33171,The successful management of a small logistics company,10.1108/09600030310503352,crossref,"""<jats:p>In this paper, a case study conducted on a small third‐party logistics (3PL) company in Hong Kong is presented. This company is interesting in that it has been designated as the “king” of Hong Kong's 3PL (in‐bound) logistics companies. The company has been successful in its overall business performance and in satisfying customers. This company's strategic alliances with both clients and customers have helped to improve the utilization of its resources, such as warehouse space and transportation fleets. Also, the company is in the process of expanding its operations across greater China, with the objective of becoming a full‐pledged 3PL company. The analysis of this case focuses on the critical success factors (strategies and technologies) that have allowed a small company started only in 1996 to become so successful in its operations. Also, a framework has been provided for the company to develop its logistics operations as a full‐pledged 3PL company.</jats:p>""",""" in this paper, a case study conducted on a small third‐party logistics (3pl) company in hong kong is presented. this company is interesting in that it has been designated as the “king” of hong kong's 3pl (in‐bound) logistics companies. the company has been successful in its overall business performance and in satisfying customers. this company's strategic alliances with both clients and customers have helped to improve the utilization of its resources, such as warehouse space and transportation fleets. also, the company is in the process of expanding its operations across greater china, with the objective of becoming a full‐pledged 3pl company. the analysis of this case focuses on the critical success factors (strategies and technologies) that have allowed a small company started only in 1996 to become so successful in its operations. also, a framework has been provided for the company to develop its logistics operations as a full‐pledged 3pl company. """
http://orkg.org/orkg/resource/R33205,An Exploratory Study of the Success Factors for Extranet Adoption in E-Supply Chain,10.4018/jgim.2004010104,crossref,"<p>Extranet is an enabler/system that enriches the information service quality in e-supply chain. This paper uses factor analysis to determine four extranet success factors: system quality, information quality, service quality, and work performance quality. A critical analysis of areas that require improvement is also conducted.</p>","extranet is an enabler/system that enriches the information service quality in e-supply chain. this paper uses factor analysis to determine four extranet success factors: system quality, information quality, service quality, and work performance quality. a critical analysis of areas that require improvement is also conducted."
http://orkg.org/orkg/resource/R33223,Successful use of e‐procurement in supply chains,10.1108/13598540510589197,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>Electronic support of internal supply chains for direct or production goods has been a major element during the implementation of enterprise resource planning (ERP) systems that has taken place since the late 1980s. However, supply chains to indirect material suppliers were not usually included due to low transaction volumes, low product values and low strategic importance of these goods. Dedicated information systems for streamlining indirect goods supply chains have emerged since the late 1990s and subsequently have faced a broad diffusion in practice. The concept of these e‐procurement solutions has also been described broadly in the literature. However, studies on how companies use these e‐procurement solutions and what factors are critical to their implementation are only emerging. This research aims to explore the introduction of e‐procurement systems and their contribution to the management of indirect goods supply chain.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>Chooses a two‐part qualitative approach. First, summarizes the results of a benchmarking study that was conducted by a consortium of 12 multinational companies. During the benchmarking process 120 questionnaires were distributed, ten phone‐based interviews were conducted, and finally five successful practice companies were selected and analyzed in detail. Second, draws together the success factors identified in the benchmarking study and maps them against the successful practice companies.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>Although e‐procurement has substantially streamlined the procurement and coordination processes for indirect goods, many companies operate multiple e‐procurement solutions. For integrated procurement solutions, the paper recognizes the need of an overall procurement strategy and organization, an alignment of various e‐procurement solutions along the procurement process and the need for integrated system architectures. Companies also have to realize that a no standardized e‐procurement solutions exists and that important success factors are “non‐technical” in nature.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This paper presents a first step towards a systematic analysis of factors that may guide companies in the implementation of e‐procurement solutions. Besides providing a direct contribution to the project work in companies it may stimulate further research in e‐procurement success factors.</jats:p></jats:sec>","purpose electronic support of internal supply chains for direct or production goods has been a major element during the implementation of enterprise resource planning (erp) systems that has taken place since the late 1980s. however, supply chains to indirect material suppliers were not usually included due to low transaction volumes, low product values and low strategic importance of these goods. dedicated information systems for streamlining indirect goods supply chains have emerged since the late 1990s and subsequently have faced a broad diffusion in practice. the concept of these e‐procurement solutions has also been described broadly in the literature. however, studies on how companies use these e‐procurement solutions and what factors are critical to their implementation are only emerging. this research aims to explore the introduction of e‐procurement systems and their contribution to the management of indirect goods supply chain. design/methodology/approach chooses a two‐part qualitative approach. first, summarizes the results of a benchmarking study that was conducted by a consortium of 12 multinational companies. during the benchmarking process 120 questionnaires were distributed, ten phone‐based interviews were conducted, and finally five successful practice companies were selected and analyzed in detail. second, draws together the success factors identified in the benchmarking study and maps them against the successful practice companies. findings although e‐procurement has substantially streamlined the procurement and coordination processes for indirect goods, many companies operate multiple e‐procurement solutions. for integrated procurement solutions, the paper recognizes the need of an overall procurement strategy and organization, an alignment of various e‐procurement solutions along the procurement process and the need for integrated system architectures. companies also have to realize that a no standardized e‐procurement solutions exists and that important success factors are “non‐technical” in nature. originality/value this paper presents a first step towards a systematic analysis of factors that may guide companies in the implementation of e‐procurement solutions. besides providing a direct contribution to the project work in companies it may stimulate further research in e‐procurement success factors."
http://orkg.org/orkg/resource/R33237,Supply chain software implementations: getting it right,10.1108/13598540510612695,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>To highlight key success factors in supply chain projects.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>The paper presents insights from a number of supply chain projects in which IT has played an important part in the business solution.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>Successful supply chain projects have four things in common: the right leadership, the right focus, the right approach and effective communication of KPIs to all stakeholders engaged in the project.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>The focus of the paper is on supply chain projects with a significant IT component, but the key success factors identified are common to the majority of supply chain projects.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>Companies must not assume that investment in IT is, by itself, a solution to their supply chain solutions. A lack of leadership, focus and communication will invariably result in sub‐optimal outcomes which are all too frequently attributed to the complex nature of the project or the inflexibility of the software when in most cases the problems are internal to the businesses involved and the project management process.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This paper provides practical tips for improving the likelihood of getting the most out of IT‐based supply chain projects.</jats:p></jats:sec>","purpose to highlight key success factors in supply chain projects. design/methodology/approach the paper presents insights from a number of supply chain projects in which it has played an important part in the business solution. findings successful supply chain projects have four things in common: the right leadership, the right focus, the right approach and effective communication of kpis to all stakeholders engaged in the project. research limitations/implications the focus of the paper is on supply chain projects with a significant it component, but the key success factors identified are common to the majority of supply chain projects. practical implications companies must not assume that investment in it is, by itself, a solution to their supply chain solutions. a lack of leadership, focus and communication will invariably result in sub‐optimal outcomes which are all too frequently attributed to the complex nature of the project or the inflexibility of the software when in most cases the problems are internal to the businesses involved and the project management process. originality/value this paper provides practical tips for improving the likelihood of getting the most out of it‐based supply chain projects."
http://orkg.org/orkg/resource/R33245,Assessing supply chain management success factors: a case study,10.1108/13598540610652573,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this study is to examine important operational issues related to strategic success factors that are necessary when implementing SCM plans in an organization.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>A questionnaire was distributed to top and middle management within a large manufacturing firm, specializing in producing consumer and building products, to examine the importance and the extent to which the selected manufacturing company practiced the strategies based on these identified operational issues.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>Reducing cost of operations, improving inventory, lead times and customer satisfaction, increasing flexibility and cross‐functional communication, and remaining competitive appear to be the most important objectives to implement SCM strategies. The responses by the survey respondents indicate that not enough resources were allocated to implement and support SCM initiatives in their divisions. In addition, they perceived that resource allocation could be improved in the areas of better information systems, greater commitment, setting clear‐cut goals, increased training, more personnel, and aligning SCM initiatives with current priorities and resource commitments.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>The results will help to provide greater understanding of strategic and operational issues that support SCM framework and implementing SCM strategies to reduce supply chain‐wide costs and meeting customer service levels.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>The results will be useful for business managers to understand and implement SCM plans in terms of their importance and the company\'s culture.</jats:p></jats:sec>","purpose the purpose of this study is to examine important operational issues related to strategic success factors that are necessary when implementing scm plans in an organization. design/methodology/approach a questionnaire was distributed to top and middle management within a large manufacturing firm, specializing in producing consumer and building products, to examine the importance and the extent to which the selected manufacturing company practiced the strategies based on these identified operational issues. findings reducing cost of operations, improving inventory, lead times and customer satisfaction, increasing flexibility and cross‐functional communication, and remaining competitive appear to be the most important objectives to implement scm strategies. the responses by the survey respondents indicate that not enough resources were allocated to implement and support scm initiatives in their divisions. in addition, they perceived that resource allocation could be improved in the areas of better information systems, greater commitment, setting clear‐cut goals, increased training, more personnel, and aligning scm initiatives with current priorities and resource commitments. practical implications the results will help to provide greater understanding of strategic and operational issues that support scm framework and implementing scm strategies to reduce supply chain‐wide costs and meeting customer service levels. originality/value the results will be useful for business managers to understand and implement scm plans in terms of their importance and the company\'s culture."
http://orkg.org/orkg/resource/R33305,Supply chain management in SMEs: development of constructs and propositions,10.1108/13555850810844896,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to review the literature on supply chain management (SCM) practices in small and medium scale enterprises (SMEs) and outlines the key insights.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>The paper describes a literature‐based research that has sought understand the issues of SCM for SMEs. The methodology is based on critical review of 77 research papers from high‐quality, international refereed journals. Mainly, issues are explored under three categories – supply chain integration, strategy and planning and implementation. This has supported the development of key constructs and propositions.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The research outcomes are three fold. Firstly, paper summarizes the reported literature and classifies it based on their nature of work and contributions. Second, paper demonstrates the overall approach towards the development of constructs, research questions, and investigative questions leading to key proposition for the further research. Lastly, paper outlines the key findings and insights gained.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>Survival of SMEs will be determined first and foremost by their ability to provide/produce more, at less cost, in less time, with few “defects”. The key to this is effective SCM. The issue is much explored in the context of large enterprises but less attention is paid to SMEs. Paper aims to surface out some facts for the same. <jats:bold>Originality/value</jats:bold> The paper reports‐classifies the literature and development of construct and propositions.</jats:p></jats:sec>","purpose the purpose of this paper is to review the literature on supply chain management (scm) practices in small and medium scale enterprises (smes) and outlines the key insights. design/methodology/approach the paper describes a literature‐based research that has sought understand the issues of scm for smes. the methodology is based on critical review of 77 research papers from high‐quality, international refereed journals. mainly, issues are explored under three categories – supply chain integration, strategy and planning and implementation. this has supported the development of key constructs and propositions. findings the research outcomes are three fold. firstly, paper summarizes the reported literature and classifies it based on their nature of work and contributions. second, paper demonstrates the overall approach towards the development of constructs, research questions, and investigative questions leading to key proposition for the further research. lastly, paper outlines the key findings and insights gained. practical implications survival of smes will be determined first and foremost by their ability to provide/produce more, at less cost, in less time, with few “defects”. the key to this is effective scm. the issue is much explored in the context of large enterprises but less attention is paid to smes. paper aims to surface out some facts for the same. originality/value the paper reports‐classifies the literature and development of construct and propositions."
http://orkg.org/orkg/resource/R33328,Critical success factors in the context of humanitarian aid supply chains,10.1108/09600030910985811,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>Critical success factors (CSFs) have been widely used in the context of commercial supply chains. However, in the context of humanitarian aid (HA) this is a poorly addressed area and this paper therefore aims to set out the key areas for research.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>This paper is based on a conceptual discussion of CSFs as applied to the HA sector. A detailed literature review is undertaken to identify CSFs in a commercial context and to consider their applicability to the HA sector.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>CSFs have not previously been identified for the HA sector, an issue addressed in this paper.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>The main constraint on this paper is that CSFs have not been previously considered in the literature as applied to HA. The relevance of CSFs will therefore need to be tested in the HA environment and qualitative research is needed to inform further work.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>This paper informs the HA community of key areas of activity which have not been fully addressed and offers.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This paper contributes to the understanding of supply chain management in an HA context.</jats:p></jats:sec>","purpose critical success factors (csfs) have been widely used in the context of commercial supply chains. however, in the context of humanitarian aid (ha) this is a poorly addressed area and this paper therefore aims to set out the key areas for research. design/methodology/approach this paper is based on a conceptual discussion of csfs as applied to the ha sector. a detailed literature review is undertaken to identify csfs in a commercial context and to consider their applicability to the ha sector. findings csfs have not previously been identified for the ha sector, an issue addressed in this paper. research limitations/implications the main constraint on this paper is that csfs have not been previously considered in the literature as applied to ha. the relevance of csfs will therefore need to be tested in the ha environment and qualitative research is needed to inform further work. practical implications this paper informs the ha community of key areas of activity which have not been fully addressed and offers. originality/value this paper contributes to the understanding of supply chain management in an ha context."
http://orkg.org/orkg/resource/R33348,Critical success factors for B2B e‐commerce use within the UK NHS pharmaceutical supply chain,10.1108/01443570911000177,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to determine those factors perceived by users to influence the successful on‐going use of e‐commerce systems in business‐to‐business (B2B) buying and selling transactions through examination of the views of individuals acting in both purchasing and selling roles within the UK National Health Service (NHS) pharmaceutical supply chain.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>Literature from the fields of operations and supply chain management (SCM) and information systems (IS) is used to determine candidate factors that might influence the success of the use of e‐commerce. A questionnaire based on these is used for primary data collection in the UK NHS pharmaceutical supply chain. Factor analysis is used to analyse the data.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The paper yields five composite factors that are perceived by users to influence successful e‐commerce use. “System quality,” “information quality,” “management and use,” “world wide web – assurance and empathy,” and “trust” are proposed as potential critical success factors. Of these, all respondents ranked information quality, system quality, and trust as being of most importance, but differences in the rankings between purchasing and selling respondents are evident.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>The empirical study is limited to a single supply network, and although the findings seem intuitively to be of relevance to other sectors and supply contexts, there remains an opportunity to test this through further research. There is also an opportunity to extend the survey research, particularly into the wholesaler organisations that operate in the sector of study.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>The managerial implications that result from this research provide practical guidance to organisations in this sector on how to ensure that e‐commerce systems for B2B buying and selling are used successfully.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This paper furthers knowledge and understanding in the fields of operations management, IS, and SCM, by suggesting potential determinants of successful e‐commerce use in both buying and selling organisations within supply networks.</jats:p></jats:sec>","purpose the purpose of this paper is to determine those factors perceived by users to influence the successful on‐going use of e‐commerce systems in business‐to‐business (b2b) buying and selling transactions through examination of the views of individuals acting in both purchasing and selling roles within the uk national health service (nhs) pharmaceutical supply chain. design/methodology/approach literature from the fields of operations and supply chain management (scm) and information systems (is) is used to determine candidate factors that might influence the success of the use of e‐commerce. a questionnaire based on these is used for primary data collection in the uk nhs pharmaceutical supply chain. factor analysis is used to analyse the data. findings the paper yields five composite factors that are perceived by users to influence successful e‐commerce use. “system quality,” “information quality,” “management and use,” “world wide web – assurance and empathy,” and “trust” are proposed as potential critical success factors. of these, all respondents ranked information quality, system quality, and trust as being of most importance, but differences in the rankings between purchasing and selling respondents are evident. research limitations/implications the empirical study is limited to a single supply network, and although the findings seem intuitively to be of relevance to other sectors and supply contexts, there remains an opportunity to test this through further research. there is also an opportunity to extend the survey research, particularly into the wholesaler organisations that operate in the sector of study. practical implications the managerial implications that result from this research provide practical guidance to organisations in this sector on how to ensure that e‐commerce systems for b2b buying and selling are used successfully. originality/value this paper furthers knowledge and understanding in the fields of operations management, is, and scm, by suggesting potential determinants of successful e‐commerce use in both buying and selling organisations within supply networks."
http://orkg.org/orkg/resource/R33375,Critical factors for implementing green supply chain management practice,10.1108/01409171011050208,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to explore critical factors for implementing green supply chain management (GSCM) practice in the Taiwanese electrical and electronics industries relative to European Union directives.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>A tentative list of critical factors of GSCM was developed based on a thorough and detailed analysis of the pertinent literature. The survey questionnaire contained 25 items, developed based on the literature and interviews with three industry experts, specifically quality and product assurance representatives. A total of 300 questionnaires were mailed out, and 87 were returned, of which 84 were valid, representing a response rate of 28 percent. Using the data collected, the identified critical factors were performed via factor analysis to establish reliability and validity.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The results show that 20 critical factors were extracted into four dimensions, which denominated supplier management, product recycling, organization involvement and life cycle management.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>This study obtained 84 valid responses from the Taiwanese electrical and electronics industries, the limitation of the study is the insufficient sampling. Future researches need to be performed using a larger sample and studying more countries.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>The Taiwanese electrical and electronics industry plays a decisive role in the global information and communications technology (ICT) industry. Consequently, the validated instrument enables decision makers at ICT manufacturers to evaluate the perceptions of GSCM in their organizations. In addition, the critical factors of implementing GSCM practices validated in this work can help enterprises identify those areas of GSCM where acceptance and improvements will be made, and in prioritizing GSCM efforts.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This study presents an empirical investigation of GSCM practices, and fills a gap in the literature on the identification and establishment of critical factors for GSCM implementation in electrical and electronics industries.</jats:p></jats:sec>","purpose the purpose of this paper is to explore critical factors for implementing green supply chain management (gscm) practice in the taiwanese electrical and electronics industries relative to european union directives. design/methodology/approach a tentative list of critical factors of gscm was developed based on a thorough and detailed analysis of the pertinent literature. the survey questionnaire contained 25 items, developed based on the literature and interviews with three industry experts, specifically quality and product assurance representatives. a total of 300 questionnaires were mailed out, and 87 were returned, of which 84 were valid, representing a response rate of 28 percent. using the data collected, the identified critical factors were performed via factor analysis to establish reliability and validity. findings the results show that 20 critical factors were extracted into four dimensions, which denominated supplier management, product recycling, organization involvement and life cycle management. research limitations/implications this study obtained 84 valid responses from the taiwanese electrical and electronics industries, the limitation of the study is the insufficient sampling. future researches need to be performed using a larger sample and studying more countries. practical implications the taiwanese electrical and electronics industry plays a decisive role in the global information and communications technology (ict) industry. consequently, the validated instrument enables decision makers at ict manufacturers to evaluate the perceptions of gscm in their organizations. in addition, the critical factors of implementing gscm practices validated in this work can help enterprises identify those areas of gscm where acceptance and improvements will be made, and in prioritizing gscm efforts. originality/value this study presents an empirical investigation of gscm practices, and fills a gap in the literature on the identification and establishment of critical factors for gscm implementation in electrical and electronics industries."
http://orkg.org/orkg/resource/R33426,Determination of the success factors in supply chain networks: a Hong Kong‐based manufacturer's perspective,10.1108/13683041111113231,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p><jats:italic>The purpose of the paper is to investigate the factors that affect the decision‐making process of Hong Kong‐based manufacturers when they select a third‐party logistics (3PL) service provider and how 3PL service providers manage to retain customer loyalty in times of financial turbulence.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p><jats:italic>The paper presents a survey‐based study targeting Hong Kong‐based manufacturers currently using 3PL companies. It investigates the relationship between the reasons for using 3PL services and the requirements for selecting a provider, and examines the relationship between customer satisfaction and loyalty. In addition, the relationships among various dimensions – in small to medium‐sized enterprises (SMEs), large enterprises and companies – of contracts of various lengths are investigated.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p><jats:italic>In general, the reasons for using 3PL services and the requirements for selecting 3PL service providers are positive‐related. The dimension of “reputation” of satisfaction influences “primary customer loyalty” positively.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p><jats:italic>Constructive suggestions are provided to help 3PL service providers allocate their limited resources to business areas that help them to meet the demands of their target customers, increase the number of customers, and improve customer loyalty.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p><jats:italic>The paper is an attempt to help 3PL service providers find ways to survive in a climate of financial crisis.</jats:italic></jats:p></jats:sec>","purpose the purpose of the paper is to investigate the factors that affect the decision‐making process of hong kong‐based manufacturers when they select a third‐party logistics (3pl) service provider and how 3pl service providers manage to retain customer loyalty in times of financial turbulence. design/methodology/approach the paper presents a survey‐based study targeting hong kong‐based manufacturers currently using 3pl companies. it investigates the relationship between the reasons for using 3pl services and the requirements for selecting a provider, and examines the relationship between customer satisfaction and loyalty. in addition, the relationships among various dimensions – in small to medium‐sized enterprises (smes), large enterprises and companies – of contracts of various lengths are investigated. findings in general, the reasons for using 3pl services and the requirements for selecting 3pl service providers are positive‐related. the dimension of “reputation” of satisfaction influences “primary customer loyalty” positively. practical implications constructive suggestions are provided to help 3pl service providers allocate their limited resources to business areas that help them to meet the demands of their target customers, increase the number of customers, and improve customer loyalty. originality/value the paper is an attempt to help 3pl service providers find ways to survive in a climate of financial crisis."
http://orkg.org/orkg/resource/R33489,Identifying critical enablers and pathways to high performance supply chain quality management,10.1108/01443571311300818,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The aim of this paper is threefold: first, to examine the content of supply chain quality management (SCQM); second, to identify the structure of SCQM; and third, to show ways for finding improvement opportunities and organizing individual institution\'s resources/actions into collective performance outcomes.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>To meet the goals of this work, the paper uses abductive reasoning and two qualitative methods: content analysis and formal concept analysis (FCA). Primary data were collected from both original design manufacturers (ODMs) and original equipment manufacturers (OEMs) in Taiwan.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>According to the qualitative empirical study, modern enterprises need to pay immediate attention to the following two pathways: a compliance approach and a voluntary approach. For the former, three strategic content variables are identified: training programs, ISO, and supplier quality audit programs. As for initiating a voluntary effort, modern lead firms need to instill “motivation” into a supply chain quality system.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>The findings based on the abductive model reveal numerous strategic and tactical enablers, key sequences to move firms from their current situation to their preferred one, and critical opportunities for supply chain‐wide quality system designs.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This study will be of great value to supply chain policy makers, supply chain operators, and decision makers in lead firms in a supply chain setting and their channel partners. The proactive use of the authors\' proposed research procedure is indispensable to effective supply chain quality planning.</jats:p></jats:sec>","purpose the aim of this paper is threefold: first, to examine the content of supply chain quality management (scqm); second, to identify the structure of scqm; and third, to show ways for finding improvement opportunities and organizing individual institution\'s resources/actions into collective performance outcomes. design/methodology/approach to meet the goals of this work, the paper uses abductive reasoning and two qualitative methods: content analysis and formal concept analysis (fca). primary data were collected from both original design manufacturers (odms) and original equipment manufacturers (oems) in taiwan. findings according to the qualitative empirical study, modern enterprises need to pay immediate attention to the following two pathways: a compliance approach and a voluntary approach. for the former, three strategic content variables are identified: training programs, iso, and supplier quality audit programs. as for initiating a voluntary effort, modern lead firms need to instill “motivation” into a supply chain quality system. practical implications the findings based on the abductive model reveal numerous strategic and tactical enablers, key sequences to move firms from their current situation to their preferred one, and critical opportunities for supply chain‐wide quality system designs. originality/value this study will be of great value to supply chain policy makers, supply chain operators, and decision makers in lead firms in a supply chain setting and their channel partners. the proactive use of the authors\' proposed research procedure is indispensable to effective supply chain quality planning."
http://orkg.org/orkg/resource/R33506,Key success factor analysis for e‐SCM project implementation and a case study in semiconductor manufacturers,10.1108/ijpdlm-03-2012-0062,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The semiconductor market exceeded US$250 billion worldwide in 2010 and has had a double‐digit compound annual growth rate (CAGR) in the last 20 years. As it is located far upstream of the electronic product market, the semiconductor industry has suffered severely from the “bullwhip” effect. Therefore, effective e‐based supply chain management (e‐SCM) has become imperative for the efficient operation of semiconductor manufacturing (SM) companies. The purpose of this research is to define and analyze the key success factors (KSF) for e‐SCM system implementation in the semiconductor industry.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>A hierarchy of KSFs is defined first by a combination of a literature review and a focus group discussion with experts who successfully implemented an inter‐organizational e‐SCM project. Fuzzy analytic hierarchy process (FAHP) is then employed to rank the importance of these identified KSFs. To confirm the research result and further explore the managerial implications, a second in‐depth interview with the e‐SCM project executives is conducted.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The KSF hierarchy is constructed with two levels: a top‐level consisting of four dimensions and a detailed‐level consisting of 15 individual factors. The research shows that, in the top‐level, strategy is the most critically successful dimension followed by process, organization, and technical; whereas in the detailed‐level, the top management commitment, clear project goal and business requirements, and business process re‐engineering are the top three critical successful factors.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>Research surveys and interviews were conducted with two leading companies: Taiwan Semiconductor Manufacturing Company (TSMC) and ASE; they are the largest front‐end and back‐end SM companies in the world, respectively. Although the data collected was primarily based on the experience of one successful e‐SCM project, the significant roles of these two companies and compelling contribution made by the e‐SCM project leading to the research resulted in valuable guidelines for the companies in the semiconductor industry and a useful reference for companies in other manufacturing industries.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>e‐SCM system has a high failure rate and there is little literature discussing the KSF of e‐SCM implementation from a holistic view for certain industries. This paper not only provides a structured and comprehensive list of KSFs but also illustrates the application of the most critical factors by examples. In addition to the contributions made to industries, the research results can also serve as a foundation for related academic research when comparing the KSFs of implementing e‐SCM by different industries.</jats:p></jats:sec>","purpose the semiconductor market exceeded us$250 billion worldwide in 2010 and has had a double‐digit compound annual growth rate (cagr) in the last 20 years. as it is located far upstream of the electronic product market, the semiconductor industry has suffered severely from the “bullwhip” effect. therefore, effective e‐based supply chain management (e‐scm) has become imperative for the efficient operation of semiconductor manufacturing (sm) companies. the purpose of this research is to define and analyze the key success factors (ksf) for e‐scm system implementation in the semiconductor industry. design/methodology/approach a hierarchy of ksfs is defined first by a combination of a literature review and a focus group discussion with experts who successfully implemented an inter‐organizational e‐scm project. fuzzy analytic hierarchy process (fahp) is then employed to rank the importance of these identified ksfs. to confirm the research result and further explore the managerial implications, a second in‐depth interview with the e‐scm project executives is conducted. findings the ksf hierarchy is constructed with two levels: a top‐level consisting of four dimensions and a detailed‐level consisting of 15 individual factors. the research shows that, in the top‐level, strategy is the most critically successful dimension followed by process, organization, and technical; whereas in the detailed‐level, the top management commitment, clear project goal and business requirements, and business process re‐engineering are the top three critical successful factors. research limitations/implications research surveys and interviews were conducted with two leading companies: taiwan semiconductor manufacturing company (tsmc) and ase; they are the largest front‐end and back‐end sm companies in the world, respectively. although the data collected was primarily based on the experience of one successful e‐scm project, the significant roles of these two companies and compelling contribution made by the e‐scm project leading to the research resulted in valuable guidelines for the companies in the semiconductor industry and a useful reference for companies in other manufacturing industries. originality/value e‐scm system has a high failure rate and there is little literature discussing the ksf of e‐scm implementation from a holistic view for certain industries. this paper not only provides a structured and comprehensive list of ksfs but also illustrates the application of the most critical factors by examples. in addition to the contributions made to industries, the research results can also serve as a foundation for related academic research when comparing the ksfs of implementing e‐scm by different industries."
http://orkg.org/orkg/resource/R33521,Evaluating the critical success factors of supplier development: a case study,10.1108/14635771311318117,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to identify and evaluate the critical success factors (CSFs) responsible for supplier development (SD) in a manufacturing supply chain environment.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>In total, 13 CSFs for SD are identified (i.e. long‐term strategic goal; top management commitment; incentives; supplier\'s supplier condition; proximity to manufacturing base; supplier certification; innovation capability; information sharing; environmental readiness; external environment; project completion experience; supplier status and direct involvement) through extensive literature review and discussion held with managers/engineers in different Indian manufacturing companies. A fuzzy analytic hierarchy process (FAHP) is proposed and developed to evaluate the degree of impact of each CSF on SD.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The degree of impact for each CSF on SD is established for an Indian company. The results are discussed in detail with managerial implications. The long‐term strategic goal is found to be the most significant CSF for successful SD implementation.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>This study has not been statistically validated in a manufacturing supply chain environment for complete acceptability.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>The simplicity and clarity of the proposed approach enhances its acceptability for evaluating CSFs in manufacturing supply chain environment. It also provides the direction for optimally allocating the efforts and resources for successful implementation of SD in short duration.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>Although both CSFs and SD have been widely researched, but no study has been reported in the literature to prioritize and rank the CSFs of SD in an Indian manufacturing environment. The paper contributes to research in the supply chain management area in general and SD in particular for manufacturing environment. The proposed approach has the ability to capture the judgment of multiple experts to prioritize and rank CSFs for SD.</jats:p></jats:sec>","purpose the purpose of this paper is to identify and evaluate the critical success factors (csfs) responsible for supplier development (sd) in a manufacturing supply chain environment. design/methodology/approach in total, 13 csfs for sd are identified (i.e. long‐term strategic goal; top management commitment; incentives; supplier\'s supplier condition; proximity to manufacturing base; supplier certification; innovation capability; information sharing; environmental readiness; external environment; project completion experience; supplier status and direct involvement) through extensive literature review and discussion held with managers/engineers in different indian manufacturing companies. a fuzzy analytic hierarchy process (fahp) is proposed and developed to evaluate the degree of impact of each csf on sd. findings the degree of impact for each csf on sd is established for an indian company. the results are discussed in detail with managerial implications. the long‐term strategic goal is found to be the most significant csf for successful sd implementation. research limitations/implications this study has not been statistically validated in a manufacturing supply chain environment for complete acceptability. practical implications the simplicity and clarity of the proposed approach enhances its acceptability for evaluating csfs in manufacturing supply chain environment. it also provides the direction for optimally allocating the efforts and resources for successful implementation of sd in short duration. originality/value although both csfs and sd have been widely researched, but no study has been reported in the literature to prioritize and rank the csfs of sd in an indian manufacturing environment. the paper contributes to research in the supply chain management area in general and sd in particular for manufacturing environment. the proposed approach has the ability to capture the judgment of multiple experts to prioritize and rank csfs for sd."
http://orkg.org/orkg/resource/R33587,Outcome for Children with Autism who Began Intensive Behavioral Treatment Between Ages 4 and 7,10.1177/0145445506291396,crossref,"<jats:p> This study extends findings on the effects of intensive applied behavior analytic treatment for children with autism who began treatment at a mean age of 5.5 years. The behavioral treatment group ( n = 13, 8 boys) was compared to an eclectic treatment group ( n = 12, 11 boys). Assignment to groups was made independently based on the availability of qualified supervisors. Both behavioral and eclectic treatment took place in public kindergartens and elementary schools for typically developing children. At a mean age of 8 years, 2 months, the behavioral treatment group showed larger increases in IQ and adaptive functioning than did the eclectic group. The behavioral treatment group also displayed fewer aberrant behaviors and social problems at follow-up. Results suggest that behavioral treatment was effective for children with autism in the study. </jats:p>","this study extends findings on the effects of intensive applied behavior analytic treatment for children with autism who began treatment at a mean age of 5.5 years. the behavioral treatment group ( n = 13, 8 boys) was compared to an eclectic treatment group ( n = 12, 11 boys). assignment to groups was made independently based on the availability of qualified supervisors. both behavioral and eclectic treatment took place in public kindergartens and elementary schools for typically developing children. at a mean age of 8 years, 2 months, the behavioral treatment group showed larger increases in iq and adaptive functioning than did the eclectic group. the behavioral treatment group also displayed fewer aberrant behaviors and social problems at follow-up. results suggest that behavioral treatment was effective for children with autism in the study."
http://orkg.org/orkg/resource/R33589,Long-term outcome of social skills intervention based on interactive LEGO play,10.1177/1362361306064403,crossref,"<jats:p> LEGO<jats:sup>©</jats:sup> building materials have been adapted as a therapeutic modality for increasing motivation to participate in social skills intervention, and providing a medium through which children with social and communication handicaps can effectively interact. A 3 year retrospective study of long-term outcome for autistic spectrum children participating in LEGO<jats:sup>©</jats:sup> therapy ( N = 60) compared Vineland Adaptive Behavior Scale socialization domain (VABS–SD) and Gilliam Autism Rating Scale social interaction subscale (GARS–SI) scores preand post-treatment with a matched comparison sample ( N = 57) who received comparable non-LEGO<jats:sup>©</jats:sup> therapy. Although both groups made significant gains on the two outcome measures, LEGO<jats:sup>©</jats:sup> participants improved significantly more than the comparison subjects. Diagnosis and pre-treatment full-scale IQ scores did not predict outcome scores; however, Vineland adaptive behavior composite, Vineland communication domain, and verbal IQ all predicted outcome on the VABS–SD, especially for the LEGO<jats:sup>©</jats:sup> therapy group. Results are discussed in terms of implications for methods of social skills intervention for autistic spectrum disorders. </jats:p>","lego © building materials have been adapted as a therapeutic modality for increasing motivation to participate in social skills intervention, and providing a medium through which children with social and communication handicaps can effectively interact. a 3 year retrospective study of long-term outcome for autistic spectrum children participating in lego © therapy ( n = 60) compared vineland adaptive behavior scale socialization domain (vabs–sd) and gilliam autism rating scale social interaction subscale (gars–si) scores preand post-treatment with a matched comparison sample ( n = 57) who received comparable non-lego © therapy. although both groups made significant gains on the two outcome measures, lego © participants improved significantly more than the comparison subjects. diagnosis and pre-treatment full-scale iq scores did not predict outcome scores; however, vineland adaptive behavior composite, vineland communication domain, and verbal iq all predicted outcome on the vabs–sd, especially for the lego © therapy group. results are discussed in terms of implications for methods of social skills intervention for autistic spectrum disorders."
http://orkg.org/orkg/resource/R33802,Assessment of algorithms for high throughput detection of genomic copy number variation in oligonucleotide microarray data,10.1186/1471-2105-8-368,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Genomic deletions and duplications are important in the pathogenesis of diseases, such as cancer and mental retardation, and have recently been shown to occur frequently in unaffected individuals as polymorphisms. Affymetrix GeneChip whole genome sampling analysis (WGSA) combined with 100 K single nucleotide polymorphism (SNP) genotyping arrays is one of several microarray-based approaches that are now being used to detect such structural genomic changes. The popularity of this technology and its associated open source data format have resulted in the development of an increasing number of software packages for the analysis of copy number changes using these SNP arrays.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>We evaluated four publicly available software packages for high throughput copy number analysis using synthetic and empirical 100 K SNP array data sets, the latter obtained from 107 mental retardation (MR) patients and their unaffected parents and siblings. We evaluated the software with regards to overall suitability for high-throughput 100 K SNP array data analysis, as well as effectiveness of normalization, scaling with various reference sets and feature extraction, as well as true and false positive rates of genomic copy number variant (CNV) detection.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusion</jats:title>\n            <jats:p>We observed considerable variation among the numbers and types of candidate CNVs detected by different analysis approaches, and found that multiple programs were needed to find all real aberrations in our test set. The frequency of false positive deletions was substantial, but could be greatly reduced by using the SNP genotype information to confirm loss of heterozygosity.</jats:p>\n          </jats:sec>","abstract \n \n background \n genomic deletions and duplications are important in the pathogenesis of diseases, such as cancer and mental retardation, and have recently been shown to occur frequently in unaffected individuals as polymorphisms. affymetrix genechip whole genome sampling analysis (wgsa) combined with 100 k single nucleotide polymorphism (snp) genotyping arrays is one of several microarray-based approaches that are now being used to detect such structural genomic changes. the popularity of this technology and its associated open source data format have resulted in the development of an increasing number of software packages for the analysis of copy number changes using these snp arrays. \n \n \n results \n we evaluated four publicly available software packages for high throughput copy number analysis using synthetic and empirical 100 k snp array data sets, the latter obtained from 107 mental retardation (mr) patients and their unaffected parents and siblings. we evaluated the software with regards to overall suitability for high-throughput 100 k snp array data analysis, as well as effectiveness of normalization, scaling with various reference sets and feature extraction, as well as true and false positive rates of genomic copy number variant (cnv) detection. \n \n \n conclusion \n we observed considerable variation among the numbers and types of candidate cnvs detected by different analysis approaches, and found that multiple programs were needed to find all real aberrations in our test set. the frequency of false positive deletions was substantial, but could be greatly reduced by using the snp genotype information to confirm loss of heterozygosity. \n"
http://orkg.org/orkg/resource/R33951,"Test Case Prioritization Using Ant Colony optimization,” Association in Computing Machinery",10.1145/1811226.1811238,crossref,"<jats:p>Regression testing is primarily a maintenance activity that is performed frequently to ensure the validity of the modified software. In such cases, due to time and cost constraints, the entire test suite cannot be run. Thus, it becomes essential to prioritize the tests in order to cover maximum faults in minimum time. In this paper, ant colony optimization is used, which is a new way to solve time constraint prioritization problem. This paper presents the regression test prioritization technique to reorder test suites in time constraint environment along with an algorithm that implements the technique.</jats:p>","regression testing is primarily a maintenance activity that is performed frequently to ensure the validity of the modified software. in such cases, due to time and cost constraints, the entire test suite cannot be run. thus, it becomes essential to prioritize the tests in order to cover maximum faults in minimum time. in this paper, ant colony optimization is used, which is a new way to solve time constraint prioritization problem. this paper presents the regression test prioritization technique to reorder test suites in time constraint environment along with an algorithm that implements the technique."
http://orkg.org/orkg/resource/R34057,Migration and rearing histories of chinook salmon (Oncorhynchus tshawytscha) determined by ion microprobe Sr isotope and Sr/Ca transects of otoliths,10.1139/f04-167,crossref,"""<jats:p> Strontium isotope and Sr/Ca ratios measured in situ by ion microprobe along radial transects of otoliths of juvenile chinook salmon (Oncorhynchus tshawytscha) vary between watersheds with contrasting geology. Otoliths from ocean-type chinook from Skagit River estuary, Washington, had prehatch regions with <jats:sup>87</jats:sup>Sr/<jats:sup>86</jats:sup>Sr ratios of ~0.709, suggesting a maternally inherited marine signature, extensive fresh water growth zones with <jats:sup>87</jats:sup>Sr/<jats:sup>86</jats:sup>Sr ratios similar to those of the Skagit River at ~0.705, and marine-like <jats:sup>87</jats:sup>Sr/<jats:sup>86</jats:sup>Sr ratios near their edges. Otoliths from stream-type chinook from central Idaho had prehatch <jats:sup>87</jats:sup>Sr/<jats:sup>86</jats:sup>Sr ratios ≥0.711, indicating that a maternal marine Sr isotopic signature is not preserved after the ~1000- to 1400-km migration from the Pacific Ocean. <jats:sup>87</jats:sup>Sr/<jats:sup>86</jats:sup>Sr ratios in the outer portions of otoliths from these Idaho juveniles were similar to those of their respective streams (~0.708\x960.722). For Skagit juveniles, fresh water growth was marked by small decreases in otolith Sr/Ca, with increases in Sr/Ca corresponding to increases in <jats:sup>87</jats:sup>Sr/<jats:sup>86</jats:sup>Sr with migration into salt water. Otoliths of Idaho fish had Sr/Ca radial variation patterns that record seasonal fluctuation in ambient water Sr/Ca ratios. The ion microprobe's ability to measure both <jats:sup>87</jats:sup>Sr/<jats:sup>86</jats:sup>Sr and Sr/Ca ratios of otoliths at high spatial resolution in situ provides a new tool for studies of fish rearing and migration. </jats:p>""",""" strontium isotope and sr/ca ratios measured in situ by ion microprobe along radial transects of otoliths of juvenile chinook salmon (oncorhynchus tshawytscha) vary between watersheds with contrasting geology. otoliths from ocean-type chinook from skagit river estuary, washington, had prehatch regions with 87 sr/ 86 sr ratios of ~0.709, suggesting a maternally inherited marine signature, extensive fresh water growth zones with 87 sr/ 86 sr ratios similar to those of the skagit river at ~0.705, and marine-like 87 sr/ 86 sr ratios near their edges. otoliths from stream-type chinook from central idaho had prehatch 87 sr/ 86 sr ratios ≥0.711, indicating that a maternal marine sr isotopic signature is not preserved after the ~1000- to 1400-km migration from the pacific ocean. 87 sr/ 86 sr ratios in the outer portions of otoliths from these idaho juveniles were similar to those of their respective streams (~0.708\x960.722). for skagit juveniles, fresh water growth was marked by small decreases in otolith sr/ca, with increases in sr/ca corresponding to increases in 87 sr/ 86 sr with migration into salt water. otoliths of idaho fish had sr/ca radial variation patterns that record seasonal fluctuation in ambient water sr/ca ratios. the ion microprobe's ability to measure both 87 sr/ 86 sr and sr/ca ratios of otoliths at high spatial resolution in situ provides a new tool for studies of fish rearing and migration. """
http://orkg.org/orkg/resource/R34060,Population structure of sympatric anadromous and nonanadromous Oncorhynchus mykiss: evidence from spawning surveys and otolith microchemistry,10.1139/f00-192,crossref,"<jats:p>Reproductive isolation between steelhead and resident rainbow trout (Oncorhynchus mykiss) was examined in the Deschutes River, Oregon, through surveys of spawning timing and location. Otolith microchemistry was used to determine the occurrence of steelhead and resident rainbow trout progeny in the adult populations of steelhead and resident rainbow trout in the Deschutes River and in the Babine River, British Columbia. In the 3 years studied, steelhead spawning occurred from mid March through May and resident rainbow trout spawning occurred from mid March through August. The timing of 50% spawning was 9-10 weeks earlier for steelhead than for resident rainbow trout. Spawning sites selected by steelhead were in deeper water and had larger substrate than those selected by resident rainbow trout. Maternal origin was identified by comparing Sr/Ca ratios in the primordia and freshwater growth regions of the otolith with a wavelength-dispersive electron microprobe. In the Deschutes River, only steelhead of steelhead maternal origin and resident rainbow trout of resident rainbow trout origin were observed. In the Babine River, steelhead of resident rainbow trout origin and resident rainbow trout of steelhead maternal origin were also observed. Based on these findings, we suggest that steelhead and resident rainbow trout in the Deschutes River may constitute reproductively isolated populations.</jats:p>","reproductive isolation between steelhead and resident rainbow trout (oncorhynchus mykiss) was examined in the deschutes river, oregon, through surveys of spawning timing and location. otolith microchemistry was used to determine the occurrence of steelhead and resident rainbow trout progeny in the adult populations of steelhead and resident rainbow trout in the deschutes river and in the babine river, british columbia. in the 3 years studied, steelhead spawning occurred from mid march through may and resident rainbow trout spawning occurred from mid march through august. the timing of 50% spawning was 9-10 weeks earlier for steelhead than for resident rainbow trout. spawning sites selected by steelhead were in deeper water and had larger substrate than those selected by resident rainbow trout. maternal origin was identified by comparing sr/ca ratios in the primordia and freshwater growth regions of the otolith with a wavelength-dispersive electron microprobe. in the deschutes river, only steelhead of steelhead maternal origin and resident rainbow trout of resident rainbow trout origin were observed. in the babine river, steelhead of resident rainbow trout origin and resident rainbow trout of steelhead maternal origin were also observed. based on these findings, we suggest that steelhead and resident rainbow trout in the deschutes river may constitute reproductively isolated populations."
http://orkg.org/orkg/resource/R34115,Predicting multiple sclerosis at optic neuritis onset,10.1191/1352458503ms895oa,crossref,"<jats:p> Using multivariate analyses, individual risk of clinically definite multiple sclerosis (C DMS) after monosymptomatic optic neuritis (MO N) was quantified in a prospective study with clinical MO N onset during 1990 -95 in Stockholm, Sweden. During a mean follow-up time of 3.8 years, the presence of MS-like brain magnetic resonance imaging (MRI) lesions and oligoclonal immunoglobulin (Ig) G bands in cerebrospinal fluid (CSF) were strong prognostic markers of C DMS, with relative hazard ratios of 4.68 {95% confidence interval (CI) 2.21 -9.91} and 5.39 (95% C I 1.56 -18.61), respectively. Age and season of clinical onset were also significant predictors, with relative hazard ratios of 1.76 (95% C I 1.02 -3.04) and 2.21 (95% C I 1.13 -3.98), respectively. Based on the above two strong predicto rs, individual probability of C DMS development after MO N was calculated in a three-quarter sample drawn from a cohort, with completion of follow-up at three years. The highest probability, 0.66 (95% C I 0.48 -0.80), was obtained for individuals presenting with three or more brain MRI lesions and oligoclonal bands in the C SF, and the lowest, 0.09 (95% C I 0.02 -0.32), for those not presenting with these traits. Medium values, 0.29 (95% C I 0.13 -0.53) and 0.32 (95% C I 0.07 -0.73), were obtained for individuals discordant for the presence of brain MRI lesions and oligoclonal bands in the C SF. These predictions were validated in an external one-quarter sample. </jats:p>","using multivariate analyses, individual risk of clinically definite multiple sclerosis (c dms) after monosymptomatic optic neuritis (mo n) was quantified in a prospective study with clinical mo n onset during 1990 -95 in stockholm, sweden. during a mean follow-up time of 3.8 years, the presence of ms-like brain magnetic resonance imaging (mri) lesions and oligoclonal immunoglobulin (ig) g bands in cerebrospinal fluid (csf) were strong prognostic markers of c dms, with relative hazard ratios of 4.68 {95% confidence interval (ci) 2.21 -9.91} and 5.39 (95% c i 1.56 -18.61), respectively. age and season of clinical onset were also significant predictors, with relative hazard ratios of 1.76 (95% c i 1.02 -3.04) and 2.21 (95% c i 1.13 -3.98), respectively. based on the above two strong predicto rs, individual probability of c dms development after mo n was calculated in a three-quarter sample drawn from a cohort, with completion of follow-up at three years. the highest probability, 0.66 (95% c i 0.48 -0.80), was obtained for individuals presenting with three or more brain mri lesions and oligoclonal bands in the c sf, and the lowest, 0.09 (95% c i 0.02 -0.32), for those not presenting with these traits. medium values, 0.29 (95% c i 0.13 -0.53) and 0.32 (95% c i 0.07 -0.73), were obtained for individuals discordant for the presence of brain mri lesions and oligoclonal bands in the c sf. these predictions were validated in an external one-quarter sample."
http://orkg.org/orkg/resource/R34124,Can CSF predict the course of optic neuritis?,10.1177/135245859800400308,crossref,"<jats:p> To discuss the implications of CSF abnormalities for the course of acute monosymptomatic optic neuritis (AMON), various CSF markers were analysed in patients being randomly selected from a population-based cohort. Paired serum and CSF were obtained within a few weeks from onset of AMON. CSF-restricted oligoclonal IgG bands, free kappa and free lambda chain bands were observed in 17, 15, and nine of 27 examined patients, respectively. Sixteen patients showed a polyspecific intrathecal synthesis of oligoclonal IgG antibodies against one or more viruses. At 1 year follow-up five patients had developed clinically definite multiple sclerosis (CDMS); all had CSF oligoclonal IgG bands and virus-specific oligoclonal IgG antibodies at onset. Due to the relative small number studied at the short-term follow-up, no firm conclusion of the prognostic value of these analyses could be reached. CSF Myelin Basic Protein-like material was increased in only two of 29 patients with AMON, but may have potential value in reflecting disease activity, as the highest values were obtained among patients with CSF sampled soon after the worst visual acuity was reached, and among patients with severe visual impairment. In most previous studies of patients with AMON qualitative and quantitative analyses of CSF IgG had a predictive value for development of CDMS, but the results are conflicting. </jats:p>","to discuss the implications of csf abnormalities for the course of acute monosymptomatic optic neuritis (amon), various csf markers were analysed in patients being randomly selected from a population-based cohort. paired serum and csf were obtained within a few weeks from onset of amon. csf-restricted oligoclonal igg bands, free kappa and free lambda chain bands were observed in 17, 15, and nine of 27 examined patients, respectively. sixteen patients showed a polyspecific intrathecal synthesis of oligoclonal igg antibodies against one or more viruses. at 1 year follow-up five patients had developed clinically definite multiple sclerosis (cdms); all had csf oligoclonal igg bands and virus-specific oligoclonal igg antibodies at onset. due to the relative small number studied at the short-term follow-up, no firm conclusion of the prognostic value of these analyses could be reached. csf myelin basic protein-like material was increased in only two of 29 patients with amon, but may have potential value in reflecting disease activity, as the highest values were obtained among patients with csf sampled soon after the worst visual acuity was reached, and among patients with severe visual impairment. in most previous studies of patients with amon qualitative and quantitative analyses of csf igg had a predictive value for development of cdms, but the results are conflicting."
http://orkg.org/orkg/resource/R34132,"Evaluation of Transgenic Corn Against European Corn Borer, Central Minnesota, 1996",10.1093/amt/22.1.424,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>This experiment was conducted to assess the performance of Bacillus thuringiensis (Bt) transgenic corn [crylA(b) gene] against a natural ECB infestation in Rosemount, MN. Plots measuring 50 ft by 8 rows (30 inch row spacing) were established in Dakota silty loam soil on 23 May at a rate of 26,100 seeds per acre. Plots were arranged in a RCB with four replications. First generation ECB measurements recorded Jul to Aug included % shot-holing, leaf injury ratings, and tunnel length and number. Measurements for second generation ECB recorded in Sept included cumulative tunnel length and number, fall larvae, and ear and shank damage. Yield data were corrected to 15.5% moisture.</jats:p>","abstract \n this experiment was conducted to assess the performance of bacillus thuringiensis (bt) transgenic corn [cryla(b) gene] against a natural ecb infestation in rosemount, mn. plots measuring 50 ft by 8 rows (30 inch row spacing) were established in dakota silty loam soil on 23 may at a rate of 26,100 seeds per acre. plots were arranged in a rcb with four replications. first generation ecb measurements recorded jul to aug included % shot-holing, leaf injury ratings, and tunnel length and number. measurements for second generation ecb recorded in sept included cumulative tunnel length and number, fall larvae, and ear and shank damage. yield data were corrected to 15.5% moisture."
http://orkg.org/orkg/resource/R34136,Case study in benefits and risks of agricultural biotechnology: Roundup Ready soybeans.,10.1079/9780851995731.0227,crossref,"<title>Abstract</title><p>This case study describes the US regulatory process governing agricultural biotechnology and traces the approval of Roundup Ready soyabeans (with transgenic tolerance of the herbicide glyphosate), summarizing the information that was submitted to US regulatory agencies by Monsanto. Estimates of the impact that the adoption of Roundup Ready soyabeans has had on US agriculture are also provided. The US regulatory structure for agricultural biotechnology has evolved over the past 25 years, as technology allowing for genetic modification developed. The system continues to evolve as new and different applications of the technology emerge. In reviewing the studies that were conducted on the safety of Roundup Ready soyabeans, no indication of greater health or environmental risks were found compared with conventional varieties. The benefits of the introduction of Roundup Ready soyabeans include cost savings of US$216 million in annual weed control and 19 million fewer soyabean herbicide applications per year.</p>","abstract this case study describes the us regulatory process governing agricultural biotechnology and traces the approval of roundup ready soyabeans (with transgenic tolerance of the herbicide glyphosate), summarizing the information that was submitted to us regulatory agencies by monsanto. estimates of the impact that the adoption of roundup ready soyabeans has had on us agriculture are also provided. the us regulatory structure for agricultural biotechnology has evolved over the past 25 years, as technology allowing for genetic modification developed. the system continues to evolve as new and different applications of the technology emerge. in reviewing the studies that were conducted on the safety of roundup ready soyabeans, no indication of greater health or environmental risks were found compared with conventional varieties. the benefits of the introduction of roundup ready soyabeans include cost savings of us$216 million in annual weed control and 19 million fewer soyabean herbicide applications per year."
http://orkg.org/orkg/resource/R34240,"Are proposed African monetary unions optimal currency areas? Real, monetary and fiscal policy convergence analysis",10.1108/ajems-02-2012-0010,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>– A spectre is hunting embryonic African monetary zones: the European Monetary Union crisis. The purpose of this paper is to assess real, monetary and fiscal policy convergence within the proposed WAM and EAM zones. The introduction of common currencies in West and East Africa is facing stiff challenges in the timing of monetary convergence, the imperative of central bankers to apply common modeling and forecasting methods of monetary policy transmission, as well as the requirements of common structural and institutional characteristics among candidate states.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>– In the analysis: monetary policy targets inflation and financial dynamics of depth, efficiency, activity and size; real sector policy targets economic performance in terms of GDP growth at macro and micro levels; while, fiscal policy targets debt-to-GDP and deficit-to-GDP ratios. A dynamic panel GMM estimation with data from different non-overlapping intervals is employed. The implied rate of convergence and the time required to achieve full (100 percent) convergence are then computed from the estimations.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>– Findings suggest overwhelming lack of convergence: initial conditions for financial development are different across countries; fundamental characteristics as common monetary policy initiatives and IMF-backed financial reform programs are implemented differently across countries; there is remarkable evidence of cross-country variations in structural characteristics of macroeconomic performance; institutional cross-country differences could also be responsible for the deficiency in convergence within the potential monetary zones; absence of fiscal policy convergence and no potential for eliminating idiosyncratic fiscal shocks due to business cycle incoherence.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>– As a policy implication, heterogeneous structural and institutional characteristics across countries are giving rise to different levels and patterns of financial intermediary development. Thus, member states should work towards harmonizing cross-country differences in structural and institutional characteristics that hamper the effectiveness of convergence in monetary, real and fiscal policies. This could be done by stringently monitoring the implementation of existing common initiatives and/or the adoption of new reforms programs.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>– It is one of the few attempts to investigate the issue of convergence within the proposed WAM and EAM unions.</jats:p></jats:sec>","purpose – a spectre is hunting embryonic african monetary zones: the european monetary union crisis. the purpose of this paper is to assess real, monetary and fiscal policy convergence within the proposed wam and eam zones. the introduction of common currencies in west and east africa is facing stiff challenges in the timing of monetary convergence, the imperative of central bankers to apply common modeling and forecasting methods of monetary policy transmission, as well as the requirements of common structural and institutional characteristics among candidate states. design/methodology/approach – in the analysis: monetary policy targets inflation and financial dynamics of depth, efficiency, activity and size; real sector policy targets economic performance in terms of gdp growth at macro and micro levels; while, fiscal policy targets debt-to-gdp and deficit-to-gdp ratios. a dynamic panel gmm estimation with data from different non-overlapping intervals is employed. the implied rate of convergence and the time required to achieve full (100 percent) convergence are then computed from the estimations. findings – findings suggest overwhelming lack of convergence: initial conditions for financial development are different across countries; fundamental characteristics as common monetary policy initiatives and imf-backed financial reform programs are implemented differently across countries; there is remarkable evidence of cross-country variations in structural characteristics of macroeconomic performance; institutional cross-country differences could also be responsible for the deficiency in convergence within the potential monetary zones; absence of fiscal policy convergence and no potential for eliminating idiosyncratic fiscal shocks due to business cycle incoherence. practical implications – as a policy implication, heterogeneous structural and institutional characteristics across countries are giving rise to different levels and patterns of financial intermediary development. thus, member states should work towards harmonizing cross-country differences in structural and institutional characteristics that hamper the effectiveness of convergence in monetary, real and fiscal policies. this could be done by stringently monitoring the implementation of existing common initiatives and/or the adoption of new reforms programs. originality/value – it is one of the few attempts to investigate the issue of convergence within the proposed wam and eam unions."
http://orkg.org/orkg/resource/R34276,Macroeconomic Shock Synchronization in the East African Community,10.1515/gej-2013-0015,crossref,"<jats:p> The East African Community’s (EAC) economic integration has gained momentum recently, with the EAC countries aiming to adopt a single currency in 2015. This article evaluates empirically the readiness of the EAC countries for monetary union. First, structural similarity in terms of similarity of production and exports of the EAC countries is measured. Second, the symmetry of shocks is examined with structural vector auto-regression analysis (SVAR). The lack of macroeconomic convergence gives evidence against a hurried transition to a monetary union. Given the divergent macroeconomic outcomes, structural reforms, including closing infrastructure gaps and harmonizing macroeconomic policies that would raise synchronization of business cycles, need to be in place before moving to monetary union. </jats:p>","the east african community’s (eac) economic integration has gained momentum recently, with the eac countries aiming to adopt a single currency in 2015. this article evaluates empirically the readiness of the eac countries for monetary union. first, structural similarity in terms of similarity of production and exports of the eac countries is measured. second, the symmetry of shocks is examined with structural vector auto-regression analysis (svar). the lack of macroeconomic convergence gives evidence against a hurried transition to a monetary union. given the divergent macroeconomic outcomes, structural reforms, including closing infrastructure gaps and harmonizing macroeconomic policies that would raise synchronization of business cycles, need to be in place before moving to monetary union."
http://orkg.org/orkg/resource/R34400,Fulminant Clostridium difficile enteritis after proctocolectomy and ileal pouch-anal anastamosis,10.1155/2008/985658,crossref,"<jats:p><jats:italic>Clostridium difficile</jats:italic>(<jats:italic>C. difficile</jats:italic>) infection of the small bowel is very rare. The disease course is more severe than that of<jats:italic>C. difficile</jats:italic>colitis, and the mortality is high. We present a case of<jats:italic>C. difficile</jats:italic>enteritis in a patient with with ileal pouch-anal anastamosis (IPAA), and review previous case reports in order to better characterize this unusual condition.</jats:p>","clostridium difficile ( c. difficile ) infection of the small bowel is very rare. the disease course is more severe than that of c. difficile colitis, and the mortality is high. we present a case of c. difficile enteritis in a patient with with ileal pouch-anal anastamosis (ipaa), and review previous case reports in order to better characterize this unusual condition."
http://orkg.org/orkg/resource/R34405,"Enteral Clostrid- ium difficile, an emerging cause for high-output ileostomy",10.1136/jcp.2008.062901,crossref,"<jats:p>The loss of fluid and electrolytes from a high-output ileostomy (&gt;1200 ml/day) can quickly result in dehydration and if not properly managed may cause acute renal failure. The management of a high-output ileostomy is based upon three principles: correction of electrolyte disturbance and fluid balance, pharmacological reduction of ileostomy output, and treatment of any underlying identifiable cause. There is an increasing body of evidence to suggest that <jats:italic>Clostridium difficile</jats:italic> may behave pathologically in the small intestine producing a spectrum of enteritis that mirrors the well-recognised colonic disease manifestation. Clinically this can range from high-output ileostomy to fulminant enteritis. This report describes two cases of high-output ileostomy associated with enteric <jats:italic>C difficile</jats:italic> infection and proposes that the management algorithm of a high-output ileostomy should include exclusion of small bowel <jats:italic>C difficile</jats:italic>.</jats:p>","the loss of fluid and electrolytes from a high-output ileostomy (&gt;1200 ml/day) can quickly result in dehydration and if not properly managed may cause acute renal failure. the management of a high-output ileostomy is based upon three principles: correction of electrolyte disturbance and fluid balance, pharmacological reduction of ileostomy output, and treatment of any underlying identifiable cause. there is an increasing body of evidence to suggest that clostridium difficile may behave pathologically in the small intestine producing a spectrum of enteritis that mirrors the well-recognised colonic disease manifestation. clinically this can range from high-output ileostomy to fulminant enteritis. this report describes two cases of high-output ileostomy associated with enteric c difficile infection and proposes that the management algorithm of a high-output ileostomy should include exclusion of small bowel c difficile ."
http://orkg.org/orkg/resource/R34596,k-ANONYMITY: A MODEL FOR PROTECTING PRIVACY,10.1142/s0218488502001648,crossref,"<jats:p> Consider a data holder, such as a hospital or a bank, that has a privately held collection of person-specific, field structured data. Suppose the data holder wants to share a version of the data with researchers. How can a data holder release a version of its private data with scientific guarantees that the individuals who are the subjects of the data cannot be re-identified while the data remain practically useful? The solution provided in this paper includes a formal protection model named k-anonymity and a set of accompanying policies for deployment. A release provides k-anonymity protection if the information for each person contained in the release cannot be distinguished from at least k-1 individuals whose information also appears in the release. This paper also examines re-identification attacks that can be realized on releases that adhere to k-anonymity unless accompanying policies are respected. The k-anonymity protection model is important because it forms the basis on which the real-world systems known as Datafly, μ-Argus and k-Similar provide guarantees of privacy protection. </jats:p>","consider a data holder, such as a hospital or a bank, that has a privately held collection of person-specific, field structured data. suppose the data holder wants to share a version of the data with researchers. how can a data holder release a version of its private data with scientific guarantees that the individuals who are the subjects of the data cannot be re-identified while the data remain practically useful? the solution provided in this paper includes a formal protection model named k-anonymity and a set of accompanying policies for deployment. a release provides k-anonymity protection if the information for each person contained in the release cannot be distinguished from at least k-1 individuals whose information also appears in the release. this paper also examines re-identification attacks that can be realized on releases that adhere to k-anonymity unless accompanying policies are respected. the k-anonymity protection model is important because it forms the basis on which the real-world systems known as datafly, μ-argus and k-similar provide guarantees of privacy protection."
http://orkg.org/orkg/resource/R34647,Electronic immunization data collection systems: application of an evaluation framework,10.1186/1472-6947-14-5,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Evaluating the features and performance of health information systems can serve to strengthen the systems themselves as well as to guide other organizations in the process of designing and implementing surveillance tools. We adapted an evaluation framework in order to assess electronic immunization data collection systems, and applied it in two Ontario public health units.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Methods</jats:title>\n            <jats:p>The Centers for Disease Control and Prevention’s <jats:italic>Guidelines for Evaluating Public Health Surveillance Systems</jats:italic> are broad in nature and serve as an organizational tool to guide the development of comprehensive evaluation materials. Based on these <jats:italic>Guidelines,</jats:italic> and informed by other evaluation resources and input from stakeholders in the public health community, we applied an evaluation framework to two examples of immunization data collection and examined several system attributes: simplicity, flexibility, data quality, timeliness, and acceptability. Data collection approaches included key informant interviews, logic and completeness assessments, client surveys, and on-site observations.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>Both evaluated systems allow high-quality immunization data to be collected, analyzed, and applied in a rapid fashion. However, neither system is currently able to link to other providers’ immunization data or provincial data sources, limiting the comprehensiveness of coverage assessments. We recommended that both organizations explore possibilities for external data linkage and collaborate with other jurisdictions to promote a provincial immunization repository or data sharing platform.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>Electronic systems such as the ones described in this paper allow immunization data to be collected, analyzed, and applied in a rapid fashion, and represent the infostructure required to establish a population-based immunization registry, critical for comprehensively assessing vaccine coverage.</jats:p>\n          </jats:sec>","abstract \n \n background \n evaluating the features and performance of health information systems can serve to strengthen the systems themselves as well as to guide other organizations in the process of designing and implementing surveillance tools. we adapted an evaluation framework in order to assess electronic immunization data collection systems, and applied it in two ontario public health units. \n \n \n methods \n the centers for disease control and prevention’s guidelines for evaluating public health surveillance systems are broad in nature and serve as an organizational tool to guide the development of comprehensive evaluation materials. based on these guidelines, and informed by other evaluation resources and input from stakeholders in the public health community, we applied an evaluation framework to two examples of immunization data collection and examined several system attributes: simplicity, flexibility, data quality, timeliness, and acceptability. data collection approaches included key informant interviews, logic and completeness assessments, client surveys, and on-site observations. \n \n \n results \n both evaluated systems allow high-quality immunization data to be collected, analyzed, and applied in a rapid fashion. however, neither system is currently able to link to other providers’ immunization data or provincial data sources, limiting the comprehensiveness of coverage assessments. we recommended that both organizations explore possibilities for external data linkage and collaborate with other jurisdictions to promote a provincial immunization repository or data sharing platform. \n \n \n conclusions \n electronic systems such as the ones described in this paper allow immunization data to be collected, analyzed, and applied in a rapid fashion, and represent the infostructure required to establish a population-based immunization registry, critical for comprehensively assessing vaccine coverage. \n"
http://orkg.org/orkg/resource/R34796,Avian embryonic development does not change the stable isotope composition of the calcite eggshell,10.1071/rd10138,crossref,"<jats:p>The avian embryo resorbs most of the calcium for bone formation from the calcite eggshell but the exact mechanisms of the resorption are unknown. The present study tested whether this process results in variable fractionation of the oxygen and carbon isotopes in shell calcium carbonate, which could provide a detailed insight into the temporal and spatial use of the eggshell by the developing embryo. Despite the uncertainty regarding changes in stable isotope composition of the eggshell across developmental stages or regions of the shell, eggshells are a popular resource for the analysis of historic and extant trophic relationships. To clarify how the stable isotope composition varies with embryonic development, the δ13C and δ18O content of the carbonate fraction in shells of black-headed gull (Larus ridibundus) eggs were sampled at four different stages of embryonic development and at five eggshell regions. No consistent relationship between the stable isotope composition of the eggshell and embryonic development, shell region or maculation was observed, although shell thickness decreased with development in all shell regions. By contrast, individual eggs differed significantly in isotope composition. These results establish that eggshells can be used to investigate a species’ carbon and oxygen sources, regardless of the egg’s developmental stage.</jats:p>","the avian embryo resorbs most of the calcium for bone formation from the calcite eggshell but the exact mechanisms of the resorption are unknown. the present study tested whether this process results in variable fractionation of the oxygen and carbon isotopes in shell calcium carbonate, which could provide a detailed insight into the temporal and spatial use of the eggshell by the developing embryo. despite the uncertainty regarding changes in stable isotope composition of the eggshell across developmental stages or regions of the shell, eggshells are a popular resource for the analysis of historic and extant trophic relationships. to clarify how the stable isotope composition varies with embryonic development, the δ13c and δ18o content of the carbonate fraction in shells of black-headed gull (larus ridibundus) eggs were sampled at four different stages of embryonic development and at five eggshell regions. no consistent relationship between the stable isotope composition of the eggshell and embryonic development, shell region or maculation was observed, although shell thickness decreased with development in all shell regions. by contrast, individual eggs differed significantly in isotope composition. these results establish that eggshells can be used to investigate a species’ carbon and oxygen sources, regardless of the egg’s developmental stage."
http://orkg.org/orkg/resource/R34961,Introduction: A Survey of the Evolutionary Computation Techniques for Software Engineering,10.4018/978-1-61520-809-8.ch001,crossref,"<jats:p>This chapter aims to present a part of the computer science literature in which the evolutionary computation techniques, optimization techniques and other bio-inspired techniques are used to solve different search and optimization problems in the area of software engineering.</jats:p>","this chapter aims to present a part of the computer science literature in which the evolutionary computation techniques, optimization techniques and other bio-inspired techniques are used to solve different search and optimization problems in the area of software engineering."
http://orkg.org/orkg/resource/R51252,Identification of inhibitors of SARS-CoV-2 in-vitro cellular toxicity in human (Caco-2) cells using a large scale drug repurposing collection,10.21203/rs.3.rs-23951/v1,crossref,"<jats:title>Abstract</jats:title>\n        <jats:p>To identify possible candidates for progression towards clinical studies against SARS-CoV-2, we screened a well-defined collection of 5632 compounds including 3488 compounds which have undergone clinical investigations (marketed drugs, phases 1 -3, and withdrawn) across 600 indications. Compounds were screened for their inhibition of viral induced cytotoxicity using the human epithelial colorectal adenocarcinoma cell line Caco-2 and a SARS-CoV-2 isolate. The primary screen of 5632 compounds gave 271 hits. A total of 64 compounds with IC50 &lt;20 µM were identified, including 19 compounds with IC50 &lt; 1 µM. Of this confirmed hit population, 90% have not yet been previously reported as active against SARS-CoV-2 in-vitro cell assays. Some 37 of the actives are launched drugs, 19 are in phases 1-3 and 10 pre-clinical. Several inhibitors were associated with modulation of host pathways including kinase signaling P53 activation, ubiquitin pathways and PDE activity modulation, with long chain acyl transferases were effective viral inhibitors.</jats:p>","abstract \n to identify possible candidates for progression towards clinical studies against sars-cov-2, we screened a well-defined collection of 5632 compounds including 3488 compounds which have undergone clinical investigations (marketed drugs, phases 1 -3, and withdrawn) across 600 indications. compounds were screened for their inhibition of viral induced cytotoxicity using the human epithelial colorectal adenocarcinoma cell line caco-2 and a sars-cov-2 isolate. the primary screen of 5632 compounds gave 271 hits. a total of 64 compounds with ic50 &lt;20 µm were identified, including 19 compounds with ic50 &lt; 1 µm. of this confirmed hit population, 90% have not yet been previously reported as active against sars-cov-2 in-vitro cell assays. some 37 of the actives are launched drugs, 19 are in phases 1-3 and 10 pre-clinical. several inhibitors were associated with modulation of host pathways including kinase signaling p53 activation, ubiquitin pathways and pde activity modulation, with long chain acyl transferases were effective viral inhibitors."
http://orkg.org/orkg/resource/R36097,Schema extraction for tabular data on the web,https://doi.org/10.14778/2536336.2536343,crossref,"""<jats:p>Tabular data is an abundant source of information on the Web, but remains mostly isolated from the latter's interconnections since tables lack links and computer-accessible descriptions of their structure. In other words, the schemas of these tables -- attribute names, values, data types, etc. -- are not explicitly stored as table metadata. Consequently, the structure that these tables contain is not accessible to the crawlers that power search engines and thus not accessible to user search queries. We address this lack of structure with a new method for leveraging the principles of table construction in order to extract table schemas. Discovering the schema by which a table is constructed is achieved by harnessing the similarities and differences of nearby table rows through the use of a novel set of features and a feature processing scheme. The schemas of these data tables are determined using a classification technique based on conditional random fields in combination with a novel feature encoding method called logarithmic binning, which is specifically designed for the data table extraction task. Our method provides considerable improvement over the well-known WebTables schema extraction method. In contrast with previous work that focuses on extracting individual relations, our method excels at correctly interpreting full tables, thereby being capable of handling general tables such as those found in spreadsheets, instead of being restricted to HTML tables as is the case with the WebTables method. We also extract additional schema characteristics, such as row groupings, which are important for supporting information retrieval tasks on tabular data.</jats:p>""",""" tabular data is an abundant source of information on the web, but remains mostly isolated from the latter's interconnections since tables lack links and computer-accessible descriptions of their structure. in other words, the schemas of these tables -- attribute names, values, data types, etc. -- are not explicitly stored as table metadata. consequently, the structure that these tables contain is not accessible to the crawlers that power search engines and thus not accessible to user search queries. we address this lack of structure with a new method for leveraging the principles of table construction in order to extract table schemas. discovering the schema by which a table is constructed is achieved by harnessing the similarities and differences of nearby table rows through the use of a novel set of features and a feature processing scheme. the schemas of these data tables are determined using a classification technique based on conditional random fields in combination with a novel feature encoding method called logarithmic binning, which is specifically designed for the data table extraction task. our method provides considerable improvement over the well-known webtables schema extraction method. in contrast with previous work that focuses on extracting individual relations, our method excels at correctly interpreting full tables, thereby being capable of handling general tables such as those found in spreadsheets, instead of being restricted to html tables as is the case with the webtables method. we also extract additional schema characteristics, such as row groupings, which are important for supporting information retrieval tasks on tabular data. """
http://orkg.org/orkg/resource/R36106,Characterizing the transmission and identifying the control strategy for COVID-19 through epidemiological modeling,10.1101/2020.02.24.20026773,crossref,"<jats:title>ABSTRACT</jats:title><jats:p>The outbreak of the novel coronavirus disease, COVID-19, originating from Wuhan, China in early December, has infected more than 70,000 people in China and other countries and has caused more than 2,000 deaths. As the disease continues to spread, the biomedical society urgently began identifying effective approaches to prevent further outbreaks. Through rigorous epidemiological analysis, we characterized the fast transmission of COVID-19 with a basic reproductive number 5.6 and proved a sole zoonotic source to originate in Wuhan. No changes in transmission have been noted across generations. By evaluating different control strategies through predictive modeling and Monte carlo simulations, a comprehensive quarantine in hospitals and quarantine stations has been found to be the most effective approach. Government action to immediately enforce this quarantine is highly recommended.</jats:p>","abstract the outbreak of the novel coronavirus disease, covid-19, originating from wuhan, china in early december, has infected more than 70,000 people in china and other countries and has caused more than 2,000 deaths. as the disease continues to spread, the biomedical society urgently began identifying effective approaches to prevent further outbreaks. through rigorous epidemiological analysis, we characterized the fast transmission of covid-19 with a basic reproductive number 5.6 and proved a sole zoonotic source to originate in wuhan. no changes in transmission have been noted across generations. by evaluating different control strategies through predictive modeling and monte carlo simulations, a comprehensive quarantine in hospitals and quarantine stations has been found to be the most effective approach. government action to immediately enforce this quarantine is highly recommended."
http://orkg.org/orkg/resource/R36109,Transmission interval estimates suggest pre-symptomatic spread of COVID-19,10.1101/2020.03.03.20029983,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>As the COVID-19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic. We determine the incubation period and serial interval distribution for transmission clusters in Singapore and in Tianjin. We infer the basic reproduction number and identify the extent of pre-symptomatic transmission.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>We collected outbreak information from Singapore and Tianjin, China, reported from Jan.19-Feb.26 and Jan.21-Feb.27, respectively. We estimated incubation periods and serial intervals in both populations.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The mean incubation period was 7.1 (6.13, 8.25) days for Singapore and 9 (7.92, 10.2) days for Tianjin. Both datasets had shorter incubation periods for earlier-occurring cases. The mean serial interval was 4.56 (2.69, 6.42) days for Singapore and 4.22 (3.43, 5.01) for Tianjin. We inferred that early in the outbreaks, infection was transmitted on average 2.55 and 2.89 days before symptom onset (Singapore, Tianjin). The estimated basic reproduction number for Singapore was 1.97 (1.45, 2.48) secondary cases per infective; for Tianjin it was 1.87 (1.65, 2.09) secondary cases per infective.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>Estimated serial intervals are shorter than incubation periods in both Singapore and Tianjin, suggesting that pre-symptomatic transmission is occurring. Shorter serial intervals lead to lower estimates of R0, which suggest that half of all secondary infections should be prevented to control spread.</jats:p></jats:sec>","abstract background as the covid-19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic. we determine the incubation period and serial interval distribution for transmission clusters in singapore and in tianjin. we infer the basic reproduction number and identify the extent of pre-symptomatic transmission. methods we collected outbreak information from singapore and tianjin, china, reported from jan.19-feb.26 and jan.21-feb.27, respectively. we estimated incubation periods and serial intervals in both populations. results the mean incubation period was 7.1 (6.13, 8.25) days for singapore and 9 (7.92, 10.2) days for tianjin. both datasets had shorter incubation periods for earlier-occurring cases. the mean serial interval was 4.56 (2.69, 6.42) days for singapore and 4.22 (3.43, 5.01) for tianjin. we inferred that early in the outbreaks, infection was transmitted on average 2.55 and 2.89 days before symptom onset (singapore, tianjin). the estimated basic reproduction number for singapore was 1.97 (1.45, 2.48) secondary cases per infective; for tianjin it was 1.87 (1.65, 2.09) secondary cases per infective. conclusions estimated serial intervals are shorter than incubation periods in both singapore and tianjin, suggesting that pre-symptomatic transmission is occurring. shorter serial intervals lead to lower estimates of r0, which suggest that half of all secondary infections should be prevented to control spread."
http://orkg.org/orkg/resource/R36114,Estimation of the epidemic properties of the 2019 novel coronavirus: A mathematical modeling study,10.1101/2020.02.18.20024315,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>The 2019 novel Coronavirus (COVID-19) emerged in Wuhan, China in December 2019 and has been spreading rapidly in China. Decisions about its pandemic threat and the appropriate level of public health response depend heavily on estimates of its basic reproduction number and assessments of interventions conducted in the early stages of the epidemic.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>We conducted a mathematical modeling study using five independent methods to assess the basic reproduction number (R0) of COVID-19, using data on confirmed cases obtained from the China National Health Commission for the period 10<jats:sup>th</jats:sup> January – 8<jats:sup>th</jats:sup> February. We analyzed the data for the period before the closure of Wuhan city (10<jats:sup>th</jats:sup> January – 23<jats:sup>rd</jats:sup> January) and the post-closure period (23<jats:sup>rd</jats:sup> January – 8<jats:sup>th</jats:sup> February) and for the whole period, to assess both the epidemic risk of the virus and the effectiveness of the closure of Wuhan city on spread of COVID-19.</jats:p></jats:sec><jats:sec><jats:title>Findings</jats:title><jats:p>Before the closure of Wuhan city the basic reproduction number of COVID-19 was 4.38 (95% CI: 3.63 – 5.13), dropping to 3.41 (95% CI: 3.16 – 3.65) after the closure of Wuhan city. Over the entire epidemic period COVID-19 had a basic reproduction number of 3.39 (95% CI: 3.09 – 3.70), indicating it has a very high transmissibility.</jats:p></jats:sec><jats:sec><jats:title>Interpretation</jats:title><jats:p>COVID-19 is a highly transmissible virus with a very high risk of epidemic outbreak once it emerges in metropolitan areas. The closure of Wuhan city was effective in reducing the severity of the epidemic, but even after closure of the city and the subsequent expansion of that closure to other parts of Hubei the virus remained extremely infectious. Emergency planners in other cities should consider this high infectiousness when considering responses to this virus.</jats:p></jats:sec><jats:sec><jats:title>Funding</jats:title><jats:p>National Natural Science Foundation of China, China Medical Board, National Science and Technology Major Project of China</jats:p></jats:sec>","abstract background the 2019 novel coronavirus (covid-19) emerged in wuhan, china in december 2019 and has been spreading rapidly in china. decisions about its pandemic threat and the appropriate level of public health response depend heavily on estimates of its basic reproduction number and assessments of interventions conducted in the early stages of the epidemic. methods we conducted a mathematical modeling study using five independent methods to assess the basic reproduction number (r0) of covid-19, using data on confirmed cases obtained from the china national health commission for the period 10 th january – 8 th february. we analyzed the data for the period before the closure of wuhan city (10 th january – 23 rd january) and the post-closure period (23 rd january – 8 th february) and for the whole period, to assess both the epidemic risk of the virus and the effectiveness of the closure of wuhan city on spread of covid-19. findings before the closure of wuhan city the basic reproduction number of covid-19 was 4.38 (95% ci: 3.63 – 5.13), dropping to 3.41 (95% ci: 3.16 – 3.65) after the closure of wuhan city. over the entire epidemic period covid-19 had a basic reproduction number of 3.39 (95% ci: 3.09 – 3.70), indicating it has a very high transmissibility. interpretation covid-19 is a highly transmissible virus with a very high risk of epidemic outbreak once it emerges in metropolitan areas. the closure of wuhan city was effective in reducing the severity of the epidemic, but even after closure of the city and the subsequent expansion of that closure to other parts of hubei the virus remained extremely infectious. emergency planners in other cities should consider this high infectiousness when considering responses to this virus. funding national natural science foundation of china, china medical board, national science and technology major project of china"
http://orkg.org/orkg/resource/R36128,Risk estimation and prediction by modeling the transmission of the novel coronavirus (COVID-19) in mainland China excluding Hubei province,10.1101/2020.03.01.20029629,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>In December 2019, an outbreak of coronavirus disease (COVID-19) was identified in Wuhan, China and, later on, detected in other parts of China. Our aim is to evaluate the effectiveness of the evolution of interventions and self-protection measures, estimate the risk of partial lifting control measures and predict the epidemic trend of the virus in mainland China excluding Hubei province based on the published data and a novel mathematical model.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>A novel COVID-19 transmission dynamic model incorporating the intervention measures implemented in China is proposed. COVID-19 daily data of mainland China excluding Hubei province, including the cumulative confirmed cases, the cumulative deaths, newly confirmed cases and the cumulative recovered cases for the period January 20th-March 3rd, 2020, were archived from the National Health Commission of China (NHCC). We parameterize the model by using the Markov Chain Monte Carlo (MCMC) method and estimate the control reproduction number <jats:italic>R</jats:italic><jats:sub><jats:italic>c</jats:italic></jats:sub>, as well as the effective daily reproduction ratio <jats:italic>R</jats:italic><jats:sub><jats:italic>e</jats:italic></jats:sub>(<jats:italic>t</jats:italic>), of the disease transmission in mainland China excluding Hubei province.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The estimation outcomes indicate that <jats:italic>R</jats:italic><jats:sub><jats:italic>c</jats:italic></jats:sub> is 3.36 (95% CI 3.20-3.64) and <jats:italic>R</jats:italic><jats:sub><jats:italic>e</jats:italic></jats:sub>(<jats:italic>t</jats:italic>) has dropped below 1 since January 31st, 2020, which implies that the containment strategies implemented by the Chinese government in mainland China excluding Hubei province are indeed effective and magnificently suppressed COVID-19 transmission. Moreover, our results show that relieving personal protection too early may lead to the spread of disease for a longer time and more people would be infected, and may even cause epidemic or outbreak again. By calculating the effective reproduction ratio, we prove that the contact rate should be kept at least less than 30% of the normal level by April, 2020.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>To ensure the epidemic ending rapidly, it is necessary to maintain the current integrated restrict interventions and self-protection measures, including travel restriction, quarantine of entry, contact tracing followed by quarantine and isolation and reduction of contact, like wearing masks, etc. People should be fully aware of the real-time epidemic situation and keep sufficient personal protection until April. If all the above conditions are met, the outbreak is expected to be ended by April in mainland China apart from Hubei province.</jats:p></jats:sec>","abstract background in december 2019, an outbreak of coronavirus disease (covid-19) was identified in wuhan, china and, later on, detected in other parts of china. our aim is to evaluate the effectiveness of the evolution of interventions and self-protection measures, estimate the risk of partial lifting control measures and predict the epidemic trend of the virus in mainland china excluding hubei province based on the published data and a novel mathematical model. methods a novel covid-19 transmission dynamic model incorporating the intervention measures implemented in china is proposed. covid-19 daily data of mainland china excluding hubei province, including the cumulative confirmed cases, the cumulative deaths, newly confirmed cases and the cumulative recovered cases for the period january 20th-march 3rd, 2020, were archived from the national health commission of china (nhcc). we parameterize the model by using the markov chain monte carlo (mcmc) method and estimate the control reproduction number r c , as well as the effective daily reproduction ratio r e ( t ), of the disease transmission in mainland china excluding hubei province. results the estimation outcomes indicate that r c is 3.36 (95% ci 3.20-3.64) and r e ( t ) has dropped below 1 since january 31st, 2020, which implies that the containment strategies implemented by the chinese government in mainland china excluding hubei province are indeed effective and magnificently suppressed covid-19 transmission. moreover, our results show that relieving personal protection too early may lead to the spread of disease for a longer time and more people would be infected, and may even cause epidemic or outbreak again. by calculating the effective reproduction ratio, we prove that the contact rate should be kept at least less than 30% of the normal level by april, 2020. conclusions to ensure the epidemic ending rapidly, it is necessary to maintain the current integrated restrict interventions and self-protection measures, including travel restriction, quarantine of entry, contact tracing followed by quarantine and isolation and reduction of contact, like wearing masks, etc. people should be fully aware of the real-time epidemic situation and keep sufficient personal protection until april. if all the above conditions are met, the outbreak is expected to be ended by april in mainland china apart from hubei province."
http://orkg.org/orkg/resource/R36118,"The Novel Coronavirus, 2019-nCoV, is Highly Contagious and More Infectious Than Initially Estimated",10.1101/2020.02.07.20021154,crossref,"<jats:title>Abstract</jats:title><jats:p>The novel coronavirus (2019-nCoV) is a recently emerged human pathogen that has spread widely since January 2020. Initially, the basic reproductive number, <jats:italic>R</jats:italic><jats:sub>0</jats:sub>, was estimated to be 2.2 to 2.7. Here we provide a new estimate of this quantity. We collected extensive individual case reports and estimated key epidemiology parameters, including the incubation period. Integrating these estimates and high-resolution real-time human travel and infection data with mathematical models, we estimated that the number of infected individuals during early epidemic double every 2.4 days, and the <jats:italic>R</jats:italic><jats:sub>0</jats:sub> value is likely to be between 4.7 and 6.6. We further show that quarantine and contact tracing of symptomatic individuals alone may not be effective and early, strong control measures are needed to stop transmission of the virus.</jats:p><jats:sec><jats:title>One-sentence summary</jats:title><jats:p>By collecting and analyzing spatiotemporal data, we estimated the transmission potential for 2019-nCoV.</jats:p></jats:sec>","abstract the novel coronavirus (2019-ncov) is a recently emerged human pathogen that has spread widely since january 2020. initially, the basic reproductive number, r 0 , was estimated to be 2.2 to 2.7. here we provide a new estimate of this quantity. we collected extensive individual case reports and estimated key epidemiology parameters, including the incubation period. integrating these estimates and high-resolution real-time human travel and infection data with mathematical models, we estimated that the number of infected individuals during early epidemic double every 2.4 days, and the r 0 value is likely to be between 4.7 and 6.6. we further show that quarantine and contact tracing of symptomatic individuals alone may not be effective and early, strong control measures are needed to stop transmission of the virus. one-sentence summary by collecting and analyzing spatiotemporal data, we estimated the transmission potential for 2019-ncov."
http://orkg.org/orkg/resource/R36130,Assessing the plausibility of subcritical transmission of 2019-nCoV in the United States,10.1101/2020.02.08.20021311,crossref,<jats:title>Abstract</jats:title><jats:p>Rapid assessment of the transmission potential of an emerging or reemerging pathogen is a cornerstone of public health response. A simple approach is shown for using the number of disease introductions and secondary cases to determine whether the upper bound of the reproduction number exceeds the critical value of one.</jats:p>,abstract rapid assessment of the transmission potential of an emerging or reemerging pathogen is a cornerstone of public health response. a simple approach is shown for using the number of disease introductions and secondary cases to determine whether the upper bound of the reproduction number exceeds the critical value of one.
http://orkg.org/orkg/resource/R36132,Lessons drawn from China and South Korea for managing COVID-19 epidemic: insights from a comparative modeling study,10.1101/2020.03.09.20033464,crossref,"<jats:title>Abstract</jats:title><jats:p>We conducted a comparative study of COVID-19 epidemic in three different settings: mainland China, the Guangdong province of China and South Korea, by formulating two disease transmission dynamics models incorporating epidemic characteristics and setting-specific interventions, and fitting the models to multi-source data to identify initial and effective reproduction numbers and evaluate effectiveness of interventions. We estimated the initial basic reproduction number for South Korea, the Guangdong province and mainland China as 2.6 (95% confidence interval (CI): (2.5, 2.7)), 3.0 (95%CI: (2.6, 3.3)) and 3.8 (95%CI: (3.5,4.2)), respectively, given a serial interval with mean of 5 days with standard deviation of 3 days. We found that the effective reproduction number for the Guangdong province and mainland China has fallen below the threshold 1 since February 8<jats:sup>th</jats:sup> and 18<jats:sup>th</jats:sup> respectively, while the effective reproduction number for South Korea remains high, suggesting that the interventions implemented need to be enhanced in order to halt further infections. We also project the epidemic trend in South Korea under different scenarios where a portion or the entirety of the integrated package of interventions in China is used. We show that a coherent and integrated approach with stringent public health interventions is the key to the success of containing the epidemic in China and specially its provinces outside its epicenter, and we show that this approach can also be effective to mitigate the burden of the COVID-19 epidemic in South Korea. The experience of outbreak control in mainland China should be a guiding reference for the rest of the world including South Korea.</jats:p>","abstract we conducted a comparative study of covid-19 epidemic in three different settings: mainland china, the guangdong province of china and south korea, by formulating two disease transmission dynamics models incorporating epidemic characteristics and setting-specific interventions, and fitting the models to multi-source data to identify initial and effective reproduction numbers and evaluate effectiveness of interventions. we estimated the initial basic reproduction number for south korea, the guangdong province and mainland china as 2.6 (95% confidence interval (ci): (2.5, 2.7)), 3.0 (95%ci: (2.6, 3.3)) and 3.8 (95%ci: (3.5,4.2)), respectively, given a serial interval with mean of 5 days with standard deviation of 3 days. we found that the effective reproduction number for the guangdong province and mainland china has fallen below the threshold 1 since february 8 th and 18 th respectively, while the effective reproduction number for south korea remains high, suggesting that the interventions implemented need to be enhanced in order to halt further infections. we also project the epidemic trend in south korea under different scenarios where a portion or the entirety of the integrated package of interventions in china is used. we show that a coherent and integrated approach with stringent public health interventions is the key to the success of containing the epidemic in china and specially its provinces outside its epicenter, and we show that this approach can also be effective to mitigate the burden of the covid-19 epidemic in south korea. the experience of outbreak control in mainland china should be a guiding reference for the rest of the world including south korea."
http://orkg.org/orkg/resource/R36138,Estimating the generation interval for COVID-19 based on symptom onset data,10.1101/2020.03.05.20031815,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>Estimating key infectious disease parameters from the COVID-19 outbreak is quintessential for modelling studies and guiding intervention strategies. Whereas different estimates for the incubation period distribution and the serial interval distribution have been reported, estimates of the generation interval for COVID-19 have not been provided.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>We used outbreak data from clusters in Singapore and Tianjin, China to estimate the generation interval from symptom onset data while acknowledging uncertainty about the incubation period distribution and the underlying transmission network. From those estimates we obtained the proportions pre-symptomatic transmission and reproduction numbers.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The mean generation interval was 5.20 (95%CI 3.78-6.78) days for Singapore and 3.95 (95%CI 3.01-4.91) days for Tianjin, China when relying on a previously reported incubation period with mean 5.2 and SD 2.8 days. The proportion of pre-symptomatic transmission was 48% (95%CI 32-67%) for Singapore and 62% (95%CI 50-76%) for Tianjin, China. Estimates of the reproduction number based on the generation interval distribution were slightly higher than those based on the serial interval distribution.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>Estimating generation and serial interval distributions from outbreak data requires careful investigation of the underlying transmission network. Detailed contact tracing information is essential for correctly estimating these quantities.</jats:p></jats:sec>","abstract background estimating key infectious disease parameters from the covid-19 outbreak is quintessential for modelling studies and guiding intervention strategies. whereas different estimates for the incubation period distribution and the serial interval distribution have been reported, estimates of the generation interval for covid-19 have not been provided. methods we used outbreak data from clusters in singapore and tianjin, china to estimate the generation interval from symptom onset data while acknowledging uncertainty about the incubation period distribution and the underlying transmission network. from those estimates we obtained the proportions pre-symptomatic transmission and reproduction numbers. results the mean generation interval was 5.20 (95%ci 3.78-6.78) days for singapore and 3.95 (95%ci 3.01-4.91) days for tianjin, china when relying on a previously reported incubation period with mean 5.2 and sd 2.8 days. the proportion of pre-symptomatic transmission was 48% (95%ci 32-67%) for singapore and 62% (95%ci 50-76%) for tianjin, china. estimates of the reproduction number based on the generation interval distribution were slightly higher than those based on the serial interval distribution. conclusions estimating generation and serial interval distributions from outbreak data requires careful investigation of the underlying transmission network. detailed contact tracing information is essential for correctly estimating these quantities."
http://orkg.org/orkg/resource/R36143,A Cybernetics-based Dynamic Infection Model for Analyzing SARS-COV-2 Infection Stability and Predicting Uncontrollable Risks,10.1101/2020.03.13.20034082,crossref,"<jats:title>Abstract</jats:title><jats:p>Since December 2019, COVID-19 has raged in Wuhan and subsequently all over China and the world. We propose a Cybernetics-based Dynamic Infection Model (CDIM) to the dynamic infection process with a probability distributed incubation delay and feedback principle. Reproductive trends and the stability of the SARS-COV-2 infection in a city can then be analyzed, and the uncontrollable risks can be forecasted before they really happen. The infection mechanism of a city is depicted using the philosophy of cybernetics and approaches of the control engineering. Distinguished with other epidemiological models, such as SIR, SEIR, etc., that compute the theoretical number of infected people in a closed population, CDIM considers the immigration and emigration population as system inputs, and administrative and medical resources as dynamic control variables. The epidemic regulation can be simulated in the model to support the decision-making for containing the outbreak. City case studies are demonstrated for verification and validation.</jats:p>","abstract since december 2019, covid-19 has raged in wuhan and subsequently all over china and the world. we propose a cybernetics-based dynamic infection model (cdim) to the dynamic infection process with a probability distributed incubation delay and feedback principle. reproductive trends and the stability of the sars-cov-2 infection in a city can then be analyzed, and the uncontrollable risks can be forecasted before they really happen. the infection mechanism of a city is depicted using the philosophy of cybernetics and approaches of the control engineering. distinguished with other epidemiological models, such as sir, seir, etc., that compute the theoretical number of infected people in a closed population, cdim considers the immigration and emigration population as system inputs, and administrative and medical resources as dynamic control variables. the epidemic regulation can be simulated in the model to support the decision-making for containing the outbreak. city case studies are demonstrated for verification and validation."
http://orkg.org/orkg/resource/R36146,COVID-19 outbreak in Algeria: A mathematical model to predict the incidence,10.1101/2020.03.20.20039891,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Introduction</jats:title><jats:p>Since December 29, 2019 a pandemic of new novel coronavirus-infected pneumonia named COVID-19 has started from Wuhan, China, has led to 254 996 confirmed cases until midday March 20, 2020. Sporadic cases have been imported worldwide, in Algeria, the first case reported on February 25, 2020 was imported from Italy, and then the epidemic has spread to other parts of the country very quickly with 139 confirmed cases until March 21, 2020.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>It is crucial to estimate the cases number growth in the early stages of the outbreak, to this end, we have implemented the Alg-COVID-19 Model which allows to predict the incidence and the reproduction number R0 in the coming months in order to help decision makers.</jats:p><jats:p>The Alg-COVIS-19 Model initial equation 1, estimates the cumulative cases at t prediction time using two parameters: the reproduction number R0 and the serial interval SI.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>We found R0=2.55 based on actual incidence at the first 25 days, using the serial interval SI= 4,4 and the prediction time t=26. The herd immunity HI estimated is HI=61%. Also, The Covid-19 incidence predicted with the Alg-COVID-19 Model fits closely the actual incidence during the first 26 days of the epidemic in Algeria Fig. 1.A. which allows us to use it.</jats:p><jats:p>According to Alg-COVID-19 Model, the number of cases will exceed 5000 on the 42<jats:sup>th</jats:sup> day (April 7<jats:sup>th</jats:sup>) and it will double to 10000 on 46th day of the epidemic (April 11<jats:sup>th</jats:sup>), thus, exponential phase will begin (Table 1; Fig.1.B) and increases continuously until reaching à herd immunity of 61% unless serious preventive measures are considered.</jats:p></jats:sec><jats:sec><jats:title>Discussion</jats:title><jats:p>This model is valid only when the majority of the population is vulnerable to COVID-19 infection, however, it can be updated to fit the new parameters values.</jats:p></jats:sec>","abstract introduction since december 29, 2019 a pandemic of new novel coronavirus-infected pneumonia named covid-19 has started from wuhan, china, has led to 254 996 confirmed cases until midday march 20, 2020. sporadic cases have been imported worldwide, in algeria, the first case reported on february 25, 2020 was imported from italy, and then the epidemic has spread to other parts of the country very quickly with 139 confirmed cases until march 21, 2020. methods it is crucial to estimate the cases number growth in the early stages of the outbreak, to this end, we have implemented the alg-covid-19 model which allows to predict the incidence and the reproduction number r0 in the coming months in order to help decision makers. the alg-covis-19 model initial equation 1, estimates the cumulative cases at t prediction time using two parameters: the reproduction number r0 and the serial interval si. results we found r0=2.55 based on actual incidence at the first 25 days, using the serial interval si= 4,4 and the prediction time t=26. the herd immunity hi estimated is hi=61%. also, the covid-19 incidence predicted with the alg-covid-19 model fits closely the actual incidence during the first 26 days of the epidemic in algeria fig. 1.a. which allows us to use it. according to alg-covid-19 model, the number of cases will exceed 5000 on the 42 th day (april 7 th ) and it will double to 10000 on 46th day of the epidemic (april 11 th ), thus, exponential phase will begin (table 1; fig.1.b) and increases continuously until reaching à herd immunity of 61% unless serious preventive measures are considered. discussion this model is valid only when the majority of the population is vulnerable to covid-19 infection, however, it can be updated to fit the new parameters values."
http://orkg.org/orkg/resource/R36149,Analysis of the epidemic growth of the early 2019-nCoV outbreak using internationally confirmed cases,10.1101/2020.02.06.20020941,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>On January 23, 2020, a quarantine was imposed on travel in and out of Wuhan, where the 2019 novel coronavirus (2019-nCoV) outbreak originated from. Previous analyses estimated the basic epidemiological parameters using symptom onset dates of the confirmed cases in Wuhan and outside China.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>We obtained information on the 46 coronavirus cases who traveled from Wuhan before January 23 and have been subsequently confirmed in Hong Kong, Japan, Korea, Macau, Singapore, and Taiwan as of February 5, 2020. Most cases have detailed travel history and disease progress. Compared to previous analyses, an important distinction is that we used this data to informatively simulate the infection time of each case using the symptom onset time, previously reported incubation interval, and travel history. We then fitted a simple exponential growth model with adjustment for the January 23 travel ban to the distribution of the simulated infection time. We used a Bayesian analysis with diffuse priors to quantify the uncertainty of the estimated epidemiological parameters. We performed sensitivity analysis to different choices of incubation interval and the hyperparameters in the prior specification.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>We found that our model provides good fit to the distribution of the infection time. Assuming the travel rate to the selected countries and regions is constant over the study period, we found that the epidemic was doubling in size every 2.9 days (95% credible interval [CrI], 2 days—4.1 days). Using previously reported serial interval for 2019-nCoV, the estimated basic reproduction number is 5.7 (95% CrI, 3.4—9.2). The estimates did not change substantially if we assumed the travel rate doubled in the last 3 days before January 23, when we used previously reported incubation interval for severe acute respiratory syndrome (SARS), or when we changed the hyperparameters in our prior specification.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>Our estimated epidemiological parameters are higher than an earlier report using confirmed cases in Wuhan. This indicates the 2019-nCoV could have been spreading faster than previous estimates.</jats:p></jats:sec>","abstract background on january 23, 2020, a quarantine was imposed on travel in and out of wuhan, where the 2019 novel coronavirus (2019-ncov) outbreak originated from. previous analyses estimated the basic epidemiological parameters using symptom onset dates of the confirmed cases in wuhan and outside china. methods we obtained information on the 46 coronavirus cases who traveled from wuhan before january 23 and have been subsequently confirmed in hong kong, japan, korea, macau, singapore, and taiwan as of february 5, 2020. most cases have detailed travel history and disease progress. compared to previous analyses, an important distinction is that we used this data to informatively simulate the infection time of each case using the symptom onset time, previously reported incubation interval, and travel history. we then fitted a simple exponential growth model with adjustment for the january 23 travel ban to the distribution of the simulated infection time. we used a bayesian analysis with diffuse priors to quantify the uncertainty of the estimated epidemiological parameters. we performed sensitivity analysis to different choices of incubation interval and the hyperparameters in the prior specification. results we found that our model provides good fit to the distribution of the infection time. assuming the travel rate to the selected countries and regions is constant over the study period, we found that the epidemic was doubling in size every 2.9 days (95% credible interval [cri], 2 days—4.1 days). using previously reported serial interval for 2019-ncov, the estimated basic reproduction number is 5.7 (95% cri, 3.4—9.2). the estimates did not change substantially if we assumed the travel rate doubled in the last 3 days before january 23, when we used previously reported incubation interval for severe acute respiratory syndrome (sars), or when we changed the hyperparameters in our prior specification. conclusions our estimated epidemiological parameters are higher than an earlier report using confirmed cases in wuhan. this indicates the 2019-ncov could have been spreading faster than previous estimates."
http://orkg.org/orkg/resource/R36151,Effects of voluntary event cancellation and school closure as countermeasures against COVID-19 outbreak in Japan,10.1101/2020.03.19.20037945,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>To control the COVID-19 outbreak in Japan, sports and entertainment events were canceled and schools were closed throughout Japan from February 26 through March 19. That policy has been designated as voluntary event cancellation and school closure (VECSC).</jats:p></jats:sec><jats:sec><jats:title>Object</jats:title><jats:p>This study assesses VECSC effectiveness based on predicted outcomes. Method: A simple susceptible–infected–recovery model was applied to data of patients with symptoms in Japan during January 14 through March 25. The respective reproduction numbers were estimated before VECSC (R), during VECSC (R<jats:sub>e</jats:sub>), and after VECSC (R<jats:sub>a</jats:sub>).</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Results suggest R before VECSC as 1.987 [1.908, 2.055], R<jats:sub>e</jats:sub> during VECSC as 1.122 [0.980, 1.260], and R<jats:sub>a</jats:sub> after VECSC as 3.086 [2.529, 3.739].</jats:p></jats:sec><jats:sec><jats:title>Discussion and Conclusion</jats:title><jats:p>Results demonstrated that VECSC can reduce COVID-19 infectiousness considerably, but the value of R rose to exceed 2.5 after VECSC.</jats:p></jats:sec>","abstract background to control the covid-19 outbreak in japan, sports and entertainment events were canceled and schools were closed throughout japan from february 26 through march 19. that policy has been designated as voluntary event cancellation and school closure (vecsc). object this study assesses vecsc effectiveness based on predicted outcomes. method: a simple susceptible–infected–recovery model was applied to data of patients with symptoms in japan during january 14 through march 25. the respective reproduction numbers were estimated before vecsc (r), during vecsc (r e ), and after vecsc (r a ). results results suggest r before vecsc as 1.987 [1.908, 2.055], r e during vecsc as 1.122 [0.980, 1.260], and r a after vecsc as 3.086 [2.529, 3.739]. discussion and conclusion results demonstrated that vecsc can reduce covid-19 infectiousness considerably, but the value of r rose to exceed 2.5 after vecsc."
http://orkg.org/orkg/resource/R37003,Real-Time Estimation of the Risk of Death from Novel Coronavirus (COVID-19) Infection: Inference Using Exported Cases,10.3390/jcm9020523,crossref,"<jats:p>The exported cases of 2019 novel coronavirus (COVID-19) infection that were confirmed outside China provide an opportunity to estimate the cumulative incidence and confirmed case fatality risk (cCFR) in mainland China. Knowledge of the cCFR is critical to characterize the severity and understand the pandemic potential of COVID-19 in the early stage of the epidemic. Using the exponential growth rate of the incidence, the present study statistically estimated the cCFR and the basic reproduction number—the average number of secondary cases generated by a single primary case in a naïve population. We modeled epidemic growth either from a single index case with illness onset on 8 December 2019 (Scenario 1), or using the growth rate fitted along with the other parameters (Scenario 2) based on data from 20 exported cases reported by 24 January 2020. The cumulative incidence in China by 24 January was estimated at 6924 cases (95% confidence interval [CI]: 4885, 9211) and 19,289 cases (95% CI: 10,901, 30,158), respectively. The latest estimated values of the cCFR were 5.3% (95% CI: 3.5%, 7.5%) for Scenario 1 and 8.4% (95% CI: 5.3%, 12.3%) for Scenario 2. The basic reproduction number was estimated to be 2.1 (95% CI: 2.0, 2.2) and 3.2 (95% CI: 2.7, 3.7) for Scenarios 1 and 2, respectively. Based on these results, we argued that the current COVID-19 epidemic has a substantial potential for causing a pandemic. The proposed approach provides insights in early risk assessment using publicly available data.</jats:p>","the exported cases of 2019 novel coronavirus (covid-19) infection that were confirmed outside china provide an opportunity to estimate the cumulative incidence and confirmed case fatality risk (ccfr) in mainland china. knowledge of the ccfr is critical to characterize the severity and understand the pandemic potential of covid-19 in the early stage of the epidemic. using the exponential growth rate of the incidence, the present study statistically estimated the ccfr and the basic reproduction number—the average number of secondary cases generated by a single primary case in a naïve population. we modeled epidemic growth either from a single index case with illness onset on 8 december 2019 (scenario 1), or using the growth rate fitted along with the other parameters (scenario 2) based on data from 20 exported cases reported by 24 january 2020. the cumulative incidence in china by 24 january was estimated at 6924 cases (95% confidence interval [ci]: 4885, 9211) and 19,289 cases (95% ci: 10,901, 30,158), respectively. the latest estimated values of the ccfr were 5.3% (95% ci: 3.5%, 7.5%) for scenario 1 and 8.4% (95% ci: 5.3%, 12.3%) for scenario 2. the basic reproduction number was estimated to be 2.1 (95% ci: 2.0, 2.2) and 3.2 (95% ci: 2.7, 3.7) for scenarios 1 and 2, respectively. based on these results, we argued that the current covid-19 epidemic has a substantial potential for causing a pandemic. the proposed approach provides insights in early risk assessment using publicly available data."
http://orkg.org/orkg/resource/R37006,Estimating the Unreported Number of Novel Coronavirus (2019-nCoV) Cases in China in the First Half of January 2020: A Data-Driven Modelling Analysis of the Early Outbreak,10.3390/jcm9020388,crossref,"<jats:p>Background: In December 2019, an outbreak of respiratory illness caused by a novel coronavirus (2019-nCoV) emerged in Wuhan, China and has swiftly spread to other parts of China and a number of foreign countries. The 2019-nCoV cases might have been under-reported roughly from 1 to 15 January 2020, and thus we estimated the number of unreported cases and the basic reproduction number, R0, of 2019-nCoV. Methods: We modelled the epidemic curve of 2019-nCoV cases, in mainland China from 1 December 2019 to 24 January 2020 through the exponential growth. The number of unreported cases was determined by the maximum likelihood estimation. We used the serial intervals (SI) of infection caused by two other well-known coronaviruses (CoV), Severe Acute Respiratory Syndrome (SARS) and Middle East Respiratory Syndrome (MERS) CoVs, as approximations of the unknown SI for 2019-nCoV to estimate R0. Results: We confirmed that the initial growth phase followed an exponential growth pattern. The under-reporting was likely to have resulted in 469 (95% CI: 403–540) unreported cases from 1 to 15 January 2020. The reporting rate after 17 January 2020 was likely to have increased 21-fold (95% CI: 18–25) in comparison to the situation from 1 to 17 January 2020 on average. We estimated the R0 of 2019-nCoV at 2.56 (95% CI: 2.49–2.63). Conclusion: The under-reporting was likely to have occurred during the first half of January 2020 and should be considered in future investigation.</jats:p>","background: in december 2019, an outbreak of respiratory illness caused by a novel coronavirus (2019-ncov) emerged in wuhan, china and has swiftly spread to other parts of china and a number of foreign countries. the 2019-ncov cases might have been under-reported roughly from 1 to 15 january 2020, and thus we estimated the number of unreported cases and the basic reproduction number, r0, of 2019-ncov. methods: we modelled the epidemic curve of 2019-ncov cases, in mainland china from 1 december 2019 to 24 january 2020 through the exponential growth. the number of unreported cases was determined by the maximum likelihood estimation. we used the serial intervals (si) of infection caused by two other well-known coronaviruses (cov), severe acute respiratory syndrome (sars) and middle east respiratory syndrome (mers) covs, as approximations of the unknown si for 2019-ncov to estimate r0. results: we confirmed that the initial growth phase followed an exponential growth pattern. the under-reporting was likely to have resulted in 469 (95% ci: 403–540) unreported cases from 1 to 15 january 2020. the reporting rate after 17 january 2020 was likely to have increased 21-fold (95% ci: 18–25) in comparison to the situation from 1 to 17 january 2020 on average. we estimated the r0 of 2019-ncov at 2.56 (95% ci: 2.49–2.63). conclusion: the under-reporting was likely to have occurred during the first half of january 2020 and should be considered in future investigation."
http://orkg.org/orkg/resource/R37008,Estimation of the Transmission Risk of the 2019-nCoV and Its Implication for Public Health Interventions,10.3390/jcm9020462,crossref,"<jats:p>Since the emergence of the first cases in Wuhan, China, the novel coronavirus (2019-nCoV) infection has been quickly spreading out to other provinces and neighboring countries. Estimation of the basic reproduction number by means of mathematical modeling can be helpful for determining the potential and severity of an outbreak and providing critical information for identifying the type of disease interventions and intensity. A deterministic compartmental model was devised based on the clinical progression of the disease, epidemiological status of the individuals, and intervention measures. The estimations based on likelihood and model analysis show that the control reproduction number may be as high as 6.47 (95% CI 5.71–7.23). Sensitivity analyses show that interventions, such as intensive contact tracing followed by quarantine and isolation, can effectively reduce the control reproduction number and transmission risk, with the effect of travel restriction adopted by Wuhan on 2019-nCoV infection in Beijing being almost equivalent to increasing quarantine by a 100 thousand baseline value. It is essential to assess how the expensive, resource-intensive measures implemented by the Chinese authorities can contribute to the prevention and control of the 2019-nCoV infection, and how long they should be maintained. Under the most restrictive measures, the outbreak is expected to peak within two weeks (since 23 January 2020) with a significant low peak value. With travel restriction (no imported exposed individuals to Beijing), the number of infected individuals in seven days will decrease by 91.14% in Beijing, compared with the scenario of no travel restriction.</jats:p>","since the emergence of the first cases in wuhan, china, the novel coronavirus (2019-ncov) infection has been quickly spreading out to other provinces and neighboring countries. estimation of the basic reproduction number by means of mathematical modeling can be helpful for determining the potential and severity of an outbreak and providing critical information for identifying the type of disease interventions and intensity. a deterministic compartmental model was devised based on the clinical progression of the disease, epidemiological status of the individuals, and intervention measures. the estimations based on likelihood and model analysis show that the control reproduction number may be as high as 6.47 (95% ci 5.71–7.23). sensitivity analyses show that interventions, such as intensive contact tracing followed by quarantine and isolation, can effectively reduce the control reproduction number and transmission risk, with the effect of travel restriction adopted by wuhan on 2019-ncov infection in beijing being almost equivalent to increasing quarantine by a 100 thousand baseline value. it is essential to assess how the expensive, resource-intensive measures implemented by the chinese authorities can contribute to the prevention and control of the 2019-ncov infection, and how long they should be maintained. under the most restrictive measures, the outbreak is expected to peak within two weeks (since 23 january 2020) with a significant low peak value. with travel restriction (no imported exposed individuals to beijing), the number of infected individuals in seven days will decrease by 91.14% in beijing, compared with the scenario of no travel restriction."
http://orkg.org/orkg/resource/R49453,MetaboMAPS: Pathway sharing and multi-omics data visualization in metabolic context,10.12688/f1000research.23427.2,crossref,"<ns4:p>Metabolic pathways are an important part of systems biology research since they illustrate complex interactions between metabolites, enzymes, and regulators. Pathway maps are drawn to elucidate metabolism or to set data in a metabolic context. We present MetaboMAPS, a web-based platform to visualize numerical data on individual metabolic pathway maps. Metabolic maps can be stored, distributed and downloaded in SVG-format. MetaboMAPS was designed for users without computational background and supports pathway sharing without strict conventions. In addition to existing applications that established standards for well-studied pathways, MetaboMAPS offers a niche for individual, customized pathways beyond common knowledge, supporting ongoing research by creating publication-ready visualizations of experimental data.</ns4:p>","metabolic pathways are an important part of systems biology research since they illustrate complex interactions between metabolites, enzymes, and regulators. pathway maps are drawn to elucidate metabolism or to set data in a metabolic context. we present metabomaps, a web-based platform to visualize numerical data on individual metabolic pathway maps. metabolic maps can be stored, distributed and downloaded in svg-format. metabomaps was designed for users without computational background and supports pathway sharing without strict conventions. in addition to existing applications that established standards for well-studied pathways, metabomaps offers a niche for individual, customized pathways beyond common knowledge, supporting ongoing research by creating publication-ready visualizations of experimental data."
http://orkg.org/orkg/resource/R38043,Semantic federation of product information from structured and unstructured sources,,crossref,"<jats:p>Product-related information can be found in various data sources and formats across the product lifecycle. Effectively exploiting this information requires the federation of these sources, the extraction of implicit information, and the efficient access to this comprehensive knowledge base. Existing solutions for product information management (PIM) are usually restricted to structured information, but most of the business-critical information resides in unstructured documents. We present a generic architecture for federating heterogeneous information from various sources, including the Internet of Things, and argue how this process benefits from using semantic representations. A reference implementation tailor-made to business users is explained and evaluated. We also discuss several issues we experienced that we believe to be valuable for researchers and implementers of semantic information systems, as well as the information retrieval community.</jats:p>","product-related information can be found in various data sources and formats across the product lifecycle. effectively exploiting this information requires the federation of these sources, the extraction of implicit information, and the efficient access to this comprehensive knowledge base. existing solutions for product information management (pim) are usually restricted to structured information, but most of the business-critical information resides in unstructured documents. we present a generic architecture for federating heterogeneous information from various sources, including the internet of things, and argue how this process benefits from using semantic representations. a reference implementation tailor-made to business users is explained and evaluated. we also discuss several issues we experienced that we believe to be valuable for researchers and implementers of semantic information systems, as well as the information retrieval community."
http://orkg.org/orkg/resource/R38074,OntoIMM: An Ontology for Product Intelligent Master Model,,crossref,"<jats:p>Information organizing principle is one of the key issues of intelligent master model (IMM), which is an enhancement of the master model (MM) based on KBE (knowledge-based engineering). Despite the fact that the core product model (CPM) has been confirmed to be an organizing mechanism for product master model, the key issue of supporting the information organizing for IMM is not yet well addressed, mainly due to the following two reasons; (1) lack of representation of complete information and knowledge with regard to product and process, including the know-why, know-how, and know-what information and knowledge, and (2) lack of semantic richness. Therefore, a multiaspect extension to CPM was first defined, and then an ontology was constructed to represent the information and design knowledge. The extension refers to adding a design process model, context model, product control structure model, and design rationale model to CPM concerning the enhancement of master model, which is to comprehensively represent the reason, process, and result information and knowledge of theproduct. The ontology construction refers to representing the concepts, relationships among these concepts and consistency rules of IMM information structure. Finally, an example of barrel design and analysis process is illustrated to verify the effectiveness of proposed method.</jats:p>","information organizing principle is one of the key issues of intelligent master model (imm), which is an enhancement of the master model (mm) based on kbe (knowledge-based engineering). despite the fact that the core product model (cpm) has been confirmed to be an organizing mechanism for product master model, the key issue of supporting the information organizing for imm is not yet well addressed, mainly due to the following two reasons; (1) lack of representation of complete information and knowledge with regard to product and process, including the know-why, know-how, and know-what information and knowledge, and (2) lack of semantic richness. therefore, a multiaspect extension to cpm was first defined, and then an ontology was constructed to represent the information and design knowledge. the extension refers to adding a design process model, context model, product control structure model, and design rationale model to cpm concerning the enhancement of master model, which is to comprehensively represent the reason, process, and result information and knowledge of theproduct. the ontology construction refers to representing the concepts, relationships among these concepts and consistency rules of imm information structure. finally, an example of barrel design and analysis process is illustrated to verify the effectiveness of proposed method."
http://orkg.org/orkg/resource/R44865,Modelling the epidemic trend of the 2019 novel coronavirus outbreak in China,10.1101/2020.01.23.916726,crossref,"<jats:p>We present a timely evaluation of the Chinese 2019-nCov epidemic in its initial phase, where 2019-nCov demonstrates comparable transmissibility but lower fatality rates than SARS and MERS. A quick diagnosis that leads to case isolation and integrated interventions will have a major impact on its future trend. Nevertheless, as China is facing its Spring Festival travel rush and the epidemic has spread beyond its borders, further investigation on its potential spatiotemporal transmission pattern and novel intervention strategies are warranted.</jats:p>","we present a timely evaluation of the chinese 2019-ncov epidemic in its initial phase, where 2019-ncov demonstrates comparable transmissibility but lower fatality rates than sars and mers. a quick diagnosis that leads to case isolation and integrated interventions will have a major impact on its future trend. nevertheless, as china is facing its spring festival travel rush and the epidemic has spread beyond its borders, further investigation on its potential spatiotemporal transmission pattern and novel intervention strategies are warranted."
http://orkg.org/orkg/resource/R70864,"COVID-19 Knowledge Graph: a computable, multi-modal, cause-and-effect knowledge model of COVID-19 pathophysiology",10.1101/2020.04.14.040667,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Summary</jats:title><jats:p>The past few weeks have witnessed a worldwide mobilization of the research community in response to the novel coronavirus (COVID-19). This global response has led to a burst of publications on the pathophysiology of the virus, yet without coordinated efforts to organize this knowledge, it can remain hidden away from individual research groups. By extracting and formalizing this knowledge in a structured and computable form, as in the form of a knowledge graph, researchers can readily reason and analyze this information on a much larger scale. Here, we present the COVID-19 Knowledge Graph, an expansive cause-and-effect network constructed from scientific literature on the new coronavirus that aims to provide a comprehensive view of its pathophysiology. To make this resource available to the research community and facilitate its exploration and analysis, we also implemented a web application and released the KG in multiple standard formats.</jats:p></jats:sec><jats:sec><jats:title>Availability</jats:title><jats:p>The COVID-19 Knowledge Graph is publicly available under CC-0 license at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://github.com/covid19kg"">https://github.com/covid19kg</jats:ext-link> and <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://bikmi.covid19-knowledgespace.de"">https://bikmi.covid19-knowledgespace.de</jats:ext-link>.</jats:p></jats:sec><jats:sec><jats:title>Contact</jats:title><jats:p><jats:email>alpha.tom.kodamullil@scai.fraunhofer.de</jats:email></jats:p></jats:sec><jats:sec><jats:title>Supplementary information</jats:title><jats:p>Supplementary data are available online.</jats:p></jats:sec>","abstract summary the past few weeks have witnessed a worldwide mobilization of the research community in response to the novel coronavirus (covid-19). this global response has led to a burst of publications on the pathophysiology of the virus, yet without coordinated efforts to organize this knowledge, it can remain hidden away from individual research groups. by extracting and formalizing this knowledge in a structured and computable form, as in the form of a knowledge graph, researchers can readily reason and analyze this information on a much larger scale. here, we present the covid-19 knowledge graph, an expansive cause-and-effect network constructed from scientific literature on the new coronavirus that aims to provide a comprehensive view of its pathophysiology. to make this resource available to the research community and facilitate its exploration and analysis, we also implemented a web application and released the kg in multiple standard formats. availability the covid-19 knowledge graph is publicly available under cc-0 license at https://github.com/covid19kg and https://bikmi.covid19-knowledgespace.de . contact alpha.tom.kodamullil@scai.fraunhofer.de supplementary information supplementary data are available online."
http://orkg.org/orkg/resource/R41605,"Serological and molecular findings during SARS-CoV-2 infection: the first case study in Finland, January to February 2020",10.2807/1560-7917.es.2020.25.11.2000266,crossref,"<jats:p>The first case of coronavirus disease (COVID-19) in Finland was confirmed on 29 January 2020. No secondary cases were detected. We describe the clinical picture and laboratory findings 3–23 days since the first symptoms. The SARS-CoV-2/Finland/1/2020 virus strain was isolated, the genome showing a single nucleotide substitution to the reference strain from Wuhan. Neutralising antibody response appeared within 9 days along with specific IgM and IgG response, targeting particularly nucleocapsid and spike proteins.</jats:p>","the first case of coronavirus disease (covid-19) in finland was confirmed on 29 january 2020. no secondary cases were detected. we describe the clinical picture and laboratory findings 3–23 days since the first symptoms. the sars-cov-2/finland/1/2020 virus strain was isolated, the genome showing a single nucleotide substitution to the reference strain from wuhan. neutralising antibody response appeared within 9 days along with specific igm and igg response, targeting particularly nucleocapsid and spike proteins."
http://orkg.org/orkg/resource/R70556,Detecting pathogen exposure during the non-symptomatic incubation period using physiological data,,crossref,"<jats:title>Abstract</jats:title><jats:p>Early pathogen exposure detection allows better patient care and faster implementation of public health measures (patient isolation, contact tracing). Existing exposure detection most frequently relies on overt clinical symptoms, namely fever, during the infectious prodromal period. We have developed a robust machine learning based method to better detect asymptomatic states during the incubation period using subtle, sub-clinical physiological markers. Starting with high-resolution physiological waveform data from non-human primate studies of viral (Ebola, Marburg, Lassa, and Nipah viruses) and bacterial (<jats:italic>Y. pestis</jats:italic>) exposure, we processed the data to reduce short-term variability and normalize diurnal variations, then provided these to a supervised random forest classification algorithm and post-classifier declaration logic step to reduce false alarms. In most subjects detection is achieved well before the onset of fever; subject cross-validation across exposure studies (varying viruses, exposure routes, animal species, and target dose) lead to 51h mean early detection (at 0.93 area under the receiver-operating characteristic curve [AUCROC]). Evaluating the algorithm against entirely independent datasets for Lassa, Nipah, and<jats:italic>Y. pestis</jats:italic>exposures un-used in algorithm training and development yields a mean 51h early warning time (at AUCROC=0.95). We discuss which physiological indicators are most informative for early detection and options for extending this capability to limited datasets such as those available from wearable, non-invasive, ECG-based sensors.</jats:p>","abstract early pathogen exposure detection allows better patient care and faster implementation of public health measures (patient isolation, contact tracing). existing exposure detection most frequently relies on overt clinical symptoms, namely fever, during the infectious prodromal period. we have developed a robust machine learning based method to better detect asymptomatic states during the incubation period using subtle, sub-clinical physiological markers. starting with high-resolution physiological waveform data from non-human primate studies of viral (ebola, marburg, lassa, and nipah viruses) and bacterial ( y. pestis ) exposure, we processed the data to reduce short-term variability and normalize diurnal variations, then provided these to a supervised random forest classification algorithm and post-classifier declaration logic step to reduce false alarms. in most subjects detection is achieved well before the onset of fever; subject cross-validation across exposure studies (varying viruses, exposure routes, animal species, and target dose) lead to 51h mean early detection (at 0.93 area under the receiver-operating characteristic curve [aucroc]). evaluating the algorithm against entirely independent datasets for lassa, nipah, and y. pestis exposures un-used in algorithm training and development yields a mean 51h early warning time (at aucroc=0.95). we discuss which physiological indicators are most informative for early detection and options for extending this capability to limited datasets such as those available from wearable, non-invasive, ecg-based sensors."
http://orkg.org/orkg/resource/R41250,The impact of social distancing and epicenter lockdown on the COVID-19 epidemic in mainland China: A data-driven SEIQR model study,10.1101/2020.03.04.20031187,crossref,"<jats:title>Abstract</jats:title><jats:p>The outbreak of coronavirus disease 2019 (COVID-19) which originated in Wuhan, China, constitutes a public health emergency of international concern with a very high risk of spread and impact at the global level. We developed data-driven susceptible-exposed-infectious-quarantine-recovered (SEIQR) models to simulate the epidemic with the interventions of social distancing and epicenter lockdown. Population migration data combined with officially reported data were used to estimate model parameters, and then calculated the daily exported infected individuals by estimating the daily infected ratio and daily susceptible population size. As of Jan 01, 2020, the estimated initial number of latently infected individuals was 380.1 (95%-CI: 379.8∼381.0). With 30 days of substantial social distancing, the reproductive number in Wuhan and Hubei was reduced from 2.2 (95%-CI: 1.4∼3.9) to 1.58 (95%-CI: 1.34∼2.07), and in other provinces from 2.56 (95%-CI: 2.43∼2.63) to 1.65 (95%-CI: 1.56∼1.76). We found that earlier intervention of social distancing could significantly limit the epidemic in mainland China. The number of infections could be reduced up to 98.9%, and the number of deaths could be reduced by up to 99.3% as of Feb 23, 2020. However, earlier epicenter lockdown would partially neutralize this favorable effect. Because it would cause in situ deteriorating, which overwhelms the improvement out of the epicenter. To minimize the epidemic size and death, stepwise implementation of social distancing in the epicenter city first, then in the province, and later the whole nation without the epicenter lockdown would be practical and cost-effective.</jats:p>","abstract the outbreak of coronavirus disease 2019 (covid-19) which originated in wuhan, china, constitutes a public health emergency of international concern with a very high risk of spread and impact at the global level. we developed data-driven susceptible-exposed-infectious-quarantine-recovered (seiqr) models to simulate the epidemic with the interventions of social distancing and epicenter lockdown. population migration data combined with officially reported data were used to estimate model parameters, and then calculated the daily exported infected individuals by estimating the daily infected ratio and daily susceptible population size. as of jan 01, 2020, the estimated initial number of latently infected individuals was 380.1 (95%-ci: 379.8∼381.0). with 30 days of substantial social distancing, the reproductive number in wuhan and hubei was reduced from 2.2 (95%-ci: 1.4∼3.9) to 1.58 (95%-ci: 1.34∼2.07), and in other provinces from 2.56 (95%-ci: 2.43∼2.63) to 1.65 (95%-ci: 1.56∼1.76). we found that earlier intervention of social distancing could significantly limit the epidemic in mainland china. the number of infections could be reduced up to 98.9%, and the number of deaths could be reduced by up to 99.3% as of feb 23, 2020. however, earlier epicenter lockdown would partially neutralize this favorable effect. because it would cause in situ deteriorating, which overwhelms the improvement out of the epicenter. to minimize the epidemic size and death, stepwise implementation of social distancing in the epicenter city first, then in the province, and later the whole nation without the epicenter lockdown would be practical and cost-effective."
http://orkg.org/orkg/resource/R51006,Are the FAIR Data Principles fair?,10.2218/ijdc.v12i2.567,crossref,"<jats:p>This practice paper describes an ongoing research project to test the effectiveness and relevance of the FAIR Data Principles. Simultaneously, it will analyse how easy it is for data archives to adhere to the principles. The research took place from November 2016 to January 2017, and will be underpinned with feedback from the repositories.&#x0D;\nThe FAIR Data Principles feature 15 facets corresponding to the four letters of FAIR - Findable, Accessible, Interoperable, Reusable. These principles have already gained traction within the research world. The European Commission has recently expanded its demand for research to produce open data. The relevant guidelines1are explicitly written in the context of the FAIR Data Principles. Given an increasing number of researchers will have exposure to the guidelines, understanding their viability and suggesting where there may be room for modification and adjustment is of vital importance.&#x0D;\nThis practice paper is connected to a dataset(Dunning et al.,2017) containing the original overview of the sample group statistics and graphs, in an Excel spreadsheet. Over the course of two months, the web-interfaces, help-pages and metadata-records of over 40 data repositories have been examined, to score the individual data repository against the FAIR principles and facets. The traffic-light rating system enables colour-coding according to compliance and vagueness. The statistical analysis provides overall, categorised, on the principles focussing, and on the facet focussing results.&#x0D;\nThe analysis includes the statistical and descriptive evaluation, followed by elaborations on Elements of the FAIR Data Principles, the subject specific or repository specific differences, and subsequently what repositories can do to improve their information architecture.&#x0D;\n&#x0D;\n(1) H2020 Guidelines on FAIR Data Management:http://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf&#x0D;\n</jats:p>","this practice paper describes an ongoing research project to test the effectiveness and relevance of the fair data principles. simultaneously, it will analyse how easy it is for data archives to adhere to the principles. the research took place from november 2016 to january 2017, and will be underpinned with feedback from the repositories.&#x0d;\nthe fair data principles feature 15 facets corresponding to the four letters of fair - findable, accessible, interoperable, reusable. these principles have already gained traction within the research world. the european commission has recently expanded its demand for research to produce open data. the relevant guidelines1are explicitly written in the context of the fair data principles. given an increasing number of researchers will have exposure to the guidelines, understanding their viability and suggesting where there may be room for modification and adjustment is of vital importance.&#x0d;\nthis practice paper is connected to a dataset(dunning et al.,2017) containing the original overview of the sample group statistics and graphs, in an excel spreadsheet. over the course of two months, the web-interfaces, help-pages and metadata-records of over 40 data repositories have been examined, to score the individual data repository against the fair principles and facets. the traffic-light rating system enables colour-coding according to compliance and vagueness. the statistical analysis provides overall, categorised, on the principles focussing, and on the facet focussing results.&#x0d;\nthe analysis includes the statistical and descriptive evaluation, followed by elaborations on elements of the fair data principles, the subject specific or repository specific differences, and subsequently what repositories can do to improve their information architecture.&#x0d;\n&#x0d;\n(1) h2020 guidelines on fair data management:http://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf&#x0d;\n"
http://orkg.org/orkg/resource/R49045,Differential climate impacts for policy-relevant limits to global warming: the case of 1.5 °C and 2 °C,10.5194/esd-7-327-2016,crossref,"<jats:p>Abstract. Robust appraisals of climate impacts at different levels of global-mean temperature increase are vital to guide assessments of dangerous anthropogenic interference with the climate system. The 2015\xa0Paris Agreement includes a two-headed temperature goal: ""holding the increase in the global average temperature to well below 2\u202f°C above pre-industrial levels and pursuing efforts to limit the temperature increase to 1.5\u202f°C"". Despite the prominence of these two temperature limits, a comprehensive overview of the differences in climate impacts at these levels is still missing. Here we provide an assessment of key impacts of climate change at warming levels of 1.5\u202f°C and 2\u202f°C, including extreme weather events, water availability, agricultural yields, sea-level rise and risk of coral reef loss. Our results reveal substantial differences in impacts between a 1.5\u202f°C and 2\u202f°C warming that are highly relevant for the assessment of dangerous anthropogenic interference with the climate system. For heat-related extremes, the additional 0.5\u202f°C increase in global-mean temperature marks the difference between events at the upper limit of present-day natural variability and a new climate regime, particularly in tropical regions. Similarly, this warming difference is likely to be decisive for the future of tropical coral reefs. In a scenario with an end-of-century warming of 2\u202f°C, virtually all tropical coral reefs are projected to be at risk of severe degradation due to temperature-induced bleaching from\xa02050 onwards. This fraction is reduced to about 90\u202f% in\xa02050 and projected to decline to 70\u202f% by\xa02100 for a 1.5\u202f°C scenario. Analyses of precipitation-related impacts reveal distinct regional differences and hot-spots of change emerge. Regional reduction in median water availability for the Mediterranean is found to nearly double from 9\u202f% to 17\u202f% between 1.5\u202f°C and 2\u202f°C, and the projected lengthening of regional dry spells increases from 7\xa0to 11\u202f%. Projections for agricultural yields differ between crop types as well as world regions. While some (in particular high-latitude) regions may benefit, tropical regions like West Africa, South-East Asia, as well as Central and northern South America are projected to face substantial local yield reductions, particularly for wheat and maize. Best estimate sea-level rise projections based on two illustrative scenarios indicate a 50\u202fcm rise by 2100\xa0relative to year\xa02000-levels for a 2\u202f°C scenario, and about 10\xa0cm lower levels for a 1.5\u202f°C scenario. In a 1.5\u202f°C scenario, the rate of sea-level rise in\xa02100 would be reduced by about 30\u202f% compared to a 2\u202f°C scenario. Our findings highlight the importance of regional differentiation to assess both future climate risks and different vulnerabilities to incremental increases in global-mean temperature. The article provides a consistent and comprehensive assessment of existing projections and a good basis for future work on refining our understanding of the difference between impacts at 1.5\u202f°C and 2\u202f°C warming.</jats:p>","abstract. robust appraisals of climate impacts at different levels of global-mean temperature increase are vital to guide assessments of dangerous anthropogenic interference with the climate system. the 2015\xa0paris agreement includes a two-headed temperature goal: ""holding the increase in the global average temperature to well below 2\u202f°c above pre-industrial levels and pursuing efforts to limit the temperature increase to 1.5\u202f°c"". despite the prominence of these two temperature limits, a comprehensive overview of the differences in climate impacts at these levels is still missing. here we provide an assessment of key impacts of climate change at warming levels of 1.5\u202f°c and 2\u202f°c, including extreme weather events, water availability, agricultural yields, sea-level rise and risk of coral reef loss. our results reveal substantial differences in impacts between a 1.5\u202f°c and 2\u202f°c warming that are highly relevant for the assessment of dangerous anthropogenic interference with the climate system. for heat-related extremes, the additional 0.5\u202f°c increase in global-mean temperature marks the difference between events at the upper limit of present-day natural variability and a new climate regime, particularly in tropical regions. similarly, this warming difference is likely to be decisive for the future of tropical coral reefs. in a scenario with an end-of-century warming of 2\u202f°c, virtually all tropical coral reefs are projected to be at risk of severe degradation due to temperature-induced bleaching from\xa02050 onwards. this fraction is reduced to about 90\u202f% in\xa02050 and projected to decline to 70\u202f% by\xa02100 for a 1.5\u202f°c scenario. analyses of precipitation-related impacts reveal distinct regional differences and hot-spots of change emerge. regional reduction in median water availability for the mediterranean is found to nearly double from 9\u202f% to 17\u202f% between 1.5\u202f°c and 2\u202f°c, and the projected lengthening of regional dry spells increases from 7\xa0to 11\u202f%. projections for agricultural yields differ between crop types as well as world regions. while some (in particular high-latitude) regions may benefit, tropical regions like west africa, south-east asia, as well as central and northern south america are projected to face substantial local yield reductions, particularly for wheat and maize. best estimate sea-level rise projections based on two illustrative scenarios indicate a 50\u202fcm rise by 2100\xa0relative to year\xa02000-levels for a 2\u202f°c scenario, and about 10\xa0cm lower levels for a 1.5\u202f°c scenario. in a 1.5\u202f°c scenario, the rate of sea-level rise in\xa02100 would be reduced by about 30\u202f% compared to a 2\u202f°c scenario. our findings highlight the importance of regional differentiation to assess both future climate risks and different vulnerabilities to incremental increases in global-mean temperature. the article provides a consistent and comprehensive assessment of existing projections and a good basis for future work on refining our understanding of the difference between impacts at 1.5\u202f°c and 2\u202f°c warming."
http://orkg.org/orkg/resource/R39020,A data-driven assessment of early travel restrictions related to the spreading of the novel COVID-19 within mainland China,10.1101/2020.03.05.20031740,crossref,"<jats:p>Two months after it was firstly reported, the novel coronavirus disease COVID-19 has already spread worldwide. However, the vast majority of reported infections have occurred in China. To assess the effect of early travel restrictions adopted by the health authorities in China, we have implemented an epidemic metapopulation model that is fed with mobility data corresponding to 2019 and 2020. This allows to compare two radically different scenarios, one with no travel restrictions and another in which mobility is reduced by a travel ban. Our findings indicate that <jats:italic>i)</jats:italic> travel restrictions are an effective measure in the short term, however, <jats:italic>ii)</jats:italic> they are ineffective when it comes to completely eliminate the disease. The latter is due to the impossibility of removing the risk of seeding the disease to other regions. Our study also highlights the importance of developing more realistic models of behavioral changes when a disease outbreak is unfolding.</jats:p>","two months after it was firstly reported, the novel coronavirus disease covid-19 has already spread worldwide. however, the vast majority of reported infections have occurred in china. to assess the effect of early travel restrictions adopted by the health authorities in china, we have implemented an epidemic metapopulation model that is fed with mobility data corresponding to 2019 and 2020. this allows to compare two radically different scenarios, one with no travel restrictions and another in which mobility is reduced by a travel ban. our findings indicate that i) travel restrictions are an effective measure in the short term, however, ii) they are ineffective when it comes to completely eliminate the disease. the latter is due to the impossibility of removing the risk of seeding the disease to other regions. our study also highlights the importance of developing more realistic models of behavioral changes when a disease outbreak is unfolding."
http://orkg.org/orkg/resource/R49468,Retinal Blood Vessel Segmentation Using Hybrid Features and Multi-Layer Perceptron Neural Networks,10.3390/sym12060894,crossref,"<jats:p>Segmentation of retinal blood vessels is the first step for several computer aided-diagnosis systems (CAD), not only for ocular disease diagnosis such as diabetic retinopathy (DR) but also of non-ocular disease, such as hypertension, stroke and cardiovascular diseases. In this paper, a supervised learning-based method, using a multi-layer perceptron neural network and carefully selected vector of features, is proposed. In particular, for each pixel of a retinal fundus image, we construct a 24-D feature vector, encoding information on the local intensity, morphology transformation, principal moments of phase congruency, Hessian, and difference of Gaussian values. A post-processing technique depending on mathematical morphological operators is used to optimise the segmentation. Moreover, the selected feature vector succeeded in outfitting the symmetric features that provided the final blood vessel probability as a binary map image. The proposed method is tested on three known datasets: Digital Retinal Image for Extraction (DRIVE), Structure Analysis of the Retina (STARE), and CHASED_DB1 datasets. The experimental results, both visual and quantitative, testify to the robustness of the proposed method. This proposed method achieved 0.9607, 0.7542, and 0.9843 in DRIVE, 0.9632, 0.7806, and 0.9825 on STARE, 0.9577, 0.7585 and 0.9846 in CHASE_DB1, with respectable accuracy, sensitivity, and specificity performance metrics. Furthermore, they testify that the method is superior to seven similar state-of-the-art methods.</jats:p>","segmentation of retinal blood vessels is the first step for several computer aided-diagnosis systems (cad), not only for ocular disease diagnosis such as diabetic retinopathy (dr) but also of non-ocular disease, such as hypertension, stroke and cardiovascular diseases. in this paper, a supervised learning-based method, using a multi-layer perceptron neural network and carefully selected vector of features, is proposed. in particular, for each pixel of a retinal fundus image, we construct a 24-d feature vector, encoding information on the local intensity, morphology transformation, principal moments of phase congruency, hessian, and difference of gaussian values. a post-processing technique depending on mathematical morphological operators is used to optimise the segmentation. moreover, the selected feature vector succeeded in outfitting the symmetric features that provided the final blood vessel probability as a binary map image. the proposed method is tested on three known datasets: digital retinal image for extraction (drive), structure analysis of the retina (stare), and chased_db1 datasets. the experimental results, both visual and quantitative, testify to the robustness of the proposed method. this proposed method achieved 0.9607, 0.7542, and 0.9843 in drive, 0.9632, 0.7806, and 0.9825 on stare, 0.9577, 0.7585 and 0.9846 in chase_db1, with respectable accuracy, sensitivity, and specificity performance metrics. furthermore, they testify that the method is superior to seven similar state-of-the-art methods."
http://orkg.org/orkg/resource/R41008,Communicating the Risk of Death from Novel Coronavirus Disease (COVID-19),10.3390/jcm9020580,crossref,"<jats:p>To understand the severity of infection for a given disease, it is common epidemiological practice to estimate the case fatality risk, defined as the risk of death among cases. However, there are three technical obstacles that should be addressed to appropriately measure this risk. First, division of the cumulative number of deaths by that of cases tends to underestimate the actual risk because deaths that will occur have not yet observed, and so the delay in time from illness onset to death must be addressed. Second, the observed dataset of reported cases represents only a proportion of all infected individuals and there can be a substantial number of asymptomatic and mildly infected individuals who are never diagnosed. Third, ascertainment bias and risk of death among all those infected would be smaller when estimated using shorter virus detection windows and less sensitive diagnostic laboratory tests. In the ongoing COVID-19 epidemic, health authorities must cope with the uncertainty in the risk of death from COVID-19, and high-risk individuals should be identified using approaches that can address the abovementioned three problems. Although COVID-19 involves mostly mild infections among the majority of the general population, the risk of death among young adults is higher than that of seasonal influenza, and elderly with underlying comorbidities require additional care.</jats:p>","to understand the severity of infection for a given disease, it is common epidemiological practice to estimate the case fatality risk, defined as the risk of death among cases. however, there are three technical obstacles that should be addressed to appropriately measure this risk. first, division of the cumulative number of deaths by that of cases tends to underestimate the actual risk because deaths that will occur have not yet observed, and so the delay in time from illness onset to death must be addressed. second, the observed dataset of reported cases represents only a proportion of all infected individuals and there can be a substantial number of asymptomatic and mildly infected individuals who are never diagnosed. third, ascertainment bias and risk of death among all those infected would be smaller when estimated using shorter virus detection windows and less sensitive diagnostic laboratory tests. in the ongoing covid-19 epidemic, health authorities must cope with the uncertainty in the risk of death from covid-19, and high-risk individuals should be identified using approaches that can address the abovementioned three problems. although covid-19 involves mostly mild infections among the majority of the general population, the risk of death among young adults is higher than that of seasonal influenza, and elderly with underlying comorbidities require additional care."
http://orkg.org/orkg/resource/R57265,Species diversity and biological invasions: relating local process to community pattern,,crossref,"<jats:p>In a California riparian system, the most diverse natural assemblages are the most invaded by exotic plants. A direct in situ manipulation of local diversity and a seed addition experiment showed that these patterns emerge despite the intrinsic negative effects of diversity on invasions. The results suggest that species loss at small scales may reduce invasion resistance. At community-wide scales, the overwhelming effects of ecological factors spatially covarying with diversity, such as propagule supply, make the most diverse communities most likely to be invaded.</jats:p>","in a california riparian system, the most diverse natural assemblages are the most invaded by exotic plants. a direct in situ manipulation of local diversity and a seed addition experiment showed that these patterns emerge despite the intrinsic negative effects of diversity on invasions. the results suggest that species loss at small scales may reduce invasion resistance. at community-wide scales, the overwhelming effects of ecological factors spatially covarying with diversity, such as propagule supply, make the most diverse communities most likely to be invaded."
http://orkg.org/orkg/resource/R41169,Statistics based predictions of coronavirus 2019-nCoV spreading in mainland China,10.1101/2020.02.12.20021931,crossref,"<jats:title>ABSTRACT</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>The epidemic outbreak cased by coronavirus 2019-nCoV is of great interest to researches because of the high rate of spread of the infection and the significant number of fatalities. A detailed scientific analysis of the phenomenon is yet to come, but the public is already interested in the questions of the duration of the epidemic, the expected number of patients and deaths. For long time predictions, the complicated mathematical models are necessary which need many efforts for unknown parameters identification and calculations. In this article, some preliminary estimates will be presented.</jats:p></jats:sec><jats:sec><jats:title>Objective</jats:title><jats:p>Since the reliable long time data are available only for mainland China, we will try to predict the epidemic characteristics only in this area. We will estimate some of the epidemic characteristics and present the most reliable dependences for victim numbers, infected and removed persons versus time.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>In this study we use the known SIR model for the dynamics of an epidemic, the known exact solution of the linear equations and statistical approach developed before for investigation of the children disease, which occurred in Chernivtsi (Ukraine) in 1988-1989.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The optimal values of the SIR model parameters were identified with the use of statistical approach. The numbers of infected, susceptible and removed persons versus time were predicted.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>Simple mathematical model was used to predict the characteristics of the epidemic caused by coronavirus 2019-nCoV in mainland China. The further research should focus on updating the predictions with the use of fresh data and using more complicated mathematical models.</jats:p></jats:sec>","abstract background the epidemic outbreak cased by coronavirus 2019-ncov is of great interest to researches because of the high rate of spread of the infection and the significant number of fatalities. a detailed scientific analysis of the phenomenon is yet to come, but the public is already interested in the questions of the duration of the epidemic, the expected number of patients and deaths. for long time predictions, the complicated mathematical models are necessary which need many efforts for unknown parameters identification and calculations. in this article, some preliminary estimates will be presented. objective since the reliable long time data are available only for mainland china, we will try to predict the epidemic characteristics only in this area. we will estimate some of the epidemic characteristics and present the most reliable dependences for victim numbers, infected and removed persons versus time. methods in this study we use the known sir model for the dynamics of an epidemic, the known exact solution of the linear equations and statistical approach developed before for investigation of the children disease, which occurred in chernivtsi (ukraine) in 1988-1989. results the optimal values of the sir model parameters were identified with the use of statistical approach. the numbers of infected, susceptible and removed persons versus time were predicted. conclusions simple mathematical model was used to predict the characteristics of the epidemic caused by coronavirus 2019-ncov in mainland china. the further research should focus on updating the predictions with the use of fresh data and using more complicated mathematical models."
http://orkg.org/orkg/resource/R49078,Community climate simulations to assess avoided impacts in 1.5 and 2  °C futures,10.5194/esd-8-827-2017,crossref,"<jats:p>Abstract. The Paris Agreement of December 2015 stated a\xa0goal to pursue efforts to keep global temperatures below 1.5\u202f°C above preindustrial levels and well below 2\u202f°C. The IPCC was charged with assessing climate impacts at these temperature levels, but fully coupled equilibrium climate simulations do not currently exist to inform such assessments. In this study, we produce a\xa0set of scenarios using a\xa0simple model designed to achieve long-term 1.5 and 2\u202f°C temperatures in a\xa0stable climate. These scenarios are then used to produce century-scale ensemble simulations using the Community Earth System Model, providing impact-relevant long-term climate data for stabilization pathways at 1.5 and 2\u202f°C levels and an overshoot 1.5\u202f°C case, which are realized (for the 21st century) in the coupled model and are freely available to the community. Here we describe the design of the simulations and a\xa0brief overview of their impact-relevant climate response. Exceedance of historical record temperature occurs with 60\u202f% greater frequency in the 2\u202f°C climate than in a\xa01.5\u202f°C climate aggregated globally, and with twice the frequency in equatorial and arid regions. Extreme precipitation intensity is statistically significantly higher in a\xa02.0\u202f°C climate than a\xa01.5\u202f°C climate in some specific regions (but not all). The model exhibits large differences in the Arctic, which is ice-free with a\xa0frequency of 1 in 3\xa0years in the 2.0\u202f°C scenario, and 1 in 40\xa0years in the 1.5\u202f°C scenario. Significance of impact differences with respect to multi-model variability is not assessed.\n                    </jats:p>","abstract. the paris agreement of december 2015 stated a\xa0goal to pursue efforts to keep global temperatures below 1.5\u202f°c above preindustrial levels and well below 2\u202f°c. the ipcc was charged with assessing climate impacts at these temperature levels, but fully coupled equilibrium climate simulations do not currently exist to inform such assessments. in this study, we produce a\xa0set of scenarios using a\xa0simple model designed to achieve long-term 1.5 and 2\u202f°c temperatures in a\xa0stable climate. these scenarios are then used to produce century-scale ensemble simulations using the community earth system model, providing impact-relevant long-term climate data for stabilization pathways at 1.5 and 2\u202f°c levels and an overshoot 1.5\u202f°c case, which are realized (for the 21st century) in the coupled model and are freely available to the community. here we describe the design of the simulations and a\xa0brief overview of their impact-relevant climate response. exceedance of historical record temperature occurs with 60\u202f% greater frequency in the 2\u202f°c climate than in a\xa01.5\u202f°c climate aggregated globally, and with twice the frequency in equatorial and arid regions. extreme precipitation intensity is statistically significantly higher in a\xa02.0\u202f°c climate than a\xa01.5\u202f°c climate in some specific regions (but not all). the model exhibits large differences in the arctic, which is ice-free with a\xa0frequency of 1 in 3\xa0years in the 2.0\u202f°c scenario, and 1 in 40\xa0years in the 1.5\u202f°c scenario. significance of impact differences with respect to multi-model variability is not assessed.\n"
http://orkg.org/orkg/resource/R41295,Spanbert: Improving pre-training by representing and predicting spans,,crossref,"<jats:p> We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT<jats:sub>large</jats:sub>, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE. <jats:sup>1</jats:sup> </jats:p>","we present spanbert, a pre-training method that is designed to better represent and predict spans of text. our approach extends bert by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. spanbert consistently outperforms bert and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. in particular, with the same training data and model size as bert large , our single model obtains 94.6% and 88.7% f1 on squad 1.1 and 2.0 respectively. we also achieve a new state of the art on the ontonotes coreference resolution task (79.6% f1), strong performance on the tacred relation extraction benchmark, and even gains on glue. 1"
http://orkg.org/orkg/resource/R41454,Individual homogenization in large-scale systems: on the politics of computer and social architectures,10.1057/s41599-020-0425-4,crossref,"<jats:title>Abstract</jats:title><jats:p>One determining characteristic of contemporary sociopolitical systems is their power over increasingly large and diverse populations. This raises questions about power relations between heterogeneous individuals and increasingly dominant and homogenizing system objectives. This article crosses epistemic boundaries by integrating computer engineering and a historicalphilosophical approach making the general organization of individuals within large-scale systems and corresponding individual homogenization intelligible. From a versatile archeological-genealogical perspective, an analysis of computer and social architectures is conducted that reinterprets Foucault’s <jats:italic>disciplines</jats:italic> and <jats:italic>political anatomy</jats:italic> to establish the notion of politics for a purely technical system. This permits an understanding of system organization as modern technology with application to technical and social systems alike. Connecting to Heidegger’s notions of the <jats:italic>enframing</jats:italic> (<jats:italic>Gestell</jats:italic>) and a more <jats:italic>primal truth</jats:italic> (<jats:italic>anfänglicheren Wahrheit)</jats:italic>, the recognition of politics in differently developing systems then challenges the immutability of contemporary organization. Following this critique of modernity and within the conceptualization of system organization, Derrida’s <jats:italic>democracy to come (à venir)</jats:italic> is then reformulated more abstractly as <jats:italic>organizations to come</jats:italic>. Through the integration of the discussed concepts, the framework of <jats:italic>Large-Scale Systems Composed of Homogeneous Individuals</jats:italic> (LSSCHI) is proposed, problematizing the relationships between individuals, structure, activity, and power within large-scale systems. The LSSCHI framework highlights the conflict of homogenizing system-level objectives and individual heterogeneity, and outlines power relations and mechanisms of control shared across different social and technical systems.</jats:p>","abstract one determining characteristic of contemporary sociopolitical systems is their power over increasingly large and diverse populations. this raises questions about power relations between heterogeneous individuals and increasingly dominant and homogenizing system objectives. this article crosses epistemic boundaries by integrating computer engineering and a historicalphilosophical approach making the general organization of individuals within large-scale systems and corresponding individual homogenization intelligible. from a versatile archeological-genealogical perspective, an analysis of computer and social architectures is conducted that reinterprets foucault’s disciplines and political anatomy to establish the notion of politics for a purely technical system. this permits an understanding of system organization as modern technology with application to technical and social systems alike. connecting to heidegger’s notions of the enframing ( gestell ) and a more primal truth ( anfänglicheren wahrheit) , the recognition of politics in differently developing systems then challenges the immutability of contemporary organization. following this critique of modernity and within the conceptualization of system organization, derrida’s democracy to come (à venir) is then reformulated more abstractly as organizations to come . through the integration of the discussed concepts, the framework of large-scale systems composed of homogeneous individuals (lsschi) is proposed, problematizing the relationships between individuals, structure, activity, and power within large-scale systems. the lsschi framework highlights the conflict of homogenizing system-level objectives and individual heterogeneity, and outlines power relations and mechanisms of control shared across different social and technical systems."
http://orkg.org/orkg/resource/R49446,The Impact of Pyroglutamate: Sulfolobus acidocaldarius Has a Growth Advantage over Saccharolobus solfataricus in Glutamate-Containing Media,10.1155/2019/3208051,crossref,"<jats:p>Microorganisms are well adapted to their habitat but are partially sensitive to toxic metabolites or abiotic compounds secreted by other organisms or chemically formed under the respective environmental conditions. Thermoacidophiles are challenged by pyroglutamate, a lactam that is spontaneously formed by cyclization of glutamate under aerobic thermoacidophilic conditions. It is known that growth of the thermoacidophilic crenarchaeon<jats:italic>Saccharolobus solfataricus</jats:italic>(formerly<jats:italic>Sulfolobus solfataricus</jats:italic>) is completely inhibited by pyroglutamate. In the present study, we investigated the effect of pyroglutamate on the growth of<jats:italic>S. solfataricus</jats:italic>and the closely related crenarchaeon<jats:italic>Sulfolobus acidocaldarius.</jats:italic>In contrast to<jats:italic>S. solfataricus</jats:italic>,<jats:italic>S. acidocaldarius</jats:italic>was successfully cultivated with pyroglutamate as a sole carbon source. Bioinformatical analyses showed that both members of the<jats:italic>Sulfolobaceae</jats:italic>have at least one candidate for a 5-oxoprolinase, which catalyses the ATP-dependent conversion of pyroglutamate to glutamate. In<jats:italic>S. solfataricus</jats:italic>, we observed the intracellular accumulation of pyroglutamate and crude cell extract assays showed a less effective degradation of pyroglutamate. Apparently,<jats:italic>S. acidocaldarius</jats:italic>seems to be less versatile regarding carbohydrates and prefers peptidolytic growth compared to<jats:italic>S. solfataricus</jats:italic>. Concludingly,<jats:italic>S. acidocaldarius</jats:italic>exhibits a more efficient utilization of pyroglutamate and is not inhibited by this compound, making it a better candidate for applications with glutamate-containing media at high temperatures.</jats:p>","microorganisms are well adapted to their habitat but are partially sensitive to toxic metabolites or abiotic compounds secreted by other organisms or chemically formed under the respective environmental conditions. thermoacidophiles are challenged by pyroglutamate, a lactam that is spontaneously formed by cyclization of glutamate under aerobic thermoacidophilic conditions. it is known that growth of the thermoacidophilic crenarchaeon saccharolobus solfataricus (formerly sulfolobus solfataricus ) is completely inhibited by pyroglutamate. in the present study, we investigated the effect of pyroglutamate on the growth of s. solfataricus and the closely related crenarchaeon sulfolobus acidocaldarius. in contrast to s. solfataricus , s. acidocaldarius was successfully cultivated with pyroglutamate as a sole carbon source. bioinformatical analyses showed that both members of the sulfolobaceae have at least one candidate for a 5-oxoprolinase, which catalyses the atp-dependent conversion of pyroglutamate to glutamate. in s. solfataricus , we observed the intracellular accumulation of pyroglutamate and crude cell extract assays showed a less effective degradation of pyroglutamate. apparently, s. acidocaldarius seems to be less versatile regarding carbohydrates and prefers peptidolytic growth compared to s. solfataricus . concludingly, s. acidocaldarius exhibits a more efficient utilization of pyroglutamate and is not inhibited by this compound, making it a better candidate for applications with glutamate-containing media at high temperatures."
http://orkg.org/orkg/resource/R44087,Modelling the Potential Health Impact of the COVID-19 Pandemic on a Hypothetical European Country,10.1101/2020.03.20.20039776,crossref,"<jats:title>Abstract</jats:title><jats:p>A SEIR simulation model for the COVID-19 pandemic was developed (<jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://covidsim.eu"">http://covidsim.eu</jats:ext-link>) and applied to a hypothetical European country of 10 million population. Our results show which interventions potentially push the epidemic peak into the subsequent year (when vaccinations may be available) or which fail. Different levels of control (via contact reduction) resulted in 22% to 63% of the population sick, 0.2% to 0.6% hospitalised, and 0.07% to 0.28% dead (n=6,450 to 28,228).</jats:p>","abstract a seir simulation model for the covid-19 pandemic was developed ( http://covidsim.eu ) and applied to a hypothetical european country of 10 million population. our results show which interventions potentially push the epidemic peak into the subsequent year (when vaccinations may be available) or which fail. different levels of control (via contact reduction) resulted in 22% to 63% of the population sick, 0.2% to 0.6% hospitalised, and 0.07% to 0.28% dead (n=6,450 to 28,228)."
http://orkg.org/orkg/resource/R51373,Identification of antiviral drug candidates against SARS-CoV-2 from FDA-approved drugs,10.1101/2020.03.20.999730,crossref,"<jats:title>Abstract</jats:title><jats:p>COVID-19 is an emerging infectious disease and was recently declared as a pandemic by WHO. Currently, there is no vaccine or therapeutic available for this disease. Drug repositioning represents the only feasible option to address this global challenge and a panel of 48 FDA-approved drugs that have been pre-selected by an assay of SARS-CoV was screened to identify potential antiviral drug candidates against SARS-CoV-2 infection. We found a total of 24 drugs which exhibited antiviral efficacy (0.1 μM &lt; IC<jats:sub>50</jats:sub> &lt; 10 μM) against SARS-CoV-2. In particular, two FDA-approved drugs - niclosamide and ciclesonide – were notable in some respects. These drugs will be tested in an appropriate animal model for their antiviral activities. In near future, these already FDA-approved drugs could be further developed following clinical trials in order to provide additional therapeutic options for patients with COVID-19.</jats:p>","abstract covid-19 is an emerging infectious disease and was recently declared as a pandemic by who. currently, there is no vaccine or therapeutic available for this disease. drug repositioning represents the only feasible option to address this global challenge and a panel of 48 fda-approved drugs that have been pre-selected by an assay of sars-cov was screened to identify potential antiviral drug candidates against sars-cov-2 infection. we found a total of 24 drugs which exhibited antiviral efficacy (0.1 μm &lt; ic 50 &lt; 10 μm) against sars-cov-2. in particular, two fda-approved drugs - niclosamide and ciclesonide – were notable in some respects. these drugs will be tested in an appropriate animal model for their antiviral activities. in near future, these already fda-approved drugs could be further developed following clinical trials in order to provide additional therapeutic options for patients with covid-19."
http://orkg.org/orkg/resource/R44157,BioBERT: a pre-trained biomedical language representation model for biomedical text mining,10.1093/bioinformatics/btz682,crossref,"<jats:title>Abstract</jats:title>\n               <jats:sec>\n                  <jats:title>Motivation</jats:title>\n                  <jats:p>Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Results</jats:title>\n                  <jats:p>We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Availability and implementation</jats:title>\n                  <jats:p>We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.</jats:p>\n               </jats:sec>","abstract \n \n motivation \n biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. with the progress in natural language processing (nlp), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. however, directly applying the advancements in nlp to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. in this article, we investigate how the recently introduced pre-trained language model bert can be adapted for biomedical corpora. \n \n \n results \n we introduce biobert (bidirectional encoder representations from transformers for biomedical text mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. with almost the same architecture across tasks, biobert largely outperforms bert and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. while bert obtains performance comparable to that of previous state-of-the-art models, biobert significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% f1 score improvement), biomedical relation extraction (2.80% f1 score improvement) and biomedical question answering (12.24% mrr improvement). our analysis results show that pre-training bert on biomedical corpora helps it to understand complex biomedical texts. \n \n \n availability and implementation \n we make the pre-trained weights of biobert freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning biobert available at https://github.com/dmis-lab/biobert. \n"
http://orkg.org/orkg/resource/R44210,Semantic relation classification via bidirectional LSTM networks with entity-aware attention using latent entity typing,,crossref,"<jats:p>Classifying semantic relations between entity pairs in sentences is an important task in natural language processing (NLP). Most previous models applied to relation classification rely on high-level lexical and syntactic features obtained by NLP tools such as WordNet, the dependency parser, part-of-speech (POS) tagger, and named entity recognizers (NER). In addition, state-of-the-art neural models based on attention mechanisms do not fully utilize information related to the entity, which may be the most crucial feature for relation classification. To address these issues, we propose a novel end-to-end recurrent neural model that incorporates an entity-aware attention mechanism with a latent entity typing (LET) method. Our model not only effectively utilizes entities and their latent types as features, but also builds word representations by applying self-attention based on symmetrical similarity of a sentence itself. Moreover, the model is interpretable by visualizing applied attention mechanisms. Experimental results obtained with the SemEval-2010 Task 8 dataset, which is one of the most popular relation classification tasks, demonstrate that our model outperforms existing state-of-the-art models without any high-level features.</jats:p>","classifying semantic relations between entity pairs in sentences is an important task in natural language processing (nlp). most previous models applied to relation classification rely on high-level lexical and syntactic features obtained by nlp tools such as wordnet, the dependency parser, part-of-speech (pos) tagger, and named entity recognizers (ner). in addition, state-of-the-art neural models based on attention mechanisms do not fully utilize information related to the entity, which may be the most crucial feature for relation classification. to address these issues, we propose a novel end-to-end recurrent neural model that incorporates an entity-aware attention mechanism with a latent entity typing (let) method. our model not only effectively utilizes entities and their latent types as features, but also builds word representations by applying self-attention based on symmetrical similarity of a sentence itself. moreover, the model is interpretable by visualizing applied attention mechanisms. experimental results obtained with the semeval-2010 task 8 dataset, which is one of the most popular relation classification tasks, demonstrate that our model outperforms existing state-of-the-art models without any high-level features."
http://orkg.org/orkg/resource/R46146,A hybrid of CdS/HCa2Nb3O10 ultrathin nanosheets for promoting photocatalytic hydrogen eVolution,10.1039/C7DT03027D,crossref,<p>A hybrid of CdS/HCa<sub>2</sub>Nb<sub>3</sub>O<sub>10</sub> ultrathin nanosheets with a tough heterointerface was successfully fabricated. Efficient interfacial charge transfer from CdS to HCa<sub>2</sub>Nb<sub>3</sub>O<sub>10</sub> nanosheets was achieved to realize the enhanced photocatalytic H<sub>2</sub> evolution activity.</p>,a hybrid of cds/hca 2 nb 3 o 10 ultrathin nanosheets with a tough heterointerface was successfully fabricated. efficient interfacial charge transfer from cds to hca 2 nb 3 o 10 nanosheets was achieved to realize the enhanced photocatalytic h 2 evolution activity.
http://orkg.org/orkg/resource/R50397,The application of RNA sequencing for the diagnosis and genomic classification of pediatric acute lymphoblastic leukemia,10.1182/bloodadvances.2019001008,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Acute lymphoblastic leukemia (ALL) is the most common childhood malignancy, and implementation of risk-adapted therapy has been instrumental in the dramatic improvements in clinical outcomes. A key to risk-adapted therapies includes the identification of genomic features of individual tumors, including chromosome number (for hyper- and hypodiploidy) and gene fusions, notably ETV6-RUNX1, TCF3-PBX1, and BCR-ABL1 in B-cell ALL (B-ALL). RNA-sequencing (RNA-seq) of large ALL cohorts has expanded the number of recurrent gene fusions recognized as drivers in ALL, and identification of these new entities will contribute to refining ALL risk stratification. We used RNA-seq on 126 ALL patients from our clinical service to test the utility of including RNA-seq in standard-of-care diagnostic pipelines to detect gene rearrangements and IKZF1 deletions. RNA-seq identified 86% of rearrangements detected by standard-of-care diagnostics. KMT2A (MLL) rearrangements, although usually identified, were the most commonly missed by RNA-seq as a result of low expression. RNA-seq identified rearrangements that were not detected by standard-of-care testing in 9 patients. These were found in patients who were not classifiable using standard molecular assessment. We developed an approach to detect the most common IKZF1 deletion from RNA-seq data and validated this using an RQ-PCR assay. We applied an expression classifier to identify Philadelphia chromosome–like B-ALL patients. T-ALL proved a rich source of novel gene fusions, which have clinical implications or provide insights into disease biology. Our experience shows that RNA-seq can be implemented within an individual clinical service to enhance the current molecular diagnostic risk classification of ALL.</jats:p>","abstract \n acute lymphoblastic leukemia (all) is the most common childhood malignancy, and implementation of risk-adapted therapy has been instrumental in the dramatic improvements in clinical outcomes. a key to risk-adapted therapies includes the identification of genomic features of individual tumors, including chromosome number (for hyper- and hypodiploidy) and gene fusions, notably etv6-runx1, tcf3-pbx1, and bcr-abl1 in b-cell all (b-all). rna-sequencing (rna-seq) of large all cohorts has expanded the number of recurrent gene fusions recognized as drivers in all, and identification of these new entities will contribute to refining all risk stratification. we used rna-seq on 126 all patients from our clinical service to test the utility of including rna-seq in standard-of-care diagnostic pipelines to detect gene rearrangements and ikzf1 deletions. rna-seq identified 86% of rearrangements detected by standard-of-care diagnostics. kmt2a (mll) rearrangements, although usually identified, were the most commonly missed by rna-seq as a result of low expression. rna-seq identified rearrangements that were not detected by standard-of-care testing in 9 patients. these were found in patients who were not classifiable using standard molecular assessment. we developed an approach to detect the most common ikzf1 deletion from rna-seq data and validated this using an rq-pcr assay. we applied an expression classifier to identify philadelphia chromosome–like b-all patients. t-all proved a rich source of novel gene fusions, which have clinical implications or provide insights into disease biology. our experience shows that rna-seq can be implemented within an individual clinical service to enhance the current molecular diagnostic risk classification of all."
http://orkg.org/orkg/resource/R44425,"Levetiracetam in the management of feline audiogenic reflex seizures: a randomised, controlled, open-label study",10.1177/1098612X15622806 ,crossref,"<jats:sec><jats:title>Objectives</jats:title><jats:p> Currently, there are no published randomised, controlled veterinary trials evaluating the efficacy of antiepileptic medication in the treatment of myoclonic seizures. Myoclonic seizures are a hallmark of feline audiogenic seizures (FARS). </jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p> This prospective, randomised, open-label trial compared the efficacy and tolerability of levetiracetam (20–25 mg/kg q8h) with phenobarbital (3–5 mg/kg q12h) in cats with suspected FARS that experienced myoclonic seizures. Cats were included that had ⩾12 myoclonic seizure days during a prospective 12 week baseline period. This was followed by a 4 week titration phase (until a therapeutic serum concentration of phenobarbital was achieved) and a 12 week treatment phase. </jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p> Fifty-seven cats completed the study: 28 in the levetiracetam group and 29 in the phenobarbital group. A reduction of ⩾50% in the number of myoclonic seizure days was seen in 100% of patients in the levetiracetam group and in 3% of patients in the phenobarbital group ( P &lt;0.001) during the treatment period. Levetiracetam-treated cats had higher freedom from myoclonic seizures (50.0% vs 0%; P &lt;0.001) during the treatment period. The most common adverse events were lethargy, inappetence and ataxia, with no difference in incidence between levetiracetam and phenobarbital. Adverse events were mild and transient with levetiracetam but persistent with phenobarbital. </jats:p></jats:sec><jats:sec><jats:title>Conclusions and relevance</jats:title><jats:p> These results suggest that levetiracetam is an effective and well tolerated treatment for cats with myoclonic seizures and is more effective than phenobarbital. Whether it will prevent the occurrence of generalised tonic–clonic seizures and other forebrain signs if used early in the course of FARS is not yet clear. </jats:p></jats:sec>","objectives currently, there are no published randomised, controlled veterinary trials evaluating the efficacy of antiepileptic medication in the treatment of myoclonic seizures. myoclonic seizures are a hallmark of feline audiogenic seizures (fars). methods this prospective, randomised, open-label trial compared the efficacy and tolerability of levetiracetam (20–25 mg/kg q8h) with phenobarbital (3–5 mg/kg q12h) in cats with suspected fars that experienced myoclonic seizures. cats were included that had ⩾12 myoclonic seizure days during a prospective 12 week baseline period. this was followed by a 4 week titration phase (until a therapeutic serum concentration of phenobarbital was achieved) and a 12 week treatment phase. results fifty-seven cats completed the study: 28 in the levetiracetam group and 29 in the phenobarbital group. a reduction of ⩾50% in the number of myoclonic seizure days was seen in 100% of patients in the levetiracetam group and in 3% of patients in the phenobarbital group ( p &lt;0.001) during the treatment period. levetiracetam-treated cats had higher freedom from myoclonic seizures (50.0% vs 0%; p &lt;0.001) during the treatment period. the most common adverse events were lethargy, inappetence and ataxia, with no difference in incidence between levetiracetam and phenobarbital. adverse events were mild and transient with levetiracetam but persistent with phenobarbital. conclusions and relevance these results suggest that levetiracetam is an effective and well tolerated treatment for cats with myoclonic seizures and is more effective than phenobarbital. whether it will prevent the occurrence of generalised tonic–clonic seizures and other forebrain signs if used early in the course of fars is not yet clear."
http://orkg.org/orkg/resource/R44434,Efficacy of zonisamide on interictal electroencephalography in familial spontaneous epileptic cats,10.1177/1098612X17740247,crossref,"<jats:sec><jats:title>Objectives</jats:title><jats:p> The effectiveness of zonisamide (ZNS) against spontaneous epilepsy in cats has not yet been described. The purpose of this study was to investigate the effect of ZNS on interictal paroxysmal discharges (PDs) using scalp electroencephalography (EEG) in familial spontaneous epileptic cats (FSECs). </jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p> Eight FSECs were evaluated (six males and two females). Scalp EEG measurements were performed once a week for 3 weeks before ZNS administration (Pre-ZNS). Thereafter, administration of ZNS was started and an adjustment period was instituted until the drug in plasma achieved the steady state. When ZNS in plasma was confirmed to be within 10–40 μg/ml, scalp EEG measurements were performed once a week for 3 weeks (Post-ZNS). The number of PDs (counts/min) were compared between Pre-ZNS and Post-ZNS treatment. </jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p> The median number of PDs for Pre-ZNS and Post-ZNS were 0.43/min (0.13–0.82/min) and 0.28/min (0.07–0.87/min), respectively. The number of PDs Post-ZNS was significantly reduced compared with Pre-ZNS ( P = 0.02). </jats:p></jats:sec><jats:sec><jats:title>Conclusions and relevance</jats:title><jats:p> This study showed that ZNS, within the recommended therapeutic range suggested for use in humans and dogs (10–40 µg/ml), reduced the number of PDs recorded on EEG in FSECs that are considered a model for cats with idiopathic epilepsy. Although phenobarbital is the antiepileptic drug of choice for epileptic cats, the results of this research provide evidence to support the use of ZNS in cats with phenobarbital-resistant epilepsy or for cats that cannot use phenobarbital due to adverse side effects. </jats:p></jats:sec>","objectives the effectiveness of zonisamide (zns) against spontaneous epilepsy in cats has not yet been described. the purpose of this study was to investigate the effect of zns on interictal paroxysmal discharges (pds) using scalp electroencephalography (eeg) in familial spontaneous epileptic cats (fsecs). methods eight fsecs were evaluated (six males and two females). scalp eeg measurements were performed once a week for 3 weeks before zns administration (pre-zns). thereafter, administration of zns was started and an adjustment period was instituted until the drug in plasma achieved the steady state. when zns in plasma was confirmed to be within 10–40 μg/ml, scalp eeg measurements were performed once a week for 3 weeks (post-zns). the number of pds (counts/min) were compared between pre-zns and post-zns treatment. results the median number of pds for pre-zns and post-zns were 0.43/min (0.13–0.82/min) and 0.28/min (0.07–0.87/min), respectively. the number of pds post-zns was significantly reduced compared with pre-zns ( p = 0.02). conclusions and relevance this study showed that zns, within the recommended therapeutic range suggested for use in humans and dogs (10–40 µg/ml), reduced the number of pds recorded on eeg in fsecs that are considered a model for cats with idiopathic epilepsy. although phenobarbital is the antiepileptic drug of choice for epileptic cats, the results of this research provide evidence to support the use of zns in cats with phenobarbital-resistant epilepsy or for cats that cannot use phenobarbital due to adverse side effects."
http://orkg.org/orkg/resource/R44446,Pharmacokinetics and toxicity of zonisamide in cats,10.1016/j.jfms.2008.01.006 ,crossref,"<jats:p> With the eventual goal of making zonisamide (ZNS), a relatively new antiepileptic drug, available for the treatment of epilepsy in cats, the pharmacokinetics after a single oral administration at 10 mg/kg and the toxicity after 9-week daily administration of 20 mg/kg/day of ZNS were studied in healthy cats. Pharmacokinetic parameters obtained with a single administration of ZNS at 10 mg/day were as follows: C<jats:sub>max</jats:sub>=13.1 μg/ml; T<jats:sub>max</jats:sub>=4.0 h; T<jats:sub>1/2</jats:sub>=33.0 h; areas under the curves (AUCs)=720.3 μg/mlh (values represent the medians). The study with daily administrations revealed that the toxicity of ZNS was comparatively low in cats, suggesting that it may be an available drug for cats. However, half of the cats that were administered 20 mg/kg/day daily showed adverse reactions such as anorexia, diarrhoea, vomiting, somnolence and locomotor ataxia. </jats:p>","with the eventual goal of making zonisamide (zns), a relatively new antiepileptic drug, available for the treatment of epilepsy in cats, the pharmacokinetics after a single oral administration at 10 mg/kg and the toxicity after 9-week daily administration of 20 mg/kg/day of zns were studied in healthy cats. pharmacokinetic parameters obtained with a single administration of zns at 10 mg/day were as follows: c max =13.1 μg/ml; t max =4.0 h; t 1/2 =33.0 h; areas under the curves (aucs)=720.3 μg/mlh (values represent the medians). the study with daily administrations revealed that the toxicity of zns was comparatively low in cats, suggesting that it may be an available drug for cats. however, half of the cats that were administered 20 mg/kg/day daily showed adverse reactions such as anorexia, diarrhoea, vomiting, somnolence and locomotor ataxia."
http://orkg.org/orkg/resource/R44458,Therapeutic serum phenobarbital concentrations obtained using chronic transdermal administration of phenobarbital in healthy cats,10.1177/1098612X14545141,crossref,"<jats:p> Seizures are a common cause of neurologic disease, and phenobarbital (PB) is the most commonly used antiepileptic drug. Chronic oral dosing can be challenging for cat owners, leading to poor compliance. The purpose of this study was to determine if the transdermal administration of PB could achieve serum PB concentrations of between 15 and 45 μg/ml in healthy cats. Nineteen healthy cats were enrolled in three groups. Transdermal PB in pluronic lecithin organogel (PLO) was applied to the pinnae for 14 days at a dosage of 3 mg/kg q12h in group 1 (n = 6 cats) and 9 mg/kg q12h in group 2 (n = 7 cats). Transdermal PB in Lipoderm Activemax was similarly applied at 9 mg/kg q12h for 14 days in group 3 (n = 6 cats). Steady-state serum PB concentrations were measured at trough, and at 2, 4 and 6 h after the morning dose on day 15. In group 1, median concentrations ranged from 6.0–7.5 μg/ml throughout the day (observed range 0–11 μg/ml). Group 2 median concentrations were 26.0 μg/ml (observed range 18.0–37.0 μg/ml). For group 3, median concentrations ranged from 15.0–17.0 μg/ml throughout the day (range 5–29 μg/ml). Side effects were mild. One cat was withdrawn from group 2 owing to ataxia and sedation. These results show therapeutic serum PB concentrations can be achieved in cats following chronic transdermal administration of PB in PLO at a dosage of 9 mg/kg q12h. More individual variation was noted using Lipoderm Activemax. Transdermal administration may be an alternative for cats that are difficult to medicate orally. </jats:p>","seizures are a common cause of neurologic disease, and phenobarbital (pb) is the most commonly used antiepileptic drug. chronic oral dosing can be challenging for cat owners, leading to poor compliance. the purpose of this study was to determine if the transdermal administration of pb could achieve serum pb concentrations of between 15 and 45 μg/ml in healthy cats. nineteen healthy cats were enrolled in three groups. transdermal pb in pluronic lecithin organogel (plo) was applied to the pinnae for 14 days at a dosage of 3 mg/kg q12h in group 1 (n = 6 cats) and 9 mg/kg q12h in group 2 (n = 7 cats). transdermal pb in lipoderm activemax was similarly applied at 9 mg/kg q12h for 14 days in group 3 (n = 6 cats). steady-state serum pb concentrations were measured at trough, and at 2, 4 and 6 h after the morning dose on day 15. in group 1, median concentrations ranged from 6.0–7.5 μg/ml throughout the day (observed range 0–11 μg/ml). group 2 median concentrations were 26.0 μg/ml (observed range 18.0–37.0 μg/ml). for group 3, median concentrations ranged from 15.0–17.0 μg/ml throughout the day (range 5–29 μg/ml). side effects were mild. one cat was withdrawn from group 2 owing to ataxia and sedation. these results show therapeutic serum pb concentrations can be achieved in cats following chronic transdermal administration of pb in plo at a dosage of 9 mg/kg q12h. more individual variation was noted using lipoderm activemax. transdermal administration may be an alternative for cats that are difficult to medicate orally."
http://orkg.org/orkg/resource/R44477,Bromide-associated lower airway disease: a retrospective study of seven cats,10.1177/1098612X12445069,crossref,"<jats:p> Seven cats were presented for mild-to-moderate cough and/or dyspnoea after starting bromide (Br) therapy for neurological diseases. The thoracic auscultation was abnormal in three cats showing increased respiratory sounds and wheezes. Haematology revealed mild eosinophilia in one cat. The thoracic radiographs showed bronchial patterns with peribronchial cuffing in most of them. Bronchoalveolar lavage performed in two cats revealed neutrophilic and eosinophilic inflammation. Histopathology conducted in one cat showed endogenous lipid pneumonia (EnLP). All cats improved with steroid therapy after Br discontinuation. Five cats were completely weaned off steroids, with no recurrence of clinical signs. In one cat, the treatment was discontinued despite persistent clinical signs. The cat presenting with EnLP developed secondary pneumothorax and did not recover. Br-associated lower airway disease can appear in cats after months of treatment and clinical improvement occurs only after discontinuing Br therapy. </jats:p>","seven cats were presented for mild-to-moderate cough and/or dyspnoea after starting bromide (br) therapy for neurological diseases. the thoracic auscultation was abnormal in three cats showing increased respiratory sounds and wheezes. haematology revealed mild eosinophilia in one cat. the thoracic radiographs showed bronchial patterns with peribronchial cuffing in most of them. bronchoalveolar lavage performed in two cats revealed neutrophilic and eosinophilic inflammation. histopathology conducted in one cat showed endogenous lipid pneumonia (enlp). all cats improved with steroid therapy after br discontinuation. five cats were completely weaned off steroids, with no recurrence of clinical signs. in one cat, the treatment was discontinued despite persistent clinical signs. the cat presenting with enlp developed secondary pneumothorax and did not recover. br-associated lower airway disease can appear in cats after months of treatment and clinical improvement occurs only after discontinuing br therapy."
http://orkg.org/orkg/resource/R44479,Treatment and long-term follow-up of cats with suspected primary epilepsy,10.1177/1098612X12464627,crossref,"<jats:p> We report an evaluation of the treatment and outcome of cats with suspected primary epilepsy. Phenobarbital therapy was used alone or in combination with other anti-epileptic drugs. Outcome after treatment was evaluated mainly on the basis of number of seizures per year and categorised into four groups: seizure-free, good control (1–5 seizures per year), moderate control (6–10 seizures per year) and poor control (more than 10 seizures per year). About 40–50% of cases became seizure-free, 20–30% were considered good-to-moderately controlled and about 30% were poorly controlled depending on the year of treatment considered. The duration of seizure events after treatment decreased in 26/36 cats and was unchanged in eight cats. The subjective severity of seizure also decreased in 25 cats and was unchanged in nine cats. Twenty-six cats had a good quality of life, nine cats an impaired quality of life and one cat a bad quality of life. Despite being free of seizures for years, cessation of treatment may lead to recurrence of seizures in most cats. </jats:p>","we report an evaluation of the treatment and outcome of cats with suspected primary epilepsy. phenobarbital therapy was used alone or in combination with other anti-epileptic drugs. outcome after treatment was evaluated mainly on the basis of number of seizures per year and categorised into four groups: seizure-free, good control (1–5 seizures per year), moderate control (6–10 seizures per year) and poor control (more than 10 seizures per year). about 40–50% of cases became seizure-free, 20–30% were considered good-to-moderately controlled and about 30% were poorly controlled depending on the year of treatment considered. the duration of seizure events after treatment decreased in 26/36 cats and was unchanged in eight cats. the subjective severity of seizure also decreased in 25 cats and was unchanged in nine cats. twenty-six cats had a good quality of life, nine cats an impaired quality of life and one cat a bad quality of life. despite being free of seizures for years, cessation of treatment may lead to recurrence of seizures in most cats."
http://orkg.org/orkg/resource/R44700,Telephone-based treatment for family practice patients with mild depression,10.2466/pr0.94.3.785-792,crossref,"<jats:p> The need for treating milder forms of depression has recently been of increased interest. This was a randomized, controlled study to evaluate the effects of telephone-based problem-solving therapy for mild depression. Comparison groups were a treatment-as-usual group and another group receiving stress-management training by telephone. From 1,742 family practice patients screened for depression, 54 with mild depression entered the study. Treatment was provided by experienced family practice nurses, trained and supervised in the treatments. The Hamilton Rating Scale for Depression was administered before and after the intervention period, and the Beck Depression Inventory and Duke Health Profile were administered at the end of the intervention period. Of the 36 subjects assigned to the problem-solving and stress-management groups, half dropped out early in the study. Five from the treatment-as-usual group were lost to follow-up. In the remaining subjects, there was a significant decrease in depression scores. There were no significant differences in the amount of decrease between the groups on any scores. The small sample and high dropout rate limit the interpretation of the findings. However, since all subjects tended to improve, regardless of treatment received, mild levels of depression may generally remit even without focal intervention, and watchful waiting may be a reasonable alternative for management. </jats:p>","the need for treating milder forms of depression has recently been of increased interest. this was a randomized, controlled study to evaluate the effects of telephone-based problem-solving therapy for mild depression. comparison groups were a treatment-as-usual group and another group receiving stress-management training by telephone. from 1,742 family practice patients screened for depression, 54 with mild depression entered the study. treatment was provided by experienced family practice nurses, trained and supervised in the treatments. the hamilton rating scale for depression was administered before and after the intervention period, and the beck depression inventory and duke health profile were administered at the end of the intervention period. of the 36 subjects assigned to the problem-solving and stress-management groups, half dropped out early in the study. five from the treatment-as-usual group were lost to follow-up. in the remaining subjects, there was a significant decrease in depression scores. there were no significant differences in the amount of decrease between the groups on any scores. the small sample and high dropout rate limit the interpretation of the findings. however, since all subjects tended to improve, regardless of treatment received, mild levels of depression may generally remit even without focal intervention, and watchful waiting may be a reasonable alternative for management."
http://orkg.org/orkg/resource/R44709,Acute and one-year outcome of a randomised controlled trial of brief cognitive therapy for major depressive disorder in primary care,10.1192/bjp.171.2.131,crossref,"<jats:sec><jats:title>Background</jats:title><jats:p>The consensus statement on the treatment of depression (Paykel &amp; Priest, 1992) advocates the use of cognitive therapy techniques as an adjunct to medication.</jats:p></jats:sec><jats:sec><jats:title>Method</jats:title><jats:p>This paper describes a randomised controlled trial of brief cognitive therapy (BCT) plus ‘treatment as usual’ versus treatment as usual in the management of 48 patients with major depressive disorder presenting in primary care.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>At the end of the acute phase, significantly more subjects (<jats:italic>P</jats:italic> &lt; 0.05) met recovery criteria in the intervention group (<jats:italic>n</jats:italic>=15) compared with the control group (<jats:italic>n</jats:italic>=8). When initial neuroticism scores were controlled for, reductions in Beck Depression Inventory and Hamilton Rating Scale for Depression scores favoured the BCT group throughout the 12 months of follow-up.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>BCT may be beneficial, but given the time constraints, therapists need to be more rather than less skilled in cognitive therapy. This, plus methodological limitations, leads us to advise caution before applying this approach more widely in primary care.</jats:p></jats:sec>","background the consensus statement on the treatment of depression (paykel &amp; priest, 1992) advocates the use of cognitive therapy techniques as an adjunct to medication. method this paper describes a randomised controlled trial of brief cognitive therapy (bct) plus ‘treatment as usual’ versus treatment as usual in the management of 48 patients with major depressive disorder presenting in primary care. results at the end of the acute phase, significantly more subjects ( p &lt; 0.05) met recovery criteria in the intervention group ( n =15) compared with the control group ( n =8). when initial neuroticism scores were controlled for, reductions in beck depression inventory and hamilton rating scale for depression scores favoured the bct group throughout the 12 months of follow-up. conclusions bct may be beneficial, but given the time constraints, therapists need to be more rather than less skilled in cognitive therapy. this, plus methodological limitations, leads us to advise caution before applying this approach more widely in primary care."
http://orkg.org/orkg/resource/R44731,Transmission interval estimates suggest pre-symptomatic spread of COVID-19,10.1101/2020.03.03.20029983,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>As the COVID-19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic. We determine the incubation period and serial interval distribution for transmission clusters in Singapore and in Tianjin. We infer the basic reproduction number and identify the extent of pre-symptomatic transmission.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>We collected outbreak information from Singapore and Tianjin, China, reported from Jan.19-Feb.26 and Jan.21-Feb.27, respectively. We estimated incubation periods and serial intervals in both populations.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The mean incubation period was 7.1 (6.13, 8.25) days for Singapore and 9 (7.92, 10.2) days for Tianjin. Both datasets had shorter incubation periods for earlier-occurring cases. The mean serial interval was 4.56 (2.69, 6.42) days for Singapore and 4.22 (3.43, 5.01) for Tianjin. We inferred that early in the outbreaks, infection was transmitted on average 2.55 and 2.89 days before symptom onset (Singapore, Tianjin). The estimated basic reproduction number for Singapore was 1.97 (1.45, 2.48) secondary cases per infective; for Tianjin it was 1.87 (1.65, 2.09) secondary cases per infective.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>Estimated serial intervals are shorter than incubation periods in both Singapore and Tianjin, suggesting that pre-symptomatic transmission is occurring. Shorter serial intervals lead to lower estimates of R0, which suggest that half of all secondary infections should be prevented to control spread.</jats:p></jats:sec>","abstract background as the covid-19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic. we determine the incubation period and serial interval distribution for transmission clusters in singapore and in tianjin. we infer the basic reproduction number and identify the extent of pre-symptomatic transmission. methods we collected outbreak information from singapore and tianjin, china, reported from jan.19-feb.26 and jan.21-feb.27, respectively. we estimated incubation periods and serial intervals in both populations. results the mean incubation period was 7.1 (6.13, 8.25) days for singapore and 9 (7.92, 10.2) days for tianjin. both datasets had shorter incubation periods for earlier-occurring cases. the mean serial interval was 4.56 (2.69, 6.42) days for singapore and 4.22 (3.43, 5.01) for tianjin. we inferred that early in the outbreaks, infection was transmitted on average 2.55 and 2.89 days before symptom onset (singapore, tianjin). the estimated basic reproduction number for singapore was 1.97 (1.45, 2.48) secondary cases per infective; for tianjin it was 1.87 (1.65, 2.09) secondary cases per infective. conclusions estimated serial intervals are shorter than incubation periods in both singapore and tianjin, suggesting that pre-symptomatic transmission is occurring. shorter serial intervals lead to lower estimates of r0, which suggest that half of all secondary infections should be prevented to control spread."
http://orkg.org/orkg/resource/R44743,Estimation of the epidemic properties of the 2019 novel coronavirus: A mathematical modeling study,10.1101/2020.02.18.20024315,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>The 2019 novel Coronavirus (COVID-19) emerged in Wuhan, China in December 2019 and has been spreading rapidly in China. Decisions about its pandemic threat and the appropriate level of public health response depend heavily on estimates of its basic reproduction number and assessments of interventions conducted in the early stages of the epidemic.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>We conducted a mathematical modeling study using five independent methods to assess the basic reproduction number (R0) of COVID-19, using data on confirmed cases obtained from the China National Health Commission for the period 10<jats:sup>th</jats:sup> January – 8<jats:sup>th</jats:sup> February. We analyzed the data for the period before the closure of Wuhan city (10<jats:sup>th</jats:sup> January – 23<jats:sup>rd</jats:sup> January) and the post-closure period (23<jats:sup>rd</jats:sup> January – 8<jats:sup>th</jats:sup> February) and for the whole period, to assess both the epidemic risk of the virus and the effectiveness of the closure of Wuhan city on spread of COVID-19.</jats:p></jats:sec><jats:sec><jats:title>Findings</jats:title><jats:p>Before the closure of Wuhan city the basic reproduction number of COVID-19 was 4.38 (95% CI: 3.63 – 5.13), dropping to 3.41 (95% CI: 3.16 – 3.65) after the closure of Wuhan city. Over the entire epidemic period COVID-19 had a basic reproduction number of 3.39 (95% CI: 3.09 – 3.70), indicating it has a very high transmissibility.</jats:p></jats:sec><jats:sec><jats:title>Interpretation</jats:title><jats:p>COVID-19 is a highly transmissible virus with a very high risk of epidemic outbreak once it emerges in metropolitan areas. The closure of Wuhan city was effective in reducing the severity of the epidemic, but even after closure of the city and the subsequent expansion of that closure to other parts of Hubei the virus remained extremely infectious. Emergency planners in other cities should consider this high infectiousness when considering responses to this virus.</jats:p></jats:sec><jats:sec><jats:title>Funding</jats:title><jats:p>National Natural Science Foundation of China, China Medical Board, National Science and Technology Major Project of China</jats:p></jats:sec>","abstract background the 2019 novel coronavirus (covid-19) emerged in wuhan, china in december 2019 and has been spreading rapidly in china. decisions about its pandemic threat and the appropriate level of public health response depend heavily on estimates of its basic reproduction number and assessments of interventions conducted in the early stages of the epidemic. methods we conducted a mathematical modeling study using five independent methods to assess the basic reproduction number (r0) of covid-19, using data on confirmed cases obtained from the china national health commission for the period 10 th january – 8 th february. we analyzed the data for the period before the closure of wuhan city (10 th january – 23 rd january) and the post-closure period (23 rd january – 8 th february) and for the whole period, to assess both the epidemic risk of the virus and the effectiveness of the closure of wuhan city on spread of covid-19. findings before the closure of wuhan city the basic reproduction number of covid-19 was 4.38 (95% ci: 3.63 – 5.13), dropping to 3.41 (95% ci: 3.16 – 3.65) after the closure of wuhan city. over the entire epidemic period covid-19 had a basic reproduction number of 3.39 (95% ci: 3.09 – 3.70), indicating it has a very high transmissibility. interpretation covid-19 is a highly transmissible virus with a very high risk of epidemic outbreak once it emerges in metropolitan areas. the closure of wuhan city was effective in reducing the severity of the epidemic, but even after closure of the city and the subsequent expansion of that closure to other parts of hubei the virus remained extremely infectious. emergency planners in other cities should consider this high infectiousness when considering responses to this virus. funding national natural science foundation of china, china medical board, national science and technology major project of china"
http://orkg.org/orkg/resource/R44759,Transmission potential of COVID-19 in Iran,10.1101/2020.03.08.20030643,crossref,"<jats:title>Abstract</jats:title><jats:p>We estimated the reproduction number of 2020 Iranian COVID-19 epidemic using two different methods: R<jats:sub>0</jats:sub> was estimated at 4.4 (95% CI, 3.9, 4.9) (generalized growth model) and 3.50 (1.28, 8.14) (epidemic doubling time) (February 19 - March 1) while the effective R was estimated at 1.55 (1.06, 2.57) (March 6-19).</jats:p>","abstract we estimated the reproduction number of 2020 iranian covid-19 epidemic using two different methods: r 0 was estimated at 4.4 (95% ci, 3.9, 4.9) (generalized growth model) and 3.50 (1.28, 8.14) (epidemic doubling time) (february 19 - march 1) while the effective r was estimated at 1.55 (1.06, 2.57) (march 6-19)."
http://orkg.org/orkg/resource/R44776,Estimating the generation interval for COVID-19 based on symptom onset data,10.1101/2020.03.05.20031815,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>Estimating key infectious disease parameters from the COVID-19 outbreak is quintessential for modelling studies and guiding intervention strategies. Whereas different estimates for the incubation period distribution and the serial interval distribution have been reported, estimates of the generation interval for COVID-19 have not been provided.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>We used outbreak data from clusters in Singapore and Tianjin, China to estimate the generation interval from symptom onset data while acknowledging uncertainty about the incubation period distribution and the underlying transmission network. From those estimates we obtained the proportions pre-symptomatic transmission and reproduction numbers.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>The mean generation interval was 5.20 (95%CI 3.78-6.78) days for Singapore and 3.95 (95%CI 3.01-4.91) days for Tianjin, China when relying on a previously reported incubation period with mean 5.2 and SD 2.8 days. The proportion of pre-symptomatic transmission was 48% (95%CI 32-67%) for Singapore and 62% (95%CI 50-76%) for Tianjin, China. Estimates of the reproduction number based on the generation interval distribution were slightly higher than those based on the serial interval distribution.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>Estimating generation and serial interval distributions from outbreak data requires careful investigation of the underlying transmission network. Detailed contact tracing information is essential for correctly estimating these quantities.</jats:p></jats:sec>","abstract background estimating key infectious disease parameters from the covid-19 outbreak is quintessential for modelling studies and guiding intervention strategies. whereas different estimates for the incubation period distribution and the serial interval distribution have been reported, estimates of the generation interval for covid-19 have not been provided. methods we used outbreak data from clusters in singapore and tianjin, china to estimate the generation interval from symptom onset data while acknowledging uncertainty about the incubation period distribution and the underlying transmission network. from those estimates we obtained the proportions pre-symptomatic transmission and reproduction numbers. results the mean generation interval was 5.20 (95%ci 3.78-6.78) days for singapore and 3.95 (95%ci 3.01-4.91) days for tianjin, china when relying on a previously reported incubation period with mean 5.2 and sd 2.8 days. the proportion of pre-symptomatic transmission was 48% (95%ci 32-67%) for singapore and 62% (95%ci 50-76%) for tianjin, china. estimates of the reproduction number based on the generation interval distribution were slightly higher than those based on the serial interval distribution. conclusions estimating generation and serial interval distributions from outbreak data requires careful investigation of the underlying transmission network. detailed contact tracing information is essential for correctly estimating these quantities."
http://orkg.org/orkg/resource/R44812,Pattern of early human-to-human transmission of Wuhan 2019-nCoV,10.1101/2020.01.23.917351,crossref,"<jats:title>ABSTRACT</jats:title><jats:p>On December 31, 2019, the World Health Organization was notified about a cluster of pneumonia of unknown aetiology in the city of Wuhan, China. Chinese authorities later identified a new coronavirus (2019-nCoV) as the causative agent of the outbreak. As of January 23, 2020, 655 cases have been confirmed in China and several other countries. Understanding the transmission characteristics and the potential for sustained human-to-human transmission of 2019-nCoV is critically important for coordinating current screening and containment strategies, and determining whether the outbreak constitutes a public health emergency of international concern (PHEIC). We performed stochastic simulations of early outbreak trajectories that are consistent with the epidemiological findings to date. We found the basic reproduction number, <jats:italic>R</jats:italic><jats:sub>0</jats:sub>, to be around 2.2 (90% high density interval 1.4—3.8), indicating the potential for sustained human-to-human transmission. Transmission characteristics appear to be of a similar magnitude to severe acute respiratory syndrome-related coronavirus (SARS-CoV) and the 1918 pandemic influenza. These findings underline the importance of heightened screening, surveillance and control efforts, particularly at airports and other travel hubs, in order to prevent further international spread of 2019-nCoV.</jats:p>","abstract on december 31, 2019, the world health organization was notified about a cluster of pneumonia of unknown aetiology in the city of wuhan, china. chinese authorities later identified a new coronavirus (2019-ncov) as the causative agent of the outbreak. as of january 23, 2020, 655 cases have been confirmed in china and several other countries. understanding the transmission characteristics and the potential for sustained human-to-human transmission of 2019-ncov is critically important for coordinating current screening and containment strategies, and determining whether the outbreak constitutes a public health emergency of international concern (pheic). we performed stochastic simulations of early outbreak trajectories that are consistent with the epidemiological findings to date. we found the basic reproduction number, r 0 , to be around 2.2 (90% high density interval 1.4—3.8), indicating the potential for sustained human-to-human transmission. transmission characteristics appear to be of a similar magnitude to severe acute respiratory syndrome-related coronavirus (sars-cov) and the 1918 pandemic influenza. these findings underline the importance of heightened screening, surveillance and control efforts, particularly at airports and other travel hubs, in order to prevent further international spread of 2019-ncov."
http://orkg.org/orkg/resource/R44793,Effects of voluntary event cancellation and school closure as countermeasures against COVID−19 outbreak in Japan,10.1101/2020.03.19.20037945,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>To control the COVID-19 outbreak in Japan, sports and entertainment events were canceled and schools were closed throughout Japan from February 26 through March 19. That policy has been designated as voluntary event cancellation and school closure (VECSC).</jats:p></jats:sec><jats:sec><jats:title>Object</jats:title><jats:p>This study assesses VECSC effectiveness based on predicted outcomes. Method: A simple susceptible–infected–recovery model was applied to data of patients with symptoms in Japan during January 14 through March 25. The respective reproduction numbers were estimated before VECSC (R), during VECSC (R<jats:sub>e</jats:sub>), and after VECSC (R<jats:sub>a</jats:sub>).</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Results suggest R before VECSC as 1.987 [1.908, 2.055], R<jats:sub>e</jats:sub> during VECSC as 1.122 [0.980, 1.260], and R<jats:sub>a</jats:sub> after VECSC as 3.086 [2.529, 3.739].</jats:p></jats:sec><jats:sec><jats:title>Discussion and Conclusion</jats:title><jats:p>Results demonstrated that VECSC can reduce COVID-19 infectiousness considerably, but the value of R rose to exceed 2.5 after VECSC.</jats:p></jats:sec>","abstract background to control the covid-19 outbreak in japan, sports and entertainment events were canceled and schools were closed throughout japan from february 26 through march 19. that policy has been designated as voluntary event cancellation and school closure (vecsc). object this study assesses vecsc effectiveness based on predicted outcomes. method: a simple susceptible–infected–recovery model was applied to data of patients with symptoms in japan during january 14 through march 25. the respective reproduction numbers were estimated before vecsc (r), during vecsc (r e ), and after vecsc (r a ). results results suggest r before vecsc as 1.987 [1.908, 2.055], r e during vecsc as 1.122 [0.980, 1.260], and r a after vecsc as 3.086 [2.529, 3.739]. discussion and conclusion results demonstrated that vecsc can reduce covid-19 infectiousness considerably, but the value of r rose to exceed 2.5 after vecsc."
http://orkg.org/orkg/resource/R44836,Estimating the effective reproduction number of the 2019-nCoV in China,10.1101/2020.01.27.20018952,crossref,<jats:title>Abstract</jats:title><jats:p>We estimate the effective reproduction number for 2019-nCoV based on the daily reported cases from China CDC. The results indicate that 2019-nCoV has a higher effective reproduction number than SARS with a comparable fatality rate.</jats:p><jats:sec><jats:title>Article Summary Line</jats:title><jats:p>This modeling study indicates that 2019-nCoV has a higher effective reproduction number than SARS with a comparable fatality rate.</jats:p></jats:sec>,abstract we estimate the effective reproduction number for 2019-ncov based on the daily reported cases from china cdc. the results indicate that 2019-ncov has a higher effective reproduction number than sars with a comparable fatality rate. article summary line this modeling study indicates that 2019-ncov has a higher effective reproduction number than sars with a comparable fatality rate.
http://orkg.org/orkg/resource/R44847,Novel coronavirus 2019-nCoV: early estimation of epidemiological parameters and epidemic predictions,10.1101/2020.01.23.20018549,crossref,"<jats:title>Abstract</jats:title><jats:p>Since first identified, the epidemic scale of the recently emerged novel coronavirus (2019-nCoV) in Wuhan, China, has increased rapidly, with cases arising across China and other countries and regions. using a transmission model, we estimate a basic reproductive number of 3.11 (95%CI, 2.39–4.13); 58–76% of transmissions must be prevented to stop increasing; Wuhan case ascertainment of 5.0% (3.6–7.4); 21022 (11090–33490) total infections in Wuhan 1 to 22 January.</jats:p><jats:sec><jats:title>Changes to previous version</jats:title><jats:list list-type=""bullet""><jats:list-item><jats:p>case data updated to include 22 Jan 2020; we did not use cases reported after this period as cases were reported at the province level hereafter, and large-scale control interventions were initiated on 23 Jan 2020;</jats:p></jats:list-item><jats:list-item><jats:p>improved likelihood function, better accounting for first 41 confirmed cases, and now using all infections (rather than just cases detected) in Wuhan for prediction of infection in international travellers;</jats:p></jats:list-item><jats:list-item><jats:p>improved characterization of uncertainty in parameters, and calculation of epidemic trajectory confidence intervals using a more statistically rigorous method;</jats:p></jats:list-item><jats:list-item><jats:p>extended range of latent period in sensitivity analysis to reflect reports of up to 6 day incubation period in household clusters;</jats:p></jats:list-item><jats:list-item><jats:p>removed travel restriction analysis, as different modelling approaches (e.g. stochastic transmission, rather than deterministic transmission) are more appropriate to such analyses.</jats:p></jats:list-item></jats:list></jats:sec>","abstract since first identified, the epidemic scale of the recently emerged novel coronavirus (2019-ncov) in wuhan, china, has increased rapidly, with cases arising across china and other countries and regions. using a transmission model, we estimate a basic reproductive number of 3.11 (95%ci, 2.39–4.13); 58–76% of transmissions must be prevented to stop increasing; wuhan case ascertainment of 5.0% (3.6–7.4); 21022 (11090–33490) total infections in wuhan 1 to 22 january. changes to previous version case data updated to include 22 jan 2020; we did not use cases reported after this period as cases were reported at the province level hereafter, and large-scale control interventions were initiated on 23 jan 2020; improved likelihood function, better accounting for first 41 confirmed cases, and now using all infections (rather than just cases detected) in wuhan for prediction of infection in international travellers; improved characterization of uncertainty in parameters, and calculation of epidemic trajectory confidence intervals using a more statistically rigorous method; extended range of latent period in sensitivity analysis to reflect reports of up to 6 day incubation period in household clusters; removed travel restriction analysis, as different modelling approaches (e.g. stochastic transmission, rather than deterministic transmission) are more appropriate to such analyses."
http://orkg.org/orkg/resource/R44856,Time-varying transmission dynamics of Novel Coronavirus Pneumonia in China,10.1101/2020.01.25.919787,crossref,"<jats:title>ABSTRACT</jats:title><jats:sec><jats:title>Rationale</jats:title><jats:p>Several studies have estimated basic production number of novel coronavirus pneumonia (NCP). However, the time-varying transmission dynamics of NCP during the outbreak remain unclear.</jats:p></jats:sec><jats:sec><jats:title>Objectives</jats:title><jats:p>We aimed to estimate the basic and time-varying transmission dynamics of NCP across China, and compared them with SARS.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>Data on NCP cases by February 7, 2020 were collected from epidemiological investigations or official websites. Data on severe acute respiratory syndrome (SARS) cases in Guangdong Province, Beijing and Hong Kong during 2002-2003 were also obtained. We estimated the doubling time, basic reproduction number (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>) and time-varying reproduction number (<jats:italic>R<jats:sub>t</jats:sub></jats:italic>) of NCP and SARS.</jats:p></jats:sec><jats:sec><jats:title>Measurements and main results</jats:title><jats:p>As of February 7, 2020, 34,598 NCP cases were identified in China, and daily confirmed cases decreased after February 4. The doubling time of NCP nationwide was 2.4 days which was shorter than that of SARS in Guangdong (14.3 days), Hong Kong (5.7 days) and Beijing (12.4 days). The<jats:italic>R<jats:sub>0</jats:sub></jats:italic>of NCP cases nationwide and in Wuhan were 4.5 and 4.4 respectively, which were higher than<jats:italic>R<jats:sub>0</jats:sub></jats:italic>of SARS in Guangdong (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>=2.3), Hongkong (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>=2.3), and Beijing (<jats:italic>R<jats:sub>0</jats:sub></jats:italic>=2.6). The<jats:italic>R<jats:sub>t</jats:sub></jats:italic>for NCP continuously decreased especially after January 16 nationwide and in Wuhan. The<jats:italic>R<jats:sub>0</jats:sub></jats:italic>for secondary NCP cases in Guangdong was 0.6, and the<jats:italic>R<jats:sub>t</jats:sub></jats:italic>values were less than 1 during the epidemic.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>NCP may have a higher transmissibility than SARS, and the efforts of containing the outbreak are effective. However, the efforts are needed to persist in for reducing time-varying reproduction number below one.</jats:p></jats:sec><jats:sec><jats:title>At a Glance Commentary</jats:title><jats:sec><jats:title>Scientific Knowledge on the Subject</jats:title><jats:p>Since December 29, 2019, pneumonia infection with 2019-nCoV, now named as Novel Coronavirus Pneumonia (NCP), occurred in Wuhan, Hubei Province, China. The disease has rapidly spread from Wuhan to other areas. As a novel virus, the time-varying transmission dynamics of NCP remain unclear, and it is also important to compare it with SARS.</jats:p></jats:sec><jats:sec><jats:title>What This Study Adds to the Field</jats:title><jats:p>We compared the transmission dynamics of NCP with SARS, and found that NCP has a higher transmissibility than SARS. Time-varying production number indicates that rigorous control measures taken by governments are effective across China, and persistent efforts are needed to be taken for reducing instantaneous reproduction number below one.</jats:p></jats:sec></jats:sec>","abstract rationale several studies have estimated basic production number of novel coronavirus pneumonia (ncp). however, the time-varying transmission dynamics of ncp during the outbreak remain unclear. objectives we aimed to estimate the basic and time-varying transmission dynamics of ncp across china, and compared them with sars. methods data on ncp cases by february 7, 2020 were collected from epidemiological investigations or official websites. data on severe acute respiratory syndrome (sars) cases in guangdong province, beijing and hong kong during 2002-2003 were also obtained. we estimated the doubling time, basic reproduction number ( r 0 ) and time-varying reproduction number ( r t ) of ncp and sars. measurements and main results as of february 7, 2020, 34,598 ncp cases were identified in china, and daily confirmed cases decreased after february 4. the doubling time of ncp nationwide was 2.4 days which was shorter than that of sars in guangdong (14.3 days), hong kong (5.7 days) and beijing (12.4 days). the r 0 of ncp cases nationwide and in wuhan were 4.5 and 4.4 respectively, which were higher than r 0 of sars in guangdong ( r 0 =2.3), hongkong ( r 0 =2.3), and beijing ( r 0 =2.6). the r t for ncp continuously decreased especially after january 16 nationwide and in wuhan. the r 0 for secondary ncp cases in guangdong was 0.6, and the r t values were less than 1 during the epidemic. conclusions ncp may have a higher transmissibility than sars, and the efforts of containing the outbreak are effective. however, the efforts are needed to persist in for reducing time-varying reproduction number below one. at a glance commentary scientific knowledge on the subject since december 29, 2019, pneumonia infection with 2019-ncov, now named as novel coronavirus pneumonia (ncp), occurred in wuhan, hubei province, china. the disease has rapidly spread from wuhan to other areas. as a novel virus, the time-varying transmission dynamics of ncp remain unclear, and it is also important to compare it with sars. what this study adds to the field we compared the transmission dynamics of ncp with sars, and found that ncp has a higher transmissibility than sars. time-varying production number indicates that rigorous control measures taken by governments are effective across china, and persistent efforts are needed to be taken for reducing instantaneous reproduction number below one."
http://orkg.org/orkg/resource/R44910,Estimating the Unreported Number of Novel Coronavirus (2019-nCoV) Cases in China in the First Half of January 2020: A Data-Driven Modelling Analysis of the Early Outbreak,10.3390/jcm9020388,crossref,"<jats:p>Background: In December 2019, an outbreak of respiratory illness caused by a novel coronavirus (2019-nCoV) emerged in Wuhan, China and has swiftly spread to other parts of China and a number of foreign countries. The 2019-nCoV cases might have been under-reported roughly from 1 to 15 January 2020, and thus we estimated the number of unreported cases and the basic reproduction number, R0, of 2019-nCoV. Methods: We modelled the epidemic curve of 2019-nCoV cases, in mainland China from 1 December 2019 to 24 January 2020 through the exponential growth. The number of unreported cases was determined by the maximum likelihood estimation. We used the serial intervals (SI) of infection caused by two other well-known coronaviruses (CoV), Severe Acute Respiratory Syndrome (SARS) and Middle East Respiratory Syndrome (MERS) CoVs, as approximations of the unknown SI for 2019-nCoV to estimate R0. Results: We confirmed that the initial growth phase followed an exponential growth pattern. The under-reporting was likely to have resulted in 469 (95% CI: 403–540) unreported cases from 1 to 15 January 2020. The reporting rate after 17 January 2020 was likely to have increased 21-fold (95% CI: 18–25) in comparison to the situation from 1 to 17 January 2020 on average. We estimated the R0 of 2019-nCoV at 2.56 (95% CI: 2.49–2.63). Conclusion: The under-reporting was likely to have occurred during the first half of January 2020 and should be considered in future investigation.</jats:p>","background: in december 2019, an outbreak of respiratory illness caused by a novel coronavirus (2019-ncov) emerged in wuhan, china and has swiftly spread to other parts of china and a number of foreign countries. the 2019-ncov cases might have been under-reported roughly from 1 to 15 january 2020, and thus we estimated the number of unreported cases and the basic reproduction number, r0, of 2019-ncov. methods: we modelled the epidemic curve of 2019-ncov cases, in mainland china from 1 december 2019 to 24 january 2020 through the exponential growth. the number of unreported cases was determined by the maximum likelihood estimation. we used the serial intervals (si) of infection caused by two other well-known coronaviruses (cov), severe acute respiratory syndrome (sars) and middle east respiratory syndrome (mers) covs, as approximations of the unknown si for 2019-ncov to estimate r0. results: we confirmed that the initial growth phase followed an exponential growth pattern. the under-reporting was likely to have resulted in 469 (95% ci: 403–540) unreported cases from 1 to 15 january 2020. the reporting rate after 17 january 2020 was likely to have increased 21-fold (95% ci: 18–25) in comparison to the situation from 1 to 17 january 2020 on average. we estimated the r0 of 2019-ncov at 2.56 (95% ci: 2.49–2.63). conclusion: the under-reporting was likely to have occurred during the first half of january 2020 and should be considered in future investigation."
http://orkg.org/orkg/resource/R44957,Comparing relaxation training and cognitive-behavioral group therapy for women with breast cancer,10.1177/1049731506293741,crossref,"<jats:p> Objective: To assess the effectiveness of cognitive-behavior (CB) group intervention versus relaxation and guided imagery (RGI) group training. Method: A total of 114 early-stage breast cancer patients were randomly assigned to CB, RGI, or control groups, and instruments were completed at pre- and postintervention and 4 months later. Results: Psychological distress was significantly reduced in both intervention groups compared to the control group. The RGI group was more effective in reducing levels of fatigue and sleep difficulties, whereas the CB group was more effective in reducing external health locus of control. Internal health locus of control did not significantly change. Adherence to self-practice at home was significantly associated with reduction in psychological and physical symptoms. Conclusions: The study supports the use of both CB and RGI models for reducing psychological distress in breast cancer patients. RGI showed advantages in reducing fatigue and improving sleep quality, whereas CB better reduced external health locus of control perceptions. </jats:p>","objective: to assess the effectiveness of cognitive-behavior (cb) group intervention versus relaxation and guided imagery (rgi) group training. method: a total of 114 early-stage breast cancer patients were randomly assigned to cb, rgi, or control groups, and instruments were completed at pre- and postintervention and 4 months later. results: psychological distress was significantly reduced in both intervention groups compared to the control group. the rgi group was more effective in reducing levels of fatigue and sleep difficulties, whereas the cb group was more effective in reducing external health locus of control. internal health locus of control did not significantly change. adherence to self-practice at home was significantly associated with reduction in psychological and physical symptoms. conclusions: the study supports the use of both cb and rgi models for reducing psychological distress in breast cancer patients. rgi showed advantages in reducing fatigue and improving sleep quality, whereas cb better reduced external health locus of control perceptions."
http://orkg.org/orkg/resource/R44970,Evaluation of geese theatre's re-connect program: Addressing resettlement issues in prison,10.1177/0306624X10370452,crossref,"<jats:p> This study examined the impact of Geese Theatre’s Re-Connect program on a sample of offenders who attended it. This program used theatre performance, experiential exercises, skills practice role-plays, and metaphors such as the masks to invite a group of offenders to consider and explore issues connected with their release and reconnecting with a life outside prison. Pre- and postprogram psychometric tests, behavior ratings, and interviews were completed to assess the effectiveness of the program. Significant changes were observed from pre- to posttreatment in terms of self-efficacy, motivation to change, and improved confidence in skills (i.e., social and friendship, occupational, family and intimacy, dealing with authority, alternatives to aggression or offending, and self-management and self-control skills). Improved behavior and engagement within the program was observed over the 3 days of the program. Interviews also revealed the positive impact the program had on the participants. This provides evidence supporting the short-term effectiveness of the Re-Connect program. </jats:p>","this study examined the impact of geese theatre’s re-connect program on a sample of offenders who attended it. this program used theatre performance, experiential exercises, skills practice role-plays, and metaphors such as the masks to invite a group of offenders to consider and explore issues connected with their release and reconnecting with a life outside prison. pre- and postprogram psychometric tests, behavior ratings, and interviews were completed to assess the effectiveness of the program. significant changes were observed from pre- to posttreatment in terms of self-efficacy, motivation to change, and improved confidence in skills (i.e., social and friendship, occupational, family and intimacy, dealing with authority, alternatives to aggression or offending, and self-management and self-control skills). improved behavior and engagement within the program was observed over the 3 days of the program. interviews also revealed the positive impact the program had on the participants. this provides evidence supporting the short-term effectiveness of the re-connect program."
http://orkg.org/orkg/resource/R46221,CO2 reduction over NaNbO3 and NaTaO3 perovskite photocatalysts,10.1039/c6pp00235h,crossref,<p>Both NaNbO<sub>3</sub> and NaTaO<sub>3</sub> exhibit interesting intrinsic photocatalytic activities for CO<sub>2</sub> reduction in terms of conversion and selectivity.</p>,both nanbo 3 and natao 3 exhibit interesting intrinsic photocatalytic activities for co 2 reduction in terms of conversion and selectivity.
http://orkg.org/orkg/resource/R51231,Broad anti-coronaviral activity of FDA approved drugs against SARS-CoV-2 in vitro and SARS-CoV in vivo,10.1101/2020.03.25.008482,crossref,"<jats:title>Abstract</jats:title><jats:p>SARS-CoV-2 emerged in China at the end of 2019 and has rapidly become a pandemic with roughly 2.7 million recorded COVID-19 cases and greater than 189,000 recorded deaths by April 23rd, 2020 (<jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://www.WHO.org"">www.WHO.org</jats:ext-link>). There are no FDA approved antivirals or vaccines for any coronavirus, including SARS-CoV-2. Current treatments for COVID-19 are limited to supportive therapies and off-label use of FDA approved drugs. Rapid development and human testing of potential antivirals is greatly needed. A quick way to test compounds with potential antiviral activity is through drug repurposing. Numerous drugs are already approved for human use and subsequently there is a good understanding of their safety profiles and potential side effects, making them easier to fast-track to clinical studies in COVID-19 patients. Here, we present data on the antiviral activity of 20 FDA approved drugs against SARS-CoV-2 that also inhibit SARS-CoV and MERS-CoV. We found that 17 of these inhibit SARS-CoV-2 at a range of IC50 values at non-cytotoxic concentrations. We directly follow up with seven of these to demonstrate all are capable of inhibiting infectious SARS-CoV-2 production. Moreover, we have evaluated two of these, chloroquine and chlorpromazine, <jats:italic>in vivo</jats:italic> using a mouse-adapted SARS-CoV model and found both drugs protect mice from clinical disease.</jats:p>","abstract sars-cov-2 emerged in china at the end of 2019 and has rapidly become a pandemic with roughly 2.7 million recorded covid-19 cases and greater than 189,000 recorded deaths by april 23rd, 2020 ( www.who.org ). there are no fda approved antivirals or vaccines for any coronavirus, including sars-cov-2. current treatments for covid-19 are limited to supportive therapies and off-label use of fda approved drugs. rapid development and human testing of potential antivirals is greatly needed. a quick way to test compounds with potential antiviral activity is through drug repurposing. numerous drugs are already approved for human use and subsequently there is a good understanding of their safety profiles and potential side effects, making them easier to fast-track to clinical studies in covid-19 patients. here, we present data on the antiviral activity of 20 fda approved drugs against sars-cov-2 that also inhibit sars-cov and mers-cov. we found that 17 of these inhibit sars-cov-2 at a range of ic50 values at non-cytotoxic concentrations. we directly follow up with seven of these to demonstrate all are capable of inhibiting infectious sars-cov-2 production. moreover, we have evaluated two of these, chloroquine and chlorpromazine, in vivo using a mouse-adapted sars-cov model and found both drugs protect mice from clinical disease."
http://orkg.org/orkg/resource/R48233,A scaling approach to project regional sea level rise and its uncertainties,10.5194/esd-4-11-2013,crossref,"<jats:p>Abstract. Climate change causes global mean sea level to rise due to thermal expansion of seawater and loss of land ice from mountain glaciers, ice caps and ice sheets. Locally, sea level can strongly deviate from the global mean rise due to changes in wind and ocean currents. In addition, gravitational adjustments redistribute seawater away from shrinking ice masses. However, the land ice contribution to sea level rise (SLR) remains very challenging to model, and comprehensive regional sea level projections, which include appropriate gravitational adjustments, are still a nascent field (Katsman et al., 2011; Slangen et al., 2011). Here, we present an alternative approach to derive regional sea level changes for a range of emission and land ice melt scenarios, combining probabilistic forecasts of a simple climate model (MAGICC6) with the new CMIP5 general circulation models.  The contribution from ice sheets varies considerably depending on the assumptions for the ice sheet projections, and thus represents sizeable uncertainties for future sea level rise. However, several consistent and robust patterns emerge from our analysis: at low latitudes, especially in the Indian Ocean and Western Pacific, sea level will likely rise more than the global mean (mostly by 10–20%). Around the northeastern Atlantic and the northeastern Pacific coasts, sea level will rise less than the global average or, in some rare cases, even fall. In the northwestern Atlantic, along the American coast, a strong dynamic sea level rise is counteracted by gravitational depression due to Greenland ice melt; whether sea level will be above- or below-average will depend on the relative contribution of these two factors. Our regional sea level projections and the diagnosed uncertainties provide an improved basis for coastal impact analysis and infrastructure planning for adaptation to climate change.\n                    </jats:p>","abstract. climate change causes global mean sea level to rise due to thermal expansion of seawater and loss of land ice from mountain glaciers, ice caps and ice sheets. locally, sea level can strongly deviate from the global mean rise due to changes in wind and ocean currents. in addition, gravitational adjustments redistribute seawater away from shrinking ice masses. however, the land ice contribution to sea level rise (slr) remains very challenging to model, and comprehensive regional sea level projections, which include appropriate gravitational adjustments, are still a nascent field (katsman et al., 2011; slangen et al., 2011). here, we present an alternative approach to derive regional sea level changes for a range of emission and land ice melt scenarios, combining probabilistic forecasts of a simple climate model (magicc6) with the new cmip5 general circulation models. the contribution from ice sheets varies considerably depending on the assumptions for the ice sheet projections, and thus represents sizeable uncertainties for future sea level rise. however, several consistent and robust patterns emerge from our analysis: at low latitudes, especially in the indian ocean and western pacific, sea level will likely rise more than the global mean (mostly by 10–20%). around the northeastern atlantic and the northeastern pacific coasts, sea level will rise less than the global average or, in some rare cases, even fall. in the northwestern atlantic, along the american coast, a strong dynamic sea level rise is counteracted by gravitational depression due to greenland ice melt; whether sea level will be above- or below-average will depend on the relative contribution of these two factors. our regional sea level projections and the diagnosed uncertainties provide an improved basis for coastal impact analysis and infrastructure planning for adaptation to climate change.\n"
http://orkg.org/orkg/resource/R70624,Predictive Modeling of Surgical Site Infections Using Sparse Laboratory Data,,crossref,"<jats:p>As part of a data mining competition, a training and test set of laboratory test data about patients with and without surgical site infection (SSI) were provided. The task was to develop predictive models with training set and identify patients with SSI in the no label test set. Lab test results are vital resources that guide healthcare providers make decisions about all aspects of surgical patient management. Many machine learning models were developed after pre-processing and imputing the lab tests data and only the top performing methods are discussed. Overall, RANDOM FOREST algorithms performed better than Support Vector Machine and Logistic Regression. Using a set of 74 lab tests, with RF, there were only 4 false positives in the training set and predicted 35 out of 50 SSI patients in the test set (Accuracy 0.86, Sensitivity 0.68, and Specificity 0.91). Optimal ways to address healthcare data quality concerns and imputation methods as well as newer generalizable algorithms need to be explored further to decipher new associations and knowledge among laboratory biomarkers and SSI. </jats:p>","as part of a data mining competition, a training and test set of laboratory test data about patients with and without surgical site infection (ssi) were provided. the task was to develop predictive models with training set and identify patients with ssi in the no label test set. lab test results are vital resources that guide healthcare providers make decisions about all aspects of surgical patient management. many machine learning models were developed after pre-processing and imputing the lab tests data and only the top performing methods are discussed. overall, random forest algorithms performed better than support vector machine and logistic regression. using a set of 74 lab tests, with rf, there were only 4 false positives in the training set and predicted 35 out of 50 ssi patients in the test set (accuracy 0.86, sensitivity 0.68, and specificity 0.91). optimal ways to address healthcare data quality concerns and imputation methods as well as newer generalizable algorithms need to be explored further to decipher new associations and knowledge among laboratory biomarkers and ssi."
http://orkg.org/orkg/resource/R48353,Future sea level rise constrained by observations and long-term commitment,10.1073/pnas.1500515113,crossref,"""<jats:title>Significance</jats:title>\n          <jats:p>Anthropogenic sea level rise poses challenges to coastal areas worldwide, and robust projections are needed to assess mitigation options and guide adaptation measures. Here we present an approach that combines information about the equilibrium sea level response to global warming and last century's observed contribution from the individual components to constrain projections for this century. This “constrained extrapolation” overcomes limitations of earlier global semiempirical estimates because long-term changes in the partitioning of total sea level rise are accounted for. While applying semiempirical methodology, our method yields sea level projections that overlap with the process-based estimates of the Intergovernmental Panel on Climate Change. The method can thus lead to a better understanding of the gap between process-based and global semiempirical approaches.</jats:p>""",""" significance \n anthropogenic sea level rise poses challenges to coastal areas worldwide, and robust projections are needed to assess mitigation options and guide adaptation measures. here we present an approach that combines information about the equilibrium sea level response to global warming and last century's observed contribution from the individual components to constrain projections for this century. this “constrained extrapolation” overcomes limitations of earlier global semiempirical estimates because long-term changes in the partitioning of total sea level rise are accounted for. while applying semiempirical methodology, our method yields sea level projections that overlap with the process-based estimates of the intergovernmental panel on climate change. the method can thus lead to a better understanding of the gap between process-based and global semiempirical approaches. """
http://orkg.org/orkg/resource/R49096,Stabilization of global temperature at 1.5°C and 2.0°C: implications for coastal areas,10.1098/rsta.2016.0448,crossref,"<jats:p>The effectiveness of stringent climate stabilization scenarios for coastal areas in terms of reduction of impacts/adaptation needs and wider policy implications has received little attention. Here we use the Warming Acidification and Sea Level Projector Earth systems model to calculate large ensembles of global sea-level rise (SLR) and ocean pH projections to 2300 for 1.5°C and 2.0°C stabilization scenarios, and a reference unmitigated RCP8.5 scenario. The potential consequences of these projections are then considered for global coastal flooding, small islands, deltas, coastal cities and coastal ecology. Under both stabilization scenarios, global mean ocean pH (and temperature) stabilize within a century. This implies significant ecosystem impacts are avoided, but detailed quantification is lacking, reflecting scientific uncertainty. By contrast, SLR is only slowed and continues to 2300 (and beyond). Hence, while coastal impacts due to SLR are reduced significantly by climate stabilization, especially after 2100, potential impacts continue to grow for centuries. SLR in 2300 under both stabilization scenarios exceeds unmitigated SLR in 2100. Therefore, adaptation remains essential in densely populated and economically important coastal areas under climate stabilization. Given the multiple adaptation steps that this will require, an adaptation pathways approach has merits for coastal areas.</jats:p>\n          <jats:p>This article is part of the theme issue ‘The Paris Agreement: understanding the physical and social challenges for a warming world of 1.5°C above pre-industrial levels’.</jats:p>","the effectiveness of stringent climate stabilization scenarios for coastal areas in terms of reduction of impacts/adaptation needs and wider policy implications has received little attention. here we use the warming acidification and sea level projector earth systems model to calculate large ensembles of global sea-level rise (slr) and ocean ph projections to 2300 for 1.5°c and 2.0°c stabilization scenarios, and a reference unmitigated rcp8.5 scenario. the potential consequences of these projections are then considered for global coastal flooding, small islands, deltas, coastal cities and coastal ecology. under both stabilization scenarios, global mean ocean ph (and temperature) stabilize within a century. this implies significant ecosystem impacts are avoided, but detailed quantification is lacking, reflecting scientific uncertainty. by contrast, slr is only slowed and continues to 2300 (and beyond). hence, while coastal impacts due to slr are reduced significantly by climate stabilization, especially after 2100, potential impacts continue to grow for centuries. slr in 2300 under both stabilization scenarios exceeds unmitigated slr in 2100. therefore, adaptation remains essential in densely populated and economically important coastal areas under climate stabilization. given the multiple adaptation steps that this will require, an adaptation pathways approach has merits for coastal areas. \n this article is part of the theme issue ‘the paris agreement: understanding the physical and social challenges for a warming world of 1.5°c above pre-industrial levels’."
http://orkg.org/orkg/resource/R49152,REDI: Towards knowledge graph-powered scholarly information management and research networking,10.1177/0165551520944351,crossref,"<jats:p> Academic data management has become an increasingly challenging task as research evolves over time. Essential tasks such as information retrieval and research networking have turned into extremely difficult operations due to an ever-growing number of researchers and scientific articles. Numerous initiatives have emerged in the IT environments to address this issue, especially focused on web technologies. Although those approaches have individually provided solutions for diverse problems, they still can not offer integrated knowledge bases nor flexibility to exploit adequately this information. In this article, we present REDI, a Linked Data-powered framework for academic knowledge management and research networking, which introduces a new perspective of integration. REDI combines information from multiple sources into a consolidated knowledge base through state-of-the-art procedures and leverages semantic web standards to represent the information. Moreover, REDI takes advantage of such knowledge for data visualisation and analysis, which ultimately improves and simplifies many activities including research networking. </jats:p>","academic data management has become an increasingly challenging task as research evolves over time. essential tasks such as information retrieval and research networking have turned into extremely difficult operations due to an ever-growing number of researchers and scientific articles. numerous initiatives have emerged in the it environments to address this issue, especially focused on web technologies. although those approaches have individually provided solutions for diverse problems, they still can not offer integrated knowledge bases nor flexibility to exploit adequately this information. in this article, we present redi, a linked data-powered framework for academic knowledge management and research networking, which introduces a new perspective of integration. redi combines information from multiple sources into a consolidated knowledge base through state-of-the-art procedures and leverages semantic web standards to represent the information. moreover, redi takes advantage of such knowledge for data visualisation and analysis, which ultimately improves and simplifies many activities including research networking."
http://orkg.org/orkg/resource/R67866,Emergent constraints on transient climate response  (TCR) and equilibrium climate sensitivity (ECS) from historical warming in CMIP5 and CMIP6 models,10.5194/esd-11-737-2020,crossref,"<jats:p>Abstract. Climate sensitivity to CO2 remains the key uncertainty in projections of future climate change. Transient climate response\xa0(TCR) is the metric of temperature sensitivity that is most relevant to warming in the next few decades and contributes the biggest uncertainty to estimates of the carbon budgets consistent with the Paris targets. Equilibrium climate sensitivity\xa0(ECS) is vital for understanding longer-term climate change and stabilisation targets. In the IPCC 5th Assessment Report (AR5), the stated “likely” ranges (16\u2009%–84\u2009% confidence) of TCR (1.0–2.5\u2009K) and ECS (1.5–4.5\u2009K) were broadly consistent with the ensemble of CMIP5 Earth system models\xa0(ESMs) available at the time. However, many of the latest CMIP6 ESMs have larger climate sensitivities, with 5\xa0of 34\xa0models having TCR values above 2.5\u2009K and an ensemble mean TCR of 2.0±0.4\u2009K. Even starker, 12\xa0of 34\xa0models have an ECS value above 4.5\u2009K. On the face of it, these latest ESM results suggest that the IPCC likely ranges may need revising upwards, which would cast further doubt on the feasibility of the Paris targets. Here we show that rather than increasing the uncertainty in climate sensitivity, the CMIP6 models help to constrain the likely range of TCR to 1.3–2.1\u2009K, with a central estimate of 1.68\u2009K. We reach this conclusion through an emergent constraint approach which relates the value of TCR linearly to the global warming from 1975\xa0onwards. This is a period when the signal-to-noise ratio of the net radiative forcing increases strongly, so that uncertainties in aerosol forcing become progressively less problematic. We find a consistent emergent constraint on TCR when we apply the same method to CMIP5 models. Our constraints on TCR are in good agreement with other recent studies which analysed CMIP ensembles. The relationship between ECS and the post-1975 warming trend is less direct and also non-linear. However, we are able to derive a likely range of ECS of 1.9–3.4\u2009K from the CMIP6 models by assuming an underlying emergent relationship based on a two-box energy balance model. Despite some methodological differences; this is consistent with a previously published ECS constraint derived from warming trends in CMIP5 models to\xa02005. Our results seem to be part of a growing consensus amongst studies that have applied the emergent constraint approach to different model ensembles and to different aspects of the record of global warming.\n                    </jats:p>","abstract. climate sensitivity to co2 remains the key uncertainty in projections of future climate change. transient climate response\xa0(tcr) is the metric of temperature sensitivity that is most relevant to warming in the next few decades and contributes the biggest uncertainty to estimates of the carbon budgets consistent with the paris targets. equilibrium climate sensitivity\xa0(ecs) is vital for understanding longer-term climate change and stabilisation targets. in the ipcc 5th assessment report (ar5), the stated “likely” ranges (16\u2009%–84\u2009% confidence) of tcr (1.0–2.5\u2009k) and ecs (1.5–4.5\u2009k) were broadly consistent with the ensemble of cmip5 earth system models\xa0(esms) available at the time. however, many of the latest cmip6 esms have larger climate sensitivities, with 5\xa0of 34\xa0models having tcr values above 2.5\u2009k and an ensemble mean tcr of 2.0±0.4\u2009k. even starker, 12\xa0of 34\xa0models have an ecs value above 4.5\u2009k. on the face of it, these latest esm results suggest that the ipcc likely ranges may need revising upwards, which would cast further doubt on the feasibility of the paris targets. here we show that rather than increasing the uncertainty in climate sensitivity, the cmip6 models help to constrain the likely range of tcr to 1.3–2.1\u2009k, with a central estimate of 1.68\u2009k. we reach this conclusion through an emergent constraint approach which relates the value of tcr linearly to the global warming from 1975\xa0onwards. this is a period when the signal-to-noise ratio of the net radiative forcing increases strongly, so that uncertainties in aerosol forcing become progressively less problematic. we find a consistent emergent constraint on tcr when we apply the same method to cmip5 models. our constraints on tcr are in good agreement with other recent studies which analysed cmip ensembles. the relationship between ecs and the post-1975 warming trend is less direct and also non-linear. however, we are able to derive a likely range of ecs of 1.9–3.4\u2009k from the cmip6 models by assuming an underlying emergent relationship based on a two-box energy balance model. despite some methodological differences; this is consistent with a previously published ecs constraint derived from warming trends in cmip5 models to\xa02005. our results seem to be part of a growing consensus amongst studies that have applied the emergent constraint approach to different model ensembles and to different aspects of the record of global warming.\n"
http://orkg.org/orkg/resource/R68127,Wikidata as a knowledge graph for the life sciences,10.7554/elife.52614,crossref,"<jats:p>Wikidata is a community-maintained knowledge base that has been assembled from repositories in the fields of genomics, proteomics, genetic variants, pathways, chemical compounds, and diseases, and that adheres to the FAIR principles of findability, accessibility, interoperability and reusability. Here we describe the breadth and depth of the biomedical knowledge contained within Wikidata, and discuss the open-source tools we have built to add information to Wikidata and to synchronize it with source databases. We also demonstrate several use cases for Wikidata, including the crowdsourced curation of biomedical ontologies, phenotype-based diagnosis of disease, and drug repurposing.</jats:p>","wikidata is a community-maintained knowledge base that has been assembled from repositories in the fields of genomics, proteomics, genetic variants, pathways, chemical compounds, and diseases, and that adheres to the fair principles of findability, accessibility, interoperability and reusability. here we describe the breadth and depth of the biomedical knowledge contained within wikidata, and discuss the open-source tools we have built to add information to wikidata and to synchronize it with source databases. we also demonstrate several use cases for wikidata, including the crowdsourced curation of biomedical ontologies, phenotype-based diagnosis of disease, and drug repurposing."
http://orkg.org/orkg/resource/R68408,"A Systematic Review of Information Literacy Programs in Higher Education: Effects of Face-to-Face, Online, and Blended Formats on Student Skills and Views",10.18438/b86w90,crossref,"<jats:p>Abstract&#x0D;\n&#x0D;\nObjective – Evidence from systematic reviews a decade ago suggested that face-to-face and online methods to provide information literacy training in universities were equally effective in terms of skills learnt, but there was a lack of robust comparative research. The objectives of this review were (1) to update these findings with the inclusion of more recent primary research; (2) to further enhance the summary of existing evidence by including studies of blended formats (with components of both online and face-to-face teaching) compared to single format education; and (3) to explore student views on the various formats employed.&#x0D;\n&#x0D;\nMethods – Authors searched seven databases along with a range of supplementary search methods to identify comparative research studies, dated January 1995 to October 2016, exploring skill outcomes for students enrolled in higher education programs. There were 33 studies included, of which 19 also contained comparative data on student views. Where feasible, meta-analyses were carried out to provide summary estimates of skills development and a thematic analysis was completed to identify student views across the different formats.&#x0D;\n&#x0D;\nResults – A large majority of studies (27 of 33; 82%) found no statistically significant difference between formats in skills outcomes for students. Of 13 studies that could be included in a meta-analysis, the standardized mean difference (SMD) between skill test results for face-to-face versus online formats was -0.01 (95% confidence interval -0.28 to 0.26). Of ten studies comparing blended to single delivery format, seven (70%) found no statistically significant difference between formats, and the remaining studies had mixed outcomes. From the limited evidence available across all studies, there is a potential dichotomy between outcomes measured via skill test and assignment (course work) which is worthy of further investigation. The thematic analysis of student views found no preference in relation to format on a range of measures in 14 of 19 studies (74%). The remainder identified that students perceived advantages and disadvantages for each format but had no overall preference.&#x0D;\n&#x0D;\nConclusions – There is compelling evidence that information literacy training is effective and well received across a range of delivery formats. Further research looking at blended versus single format methods, and the time implications for each, as well as comparing assignment to skill test outcomes would be valuable. Future studies should adopt a methodologically robust design (such as the randomized controlled trial) with a large student population and validated outcome measures.</jats:p>","abstract&#x0d;\n&#x0d;\nobjective – evidence from systematic reviews a decade ago suggested that face-to-face and online methods to provide information literacy training in universities were equally effective in terms of skills learnt, but there was a lack of robust comparative research. the objectives of this review were (1) to update these findings with the inclusion of more recent primary research; (2) to further enhance the summary of existing evidence by including studies of blended formats (with components of both online and face-to-face teaching) compared to single format education; and (3) to explore student views on the various formats employed.&#x0d;\n&#x0d;\nmethods – authors searched seven databases along with a range of supplementary search methods to identify comparative research studies, dated january 1995 to october 2016, exploring skill outcomes for students enrolled in higher education programs. there were 33 studies included, of which 19 also contained comparative data on student views. where feasible, meta-analyses were carried out to provide summary estimates of skills development and a thematic analysis was completed to identify student views across the different formats.&#x0d;\n&#x0d;\nresults – a large majority of studies (27 of 33; 82%) found no statistically significant difference between formats in skills outcomes for students. of 13 studies that could be included in a meta-analysis, the standardized mean difference (smd) between skill test results for face-to-face versus online formats was -0.01 (95% confidence interval -0.28 to 0.26). of ten studies comparing blended to single delivery format, seven (70%) found no statistically significant difference between formats, and the remaining studies had mixed outcomes. from the limited evidence available across all studies, there is a potential dichotomy between outcomes measured via skill test and assignment (course work) which is worthy of further investigation. the thematic analysis of student views found no preference in relation to format on a range of measures in 14 of 19 studies (74%). the remainder identified that students perceived advantages and disadvantages for each format but had no overall preference.&#x0d;\n&#x0d;\nconclusions – there is compelling evidence that information literacy training is effective and well received across a range of delivery formats. further research looking at blended versus single format methods, and the time implications for each, as well as comparing assignment to skill test outcomes would be valuable. future studies should adopt a methodologically robust design (such as the randomized controlled trial) with a large student population and validated outcome measures."
http://orkg.org/orkg/resource/R68540,The biology of color,10.1126/science.aan0221,crossref,"<jats:title>In living color</jats:title>\n          <jats:p>\n            Animals live in a colorful world, but we rarely stop to think about how this color is produced and perceived, or how it evolved. Cuthill\n            <jats:italic>et al.</jats:italic>\n            review how color is used for social signals between individual animals and how it affects interactions with parasites, predators, and the physical environment. New approaches are elucidating aspects of animal coloration, from the requirements for complex cognition and perception mechanisms to the evolutionary dynamics surrounding its development and diversification.\n          </jats:p>\n          <jats:p>\n            <jats:italic>Science</jats:italic>\n            , this issue p.\n            <jats:related-article xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""doi"" issue=""6350"" page=""eaan0221"" related-article-type=""in-this-issue"" vol=""357"" xlink:href=""10.1126/science.aan0221"">eaan0221</jats:related-article>\n          </jats:p>","in living color \n \n animals live in a colorful world, but we rarely stop to think about how this color is produced and perceived, or how it evolved. cuthill\n et al. \n review how color is used for social signals between individual animals and how it affects interactions with parasites, predators, and the physical environment. new approaches are elucidating aspects of animal coloration, from the requirements for complex cognition and perception mechanisms to the evolutionary dynamics surrounding its development and diversification.\n \n \n science \n , this issue p.\n eaan0221 \n"
http://orkg.org/orkg/resource/R68629,Integrating Library Instruction into the Course Management System for a First-Year Engineering Class: An Evidence-Based Study Measuring the Effectiveness of Blended Learning on Students’ Information Literacy Levels,10.5860/crl.76.7.934,crossref,"<jats:p>This research examines students in a first-year engineering course who receive library instruction by using a newly developed online module and attending optional in-person tutorials. It aims to evaluate the outcomes of library information literacy instruction using this module combined with in-person help. Results show a significant improvement in information literacy skills from a pre-test to a post-test. Focus group and survey data indicate that most students preferred the self-paced learning style of the online module and that the content of the module helped them to conduct library research for the course. This study also considers best practices for online library instruction. A blended instruction approach provides students with the flexibility to learn from a variety of formats at their own pace and also reduces library staff workload, especially for a large course.</jats:p>","this research examines students in a first-year engineering course who receive library instruction by using a newly developed online module and attending optional in-person tutorials. it aims to evaluate the outcomes of library information literacy instruction using this module combined with in-person help. results show a significant improvement in information literacy skills from a pre-test to a post-test. focus group and survey data indicate that most students preferred the self-paced learning style of the online module and that the content of the module helped them to conduct library research for the course. this study also considers best practices for online library instruction. a blended instruction approach provides students with the flexibility to learn from a variety of formats at their own pace and also reduces library staff workload, especially for a large course."
http://orkg.org/orkg/resource/R68880,Principles for Measuring Teamwork Skills,10.1177/001872089203400408,crossref,"<jats:p> Performance measurement research has characteristically focused on the performance of individuals and has not addressed the measurement of teamwork skills. Dyer Suggested that this lack of teamwork measures presents serious problems for conducting team-oriented research because research efforts are often judged by the quality of the measurement device employed. In an effort to assess and advance the science of measuring teamwork, the current review examines the measurement of teamwork skills on theoretical, methodological, and psychometric grounds. Furthermore, we present a series of general principles to guide research in the future. Although some progress has been made, there is still much to be learned about the measurement of teamwork. </jats:p>","performance measurement research has characteristically focused on the performance of individuals and has not addressed the measurement of teamwork skills. dyer suggested that this lack of teamwork measures presents serious problems for conducting team-oriented research because research efforts are often judged by the quality of the measurement device employed. in an effort to assess and advance the science of measuring teamwork, the current review examines the measurement of teamwork skills on theoretical, methodological, and psychometric grounds. furthermore, we present a series of general principles to guide research in the future. although some progress has been made, there is still much to be learned about the measurement of teamwork."
http://orkg.org/orkg/resource/R69543,How a general-purpose common- sense ontology can improve performance of learning-based image retrieval,10.24963/ijcai.2017/178,crossref,"<jats:p>The knowledge representation community has built general-purpose ontologies which contain large amounts of commonsense knowledge over relevant aspects of the world, including useful visual information, e.g.: ""a ball is used by a football player"", ""a tennis player is located at a tennis court"". Current state-of-the-art approaches for visual recognition do not exploit these rule-based knowledge sources. Instead, they learn recognition models directly from training examples. In this paper, we study how general-purpose ontologies—specifically, MIT\'s ConceptNet ontology—can improve the performance of state-of-the-art vision systems. As a testbed, we tackle the problem of sentence-based image retrieval. Our retrieval approach incorporates knowledge from ConceptNet on top of a large pool of object detectors derived from a deep learning technique. In our experiments, we show that ConceptNet can improve performance on a common benchmark dataset. Key to our performance is the use of the ESPGAME dataset to select visually relevant relations from ConceptNet. Consequently, a main conclusion of this work is that general-purpose commonsense ontologies improve performance on visual reasoning tasks when properly filtered to select meaningful visual relations.</jats:p>","the knowledge representation community has built general-purpose ontologies which contain large amounts of commonsense knowledge over relevant aspects of the world, including useful visual information, e.g.: ""a ball is used by a football player"", ""a tennis player is located at a tennis court"". current state-of-the-art approaches for visual recognition do not exploit these rule-based knowledge sources. instead, they learn recognition models directly from training examples. in this paper, we study how general-purpose ontologies—specifically, mit\'s conceptnet ontology—can improve the performance of state-of-the-art vision systems. as a testbed, we tackle the problem of sentence-based image retrieval. our retrieval approach incorporates knowledge from conceptnet on top of a large pool of object detectors derived from a deep learning technique. in our experiments, we show that conceptnet can improve performance on a common benchmark dataset. key to our performance is the use of the espgame dataset to select visually relevant relations from conceptnet. consequently, a main conclusion of this work is that general-purpose commonsense ontologies improve performance on visual reasoning tasks when properly filtered to select meaningful visual relations."
http://orkg.org/orkg/resource/R69584,"Exploring knowledge graphs in an interpretable composite approach for text entailment,",10.1609/aaai.v33i01.33017023,crossref,"<jats:p>Recognizing textual entailment is a key task for many semantic applications, such as Question Answering, Text Summarization, and Information Extraction, among others. Entailment scenarios can range from a simple syntactic variation to more complex semantic relationships between pieces of text, but most approaches try a one-size-fits-all solution that usually favors some scenario to the detriment of another. We propose a composite approach for recognizing text entailment which analyzes the entailment pair to decide whether it must be resolved syntactically or semantically. We also make the answer interpretable: whenever an entailment is solved semantically, we explore a knowledge base composed of structured lexical definitions to generate natural language humanlike justifications, explaining the semantic relationship holding between the pieces of text. Besides outperforming wellestablished entailment algorithms, our composite approach gives an important step towards Explainable AI, using world knowledge to make the semantic reasoning process explicit and understandable.</jats:p>","recognizing textual entailment is a key task for many semantic applications, such as question answering, text summarization, and information extraction, among others. entailment scenarios can range from a simple syntactic variation to more complex semantic relationships between pieces of text, but most approaches try a one-size-fits-all solution that usually favors some scenario to the detriment of another. we propose a composite approach for recognizing text entailment which analyzes the entailment pair to decide whether it must be resolved syntactically or semantically. we also make the answer interpretable: whenever an entailment is solved semantically, we explore a knowledge base composed of structured lexical definitions to generate natural language humanlike justifications, explaining the semantic relationship holding between the pieces of text. besides outperforming wellestablished entailment algorithms, our composite approach gives an important step towards explainable ai, using world knowledge to make the semantic reasoning process explicit and understandable."
http://orkg.org/orkg/resource/R69599,Explicit knowledge-based reasoning for visual question answering,10.24963/ijcai.2017/179,crossref,"<jats:p>We describe a method for visual question answering which is capable of reasoning about an image on the basis of information extracted from a large-scale knowledge base. The method not only answers natural language questions using concepts not contained in the image, but can explain the reasoning by which it developed its answer. It is capable of answering far more complex questions than the predominant long short-term memory-based approach, and outperforms it significantly in testing. We also provide a dataset and a protocol by which to evaluate general visual question answering methods.</jats:p>","we describe a method for visual question answering which is capable of reasoning about an image on the basis of information extracted from a large-scale knowledge base. the method not only answers natural language questions using concepts not contained in the image, but can explain the reasoning by which it developed its answer. it is capable of answering far more complex questions than the predominant long short-term memory-based approach, and outperforms it significantly in testing. we also provide a dataset and a protocol by which to evaluate general visual question answering methods."
http://orkg.org/orkg/resource/R69633,Learning heterogeneous knowledge base embeddings for explainable recommendation,10.3390/a11090137,crossref,"<jats:p>Providing model-generated explanations in recommender systems is important to user experience. State-of-the-art recommendation algorithms—especially the collaborative filtering (CF)- based approaches with shallow or deep models—usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. Though structured knowledge bases were considered in content-based approaches, they have been largely ignored recently due to the availability of vast amounts of data and the learning power of many complex models. However, structured knowledge bases exhibit unique advantages in personalized recommendation systems. When the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users’ historical behaviors and the knowledge is helpful for providing informed explanations regarding the recommended items. A great challenge for using knowledge bases for recommendation is how to integrate large-scale structured and unstructured data, while taking advantage of collaborative filtering for highly accurate performance. Recent achievements in knowledge-base embedding (KBE) sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge for explanation. In this work, we propose to explain knowledge-base embeddings for explainable recommendation. Specifically, we propose a knowledge-base representation learning framework to embed heterogeneous entities for recommendation, and based on the embedded knowledge base, a soft matching algorithm is proposed to generate personalized explanations for the recommended items. Experimental results on real-world e-commerce datasets verified the superior recommendation performance and the explainability power of our approach compared with state-of-the-art baselines.</jats:p>","providing model-generated explanations in recommender systems is important to user experience. state-of-the-art recommendation algorithms—especially the collaborative filtering (cf)- based approaches with shallow or deep models—usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. though structured knowledge bases were considered in content-based approaches, they have been largely ignored recently due to the availability of vast amounts of data and the learning power of many complex models. however, structured knowledge bases exhibit unique advantages in personalized recommendation systems. when the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users’ historical behaviors and the knowledge is helpful for providing informed explanations regarding the recommended items. a great challenge for using knowledge bases for recommendation is how to integrate large-scale structured and unstructured data, while taking advantage of collaborative filtering for highly accurate performance. recent achievements in knowledge-base embedding (kbe) sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge for explanation. in this work, we propose to explain knowledge-base embeddings for explainable recommendation. specifically, we propose a knowledge-base representation learning framework to embed heterogeneous entities for recommendation, and based on the embedded knowledge base, a soft matching algorithm is proposed to generate personalized explanations for the recommended items. experimental results on real-world e-commerce datasets verified the superior recommendation performance and the explainability power of our approach compared with state-of-the-art baselines."
http://orkg.org/orkg/resource/R69648,Knowledge engineering tools for reasoning with scientific observations and interpretations: a neural connectivity use case,10.1186/1471-2105-12-351,crossref,"""<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>We address the goal of curating observations from published experiments in a generalizable form; reasoning over these observations to generate interpretations and then querying this interpreted knowledge to supply the supporting evidence. We present web-application software as part of the 'BioScholar' project (R01-GM083871) that fully instantiates this process for a well-defined domain: using tract-tracing experiments to study the neural connectivity of the rat brain.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>The main contribution of this work is to provide the first instantiation of a knowledge representation for experimental observations called 'Knowledge Engineering from Experimental Design' (KEfED) based on experimental variables and their interdependencies. The software has three parts: (a) the KEfED model editor - a design editor for creating KEfED models by drawing a flow diagram of an experimental protocol; (b) the KEfED data interface - a spreadsheet-like tool that permits users to enter experimental data pertaining to a specific model; (c) a 'neural connection matrix' interface that presents neural connectivity as a table of ordinal connection strengths representing the interpretations of tract-tracing data. This tool also allows the user to view experimental evidence pertaining to a specific connection. BioScholar is built in Flex 3.5. It uses Persevere (a <jats:italic>noSQL</jats:italic> database) as a flexible data store and PowerLoom<jats:sup>®</jats:sup> (a mature First Order Logic reasoning system) to execute queries using spatial reasoning over the BAMS neuroanatomical ontology.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>We first introduce the KEfED approach as a general approach and describe its possible role as a way of introducing structured reasoning into models of argumentation within new models of scientific publication. We then describe the design and implementation of our example application: the BioScholar software. This is presented as a possible biocuration interface and supplementary reasoning toolkit for a larger, more specialized bioinformatics system: the Brain Architecture Management System (BAMS).</jats:p>\n          </jats:sec>""",""" abstract \n \n background \n we address the goal of curating observations from published experiments in a generalizable form; reasoning over these observations to generate interpretations and then querying this interpreted knowledge to supply the supporting evidence. we present web-application software as part of the 'bioscholar' project (r01-gm083871) that fully instantiates this process for a well-defined domain: using tract-tracing experiments to study the neural connectivity of the rat brain. \n \n \n results \n the main contribution of this work is to provide the first instantiation of a knowledge representation for experimental observations called 'knowledge engineering from experimental design' (kefed) based on experimental variables and their interdependencies. the software has three parts: (a) the kefed model editor - a design editor for creating kefed models by drawing a flow diagram of an experimental protocol; (b) the kefed data interface - a spreadsheet-like tool that permits users to enter experimental data pertaining to a specific model; (c) a 'neural connection matrix' interface that presents neural connectivity as a table of ordinal connection strengths representing the interpretations of tract-tracing data. this tool also allows the user to view experimental evidence pertaining to a specific connection. bioscholar is built in flex 3.5. it uses persevere (a nosql database) as a flexible data store and powerloom ® (a mature first order logic reasoning system) to execute queries using spatial reasoning over the bams neuroanatomical ontology. \n \n \n conclusions \n we first introduce the kefed approach as a general approach and describe its possible role as a way of introducing structured reasoning into models of argumentation within new models of scientific publication. we then describe the design and implementation of our example application: the bioscholar software. this is presented as a possible biocuration interface and supplementary reasoning toolkit for a larger, more specialized bioinformatics system: the brain architecture management system (bams). \n """
http://orkg.org/orkg/resource/R70868,The Cooperation Databank,10.31234/osf.io/rveh3,crossref,"<p>Publishing studies using standardized, machine-readable formats will enable machines to perform meta-analyses on-demand. To build a semantically-enhanced technology that embodies these functions, we developed the Cooperation Databank (CoDa) – a databank that contains 2,636 studies on human cooperation (1958-2017) conducted in 78 societies involving 356,283 participants. Experts annotated these studies along 312 variables, including the quantitative results (13,959 effects). We designed an ontology that defines and relates concepts in cooperation research and that can represent the relationships between results of correlational and experimental studies. We have created a research platform that, based on the dataset, enables users to retrieve studies that test the relation of variables with cooperation, visualize these study results, and perform (1) meta-analyses, (2) meta-regressions, (3) estimates of publication bias, and (4) statistical power analyses for future studies. We leveraged the dataset with visualization tools that allow users to explore the ontology of concepts in cooperation research and to plot a citation network of the history of studies. CoDa offers a vision of how publishing studies in a machine-readable format can establish institutions and tools that improve scientific practices and knowledge.</p>","publishing studies using standardized, machine-readable formats will enable machines to perform meta-analyses on-demand. to build a semantically-enhanced technology that embodies these functions, we developed the cooperation databank (coda) – a databank that contains 2,636 studies on human cooperation (1958-2017) conducted in 78 societies involving 356,283 participants. experts annotated these studies along 312 variables, including the quantitative results (13,959 effects). we designed an ontology that defines and relates concepts in cooperation research and that can represent the relationships between results of correlational and experimental studies. we have created a research platform that, based on the dataset, enables users to retrieve studies that test the relation of variables with cooperation, visualize these study results, and perform (1) meta-analyses, (2) meta-regressions, (3) estimates of publication bias, and (4) statistical power analyses for future studies. we leveraged the dataset with visualization tools that allow users to explore the ontology of concepts in cooperation research and to plot a citation network of the history of studies. coda offers a vision of how publishing studies in a machine-readable format can establish institutions and tools that improve scientific practices and knowledge."
http://orkg.org/orkg/resource/R69926,Zeitraum und Raumzeit: Dimensionen zeitlicher und räumlicher Narration im Theater,10.1515/jlt-2019-0007,crossref,"<jats:title>Abstract</jats:title>\n<jats:p>The positioning in space and time of performed narration in theater poses a specific challenge to classical narratological categories of structuralist descent (developed, for example, by Gérard Genette or Wolf Schmid, for the analysis of narrative fiction). Time is the phenomenon which connects narratology and theater studies: on the one hand, it provides the basis for nearly every definition of narrativity; on the other, it grounds a number of different methodologies for the analysis of theater stagings, as well as theories of performance\xa0– with their emphasis on transience, the ephemeral, and the unrepeatable, singular or transitory nature of the technically unreproducible art of theater (e.\u200ag. by Erika Fischer-Lichte). This turn towards temporality is also present in theories of postdramatic theater (by Hans-Thies Lehman) and performance art. Narrating always takes place in time; likewise, every performance is a handling of and an encounter with time. Furthermore, performed narration gains a concrete spatial setting by virtue of its location on a stage or comparable performance area, so that the spatial structures contained in this setting exist in relation to the temporal structures of the act of theatrical telling, as well as the content of what is told. Both temporal and spatial structures of theater stagings can be systematically described and analyzed with a narratological vocabulary. With references to Seymour Chatman, Käte Hamburger and Markus Kuhn among others, the contribution discusses how narratological parameters for the analysis of temporal and spatial relations can be productively expanded in relation to theater and performance analysis. For exemplary purposes, it refers to Dimiter Gotscheff’s staging of Peter Handke’s <jats:italic>Immer noch Sturm</jats:italic> (which premiered in 2011 at the Thalia Theater Hamburg in cooperation with the Salzburger Festspiele), focusing on its transmedial broadening of temporal categories like order, duration, and frequency, and subsequent, prior, or simultaneous narration. The broadening itself proves feasible since all categories of temporal narration can be applied to performative narration in the theater\xa0– at times even more fruitfully than in written language, as is the case, for example, with the concept of ›duration‹. The concept of ›time of narration‹ too can be productively applied to theater. Whilst a subsequent narration is frequently considered the standard case in written-language narratives on the one hand\xa0– a conclusion that is, however, only correct if the narrator figure and narrative stand in spatiotemporal relation to one another, i.\u200ae. if a homodiegetic narrator figure is present\xa0– it is commonly held that in scenic-performed narration, on the other hand, the telling and the told take place simultaneously. The present contribution argues against this interpretation, as it stems from a misguided understanding of the ›liveness‹ of performance. ›Liveness‹ refers only to the relationship between viewers and performers and their respective presence, but not to their temporal and spatial relationship to the told. Rather, the following will argue that the time of narration in theater (as well as in film) stays unmarked in most cases. It is possible, however, to stage subsequent, prior, or simultaneous narration, too. <jats:italic>Immer noch Sturm</jats:italic> is one example for a performed subsequent narration. For audiovisual narration, then, a special case of iterative narration (telling once what happened n times) can be identified, which is to tell a few times (n minus x) what happened n times. As an additional category for the analysis of narrative temporality in audiovisual narrative media, I propose what I venture to call ›synchronized narration‹, in order to describe the specificity of spatiotemporal relations in performance. In synchronized narration, two or more events (that happen at different places or times in the narrative world) are shown at the same time on stage. This synchronized performance of several events is only realizable within the audiovisual dimension of spatial narration and not in written-language based narration. Furthermore, for narrative space relations the categories ›space covering‹, ›space extending‹, and ›space reducing narration‹ are suggested in order to analyze the relationships between discourse space and story space(s). Discourse space emerges in the concrete physical space of the performance when narrativity is present. Within this discourse space any amount of story spaces (with any expansion) can emerge. However, whilst in time-extending narration the time of the telling is longer than the time of the told, in space-extending narration the told space is bigger than the space of the telling. This principle is analogously valid for time-reducing or space-reducing narration. The transmission and media-specific broadening of temporal and spatial narratological parameters reveals how time and space form a continuum and should thus be linked and discussed alongside one another in analytical approaches to narrative artifacts. The staging of <jats:italic>Immer noch Sturm</jats:italic> actualizes a metaleptic structure, in which temporal borders are systematically dissolved and the overstepping of spatial borders becomes an indicator for the merging of different temporal levels. Referring back to established narratological parameters and developing analogous conceptual tools for narrative space facilitates a comparative analysis both of specific narratives and of narrative media and thus not only offers a productive challenge of classical narratological parameters, but allows to investigate and construct a holistic\xa0– if culture-specific\xa0– overall view of narration.</jats:p>","abstract \n the positioning in space and time of performed narration in theater poses a specific challenge to classical narratological categories of structuralist descent (developed, for example, by gérard genette or wolf schmid, for the analysis of narrative fiction). time is the phenomenon which connects narratology and theater studies: on the one hand, it provides the basis for nearly every definition of narrativity; on the other, it grounds a number of different methodologies for the analysis of theater stagings, as well as theories of performance\xa0– with their emphasis on transience, the ephemeral, and the unrepeatable, singular or transitory nature of the technically unreproducible art of theater (e.\u200ag. by erika fischer-lichte). this turn towards temporality is also present in theories of postdramatic theater (by hans-thies lehman) and performance art. narrating always takes place in time; likewise, every performance is a handling of and an encounter with time. furthermore, performed narration gains a concrete spatial setting by virtue of its location on a stage or comparable performance area, so that the spatial structures contained in this setting exist in relation to the temporal structures of the act of theatrical telling, as well as the content of what is told. both temporal and spatial structures of theater stagings can be systematically described and analyzed with a narratological vocabulary. with references to seymour chatman, käte hamburger and markus kuhn among others, the contribution discusses how narratological parameters for the analysis of temporal and spatial relations can be productively expanded in relation to theater and performance analysis. for exemplary purposes, it refers to dimiter gotscheff’s staging of peter handke’s immer noch sturm (which premiered in 2011 at the thalia theater hamburg in cooperation with the salzburger festspiele), focusing on its transmedial broadening of temporal categories like order, duration, and frequency, and subsequent, prior, or simultaneous narration. the broadening itself proves feasible since all categories of temporal narration can be applied to performative narration in the theater\xa0– at times even more fruitfully than in written language, as is the case, for example, with the concept of ›duration‹. the concept of ›time of narration‹ too can be productively applied to theater. whilst a subsequent narration is frequently considered the standard case in written-language narratives on the one hand\xa0– a conclusion that is, however, only correct if the narrator figure and narrative stand in spatiotemporal relation to one another, i.\u200ae. if a homodiegetic narrator figure is present\xa0– it is commonly held that in scenic-performed narration, on the other hand, the telling and the told take place simultaneously. the present contribution argues against this interpretation, as it stems from a misguided understanding of the ›liveness‹ of performance. ›liveness‹ refers only to the relationship between viewers and performers and their respective presence, but not to their temporal and spatial relationship to the told. rather, the following will argue that the time of narration in theater (as well as in film) stays unmarked in most cases. it is possible, however, to stage subsequent, prior, or simultaneous narration, too. immer noch sturm is one example for a performed subsequent narration. for audiovisual narration, then, a special case of iterative narration (telling once what happened n times) can be identified, which is to tell a few times (n minus x) what happened n times. as an additional category for the analysis of narrative temporality in audiovisual narrative media, i propose what i venture to call ›synchronized narration‹, in order to describe the specificity of spatiotemporal relations in performance. in synchronized narration, two or more events (that happen at different places or times in the narrative world) are shown at the same time on stage. this synchronized performance of several events is only realizable within the audiovisual dimension of spatial narration and not in written-language based narration. furthermore, for narrative space relations the categories ›space covering‹, ›space extending‹, and ›space reducing narration‹ are suggested in order to analyze the relationships between discourse space and story space(s). discourse space emerges in the concrete physical space of the performance when narrativity is present. within this discourse space any amount of story spaces (with any expansion) can emerge. however, whilst in time-extending narration the time of the telling is longer than the time of the told, in space-extending narration the told space is bigger than the space of the telling. this principle is analogously valid for time-reducing or space-reducing narration. the transmission and media-specific broadening of temporal and spatial narratological parameters reveals how time and space form a continuum and should thus be linked and discussed alongside one another in analytical approaches to narrative artifacts. the staging of immer noch sturm actualizes a metaleptic structure, in which temporal borders are systematically dissolved and the overstepping of spatial borders becomes an indicator for the merging of different temporal levels. referring back to established narratological parameters and developing analogous conceptual tools for narrative space facilitates a comparative analysis both of specific narratives and of narrative media and thus not only offers a productive challenge of classical narratological parameters, but allows to investigate and construct a holistic\xa0– if culture-specific\xa0– overall view of narration."
http://orkg.org/orkg/resource/R69928,Narrative representation and fictionality in performative media,10.1515/fns-2018-0030,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>The transmedial discussion in this article shows that the terms <jats:italic>narrative mediation</jats:italic> and <jats:italic>representation</jats:italic> should be carefully distinguished from <jats:italic>fiction</jats:italic> or <jats:italic>fictionality</jats:italic>. The constitutive use of ‘real’ (or factual) artifacts in performative media (i. e. media which present embodied events, such as theater or film) provides a good example for the necessity of this distinction. Frequently these artifacts serve the purpose of a fictional discourse and certain definitions of fictionality (cf. Walton 1990) can be said to be fulfilled. However, a real-world artifact can by no means become itself fictive, but is rather used to represent a fictive entity. By focusing on representation and mediation instead, it becomes possible to compare theater with other performative media in terms of narrative representation: even though it is sense-physiologically unmediated, the functions of narrative mediation (i. e. selecting, ordering, presenting, commenting; cf. e. g. Chatman 1990) apply. The article establishes a dynamic system of representation that can be used for the analysis of all kinds of multichannel narrative media and thus rests the ongoing scholarly discussions of transmedial narrative representation on a much sounder theoretical basis. It distinguishes representation clearly from fictionality, and highlights the significance of theater in this discussion.</jats:p>","abstract \n the transmedial discussion in this article shows that the terms narrative mediation and representation should be carefully distinguished from fiction or fictionality . the constitutive use of ‘real’ (or factual) artifacts in performative media (i. e. media which present embodied events, such as theater or film) provides a good example for the necessity of this distinction. frequently these artifacts serve the purpose of a fictional discourse and certain definitions of fictionality (cf. walton 1990) can be said to be fulfilled. however, a real-world artifact can by no means become itself fictive, but is rather used to represent a fictive entity. by focusing on representation and mediation instead, it becomes possible to compare theater with other performative media in terms of narrative representation: even though it is sense-physiologically unmediated, the functions of narrative mediation (i. e. selecting, ordering, presenting, commenting; cf. e. g. chatman 1990) apply. the article establishes a dynamic system of representation that can be used for the analysis of all kinds of multichannel narrative media and thus rests the ongoing scholarly discussions of transmedial narrative representation on a much sounder theoretical basis. it distinguishes representation clearly from fictionality, and highlights the significance of theater in this discussion."
http://orkg.org/orkg/resource/R69999,Human organ chip-enabled pipeline to rapidly repurpose therapeutics during viral pandemics,10.1101/2020.04.13.039917,crossref,"<jats:p>The rising threat of pandemic viruses, such as SARS-CoV-2, requires development of new preclinical discovery platforms that can more rapidly identify therapeutics that are active<jats:italic>in vitro</jats:italic>and also translate<jats:italic>in vivo</jats:italic>. Here we show that human organ-on-a-chip (Organ Chip) microfluidic culture devices lined by highly differentiated human primary lung airway epithelium and endothelium can be used to model virus entry, replication, strain-dependent virulence, host cytokine production, and recruitment of circulating immune cells in response to infection by respiratory viruses with great pandemic potential. We provide a first demonstration of drug repurposing by using oseltamivir in influenza A virus-infected organ chip cultures and show that co-administration of the approved anticoagulant drug, nafamostat, can double oseltamivir’s therapeutic time window. With the emergence of the COVID-19 pandemic, the Airway Chips were used to assess the inhibitory activities of approved drugs that showed inhibition in traditional cell culture assays only to find that most failed when tested in the Organ Chip platform. When administered in human Airway Chips under flow at a clinically relevant dose, one drug – amodiaquine - significantly inhibited infection by a pseudotyped SARS-CoV-2 virus. Proof of concept was provided by showing that amodiaquine and its active metabolite (desethylamodiaquine) also significantly reduce viral load in both direct infection and animal-to-animal transmission models of native SARS-CoV-2 infection in hamsters. These data highlight the value of Organ Chip technology as a more stringent and physiologically relevant platform for drug repurposing, and suggest that amodiaquine should be considered for future clinical testing.</jats:p>","the rising threat of pandemic viruses, such as sars-cov-2, requires development of new preclinical discovery platforms that can more rapidly identify therapeutics that are active in vitro and also translate in vivo . here we show that human organ-on-a-chip (organ chip) microfluidic culture devices lined by highly differentiated human primary lung airway epithelium and endothelium can be used to model virus entry, replication, strain-dependent virulence, host cytokine production, and recruitment of circulating immune cells in response to infection by respiratory viruses with great pandemic potential. we provide a first demonstration of drug repurposing by using oseltamivir in influenza a virus-infected organ chip cultures and show that co-administration of the approved anticoagulant drug, nafamostat, can double oseltamivir’s therapeutic time window. with the emergence of the covid-19 pandemic, the airway chips were used to assess the inhibitory activities of approved drugs that showed inhibition in traditional cell culture assays only to find that most failed when tested in the organ chip platform. when administered in human airway chips under flow at a clinically relevant dose, one drug – amodiaquine - significantly inhibited infection by a pseudotyped sars-cov-2 virus. proof of concept was provided by showing that amodiaquine and its active metabolite (desethylamodiaquine) also significantly reduce viral load in both direct infection and animal-to-animal transmission models of native sars-cov-2 infection in hamsters. these data highlight the value of organ chip technology as a more stringent and physiologically relevant platform for drug repurposing, and suggest that amodiaquine should be considered for future clinical testing."
http://orkg.org/orkg/resource/R70870,Microsoft Academic Graph: When experts are not enough,10.1162/qss_a_00021,crossref,"<jats:p>An ongoing project explores the extent to which artificial intelligence (AI), specifically in the areas of natural language processing and semantic reasoning, can be exploited to facilitate the studies of science by deploying software agents equipped with natural language understanding capabilities to read scholarly publications on the web. The knowledge extracted by these AI agents is organized into a heterogeneous graph, called Microsoft Academic Graph (MAG), where the nodes and the edges represent the entities engaging in scholarly communications and the relationships among them, respectively. The frequently updated data set and a few software tools central to the underlying AI components are distributed under an open data license for research and commercial applications. This paper describes the design, schema, and technical and business motivations behind MAG and elaborates how MAG can be used in analytics, search, and recommendation scenarios. How AI plays an important role in avoiding various biases and human induced errors in other data sets and how the technologies can be further improved in the future are also discussed.</jats:p>","an ongoing project explores the extent to which artificial intelligence (ai), specifically in the areas of natural language processing and semantic reasoning, can be exploited to facilitate the studies of science by deploying software agents equipped with natural language understanding capabilities to read scholarly publications on the web. the knowledge extracted by these ai agents is organized into a heterogeneous graph, called microsoft academic graph (mag), where the nodes and the edges represent the entities engaging in scholarly communications and the relationships among them, respectively. the frequently updated data set and a few software tools central to the underlying ai components are distributed under an open data license for research and commercial applications. this paper describes the design, schema, and technical and business motivations behind mag and elaborates how mag can be used in analytics, search, and recommendation scenarios. how ai plays an important role in avoiding various biases and human induced errors in other data sets and how the technologies can be further improved in the future are also discussed."
http://orkg.org/orkg/resource/R70068,Identification of potential treatments for COVID-19 through artificial intelligence-enabled phenomic analysis of human cells infected with SARS-CoV-2,10.1101/2020.04.21.054387,crossref,"<jats:title>Abstract</jats:title><jats:p>To identify potential therapeutic stop-gaps for SARS-CoV-2, we evaluated a library of 1,670 approved and reference compounds in an unbiased, cellular image-based screen for their ability to suppress the broad impacts of the SARS-CoV-2 virus on phenomic profiles of human renal cortical epithelial cells using deep learning. In our assay, remdesivir is the only antiviral tested with strong efficacy, neither chloroquine nor hydroxychloroquine have any beneficial effect in this human cell model, and a small number of compounds not currently being pursued clinically for SARS-CoV-2 have efficacy. We observed weak but beneficial class effects of β-blockers, mTOR/PI3K inhibitors and Vitamin D analogues and a mild amplification of the viral phenotype with β-agonists.</jats:p>","abstract to identify potential therapeutic stop-gaps for sars-cov-2, we evaluated a library of 1,670 approved and reference compounds in an unbiased, cellular image-based screen for their ability to suppress the broad impacts of the sars-cov-2 virus on phenomic profiles of human renal cortical epithelial cells using deep learning. in our assay, remdesivir is the only antiviral tested with strong efficacy, neither chloroquine nor hydroxychloroquine have any beneficial effect in this human cell model, and a small number of compounds not currently being pursued clinically for sars-cov-2 have efficacy. we observed weak but beneficial class effects of β-blockers, mtor/pi3k inhibitors and vitamin d analogues and a mild amplification of the viral phenotype with β-agonists."
http://orkg.org/orkg/resource/R70866,OpenBiodiv: A Knowledge Graph for Literature-Extracted Linked Open Data in Biodiversity Science,10.3390/publications7020038,crossref,"<jats:p>Hundreds of years of biodiversity research have resulted in the accumulation of a substantial pool of communal knowledge; however, most of it is stored in silos isolated from each other, such as published articles or monographs. The need for a system to store and manage collective biodiversity knowledge in a community-agreed and interoperable open format has evolved into the concept of the Open Biodiversity Knowledge Management System (OBKMS). This paper presents OpenBiodiv: An OBKMS that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. It is presented as a Linked Open Dataset generated from scientific literature. OpenBiodiv encompasses data extracted from more than 5000 scholarly articles published by Pensoft and many more taxonomic treatments extracted by Plazi from journals of other publishers. The data from both sources are converted to Resource Description Framework (RDF) and integrated in a graph database using the OpenBiodiv-O ontology and an RDF version of the Global Biodiversity Information Facility (GBIF) taxonomic backbone. Through the application of semantic technologies, the project showcases the value of open publishing of Findable, Accessible, Interoperable, Reusable (FAIR) data towards the establishment of open science practices in the biodiversity domain.</jats:p>","hundreds of years of biodiversity research have resulted in the accumulation of a substantial pool of communal knowledge; however, most of it is stored in silos isolated from each other, such as published articles or monographs. the need for a system to store and manage collective biodiversity knowledge in a community-agreed and interoperable open format has evolved into the concept of the open biodiversity knowledge management system (obkms). this paper presents openbiodiv: an obkms that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. it is presented as a linked open dataset generated from scientific literature. openbiodiv encompasses data extracted from more than 5000 scholarly articles published by pensoft and many more taxonomic treatments extracted by plazi from journals of other publishers. the data from both sources are converted to resource description framework (rdf) and integrated in a graph database using the openbiodiv-o ontology and an rdf version of the global biodiversity information facility (gbif) taxonomic backbone. through the application of semantic technologies, the project showcases the value of open publishing of findable, accessible, interoperable, reusable (fair) data towards the establishment of open science practices in the biodiversity domain."
http://orkg.org/orkg/resource/R71525,Catch crop diversity increases rhizosphere carbon input and soil microbial biomass,10.1007/s00374-020-01475-8,crossref,"<jats:title>Abstract</jats:title><jats:p>Catch crops increase plant species richness in crop rotations, but are most often grown as pure stands. Here, we investigate the impacts of increasing plant diversity in catch crop rotations on rhizosphere C input and microbial utilization. Mustard (<jats:italic>Sinapis alba</jats:italic> L.) planted as a single cultivar was compared to diversified catch crop mixtures of four (Mix4) or 12 species (Mix12). We traced the C transfer from shoots to roots towards the soil microbial community and the soil respiration in a <jats:sup>13</jats:sup>C pulse labelling field experiment. Net CO<jats:sub>2</jats:sub>-C uptake from the atmosphere increased by two times in mix 4 and more than three times in mix 12. Higher net ecosystem C production was linked to increasing catch crop diversity and increased belowground transfer rates of recently fixed photoassimilates. The higher rhizosphere C input stimulated the growth and activity of the soil microbiome, which was investigated by phospholipid fatty acid (PLFA) analyses. Total microbial biomass increased from 14 to 22\xa0g\xa0m<jats:sup>−2</jats:sup> as compared to the fallow and was 18 and 8% higher for mix 12 and mix 4 as compared to mustard. In particular, the fungal and actinobacterial communities profited the most from the higher belowground C input and their biomass increased by 3.4 and 1.3 times as compared to the fallow. The residence time of the <jats:sup>13</jats:sup>C pulse, traced in the CO<jats:sub>2</jats:sub> flux from the soil environment, increased with plant diversity by up to 1.8 times. The results of this study suggest positive impacts of plant diversity on C cycling by higher atmospheric C uptake, higher transport rates towards the rhizosphere, higher microbial incorporation and prolonged residence time in the soil environment. We conclude that diversified catch crop mixtures improve the efficiency of C cycling in cropping systems and provide a promising tool for sustainable soil management.</jats:p>","abstract catch crops increase plant species richness in crop rotations, but are most often grown as pure stands. here, we investigate the impacts of increasing plant diversity in catch crop rotations on rhizosphere c input and microbial utilization. mustard ( sinapis alba l.) planted as a single cultivar was compared to diversified catch crop mixtures of four (mix4) or 12 species (mix12). we traced the c transfer from shoots to roots towards the soil microbial community and the soil respiration in a 13 c pulse labelling field experiment. net co 2 -c uptake from the atmosphere increased by two times in mix 4 and more than three times in mix 12. higher net ecosystem c production was linked to increasing catch crop diversity and increased belowground transfer rates of recently fixed photoassimilates. the higher rhizosphere c input stimulated the growth and activity of the soil microbiome, which was investigated by phospholipid fatty acid (plfa) analyses. total microbial biomass increased from 14 to 22\xa0g\xa0m −2 as compared to the fallow and was 18 and 8% higher for mix 12 and mix 4 as compared to mustard. in particular, the fungal and actinobacterial communities profited the most from the higher belowground c input and their biomass increased by 3.4 and 1.3 times as compared to the fallow. the residence time of the 13 c pulse, traced in the co 2 flux from the soil environment, increased with plant diversity by up to 1.8 times. the results of this study suggest positive impacts of plant diversity on c cycling by higher atmospheric c uptake, higher transport rates towards the rhizosphere, higher microbial incorporation and prolonged residence time in the soil environment. we conclude that diversified catch crop mixtures improve the efficiency of c cycling in cropping systems and provide a promising tool for sustainable soil management."
http://orkg.org/orkg/resource/R73135,The data-literature interlinking service: Towards a common infrastructure for sharing data-article links,10.1108/prog-06-2016-0048,crossref,"<jats:sec>\n<jats:title content-type=""abstract-subheading"">Purpose</jats:title>\n<jats:p>Research data publishing is today widely regarded as crucial for reproducibility, proper assessment of scientific results, and as a way for researchers to get proper credit for sharing their data. However, several challenges need to be solved to fully realize its potential, one of them being the development of a global standard for links between research data and literature. Current linking solutions are mostly based on bilateral, ad hoc agreements between publishers and data centers. These operate in silos so that content cannot be readily combined to deliver a network graph connecting research data and literature in a comprehensive and reliable way. The Research Data Alliance (RDA) Publishing Data Services Working Group (PDS-WG) aims to address this issue of fragmentation by bringing together different stakeholders to agree on a common infrastructure for sharing links between datasets and literature. The paper aims to discuss these issues.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>\n<jats:p>This paper presents the synergic effort of the RDA PDS-WG and the OpenAIRE infrastructure toward enabling a common infrastructure for exchanging data-literature links by realizing and operating the Data-Literature Interlinking (DLI) Service. The DLI Service populates and provides access to a graph of data set-literature links (at the time of writing close to five million, and growing) collected from a variety of major data centers, publishers, and research organizations.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Findings</jats:title>\n<jats:p>To achieve its objectives, the Service proposes an interoperable exchange data model and format, based on which it collects and publishes links, thereby offering the opportunity to validate such common approach on real-case scenarios, with real providers and consumers. Feedback of these actors will drive continuous refinement of the both data model and exchange format, supporting the further development of the Service to become an essential part of a universal, open, cross-platform, cross-discipline solution for collecting, and sharing data set-literature links.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>\n<jats:p>This realization of the DLI Service is the first technical, cross-community, and collaborative effort in the direction of establishing a common infrastructure for facilitating the exchange of data set-literature links. As a result of its operation and underlying community effort, a new activity, name Scholix, has been initiated involving the technological level stakeholders such as DataCite and CrossRef.</jats:p>\n</jats:sec>","\n purpose \n research data publishing is today widely regarded as crucial for reproducibility, proper assessment of scientific results, and as a way for researchers to get proper credit for sharing their data. however, several challenges need to be solved to fully realize its potential, one of them being the development of a global standard for links between research data and literature. current linking solutions are mostly based on bilateral, ad hoc agreements between publishers and data centers. these operate in silos so that content cannot be readily combined to deliver a network graph connecting research data and literature in a comprehensive and reliable way. the research data alliance (rda) publishing data services working group (pds-wg) aims to address this issue of fragmentation by bringing together different stakeholders to agree on a common infrastructure for sharing links between datasets and literature. the paper aims to discuss these issues. \n \n \n design/methodology/approach \n this paper presents the synergic effort of the rda pds-wg and the openaire infrastructure toward enabling a common infrastructure for exchanging data-literature links by realizing and operating the data-literature interlinking (dli) service. the dli service populates and provides access to a graph of data set-literature links (at the time of writing close to five million, and growing) collected from a variety of major data centers, publishers, and research organizations. \n \n \n findings \n to achieve its objectives, the service proposes an interoperable exchange data model and format, based on which it collects and publishes links, thereby offering the opportunity to validate such common approach on real-case scenarios, with real providers and consumers. feedback of these actors will drive continuous refinement of the both data model and exchange format, supporting the further development of the service to become an essential part of a universal, open, cross-platform, cross-discipline solution for collecting, and sharing data set-literature links. \n \n \n originality/value \n this realization of the dli service is the first technical, cross-community, and collaborative effort in the direction of establishing a common infrastructure for facilitating the exchange of data set-literature links. as a result of its operation and underlying community effort, a new activity, name scholix, has been initiated involving the technological level stakeholders such as datacite and crossref. \n"
http://orkg.org/orkg/resource/R73154,DataCite: Lessons Learned on Persistent Identifiers for Research Data,10.2218/ijdc.v11i2.421,crossref,"<jats:p>Data are the infrastructure of science and they serve as the groundwork for scientific pursuits. Data publication has emerged as a game-changing breakthrough in scholarly communication. Data form the outputs of research but also are a gateway to new hypotheses, enabling new scientific insights and driving innovation. And yet stakeholders across the scholarly ecosystem, including practitioners, institutions, and funders of scientific research are increasingly concerned about the lack of sharing and reuse of research data. Across disciplines and countries, researchers, funders, and publishers are pushing for a more effective research environment, minimizing the duplication of work and maximizing the interaction between researchers. Availability, discoverability, and reproducibility of research outputs are key factors to support data reuse and make possible this new environment of highly collaborative research. An interoperable e-infrastructure is imperative in order to develop new platforms and services for to data publication and reuse. DataCite has been working to establish and promote methods to locate, identify and share information about research data. Along with service development, DataCite supports and advocates for the standards behind persistent identifiers (in particular DOIs, Digital Object Identifiers) for data and other research outputs. Persistent identifiers allow different platforms to exchange information consistently and unambiguously and provide a reliable way to track citations and reuse. Because of this, data publication can become a reality from a technical standpoint, but the adoption of data publication and data citation as a practice by researchers is still in its early stages. Since 2009, DataCite has been developing a series of tools and services to foster the adoption of data publication and citation among the research community. Through the years, DataCite has worked in a close collaboration with interdisciplinary partners on these issues and we have gained insight into the development of data publication workflows. This paper describes the types of different actions and the lessons learned by DataCite.\xa0</jats:p>","data are the infrastructure of science and they serve as the groundwork for scientific pursuits. data publication has emerged as a game-changing breakthrough in scholarly communication. data form the outputs of research but also are a gateway to new hypotheses, enabling new scientific insights and driving innovation. and yet stakeholders across the scholarly ecosystem, including practitioners, institutions, and funders of scientific research are increasingly concerned about the lack of sharing and reuse of research data. across disciplines and countries, researchers, funders, and publishers are pushing for a more effective research environment, minimizing the duplication of work and maximizing the interaction between researchers. availability, discoverability, and reproducibility of research outputs are key factors to support data reuse and make possible this new environment of highly collaborative research. an interoperable e-infrastructure is imperative in order to develop new platforms and services for to data publication and reuse. datacite has been working to establish and promote methods to locate, identify and share information about research data. along with service development, datacite supports and advocates for the standards behind persistent identifiers (in particular dois, digital object identifiers) for data and other research outputs. persistent identifiers allow different platforms to exchange information consistently and unambiguously and provide a reliable way to track citations and reuse. because of this, data publication can become a reality from a technical standpoint, but the adoption of data publication and data citation as a practice by researchers is still in its early stages. since 2009, datacite has been developing a series of tools and services to foster the adoption of data publication and citation among the research community. through the years, datacite has worked in a close collaboration with interdisciplinary partners on these issues and we have gained insight into the development of data publication workflows. this paper describes the types of different actions and the lessons learned by datacite.\xa0"
http://orkg.org/orkg/resource/R74113,Risk of mortality in patients infected with SARS-CoV-2 variant of concern 202012/1: matched cohort study,10.1136/bmj.n579,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Objective</jats:title>\n            <jats:p>To establish whether there is any change in mortality from infection with a new variant of SARS-CoV-2, designated a variant of concern (VOC-202012/1) in December 2020, compared with circulating SARS-CoV-2 variants.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Design</jats:title>\n            <jats:p>Matched cohort study.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Setting</jats:title>\n            <jats:p>Community based (pillar 2) covid-19 testing centres in the UK using the TaqPath assay (a proxy measure of VOC-202012/1 infection).</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Participants</jats:title>\n            <jats:p>54\u2009906 matched pairs of participants who tested positive for SARS-CoV-2 in pillar 2 between 1 October 2020 and 29 January 2021, followed-up until 12 February 2021. Participants were matched on age, sex, ethnicity, index of multiple deprivation, lower tier local authority region, and sample date of positive specimens, and differed only by detectability of the spike protein gene using the TaqPath assay.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Main outcome measure</jats:title>\n            <jats:p>Death within 28 days of the first positive SARS-CoV-2 test result.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>The mortality hazard ratio associated with infection with VOC-202012/1 compared with infection with previously circulating variants was 1.64 (95% confidence interval 1.32 to 2.04) in patients who tested positive for covid-19 in the community. In this comparatively low risk group, this represents an increase in deaths from 2.5 to 4.1 per 1000 detected cases.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>The probability that the risk of mortality is increased by infection with VOC-202012/01 is high. If this finding is generalisable to other populations, infection with VOC-202012/1 has the potential to cause substantial additional mortality compared with previously circulating variants. Healthcare capacity planning and national and international control policies are all impacted by this finding, with increased mortality lending weight to the argument that further coordinated and stringent measures are justified to reduce deaths from SARS-CoV-2.</jats:p>\n          </jats:sec>","abstract \n \n objective \n to establish whether there is any change in mortality from infection with a new variant of sars-cov-2, designated a variant of concern (voc-202012/1) in december 2020, compared with circulating sars-cov-2 variants. \n \n \n design \n matched cohort study. \n \n \n setting \n community based (pillar 2) covid-19 testing centres in the uk using the taqpath assay (a proxy measure of voc-202012/1 infection). \n \n \n participants \n 54\u2009906 matched pairs of participants who tested positive for sars-cov-2 in pillar 2 between 1 october 2020 and 29 january 2021, followed-up until 12 february 2021. participants were matched on age, sex, ethnicity, index of multiple deprivation, lower tier local authority region, and sample date of positive specimens, and differed only by detectability of the spike protein gene using the taqpath assay. \n \n \n main outcome measure \n death within 28 days of the first positive sars-cov-2 test result. \n \n \n results \n the mortality hazard ratio associated with infection with voc-202012/1 compared with infection with previously circulating variants was 1.64 (95% confidence interval 1.32 to 2.04) in patients who tested positive for covid-19 in the community. in this comparatively low risk group, this represents an increase in deaths from 2.5 to 4.1 per 1000 detected cases. \n \n \n conclusions \n the probability that the risk of mortality is increased by infection with voc-202012/01 is high. if this finding is generalisable to other populations, infection with voc-202012/1 has the potential to cause substantial additional mortality compared with previously circulating variants. healthcare capacity planning and national and international control policies are all impacted by this finding, with increased mortality lending weight to the argument that further coordinated and stringent measures are justified to reduce deaths from sars-cov-2. \n"
http://orkg.org/orkg/resource/R74580,Scholarly event characteristics in four fields of science: a metrics-based analysis,10.1007/s11192-020-03391-y,crossref,"<jats:title>Abstract</jats:title><jats:p>One of the key channels of scholarly knowledge exchange are scholarly events such as conferences, workshops, symposiums, etc.; such events are especially important and popular in Computer Science, Engineering, and Natural Sciences.\nHowever, scholars encounter problems in finding relevant information about upcoming events and statistics on their historic evolution.\nIn order to obtain a better understanding of scholarly event characteristics in four fields of science, we analyzed the metadata of scholarly events of four major fields of science, namely Computer Science, Physics, Engineering, and Mathematics using Scholarly Events Quality Assessment suite, a suite of ten metrics.\nIn particular, we analyzed renowned scholarly events belonging to five sub-fields within Computer Science, namely World Wide Web, Computer Vision, Software Engineering, Data Management, as well as Security and Privacy.\nThis analysis is based on a systematic approach using descriptive statistics as well as exploratory data analysis. The findings are on the one hand interesting to observe the general evolution and success factors of scholarly events; on the other hand, they allow (prospective) event organizers, publishers, and committee members to assess the progress of their event over time and compare it to other events in the same field; and finally, they help researchers to make more informed decisions when selecting suitable venues for presenting their work.\nBased on these findings, a set of recommendations has been concluded to different stakeholders, involving event organizers, potential authors, proceedings publishers, and sponsors. Our comprehensive dataset of scholarly events of the aforementioned fields is openly available in a semantic format and maintained collaboratively at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://OpenResearch.org"">OpenResearch.org</jats:ext-link>.</jats:p>","abstract one of the key channels of scholarly knowledge exchange are scholarly events such as conferences, workshops, symposiums, etc.; such events are especially important and popular in computer science, engineering, and natural sciences.\nhowever, scholars encounter problems in finding relevant information about upcoming events and statistics on their historic evolution.\nin order to obtain a better understanding of scholarly event characteristics in four fields of science, we analyzed the metadata of scholarly events of four major fields of science, namely computer science, physics, engineering, and mathematics using scholarly events quality assessment suite, a suite of ten metrics.\nin particular, we analyzed renowned scholarly events belonging to five sub-fields within computer science, namely world wide web, computer vision, software engineering, data management, as well as security and privacy.\nthis analysis is based on a systematic approach using descriptive statistics as well as exploratory data analysis. the findings are on the one hand interesting to observe the general evolution and success factors of scholarly events; on the other hand, they allow (prospective) event organizers, publishers, and committee members to assess the progress of their event over time and compare it to other events in the same field; and finally, they help researchers to make more informed decisions when selecting suitable venues for presenting their work.\nbased on these findings, a set of recommendations has been concluded to different stakeholders, involving event organizers, potential authors, proceedings publishers, and sponsors. our comprehensive dataset of scholarly events of the aforementioned fields is openly available in a semantic format and maintained collaboratively at openresearch.org ."
http://orkg.org/orkg/resource/R74004,From anomaly detection to rumour detection using data streams of social platforms,10.14778/3329772.3329778,crossref,"<jats:p>Social platforms became a major source of rumours. While rumours can have severe real-world implications, their detection is notoriously hard: Content on social platforms is short and lacks semantics; it spreads quickly through a dynamically evolving network; and without considering the context of content, it may be impossible to arrive at a truthful interpretation. Traditional approaches to rumour detection, however, exploit solely a single content modality, e.g., social media posts, which limits their detection accuracy. In this paper, we cope with the aforementioned challenges by means of a multi-modal approach to rumour detection that identifies anomalies in both, the entities (e.g., users, posts, and hashtags) of a social platform and their relations. Based on local anomalies, we show how to detect rumours at the network level, following a graph-based scan approach. In addition, we propose incremental methods, which enable us to detect rumours using streaming data of social platforms. We illustrate the effectiveness and efficiency of our approach with a real-world dataset of 4M tweets with more than 1000 rumours.</jats:p>","social platforms became a major source of rumours. while rumours can have severe real-world implications, their detection is notoriously hard: content on social platforms is short and lacks semantics; it spreads quickly through a dynamically evolving network; and without considering the context of content, it may be impossible to arrive at a truthful interpretation. traditional approaches to rumour detection, however, exploit solely a single content modality, e.g., social media posts, which limits their detection accuracy. in this paper, we cope with the aforementioned challenges by means of a multi-modal approach to rumour detection that identifies anomalies in both, the entities (e.g., users, posts, and hashtags) of a social platform and their relations. based on local anomalies, we show how to detect rumours at the network level, following a graph-based scan approach. in addition, we propose incremental methods, which enable us to detect rumours using streaming data of social platforms. we illustrate the effectiveness and efficiency of our approach with a real-world dataset of 4m tweets with more than 1000 rumours."
http://orkg.org/orkg/resource/R74326,A mixed-methods analysis of mobility behavior changes in the COVID-19 era in a rural case study,10.1186/s12544-021-00472-8,crossref,"<jats:title>Abstract</jats:title><jats:sec>\n                <jats:title>Background</jats:title>\n                <jats:p>As a reaction to the novel coronavirus disease (COVID-19), countries around the globe have implemented various measures to reduce the spread of the virus. The transportation sector is particularly affected by the pandemic situation. The current study aims to contribute to the empirical knowledge regarding the effects of the coronavirus situation on the mobility of people by (1) broadening the perspective to the mobility rural area’s residents and (2) providing subjective data concerning the perceived changes of affected persons’ mobility practices, as these two aspects have scarcely been considered in research so far.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Methods</jats:title>\n                <jats:p>To address these research gaps, a mixed-methods study was conducted that integrates a qualitative telephone interview study (<jats:italic>N</jats:italic>\u2009=\u200915) and a quantitative household survey (<jats:italic>N</jats:italic>\u2009=\u2009301). The rural district of <jats:italic>Altmarkkreis Salzwedel</jats:italic> in Northern Germany was chosen as a model region.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Results</jats:title>\n                <jats:p>The results provide in-depth insights into the changing mobility practices of residents of a rural area during the legal restrictions to stem the spread of the virus. A high share of respondents (62.6%) experienced no changes in their mobility behavior due to the COVID-19 pandemic situation. However, nearly one third of trips were also cancelled overall. A modal shift was observed towards the reduction of trips by car and bus, and an increase of trips by bike. The share of trips by foot was unchanged. The majority of respondents did not predict strong long-term effects of the corona pandemic on their mobility behavior.</jats:p>\n              </jats:sec>","abstract \n background \n as a reaction to the novel coronavirus disease (covid-19), countries around the globe have implemented various measures to reduce the spread of the virus. the transportation sector is particularly affected by the pandemic situation. the current study aims to contribute to the empirical knowledge regarding the effects of the coronavirus situation on the mobility of people by (1) broadening the perspective to the mobility rural area’s residents and (2) providing subjective data concerning the perceived changes of affected persons’ mobility practices, as these two aspects have scarcely been considered in research so far. \n \n methods \n to address these research gaps, a mixed-methods study was conducted that integrates a qualitative telephone interview study ( n \u2009=\u200915) and a quantitative household survey ( n \u2009=\u2009301). the rural district of altmarkkreis salzwedel in northern germany was chosen as a model region. \n \n results \n the results provide in-depth insights into the changing mobility practices of residents of a rural area during the legal restrictions to stem the spread of the virus. a high share of respondents (62.6%) experienced no changes in their mobility behavior due to the covid-19 pandemic situation. however, nearly one third of trips were also cancelled overall. a modal shift was observed towards the reduction of trips by car and bus, and an increase of trips by bike. the share of trips by foot was unchanged. the majority of respondents did not predict strong long-term effects of the corona pandemic on their mobility behavior. \n"
http://orkg.org/orkg/resource/R74055,Case fatality risk of the SARS-CoV-2 variant of concern B.1.1.7 in England,10.1101/2021.03.04.21252528,crossref,<jats:title>Abstract</jats:title><jats:p>The B.1.1.7 variant of concern (VOC) is increasing in prevalence across Europe. Accurate estimation of disease severity associated with this VOC is critical for pandemic planning. We found increased risk of death for VOC compared with non-VOC cases in England (HR: 1.67 (95% CI: 1.34 - 2.09; P&lt;.0001)). Absolute risk of death by 28-days increased with age and comorbidities. VOC has potential to spread faster with higher mortality than the pandemic to date.</jats:p>,abstract the b.1.1.7 variant of concern (voc) is increasing in prevalence across europe. accurate estimation of disease severity associated with this voc is critical for pandemic planning. we found increased risk of death for voc compared with non-voc cases in england (hr: 1.67 (95% ci: 1.34 - 2.09; p&lt;.0001)). absolute risk of death by 28-days increased with age and comorbidities. voc has potential to spread faster with higher mortality than the pandemic to date.
http://orkg.org/orkg/resource/R74069,Increased mortality in community-tested cases of SARS-CoV-2 lineage B.1.1.7,10.1038/s41586-021-03426-1,crossref,"<jats:p>SARS-CoV-2 lineage B.1.1.7, a variant first detected in the United Kingdom in September 2020<jats:sup>1</jats:sup>, has spread to multiple countries worldwide. Several studies have established that B.1.1.7 is more transmissible than preexisting variants, but have not identified whether it leads to any change in disease severity<jats:sup>2</jats:sup>. We analyse a dataset linking 2,245,263 positive SARS-CoV-2 community tests and 17,452 COVID-19 deaths in England from 1 September 2020 to 14 February 2021. For 1,146,534 (51%) of these tests, the presence or absence of B.1.1.7 can be identified because of mutations in this lineage preventing PCR amplification of the spike gene target (S gene target failure, SGTF<jats:sup>1</jats:sup>). Based on 4,945 deaths with known SGTF status, we estimate that the hazard of death associated with SGTF is 55% (95% CI 39–72%) higher after adjustment for age, sex, ethnicity, deprivation, care home residence, local authority of residence and test date. This corresponds to the absolute risk of death for a 55–69-year-old male increasing from 0.6% to 0.9% (95% CI 0.8–1.0%) within 28 days after a positive test in the community. Correcting for misclassification of SGTF and missingness in SGTF status, we estimate a 61% (42–82%) higher hazard of death associated with B.1.1.7. Our analysis suggests that B.1.1.7 is not only more transmissible than preexisting SARS-CoV-2 variants, but may also cause more severe illness.</jats:p>","sars-cov-2 lineage b.1.1.7, a variant first detected in the united kingdom in september 2020 1 , has spread to multiple countries worldwide. several studies have established that b.1.1.7 is more transmissible than preexisting variants, but have not identified whether it leads to any change in disease severity 2 . we analyse a dataset linking 2,245,263 positive sars-cov-2 community tests and 17,452 covid-19 deaths in england from 1 september 2020 to 14 february 2021. for 1,146,534 (51%) of these tests, the presence or absence of b.1.1.7 can be identified because of mutations in this lineage preventing pcr amplification of the spike gene target (s gene target failure, sgtf 1 ). based on 4,945 deaths with known sgtf status, we estimate that the hazard of death associated with sgtf is 55% (95% ci 39–72%) higher after adjustment for age, sex, ethnicity, deprivation, care home residence, local authority of residence and test date. this corresponds to the absolute risk of death for a 55–69-year-old male increasing from 0.6% to 0.9% (95% ci 0.8–1.0%) within 28 days after a positive test in the community. correcting for misclassification of sgtf and missingness in sgtf status, we estimate a 61% (42–82%) higher hazard of death associated with b.1.1.7. our analysis suggests that b.1.1.7 is not only more transmissible than preexisting sars-cov-2 variants, but may also cause more severe illness."
http://orkg.org/orkg/resource/R74081,Risk of mortality in patients infected with SARS-CoV-2 variant of concern B.1.1.7 matched cohort study,10.1136/bmj.n579,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Objective</jats:title>\n            <jats:p>To establish whether there is any change in mortality from infection with a new variant of SARS-CoV-2, designated a variant of concern (VOC-202012/1) in December 2020, compared with circulating SARS-CoV-2 variants.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Design</jats:title>\n            <jats:p>Matched cohort study.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Setting</jats:title>\n            <jats:p>Community based (pillar 2) covid-19 testing centres in the UK using the TaqPath assay (a proxy measure of VOC-202012/1 infection).</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Participants</jats:title>\n            <jats:p>54\u2009906 matched pairs of participants who tested positive for SARS-CoV-2 in pillar 2 between 1 October 2020 and 29 January 2021, followed-up until 12 February 2021. Participants were matched on age, sex, ethnicity, index of multiple deprivation, lower tier local authority region, and sample date of positive specimens, and differed only by detectability of the spike protein gene using the TaqPath assay.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Main outcome measure</jats:title>\n            <jats:p>Death within 28 days of the first positive SARS-CoV-2 test result.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>The mortality hazard ratio associated with infection with VOC-202012/1 compared with infection with previously circulating variants was 1.64 (95% confidence interval 1.32 to 2.04) in patients who tested positive for covid-19 in the community. In this comparatively low risk group, this represents an increase in deaths from 2.5 to 4.1 per 1000 detected cases.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>The probability that the risk of mortality is increased by infection with VOC-202012/01 is high. If this finding is generalisable to other populations, infection with VOC-202012/1 has the potential to cause substantial additional mortality compared with previously circulating variants. Healthcare capacity planning and national and international control policies are all impacted by this finding, with increased mortality lending weight to the argument that further coordinated and stringent measures are justified to reduce deaths from SARS-CoV-2.</jats:p>\n          </jats:sec>","abstract \n \n objective \n to establish whether there is any change in mortality from infection with a new variant of sars-cov-2, designated a variant of concern (voc-202012/1) in december 2020, compared with circulating sars-cov-2 variants. \n \n \n design \n matched cohort study. \n \n \n setting \n community based (pillar 2) covid-19 testing centres in the uk using the taqpath assay (a proxy measure of voc-202012/1 infection). \n \n \n participants \n 54\u2009906 matched pairs of participants who tested positive for sars-cov-2 in pillar 2 between 1 october 2020 and 29 january 2021, followed-up until 12 february 2021. participants were matched on age, sex, ethnicity, index of multiple deprivation, lower tier local authority region, and sample date of positive specimens, and differed only by detectability of the spike protein gene using the taqpath assay. \n \n \n main outcome measure \n death within 28 days of the first positive sars-cov-2 test result. \n \n \n results \n the mortality hazard ratio associated with infection with voc-202012/1 compared with infection with previously circulating variants was 1.64 (95% confidence interval 1.32 to 2.04) in patients who tested positive for covid-19 in the community. in this comparatively low risk group, this represents an increase in deaths from 2.5 to 4.1 per 1000 detected cases. \n \n \n conclusions \n the probability that the risk of mortality is increased by infection with voc-202012/01 is high. if this finding is generalisable to other populations, infection with voc-202012/1 has the potential to cause substantial additional mortality compared with previously circulating variants. healthcare capacity planning and national and international control policies are all impacted by this finding, with increased mortality lending weight to the argument that further coordinated and stringent measures are justified to reduce deaths from sars-cov-2. \n"
http://orkg.org/orkg/resource/R74080,Risk of mortality in patients infected with SARS-CoV-2 variant of concern B.1.1.7: matched cohort study,10.1136/bmj.n579,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Objective</jats:title>\n            <jats:p>To establish whether there is any change in mortality from infection with a new variant of SARS-CoV-2, designated a variant of concern (VOC-202012/1) in December 2020, compared with circulating SARS-CoV-2 variants.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Design</jats:title>\n            <jats:p>Matched cohort study.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Setting</jats:title>\n            <jats:p>Community based (pillar 2) covid-19 testing centres in the UK using the TaqPath assay (a proxy measure of VOC-202012/1 infection).</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Participants</jats:title>\n            <jats:p>54\u2009906 matched pairs of participants who tested positive for SARS-CoV-2 in pillar 2 between 1 October 2020 and 29 January 2021, followed-up until 12 February 2021. Participants were matched on age, sex, ethnicity, index of multiple deprivation, lower tier local authority region, and sample date of positive specimens, and differed only by detectability of the spike protein gene using the TaqPath assay.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Main outcome measure</jats:title>\n            <jats:p>Death within 28 days of the first positive SARS-CoV-2 test result.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>The mortality hazard ratio associated with infection with VOC-202012/1 compared with infection with previously circulating variants was 1.64 (95% confidence interval 1.32 to 2.04) in patients who tested positive for covid-19 in the community. In this comparatively low risk group, this represents an increase in deaths from 2.5 to 4.1 per 1000 detected cases.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>The probability that the risk of mortality is increased by infection with VOC-202012/01 is high. If this finding is generalisable to other populations, infection with VOC-202012/1 has the potential to cause substantial additional mortality compared with previously circulating variants. Healthcare capacity planning and national and international control policies are all impacted by this finding, with increased mortality lending weight to the argument that further coordinated and stringent measures are justified to reduce deaths from SARS-CoV-2.</jats:p>\n          </jats:sec>","abstract \n \n objective \n to establish whether there is any change in mortality from infection with a new variant of sars-cov-2, designated a variant of concern (voc-202012/1) in december 2020, compared with circulating sars-cov-2 variants. \n \n \n design \n matched cohort study. \n \n \n setting \n community based (pillar 2) covid-19 testing centres in the uk using the taqpath assay (a proxy measure of voc-202012/1 infection). \n \n \n participants \n 54\u2009906 matched pairs of participants who tested positive for sars-cov-2 in pillar 2 between 1 october 2020 and 29 january 2021, followed-up until 12 february 2021. participants were matched on age, sex, ethnicity, index of multiple deprivation, lower tier local authority region, and sample date of positive specimens, and differed only by detectability of the spike protein gene using the taqpath assay. \n \n \n main outcome measure \n death within 28 days of the first positive sars-cov-2 test result. \n \n \n results \n the mortality hazard ratio associated with infection with voc-202012/1 compared with infection with previously circulating variants was 1.64 (95% confidence interval 1.32 to 2.04) in patients who tested positive for covid-19 in the community. in this comparatively low risk group, this represents an increase in deaths from 2.5 to 4.1 per 1000 detected cases. \n \n \n conclusions \n the probability that the risk of mortality is increased by infection with voc-202012/01 is high. if this finding is generalisable to other populations, infection with voc-202012/1 has the potential to cause substantial additional mortality compared with previously circulating variants. healthcare capacity planning and national and international control policies are all impacted by this finding, with increased mortality lending weight to the argument that further coordinated and stringent measures are justified to reduce deaths from sars-cov-2. \n"
http://orkg.org/orkg/resource/R74362,Estimation of Water Budget Components of the Sakarya River Basin by Using the WEAP-PGM Model,10.3390/w11020271,crossref,"<jats:p>The use of water resources has increased with rapid population growth, industrial development, and agricultural activities. Besides, the problem might increase with the potential climate change impacts on water quantity. Thus, sustainable use of water resources becomes crucial. Modeling studies provide scientific support to the analysis of water resource problems and develop strategies for current and potential problems for the sustainable management of water resources. In this study, WEAP-PGM (Water Evaluation and Planning System—Plant Growth Model) was applied to the Sakarya River Basin in Turkey, where almost 50% of the area is agricultural land. The main goals in the study are compiling/integrating available data from different sources in a data-scarce region for hydrological models, and estimating the water budget components of Sakarya River Basin on an annual basis as well as investigating the applicability of WEAP-PGM. General model performance ratings indicated that model simulations represent streamflow variations at acceptable levels. Model results revealed that, runoff is 4747 million m3, flow to groundwater is 3065 million m3 and evapotranspiration is 23,011 million m3. This model setup can be used as a baseline for calculating the crop yields under climate change in the context of water-food-energy nexus in the further studies.</jats:p>","the use of water resources has increased with rapid population growth, industrial development, and agricultural activities. besides, the problem might increase with the potential climate change impacts on water quantity. thus, sustainable use of water resources becomes crucial. modeling studies provide scientific support to the analysis of water resource problems and develop strategies for current and potential problems for the sustainable management of water resources. in this study, weap-pgm (water evaluation and planning system—plant growth model) was applied to the sakarya river basin in turkey, where almost 50% of the area is agricultural land. the main goals in the study are compiling/integrating available data from different sources in a data-scarce region for hydrological models, and estimating the water budget components of sakarya river basin on an annual basis as well as investigating the applicability of weap-pgm. general model performance ratings indicated that model simulations represent streamflow variations at acceptable levels. model results revealed that, runoff is 4747 million m3, flow to groundwater is 3065 million m3 and evapotranspiration is 23,011 million m3. this model setup can be used as a baseline for calculating the crop yields under climate change in the context of water-food-energy nexus in the further studies."
http://orkg.org/orkg/resource/R74102,Risk of mortality in patients infected with SARS-CoV-2 variant of concern 202012/1: matched cohort study,10.1136/bmj.n579,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Objective</jats:title>\n            <jats:p>To establish whether there is any change in mortality from infection with a new variant of SARS-CoV-2, designated a variant of concern (VOC-202012/1) in December 2020, compared with circulating SARS-CoV-2 variants.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Design</jats:title>\n            <jats:p>Matched cohort study.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Setting</jats:title>\n            <jats:p>Community based (pillar 2) covid-19 testing centres in the UK using the TaqPath assay (a proxy measure of VOC-202012/1 infection).</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Participants</jats:title>\n            <jats:p>54\u2009906 matched pairs of participants who tested positive for SARS-CoV-2 in pillar 2 between 1 October 2020 and 29 January 2021, followed-up until 12 February 2021. Participants were matched on age, sex, ethnicity, index of multiple deprivation, lower tier local authority region, and sample date of positive specimens, and differed only by detectability of the spike protein gene using the TaqPath assay.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Main outcome measure</jats:title>\n            <jats:p>Death within 28 days of the first positive SARS-CoV-2 test result.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>The mortality hazard ratio associated with infection with VOC-202012/1 compared with infection with previously circulating variants was 1.64 (95% confidence interval 1.32 to 2.04) in patients who tested positive for covid-19 in the community. In this comparatively low risk group, this represents an increase in deaths from 2.5 to 4.1 per 1000 detected cases.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>The probability that the risk of mortality is increased by infection with VOC-202012/01 is high. If this finding is generalisable to other populations, infection with VOC-202012/1 has the potential to cause substantial additional mortality compared with previously circulating variants. Healthcare capacity planning and national and international control policies are all impacted by this finding, with increased mortality lending weight to the argument that further coordinated and stringent measures are justified to reduce deaths from SARS-CoV-2.</jats:p>\n          </jats:sec>","abstract \n \n objective \n to establish whether there is any change in mortality from infection with a new variant of sars-cov-2, designated a variant of concern (voc-202012/1) in december 2020, compared with circulating sars-cov-2 variants. \n \n \n design \n matched cohort study. \n \n \n setting \n community based (pillar 2) covid-19 testing centres in the uk using the taqpath assay (a proxy measure of voc-202012/1 infection). \n \n \n participants \n 54\u2009906 matched pairs of participants who tested positive for sars-cov-2 in pillar 2 between 1 october 2020 and 29 january 2021, followed-up until 12 february 2021. participants were matched on age, sex, ethnicity, index of multiple deprivation, lower tier local authority region, and sample date of positive specimens, and differed only by detectability of the spike protein gene using the taqpath assay. \n \n \n main outcome measure \n death within 28 days of the first positive sars-cov-2 test result. \n \n \n results \n the mortality hazard ratio associated with infection with voc-202012/1 compared with infection with previously circulating variants was 1.64 (95% confidence interval 1.32 to 2.04) in patients who tested positive for covid-19 in the community. in this comparatively low risk group, this represents an increase in deaths from 2.5 to 4.1 per 1000 detected cases. \n \n \n conclusions \n the probability that the risk of mortality is increased by infection with voc-202012/01 is high. if this finding is generalisable to other populations, infection with voc-202012/1 has the potential to cause substantial additional mortality compared with previously circulating variants. healthcare capacity planning and national and international control policies are all impacted by this finding, with increased mortality lending weight to the argument that further coordinated and stringent measures are justified to reduce deaths from sars-cov-2. \n"
http://orkg.org/orkg/resource/R74118,"Organizing Spaces: Meeting Arenas as a Social Movement Infrastructure between Organization, Network, and Institution",10.1177/0170840613479232,crossref,"<jats:p> In recent years, social movement scholars have shown increasing interest in the internal lives of social movements, but this turn from “social movements as actors” to “social movements as spaces” has not yet led to a conceptual apparatus that addresses the key role of face-to-face meetings, especially in the inter-organizational domain of mesomobilization. Building on the concept of “partial organization”, the paper develops the concept of “meeting arena” as a hybrid of three forms of social order: organization, institution, and network. It is argued that the complex figuration of meeting arenas in a social movement or protest mobilization constitutes an infrastructure that synchronizes the dispersed activities of movement actors in time and space. This infrastructure is not an entirely emergent phenomenon but is also the result of conscious decisions by organizers. Heuristic, methodological, and theoretical implications of this novel perspective on social movements are discussed, highlighting especially the potential of the distinction between organizing and mobilizing as two intertwined but essentially different types of social movement activity. </jats:p>","in recent years, social movement scholars have shown increasing interest in the internal lives of social movements, but this turn from “social movements as actors” to “social movements as spaces” has not yet led to a conceptual apparatus that addresses the key role of face-to-face meetings, especially in the inter-organizational domain of mesomobilization. building on the concept of “partial organization”, the paper develops the concept of “meeting arena” as a hybrid of three forms of social order: organization, institution, and network. it is argued that the complex figuration of meeting arenas in a social movement or protest mobilization constitutes an infrastructure that synchronizes the dispersed activities of movement actors in time and space. this infrastructure is not an entirely emergent phenomenon but is also the result of conscious decisions by organizers. heuristic, methodological, and theoretical implications of this novel perspective on social movements are discussed, highlighting especially the potential of the distinction between organizing and mobilizing as two intertwined but essentially different types of social movement activity."
http://orkg.org/orkg/resource/R74366,Estimation of Water Budget Components of the Sakarya River Basin by Using the WEAP-PGM Model,10.3390/w11020271,crossref,"<jats:p>The use of water resources has increased with rapid population growth, industrial development, and agricultural activities. Besides, the problem might increase with the potential climate change impacts on water quantity. Thus, sustainable use of water resources becomes crucial. Modeling studies provide scientific support to the analysis of water resource problems and develop strategies for current and potential problems for the sustainable management of water resources. In this study, WEAP-PGM (Water Evaluation and Planning System—Plant Growth Model) was applied to the Sakarya River Basin in Turkey, where almost 50% of the area is agricultural land. The main goals in the study are compiling/integrating available data from different sources in a data-scarce region for hydrological models, and estimating the water budget components of Sakarya River Basin on an annual basis as well as investigating the applicability of WEAP-PGM. General model performance ratings indicated that model simulations represent streamflow variations at acceptable levels. Model results revealed that, runoff is 4747 million m3, flow to groundwater is 3065 million m3 and evapotranspiration is 23,011 million m3. This model setup can be used as a baseline for calculating the crop yields under climate change in the context of water-food-energy nexus in the further studies.</jats:p>","the use of water resources has increased with rapid population growth, industrial development, and agricultural activities. besides, the problem might increase with the potential climate change impacts on water quantity. thus, sustainable use of water resources becomes crucial. modeling studies provide scientific support to the analysis of water resource problems and develop strategies for current and potential problems for the sustainable management of water resources. in this study, weap-pgm (water evaluation and planning system—plant growth model) was applied to the sakarya river basin in turkey, where almost 50% of the area is agricultural land. the main goals in the study are compiling/integrating available data from different sources in a data-scarce region for hydrological models, and estimating the water budget components of sakarya river basin on an annual basis as well as investigating the applicability of weap-pgm. general model performance ratings indicated that model simulations represent streamflow variations at acceptable levels. model results revealed that, runoff is 4747 million m3, flow to groundwater is 3065 million m3 and evapotranspiration is 23,011 million m3. this model setup can be used as a baseline for calculating the crop yields under climate change in the context of water-food-energy nexus in the further studies."
http://orkg.org/orkg/resource/R74947,Palm biomass waste as supplementary source of electricity generation in Ghana: Case of the Juaben Oil Mills,10.1177/0958305x17744079,crossref,"<jats:p> The purpose of the study was to examine the potential of palm biomass, taking a case of the Juaben Oil Mills in the Ashanti Region of Ghana, which has over the years generated electricity for its operations from its waste products and other benefits that have accrued to the company and the host community. Primary data collection and intensive desk study approaches were employed albeit qualitatively, to describe the use of palm biomass as supplementary source of electricity generation in Ghana. The study showed that there is enough potential (waste by-products) for electricity generation to supplement current production from hydropower to meet growing demand. However, policy and institutional arrangements do not easily allow generation and extension for communal benefits. The authors therefore recommend a relook at existing policy and institutional arrangements to help promote this alternative source of energy for efficient and sustainable domestic and industrial uses. We also argue that ensuring efficiency in energy generation calls for R&amp;D into its commercial potential and explore more efficient means of managing industrial and other agro biofuel wastes in developing economies. Future energy policy must also create and harness diversity of available biomass resources and reduce the delivery risks of the resources. </jats:p>","the purpose of the study was to examine the potential of palm biomass, taking a case of the juaben oil mills in the ashanti region of ghana, which has over the years generated electricity for its operations from its waste products and other benefits that have accrued to the company and the host community. primary data collection and intensive desk study approaches were employed albeit qualitatively, to describe the use of palm biomass as supplementary source of electricity generation in ghana. the study showed that there is enough potential (waste by-products) for electricity generation to supplement current production from hydropower to meet growing demand. however, policy and institutional arrangements do not easily allow generation and extension for communal benefits. the authors therefore recommend a relook at existing policy and institutional arrangements to help promote this alternative source of energy for efficient and sustainable domestic and industrial uses. we also argue that ensuring efficiency in energy generation calls for r&amp;d into its commercial potential and explore more efficient means of managing industrial and other agro biofuel wastes in developing economies. future energy policy must also create and harness diversity of available biomass resources and reduce the delivery risks of the resources."
http://orkg.org/orkg/resource/R74389,Consuming and producing linked open data: The case of OpenCourseWare,10.1108/PROG-07-2012-0045,crossref,"<jats:sec>\n               <jats:title content-type=""abstract-heading"">Purpose</jats:title>\n               <jats:p> – The aim of this paper is to present an initiative to apply the principles of Linked Data to enhance the search and discovery of OpenCourseWare (OCW) contents created and shared by the universities. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title>\n               <jats:p> – This paper is a case study of how linked data technologies can be applied for the enhancement of open learning contents. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Findings</jats:title>\n               <jats:p> – Results presented under the umbrella of OCW-Universia consortium, as the integration and access to content from different repositories OCW and the development of a query method to access these data, reveal that linked data would offer a solution to filter and select semantically those open educational contents, and automatically are linked to the linked open data cloud. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Originality/value</jats:title>\n               <jats:p> – The new OCW-Universia integration with linked data adds new features to the initial framework including improved query mechanisms and interoperability.</jats:p>\n            </jats:sec>","\n purpose \n – the aim of this paper is to present an initiative to apply the principles of linked data to enhance the search and discovery of opencourseware (ocw) contents created and shared by the universities. \n \n \n design/methodology/approach \n – this paper is a case study of how linked data technologies can be applied for the enhancement of open learning contents. \n \n \n findings \n – results presented under the umbrella of ocw-universia consortium, as the integration and access to content from different repositories ocw and the development of a query method to access these data, reveal that linked data would offer a solution to filter and select semantically those open educational contents, and automatically are linked to the linked open data cloud. \n \n \n originality/value \n – the new ocw-universia integration with linked data adds new features to the initial framework including improved query mechanisms and interoperability. \n"
http://orkg.org/orkg/resource/R74228,"THE DUBLIN DASHBOARD: DESIGN AND DEVELOPMENT OF A REAL-TIME
ANALYTICAL URBAN DASHBOARD",10.5194/isprs-annals-iv-4-w1-19-2016,crossref,"<jats:p>Abstract. As many cities increase in size across multiple dimensions such as population, economic output and physical size, new methods for understanding and managing cities are required. Data produced by and about urban environments offer insight into what is happening in cities. Real-time data from sensors within the city record current transport and environmental conditions such as noise levels, water levels, journey times and public transport delays. Similarly administrative data such as demographics, employment statistics, property prices and crime rates all provide insight into how a city is evolving. Traditionally, these data were maintained separately and managed by individual city departments. Advances in technology and a move to open-government have placed many of these data in the public domain. Urban dashboards have emerged as a technique to visualise these data in an accessible way. This paper describes the implementation of one such dashboard, the Dublin Dashboard, an interactive website which collects, analyses and visualises data from a variety of sources about Dublin in Ireland through a series of interactive maps, graphs and applications. This paper describes the approach, the data and the technology used to develop the Dublin Dashboard and acts as a guideline for developing urban dashboards in other cities.\n                    </jats:p>","abstract. as many cities increase in size across multiple dimensions such as population, economic output and physical size, new methods for understanding and managing cities are required. data produced by and about urban environments offer insight into what is happening in cities. real-time data from sensors within the city record current transport and environmental conditions such as noise levels, water levels, journey times and public transport delays. similarly administrative data such as demographics, employment statistics, property prices and crime rates all provide insight into how a city is evolving. traditionally, these data were maintained separately and managed by individual city departments. advances in technology and a move to open-government have placed many of these data in the public domain. urban dashboards have emerged as a technique to visualise these data in an accessible way. this paper describes the implementation of one such dashboard, the dublin dashboard, an interactive website which collects, analyses and visualises data from a variety of sources about dublin in ireland through a series of interactive maps, graphs and applications. this paper describes the approach, the data and the technology used to develop the dublin dashboard and acts as a guideline for developing urban dashboards in other cities.\n"
http://orkg.org/orkg/resource/R74317,Impact of COVID-19 pandemic on mobility in ten countries and associated perceived risk for all transport modes,10.1371/journal.pone.0245886,crossref,"<jats:p>The restrictive measures implemented in response to the COVID-19 pandemic have triggered sudden massive changes to travel behaviors of people all around the world. This study examines the individual mobility patterns for all transport modes (walk, bicycle, motorcycle, car driven alone, car driven in company, bus, subway, tram, train, airplane) before and during the restrictions adopted in ten countries on six continents: Australia, Brazil, China, Ghana, India, Iran, Italy, Norway, South Africa and the United States. This cross-country study also aims at understanding the predictors of protective behaviors related to the transport sector and COVID-19. Findings hinge upon an online survey conducted in May 2020 (N = 9,394). The empirical results quantify tremendous disruptions for both commuting and non-commuting travels, highlighting substantial reductions in the frequency of all types of trips and use of all modes. In terms of potential virus spread, airplanes and buses are perceived to be the riskiest transport modes, while avoidance of public transport is consistently found across the countries. According to the Protection Motivation Theory, the study sheds new light on the fact that two indicators, namely income inequality, expressed as Gini index, and the reported number of deaths due to COVID-19 per 100,000 inhabitants, aggravate respondents’ perceptions. This research indicates that socio-economic inequality and morbidity are not only related to actual health risks, as well documented in the relevant literature, but also to the perceived risks. These findings document the global impact of the COVID-19 crisis as well as provide guidance for transportation practitioners in developing future strategies.</jats:p>","the restrictive measures implemented in response to the covid-19 pandemic have triggered sudden massive changes to travel behaviors of people all around the world. this study examines the individual mobility patterns for all transport modes (walk, bicycle, motorcycle, car driven alone, car driven in company, bus, subway, tram, train, airplane) before and during the restrictions adopted in ten countries on six continents: australia, brazil, china, ghana, india, iran, italy, norway, south africa and the united states. this cross-country study also aims at understanding the predictors of protective behaviors related to the transport sector and covid-19. findings hinge upon an online survey conducted in may 2020 (n = 9,394). the empirical results quantify tremendous disruptions for both commuting and non-commuting travels, highlighting substantial reductions in the frequency of all types of trips and use of all modes. in terms of potential virus spread, airplanes and buses are perceived to be the riskiest transport modes, while avoidance of public transport is consistently found across the countries. according to the protection motivation theory, the study sheds new light on the fact that two indicators, namely income inequality, expressed as gini index, and the reported number of deaths due to covid-19 per 100,000 inhabitants, aggravate respondents’ perceptions. this research indicates that socio-economic inequality and morbidity are not only related to actual health risks, as well documented in the relevant literature, but also to the perceived risks. these findings document the global impact of the covid-19 crisis as well as provide guidance for transportation practitioners in developing future strategies."
http://orkg.org/orkg/resource/R75714,Association between physical activity and changes in intestinal microbiota composition: A systematic review,10.1371/journal.pone.0247039,crossref,"<jats:sec id=""sec001"">\n<jats:title>Introduction</jats:title>\n<jats:p>The intestinal microbiota comprises bacteria, fungi, archaea, protists, helminths and viruses that symbiotically inhabit the digestive system. To date, research has provided limited data on the possible association between an active lifestyle and a healthy composition of human microbiota. This review was aimed to summarize the results of human studies comparing the microbiome of healthy individuals with different physical activity amounts.</jats:p>\n</jats:sec>\n<jats:sec id=""sec002"">\n<jats:title>Methods</jats:title>\n<jats:p>We searched Medline/Ovid, NIH/PubMed, and Academic Search Complete between August–October 2020. Inclusion criteria comprised: (a) cross-sectional studies focused on comparing gut microbiome among subjects with different physical activity levels; (b) studies describing human gut microbiome responses to any type of exercise stimulus; (c) studies containing healthy adult women and men. We excluded studies containing diet modifications, probiotic or prebiotic consumption, as well as studies focused on diabetes, hypertension, cancer, hormonal dysfunction. Methodological quality and risk of bias for each study were assessed using the Risk Of Bias In Non-randomized Studies—of Interventions tool. The results from cross-sectional and longitudinal studies are shown independently.</jats:p>\n</jats:sec>\n<jats:sec id=""sec003"">\n<jats:title>Results</jats:title>\n<jats:p>A total of 17 articles were eligible for inclusion: ten cross-sectional and seven longitudinal studies. Main outcomes vary significantly according to physical activity amounts in longitudinal studies. We identified discrete changes in diversity indexes and relative abundance of certain bacteria in active people.</jats:p>\n</jats:sec>\n<jats:sec id=""sec004"">\n<jats:title>Conclusion</jats:title>\n<jats:p>As literature in this field is rapidly growing, it is important that studies incorporate diverse methods to evaluate other aspects related to active lifestyles such as sleep and dietary patterns. Exploration of other groups such as viruses, archaea and parasites may lead to a better understanding of gut microbiota adaptation to physical activity and sports and its potentially beneficial effects on host metabolism and endurance.</jats:p>\n</jats:sec>","\n introduction \n the intestinal microbiota comprises bacteria, fungi, archaea, protists, helminths and viruses that symbiotically inhabit the digestive system. to date, research has provided limited data on the possible association between an active lifestyle and a healthy composition of human microbiota. this review was aimed to summarize the results of human studies comparing the microbiome of healthy individuals with different physical activity amounts. \n \n \n methods \n we searched medline/ovid, nih/pubmed, and academic search complete between august–october 2020. inclusion criteria comprised: (a) cross-sectional studies focused on comparing gut microbiome among subjects with different physical activity levels; (b) studies describing human gut microbiome responses to any type of exercise stimulus; (c) studies containing healthy adult women and men. we excluded studies containing diet modifications, probiotic or prebiotic consumption, as well as studies focused on diabetes, hypertension, cancer, hormonal dysfunction. methodological quality and risk of bias for each study were assessed using the risk of bias in non-randomized studies—of interventions tool. the results from cross-sectional and longitudinal studies are shown independently. \n \n \n results \n a total of 17 articles were eligible for inclusion: ten cross-sectional and seven longitudinal studies. main outcomes vary significantly according to physical activity amounts in longitudinal studies. we identified discrete changes in diversity indexes and relative abundance of certain bacteria in active people. \n \n \n conclusion \n as literature in this field is rapidly growing, it is important that studies incorporate diverse methods to evaluate other aspects related to active lifestyles such as sleep and dietary patterns. exploration of other groups such as viruses, archaea and parasites may lead to a better understanding of gut microbiota adaptation to physical activity and sports and its potentially beneficial effects on host metabolism and endurance. \n"
http://orkg.org/orkg/resource/R74698,"Design, Setup, and Evaluation of a Compensation System for the Light Deflection Effect Occurring When Measuring Wrought-Hot Objects Using Optical Triangulation Methods",10.3390/met10070908,crossref,"<jats:p>In this paper, we present a system to compensate for the light deflection effect during the optical geometry measurement of a wrought-hot object. The acquired 3D data can be used to analyze the formed geometry of a component directly after a hot forging process without waiting for the needed cooling time to room temperature. This may be used to parameterize the process and to detect defect components early in the production process, among others. The light deflection as the deviation from the linear path of the light is caused by an inhomogeneous refractive index field surrounding the hot object. We present the design and setup for a nozzle-based forced air flow actuator, which suppresses the light deflection effect. The design process includes a simulation of the developing field, as well as of the interaction of the field with an external forced air flow. The cooling effect of the air flow is evaluated, and conclusions are drawn from the conflicting interests of good measurement conditions against the forced cooling of the hot object. The findings are then implemented in the physical setup of the suppression system. The system is evaluated using a previously established method based on optical triangulation and fringe projection. Other occurring effects and their influence on the evaluation are considered and discussed.</jats:p>","in this paper, we present a system to compensate for the light deflection effect during the optical geometry measurement of a wrought-hot object. the acquired 3d data can be used to analyze the formed geometry of a component directly after a hot forging process without waiting for the needed cooling time to room temperature. this may be used to parameterize the process and to detect defect components early in the production process, among others. the light deflection as the deviation from the linear path of the light is caused by an inhomogeneous refractive index field surrounding the hot object. we present the design and setup for a nozzle-based forced air flow actuator, which suppresses the light deflection effect. the design process includes a simulation of the developing field, as well as of the interaction of the field with an external forced air flow. the cooling effect of the air flow is evaluated, and conclusions are drawn from the conflicting interests of good measurement conditions against the forced cooling of the hot object. the findings are then implemented in the physical setup of the suppression system. the system is evaluated using a previously established method based on optical triangulation and fringe projection. other occurring effects and their influence on the evaluation are considered and discussed."
http://orkg.org/orkg/resource/R75151,Semantic assessment of smart healthcare ontology,,crossref,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>Health-care ontologies and their terminologies play a vital role in knowledge representation and data integration for health information. In health-care systems, Internet of Technology (IoT) technologies provide data exchange among various entities and ontologies offer a formal description to present the knowledge of health-care domains. These ontologies are advised to assure the quality of their adoption and applicability in the real world.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>Ontology assessment is an integral part of ontology construction and maintenance. It is always performed to identify inconsistencies and modeling errors by the experts during the ontology development. A smart health-care ontology (SHCO) has been designed to deal with health-care information and IoT devices. In this paper, an integrated approach has been proposed to assess the SHCO on different assessment tools such as Themis, Test-Driven Development (TDD)onto, Protégé and OOPs! Several test cases are framed to assess the ontology on these tools, in this research, Themis and TDDonto tools provide the verification for the test cases while Protégé and OOPs! provides validation of modeled knowledge in the ontology.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>As of the best knowledge, no other study has been presented earlier to conduct the integrated assessment on different tools. All test cases are successfully analyzed on these tools and results are drawn and compared with other ontologies.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>The developed ontology is analyzed on different verification and validation tools to assure the quality of ontologies.</jats:p></jats:sec>","purpose health-care ontologies and their terminologies play a vital role in knowledge representation and data integration for health information. in health-care systems, internet of technology (iot) technologies provide data exchange among various entities and ontologies offer a formal description to present the knowledge of health-care domains. these ontologies are advised to assure the quality of their adoption and applicability in the real world. design/methodology/approach ontology assessment is an integral part of ontology construction and maintenance. it is always performed to identify inconsistencies and modeling errors by the experts during the ontology development. a smart health-care ontology (shco) has been designed to deal with health-care information and iot devices. in this paper, an integrated approach has been proposed to assess the shco on different assessment tools such as themis, test-driven development (tdd)onto, protégé and oops! several test cases are framed to assess the ontology on these tools, in this research, themis and tddonto tools provide the verification for the test cases while protégé and oops! provides validation of modeled knowledge in the ontology. findings as of the best knowledge, no other study has been presented earlier to conduct the integrated assessment on different tools. all test cases are successfully analyzed on these tools and results are drawn and compared with other ontologies. originality/value the developed ontology is analyzed on different verification and validation tools to assure the quality of ontologies."
http://orkg.org/orkg/resource/R74963,Fpocket: An open source platform for ligand pocket detection,10.1186/1471-2105-10-168,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Virtual screening methods start to be well established as effective approaches to identify hits, candidates and leads for drug discovery research. Among those, structure based virtual screening (SBVS) approaches aim at docking collections of small compounds in the target structure to identify potent compounds. For SBVS, the identification of candidate pockets in protein structures is a key feature, and the recent years have seen increasing interest in developing methods for pocket and cavity detection on protein surfaces.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>Fpocket is an open source pocket detection package based on Voronoi tessellation and alpha spheres built on top of the publicly available package Qhull. The modular source code is organised around a central library of functions, a basis for three main programs: (i) Fpocket, to perform pocket identification, (ii) Tpocket, to organise pocket detection benchmarking on a set of known protein-ligand complexes, and (iii) Dpocket, to collect pocket descriptor values on a set of proteins. Fpocket is written in the C programming language, which makes it a platform well suited for the scientific community willing to develop new scoring functions and extract various pocket descriptors on a large scale level. Fpocket 1.0, relying on a simple scoring function, is able to detect 94% and 92% of the pockets within the best three ranked pockets from the holo and apo proteins respectively, outperforming the standards of the field, while being faster.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusion</jats:title>\n            <jats:p>Fpocket provides a rapid, open source and stable basis for further developments related to protein pocket detection, efficient pocket descriptor extraction, or drugablity prediction purposes. Fpocket is freely available under the GNU GPL license at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""http://fpocket.sourceforge.net"" ext-link-type=""uri"">http://fpocket.sourceforge.net</jats:ext-link>.</jats:p>\n          </jats:sec>","abstract \n \n background \n virtual screening methods start to be well established as effective approaches to identify hits, candidates and leads for drug discovery research. among those, structure based virtual screening (sbvs) approaches aim at docking collections of small compounds in the target structure to identify potent compounds. for sbvs, the identification of candidate pockets in protein structures is a key feature, and the recent years have seen increasing interest in developing methods for pocket and cavity detection on protein surfaces. \n \n \n results \n fpocket is an open source pocket detection package based on voronoi tessellation and alpha spheres built on top of the publicly available package qhull. the modular source code is organised around a central library of functions, a basis for three main programs: (i) fpocket, to perform pocket identification, (ii) tpocket, to organise pocket detection benchmarking on a set of known protein-ligand complexes, and (iii) dpocket, to collect pocket descriptor values on a set of proteins. fpocket is written in the c programming language, which makes it a platform well suited for the scientific community willing to develop new scoring functions and extract various pocket descriptors on a large scale level. fpocket 1.0, relying on a simple scoring function, is able to detect 94% and 92% of the pockets within the best three ranked pockets from the holo and apo proteins respectively, outperforming the standards of the field, while being faster. \n \n \n conclusion \n fpocket provides a rapid, open source and stable basis for further developments related to protein pocket detection, efficient pocket descriptor extraction, or drugablity prediction purposes. fpocket is freely available under the gnu gpl license at http://fpocket.sourceforge.net . \n"
http://orkg.org/orkg/resource/R74701,Adapted Fringe Projection Sequences for Changing Illumination Conditions on the Example of Measuring a Wrought-Hot Object Influenced by Forced Cooling ,10.3390/s21051599,crossref,"<jats:p>Optical 3D geometry reconstruction, or more specific, fringe projection profilometry, is a state-of-the-art technique for the measurement of the shape of objects in confined spaces or under rough environmental conditions, e.g., while inspecting a wrought-hot specimen after a forging operation. While the contact-less method enables the measurement of such an object, the results are influenced by the light deflection effect occurring due to the inhomogeneous refractive index field induced by the hot air around the measurand. However, the developed active compensation methods to fight this issue exhibits a major drawback, namely an additional cooling of the object and a subsequent transient illumination component. In this paper, we investigate the cooling and its effect on temporal phase reconstruction algorithms and take a theoretical approach to its compensation. The simulated compensation measures are transferred to a fringe projection profilometry setup and are evaluated using established and newly developed methods. The results show a significant improvement when measuring specimens under a transient illumination and are easily transferable to any kind of multi-frequency phase-shift measurement.</jats:p>","optical 3d geometry reconstruction, or more specific, fringe projection profilometry, is a state-of-the-art technique for the measurement of the shape of objects in confined spaces or under rough environmental conditions, e.g., while inspecting a wrought-hot specimen after a forging operation. while the contact-less method enables the measurement of such an object, the results are influenced by the light deflection effect occurring due to the inhomogeneous refractive index field induced by the hot air around the measurand. however, the developed active compensation methods to fight this issue exhibits a major drawback, namely an additional cooling of the object and a subsequent transient illumination component. in this paper, we investigate the cooling and its effect on temporal phase reconstruction algorithms and take a theoretical approach to its compensation. the simulated compensation measures are transferred to a fringe projection profilometry setup and are evaluated using established and newly developed methods. the results show a significant improvement when measuring specimens under a transient illumination and are easily transferable to any kind of multi-frequency phase-shift measurement."
http://orkg.org/orkg/resource/R75432,A method for obtaining digital signatures and public-key cryptosystems,10.1145/359340.359342,crossref,"<jats:p>\n            An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: (1) Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intented recipient. Only he can decipher the message, since only he knows the corresponding decryption key. (2) A message can be “signed” using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in “electronic mail” and “electronic funds transfer” systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product,\n            <jats:italic>n</jats:italic>\n            , of two large secret primer numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d ≡ 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor,\n            <jats:italic>n</jats:italic>\n            .\n          </jats:p>","\n an encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. this has two important consequences: (1) couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intented recipient. only he can decipher the message, since only he knows the corresponding decryption key. (2) a message can be “signed” using a privately held decryption key. anyone can verify this signature using the corresponding publicly revealed encryption key. signatures cannot be forged, and a signer cannot later deny the validity of his signature. this has obvious applications in “electronic mail” and “electronic funds transfer” systems. a message is encrypted by representing it as a number m, raising m to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product,\n n \n , of two large secret primer numbers p and q. decryption is similar; only a different, secret, power d is used, where e * d ≡ 1(mod (p - 1) * (q - 1)). the security of the system rests in part on the difficulty of factoring the published divisor,\n n \n .\n"
http://orkg.org/orkg/resource/R74776,Propositionalization and embeddings: two sides of the same coin,10.1007/s10994-020-05890-8,crossref,"<jats:title>Abstract</jats:title><jats:p>Data preprocessing is an important component of machine learning pipelines, which requires ample time and resources. An integral part of preprocessing is data transformation into the format required by a given learning algorithm. This paper outlines some of the modern data processing techniques used in relational learning that enable data fusion from different input data types and formats into a single table data representation, focusing on the propositionalization and embedding data transformation approaches. While both approaches aim at transforming data into tabular data format, they use different terminology and task definitions, are perceived to address different goals, and are used in different contexts. This paper contributes a unifying framework that allows for improved understanding of these two data transformation techniques by presenting their unified definitions, and by explaining the similarities and differences between the two approaches as variants of a unified complex data transformation task. In addition to the unifying framework, the novelty of this paper is a unifying methodology combining propositionalization and embeddings, which benefits from the advantages of both in solving complex data transformation and learning tasks. We present two efficient implementations of the unifying methodology: an instance-based PropDRM approach, and a feature-based PropStar approach to data transformation and learning, together with their empirical evaluation on several relational problems. The results show that the new algorithms can outperform existing relational learners and can solve much larger problems.</jats:p>","abstract data preprocessing is an important component of machine learning pipelines, which requires ample time and resources. an integral part of preprocessing is data transformation into the format required by a given learning algorithm. this paper outlines some of the modern data processing techniques used in relational learning that enable data fusion from different input data types and formats into a single table data representation, focusing on the propositionalization and embedding data transformation approaches. while both approaches aim at transforming data into tabular data format, they use different terminology and task definitions, are perceived to address different goals, and are used in different contexts. this paper contributes a unifying framework that allows for improved understanding of these two data transformation techniques by presenting their unified definitions, and by explaining the similarities and differences between the two approaches as variants of a unified complex data transformation task. in addition to the unifying framework, the novelty of this paper is a unifying methodology combining propositionalization and embeddings, which benefits from the advantages of both in solving complex data transformation and learning tasks. we present two efficient implementations of the unifying methodology: an instance-based propdrm approach, and a feature-based propstar approach to data transformation and learning, together with their empirical evaluation on several relational problems. the results show that the new algorithms can outperform existing relational learners and can solve much larger problems."
http://orkg.org/orkg/resource/R76325,The Visitor Effect on Zoo Animals: Implications and Opportunities for Zoo Animal Welfare,10.3390/ani9060366,crossref,"<jats:p>Achieving and maintaining high standards of animal welfare is critical to the success of a modern zoo. Research has shown that an animal’s welfare is highly dependent on how various individual animal factors (e.g., species traits, genetics, temperament and previous experience) interact with environmental features (e.g., social grouping, enclosure design and sensory environment). One prominent feature of the zoo environment is the presence of visitors. Visitor contact can be unpredictable and intense, particularly in terms of auditory and visual interaction. Depending on an animal’s perception of this interaction, visitors can have either negative, neutral or positive impacts on zoo animal behaviour and welfare. This paper reviews the literature on the implications and potential opportunities of human-zoo animal interactions on animal behaviour and welfare, with the aim of stimulating interest, understanding and exploration of this important subject. The literature to date presents a mixed range of findings on the topic. It is possible this variation in the responses of zoo animals to visitors may be due to species-specific differences, the nature and intensity of the visitor interactions, enclosure design, and individual animal characteristics. Analysing these studies and better understanding animal preferences and motivations can provide insight into what animals find negatively and positively reinforcing in terms of visitor contact in a specific zoo setting. This understanding can then be applied to either safeguard welfare in cases where visitors can have a negative impact, or, conversely, it can be applied to highlight opportunities to encourage animal-visitor interaction in situations where animals experience positive emotions associated with visitor interaction.</jats:p>","achieving and maintaining high standards of animal welfare is critical to the success of a modern zoo. research has shown that an animal’s welfare is highly dependent on how various individual animal factors (e.g., species traits, genetics, temperament and previous experience) interact with environmental features (e.g., social grouping, enclosure design and sensory environment). one prominent feature of the zoo environment is the presence of visitors. visitor contact can be unpredictable and intense, particularly in terms of auditory and visual interaction. depending on an animal’s perception of this interaction, visitors can have either negative, neutral or positive impacts on zoo animal behaviour and welfare. this paper reviews the literature on the implications and potential opportunities of human-zoo animal interactions on animal behaviour and welfare, with the aim of stimulating interest, understanding and exploration of this important subject. the literature to date presents a mixed range of findings on the topic. it is possible this variation in the responses of zoo animals to visitors may be due to species-specific differences, the nature and intensity of the visitor interactions, enclosure design, and individual animal characteristics. analysing these studies and better understanding animal preferences and motivations can provide insight into what animals find negatively and positively reinforcing in terms of visitor contact in a specific zoo setting. this understanding can then be applied to either safeguard welfare in cases where visitors can have a negative impact, or, conversely, it can be applied to highlight opportunities to encourage animal-visitor interaction in situations where animals experience positive emotions associated with visitor interaction."
http://orkg.org/orkg/resource/R76423,Expanding horizons in historical linguistics with the 400-million word Corpus of Historical American English,10.3366/cor.2012.0024,crossref,"<jats:p> The Corpus of Historical American English (COHA) contains 400 million words in more than 100,000 texts which date from the 1810s to the 2000s. The corpus contains texts from fiction, popular magazines, newspapers and non-fiction books, and is balanced by genre from decade to decade. It has been carefully lemmatised and tagged for part-of-speech, and uses the same architecture as the Corpus of Contemporary American English (COCA), BYU-BNC, the TIME Corpus and other corpora. COHA allows for a wide range of research on changes in lexis, morphology, syntax, semantics, and American culture and society (as viewed through language change), in ways that are probably not possible with any text archive (e.g., Google Books) or any other corpus of historical American English. </jats:p>","the corpus of historical american english (coha) contains 400 million words in more than 100,000 texts which date from the 1810s to the 2000s. the corpus contains texts from fiction, popular magazines, newspapers and non-fiction books, and is balanced by genre from decade to decade. it has been carefully lemmatised and tagged for part-of-speech, and uses the same architecture as the corpus of contemporary american english (coca), byu-bnc, the time corpus and other corpora. coha allows for a wide range of research on changes in lexis, morphology, syntax, semantics, and american culture and society (as viewed through language change), in ways that are probably not possible with any text archive (e.g., google books) or any other corpus of historical american english."
http://orkg.org/orkg/resource/R75295,Fascioliasis An Ongoing Zoonotic Trematode Infection,10.1155/2015/786195,crossref,<jats:p>Zoonotic trematode infections are an area of the neglected tropical diseases that have become of major interest to global and public health due to their associated morbidity. Human fascioliasis is a trematode zoonosis of interest in public health. It affects approximately 50 million people worldwide and over 180 million are at risk of infection in both developed and underdeveloped countries. The one health paradigm is an area that seeks to address the problem of zoonotic infections through a comprehensive and sustainable approach. This review attempts to address the major challenges in managing human and animal fascioliasis with valuable insights gained from the one health paradigm to global health and multidisciplinary integration.</jats:p>,zoonotic trematode infections are an area of the neglected tropical diseases that have become of major interest to global and public health due to their associated morbidity. human fascioliasis is a trematode zoonosis of interest in public health. it affects approximately 50 million people worldwide and over 180 million are at risk of infection in both developed and underdeveloped countries. the one health paradigm is an area that seeks to address the problem of zoonotic infections through a comprehensive and sustainable approach. this review attempts to address the major challenges in managing human and animal fascioliasis with valuable insights gained from the one health paradigm to global health and multidisciplinary integration.
http://orkg.org/orkg/resource/R75685,Dietary patterns and cardiometabolic risk factors among adolescents: systematic review and meta-analysis,10.1017/s0007114518000533,crossref,"<jats:title>Abstract</jats:title><jats:p>This study systematised and synthesised the results of observational studies that were aimed at supporting the association between dietary patterns and cardiometabolic risk (CMR) factors among adolescents. Relevant scientific articles were searched in PUBMED, EMBASE, SCIENCE DIRECT, LILACS, WEB OF SCIENCE and SCOPUS. Observational studies that included the measurement of any CMR factor in healthy adolescents and dietary patterns were included. The search strategy retained nineteen articles for qualitative analysis. Among retained articles, the effects of dietary pattern on the means of BMI (<jats:italic>n</jats:italic> 18), waist circumference (WC) (<jats:italic>n</jats:italic> 9), systolic blood pressure (<jats:italic>n</jats:italic> 7), diastolic blood pressure (<jats:italic>n</jats:italic> 6), blood glucose (<jats:italic>n</jats:italic> 5) and lipid profile (<jats:italic>n</jats:italic> 5) were examined. Systematised evidence showed that an unhealthy dietary pattern appears to be associated with poor mean values of CMR factors among adolescents. However, evidence of a protective effect of healthier dietary patterns in this group remains unclear. Considering the number of studies with available information, a meta-analysis of anthropometric measures showed that dietary patterns characterised by the highest intake of unhealthy foods resulted in a higher mean BMI (0·57 kg/m²; 95 % CI 0·51, 0·63) and WC (0·57 cm; 95 % CI 0·47, 0·67) compared with low intake of unhealthy foods. Controversially, patterns characterised by a low intake of healthy foods were associated with a lower mean BMI (−0·41 kg/m²; 95 % CI −0·46,−0·36) and WC (−0·43 cm; 95 % CI −0·52,−0·33). An unhealthy dietary pattern may influence markers of CMR among adolescents, but considering the small number and limitations of the studies included, further studies are warranted to strengthen the evidence of this relation.</jats:p>","abstract this study systematised and synthesised the results of observational studies that were aimed at supporting the association between dietary patterns and cardiometabolic risk (cmr) factors among adolescents. relevant scientific articles were searched in pubmed, embase, science direct, lilacs, web of science and scopus. observational studies that included the measurement of any cmr factor in healthy adolescents and dietary patterns were included. the search strategy retained nineteen articles for qualitative analysis. among retained articles, the effects of dietary pattern on the means of bmi ( n 18), waist circumference (wc) ( n 9), systolic blood pressure ( n 7), diastolic blood pressure ( n 6), blood glucose ( n 5) and lipid profile ( n 5) were examined. systematised evidence showed that an unhealthy dietary pattern appears to be associated with poor mean values of cmr factors among adolescents. however, evidence of a protective effect of healthier dietary patterns in this group remains unclear. considering the number of studies with available information, a meta-analysis of anthropometric measures showed that dietary patterns characterised by the highest intake of unhealthy foods resulted in a higher mean bmi (0·57 kg/m²; 95 % ci 0·51, 0·63) and wc (0·57 cm; 95 % ci 0·47, 0·67) compared with low intake of unhealthy foods. controversially, patterns characterised by a low intake of healthy foods were associated with a lower mean bmi (−0·41 kg/m²; 95 % ci −0·46,−0·36) and wc (−0·43 cm; 95 % ci −0·52,−0·33). an unhealthy dietary pattern may influence markers of cmr among adolescents, but considering the small number and limitations of the studies included, further studies are warranted to strengthen the evidence of this relation."
http://orkg.org/orkg/resource/R75305,Extracting ontological knowledge from Java source code using Hidden Markov Models,https://doi.org/10.1515/comp-2019-0013,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Ontologies have become a key element since many decades in information systems such as in epidemiological surveillance domain. Building domain ontologies requires the access to domain knowledge owned by domain experts or contained in knowledge sources. However, domain experts are not always available for interviews. Therefore, there is a lot of value in using ontology learning which consists in automatic or semi-automatic extraction of ontological knowledge from structured or unstructured knowledge sources such as texts, databases, etc. Many techniques have been used but they all are limited in concepts, properties and terminology extraction leaving behind axioms and rules. Source code which naturally embed domain knowledge is rarely used. In this paper, we propose an approach based on Hidden Markov Models (HMMs) for concepts, properties, axioms and rules learning from Java source code. This approach is experimented with the source code of EPICAM, an epidemiological platform developed in Java and used in Cameroon for tuberculosis surveillance. Domain experts involved in the evaluation estimated that knowledge extracted was relevant to the domain. In addition, we performed an automatic evaluation of the relevance of the terms extracted to the medical domain by aligning them with ontologies hosted on Bioportal platform through the Ontology Recommender tool. The results were interesting since the terms extracted were covered at 82.9% by many biomedical ontologies such as NCIT, SNOWMEDCT and ONTOPARON.</jats:p>","abstract \n ontologies have become a key element since many decades in information systems such as in epidemiological surveillance domain. building domain ontologies requires the access to domain knowledge owned by domain experts or contained in knowledge sources. however, domain experts are not always available for interviews. therefore, there is a lot of value in using ontology learning which consists in automatic or semi-automatic extraction of ontological knowledge from structured or unstructured knowledge sources such as texts, databases, etc. many techniques have been used but they all are limited in concepts, properties and terminology extraction leaving behind axioms and rules. source code which naturally embed domain knowledge is rarely used. in this paper, we propose an approach based on hidden markov models (hmms) for concepts, properties, axioms and rules learning from java source code. this approach is experimented with the source code of epicam, an epidemiological platform developed in java and used in cameroon for tuberculosis surveillance. domain experts involved in the evaluation estimated that knowledge extracted was relevant to the domain. in addition, we performed an automatic evaluation of the relevance of the terms extracted to the medical domain by aligning them with ontologies hosted on bioportal platform through the ontology recommender tool. the results were interesting since the terms extracted were covered at 82.9% by many biomedical ontologies such as ncit, snowmedct and ontoparon."
http://orkg.org/orkg/resource/R74999,The Economic Lives of the Poor,10.1257/jep.21.1.141,crossref,"""<jats:p> The 1990 World Development Report from the World Bank defined the “extremely poor” people of the world as those who are currently living on no more than $1 per day per person. But how actually does one live on less than $1 per day? This essay is about the economic lives of the extremely poor: the choices they face, the constraints they grapple with, and the challenges they meet. A number of recent data sets and a body of new research allow us to start building an image of the way the extremely poor live their lives. Our discussion builds on household surveys conducted in 13 countries: Cote d'Ivoire, Guatemala, India, Indonesia, Mexico, Nicaragua, Pakistan, Panama, Papua New Guinea, Peru, South Africa, Tanzania, and Timor Leste (East Timor). These surveys provide detailed information on extremely poor households around the world, from Asia to Africa to Latin America, including information on what they consume, where they work, and how they save and borrow. We consider the extremely poor—those living in households where the consumption per capita is less than $1.08 per person per day—as well as the merely “poor”—defined as those who live under $2.16 a day—using 1993 purchasing power parity as benchmark. In keeping with convention, we call these the $1 and $2 dollar poverty lines, respectively. </jats:p>""",""" the 1990 world development report from the world bank defined the “extremely poor” people of the world as those who are currently living on no more than $1 per day per person. but how actually does one live on less than $1 per day? this essay is about the economic lives of the extremely poor: the choices they face, the constraints they grapple with, and the challenges they meet. a number of recent data sets and a body of new research allow us to start building an image of the way the extremely poor live their lives. our discussion builds on household surveys conducted in 13 countries: cote d'ivoire, guatemala, india, indonesia, mexico, nicaragua, pakistan, panama, papua new guinea, peru, south africa, tanzania, and timor leste (east timor). these surveys provide detailed information on extremely poor households around the world, from asia to africa to latin america, including information on what they consume, where they work, and how they save and borrow. we consider the extremely poor—those living in households where the consumption per capita is less than $1.08 per person per day—as well as the merely “poor”—defined as those who live under $2.16 a day—using 1993 purchasing power parity as benchmark. in keeping with convention, we call these the $1 and $2 dollar poverty lines, respectively. """
http://orkg.org/orkg/resource/R75019,The Impact of Pulsed Electric Field on the Extraction of Bioactive Compounds from Beetroot,10.3390/foods8070244,crossref,"<jats:p>Beetroot is a root vegetable rich in different bioactive components, such as vitamins, minerals, phenolics, carotenoids, nitrate, ascorbic acids, and betalains, that can have a positive effect on human health. The aim of this work was to study the influence of the pulsed electric field (PEF) at different electric field strengths (4.38 and 6.25 kV/cm), pulse number 10–30, and energy input 0–12.5 kJ/kg as a pretreatment method on the extraction of betalains from beetroot. The obtained results showed that the application of PEF pre-treatment significantly (p &lt; 0.05) influenced the efficiency of extraction of bioactive compounds from beetroot. The highest increase in the content of betalain compounds in the red beet’s extract (betanin by 329%, vulgaxanthin by 244%, compared to the control sample), was noted for 20 pulses of electric field at 4.38 kV/cm of strength. Treatment of the plant material with a PEF also resulted in an increase in the electrical conductivity compared to the non-treated sample due to the increase in cell membrane permeability, which was associated with leakage of substances able to conduct electricity, including mineral salts, into the intercellular space.</jats:p>","beetroot is a root vegetable rich in different bioactive components, such as vitamins, minerals, phenolics, carotenoids, nitrate, ascorbic acids, and betalains, that can have a positive effect on human health. the aim of this work was to study the influence of the pulsed electric field (pef) at different electric field strengths (4.38 and 6.25 kv/cm), pulse number 10–30, and energy input 0–12.5 kj/kg as a pretreatment method on the extraction of betalains from beetroot. the obtained results showed that the application of pef pre-treatment significantly (p &lt; 0.05) influenced the efficiency of extraction of bioactive compounds from beetroot. the highest increase in the content of betalain compounds in the red beet’s extract (betanin by 329%, vulgaxanthin by 244%, compared to the control sample), was noted for 20 pulses of electric field at 4.38 kv/cm of strength. treatment of the plant material with a pef also resulted in an increase in the electrical conductivity compared to the non-treated sample due to the increase in cell membrane permeability, which was associated with leakage of substances able to conduct electricity, including mineral salts, into the intercellular space."
http://orkg.org/orkg/resource/R75084,"A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks",,crossref,"<jats:p>A knowledge graph (KG), also known as a knowledge base, is a particular kind of network structure in which the node indicates entity and the edge represent relation. However, with the explosion of network volume, the problem of data sparsity that causes large-scale KG systems to calculate and manage difficultly has become more significant. For alleviating the issue, knowledge graph embedding is proposed to embed entities and relations in a KG to a low-, dense and continuous feature space, and endow the yield model with abilities of knowledge inference and fusion. In recent years, many researchers have poured much attention in this approach, and we will systematically introduce the existing state-of-the-art approaches and a variety of applications that benefit from these methods in this paper. In addition, we discuss future prospects for the development of techniques and application trends. Specifically, we first introduce the embedding models that only leverage the information of observed triplets in the KG. We illustrate the overall framework and specific idea and compare the advantages and disadvantages of such approaches. Next, we introduce the advanced models that utilize additional semantic information to improve the performance of the original methods. We divide the additional information into two categories, including textual descriptions and relation paths. The extension approaches in each category are described, following the same classification criteria as those defined for the triplet fact-based models. We then describe two experiments for comparing the performance of listed methods and mention some broader domain tasks such as question answering, recommender systems, and so forth. Finally, we collect several hurdles that need to be overcome and provide a few future research directions for knowledge graph embedding.</jats:p>","a knowledge graph (kg), also known as a knowledge base, is a particular kind of network structure in which the node indicates entity and the edge represent relation. however, with the explosion of network volume, the problem of data sparsity that causes large-scale kg systems to calculate and manage difficultly has become more significant. for alleviating the issue, knowledge graph embedding is proposed to embed entities and relations in a kg to a low-, dense and continuous feature space, and endow the yield model with abilities of knowledge inference and fusion. in recent years, many researchers have poured much attention in this approach, and we will systematically introduce the existing state-of-the-art approaches and a variety of applications that benefit from these methods in this paper. in addition, we discuss future prospects for the development of techniques and application trends. specifically, we first introduce the embedding models that only leverage the information of observed triplets in the kg. we illustrate the overall framework and specific idea and compare the advantages and disadvantages of such approaches. next, we introduce the advanced models that utilize additional semantic information to improve the performance of the original methods. we divide the additional information into two categories, including textual descriptions and relation paths. the extension approaches in each category are described, following the same classification criteria as those defined for the triplet fact-based models. we then describe two experiments for comparing the performance of listed methods and mention some broader domain tasks such as question answering, recommender systems, and so forth. finally, we collect several hurdles that need to be overcome and provide a few future research directions for knowledge graph embedding."
http://orkg.org/orkg/resource/R75154,Semantic assessment of smart healthcare ontology,,crossref,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>Health-care ontologies and their terminologies play a vital role in knowledge representation and data integration for health information. In health-care systems, Internet of Technology (IoT) technologies provide data exchange among various entities and ontologies offer a formal description to present the knowledge of health-care domains. These ontologies are advised to assure the quality of their adoption and applicability in the real world.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>Ontology assessment is an integral part of ontology construction and maintenance. It is always performed to identify inconsistencies and modeling errors by the experts during the ontology development. A smart health-care ontology (SHCO) has been designed to deal with health-care information and IoT devices. In this paper, an integrated approach has been proposed to assess the SHCO on different assessment tools such as Themis, Test-Driven Development (TDD)onto, Protégé and OOPs! Several test cases are framed to assess the ontology on these tools, in this research, Themis and TDDonto tools provide the verification for the test cases while Protégé and OOPs! provides validation of modeled knowledge in the ontology.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>As of the best knowledge, no other study has been presented earlier to conduct the integrated assessment on different tools. All test cases are successfully analyzed on these tools and results are drawn and compared with other ontologies.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>The developed ontology is analyzed on different verification and validation tools to assure the quality of ontologies.</jats:p></jats:sec>","purpose health-care ontologies and their terminologies play a vital role in knowledge representation and data integration for health information. in health-care systems, internet of technology (iot) technologies provide data exchange among various entities and ontologies offer a formal description to present the knowledge of health-care domains. these ontologies are advised to assure the quality of their adoption and applicability in the real world. design/methodology/approach ontology assessment is an integral part of ontology construction and maintenance. it is always performed to identify inconsistencies and modeling errors by the experts during the ontology development. a smart health-care ontology (shco) has been designed to deal with health-care information and iot devices. in this paper, an integrated approach has been proposed to assess the shco on different assessment tools such as themis, test-driven development (tdd)onto, protégé and oops! several test cases are framed to assess the ontology on these tools, in this research, themis and tddonto tools provide the verification for the test cases while protégé and oops! provides validation of modeled knowledge in the ontology. findings as of the best knowledge, no other study has been presented earlier to conduct the integrated assessment on different tools. all test cases are successfully analyzed on these tools and results are drawn and compared with other ontologies. originality/value the developed ontology is analyzed on different verification and validation tools to assure the quality of ontologies."
http://orkg.org/orkg/resource/R75482,Prevalence and Incidence of Epilepsy in Italy Based on a Nationwide Database,10.1159/000368801,crossref,"""<jats:p>&lt;b&gt;&lt;i&gt;Objectives:&lt;/i&gt;&lt;/b&gt; To estimate the prevalence and incidence of epilepsy in Italy using a national database of general practitioners (GPs). &lt;b&gt;&lt;i&gt;Methods:&lt;/i&gt;&lt;/b&gt; The Health Search CSD Longitudinal Patient Database (HSD) has been established in 1998 by the Italian College of GPs. Participants were 700 GPs, representing a population of 912,458. For each patient, information on age and sex, EEG, CT scan, and MRI was included. Prevalent cases with a diagnosis of &amp;#8216;epilepsy' (ICD9CM: 345*) were selected in the 2011 population. Incident cases of epilepsy were identified in 2011 by excluding patients diagnosed for epilepsy and convulsions and those with EEG, CT scan, MRI prescribed for epilepsy and/or convulsions in the previous years. Crude and standardized (Italian population) prevalence and incidence were calculated. &lt;b&gt;&lt;i&gt;Results:&lt;/i&gt;&lt;/b&gt; Crude prevalence of epilepsy was 7.9 per 1,000 (men 8.1; women 7.7). The highest prevalence was in patients &lt;25 years and &amp;#8805;75 years. The incidence of epilepsy was 33.5 per 100,000 (women 35.3; men 31.5). The highest incidence was in women &lt;25 years and in men 75 years or older. &lt;b&gt;&lt;i&gt;Conclusions:&lt;/i&gt;&lt;/b&gt; Prevalence and incidence of epilepsy in this study were similar to those of other industrialized countries. HSD appears as a reliable data source for the surveillance of epilepsy in Italy. i 2014 S. Karger AG, Basel</jats:p>""",""" &lt;b&gt;&lt;i&gt;objectives:&lt;/i&gt;&lt;/b&gt; to estimate the prevalence and incidence of epilepsy in italy using a national database of general practitioners (gps). &lt;b&gt;&lt;i&gt;methods:&lt;/i&gt;&lt;/b&gt; the health search csd longitudinal patient database (hsd) has been established in 1998 by the italian college of gps. participants were 700 gps, representing a population of 912,458. for each patient, information on age and sex, eeg, ct scan, and mri was included. prevalent cases with a diagnosis of &amp;#8216;epilepsy' (icd9cm: 345*) were selected in the 2011 population. incident cases of epilepsy were identified in 2011 by excluding patients diagnosed for epilepsy and convulsions and those with eeg, ct scan, mri prescribed for epilepsy and/or convulsions in the previous years. crude and standardized (italian population) prevalence and incidence were calculated. &lt;b&gt;&lt;i&gt;results:&lt;/i&gt;&lt;/b&gt; crude prevalence of epilepsy was 7.9 per 1,000 (men 8.1; women 7.7). the highest prevalence was in patients &lt;25 years and &amp;#8805;75 years. the incidence of epilepsy was 33.5 per 100,000 (women 35.3; men 31.5). the highest incidence was in women &lt;25 years and in men 75 years or older. &lt;b&gt;&lt;i&gt;conclusions:&lt;/i&gt;&lt;/b&gt; prevalence and incidence of epilepsy in this study were similar to those of other industrialized countries. hsd appears as a reliable data source for the surveillance of epilepsy in italy. i 2014 s. karger ag, basel """
http://orkg.org/orkg/resource/R76032,Meaningful Integration of Data from Heterogeneous Health Services and Home Environment Based on Ontology,,crossref,"<jats:p>The development of electronic health records, wearable devices, health applications and Internet of Things (IoT)-empowered smart homes is promoting various applications. It also makes health self-management much more feasible, which can partially mitigate one of the challenges that the current healthcare system is facing. Effective and convenient self-management of health requires the collaborative use of health data and home environment data from different services, devices, and even open data on the Web. Although health data interoperability standards including HL7 Fast Healthcare Interoperability Resources (FHIR) and IoT ontology including Semantic Sensor Network (SSN) have been developed and promoted, it is impossible for all the different categories of services to adopt the same standard in the near future. This study presents a method that applies Semantic Web technologies to integrate the health data and home environment data from heterogeneously built services and devices. We propose a Web Ontology Language (OWL)-based integration ontology that models health data from HL7 FHIR standard implemented services, normal Web services and Web of Things (WoT) services and Linked Data together with home environment data from formal ontology-described WoT services. It works on the resource integration layer of the layered integration architecture. An example use case with a prototype implementation shows that the proposed method successfully integrates the health data and home environment data into a resource graph. The integrated data are annotated with semantics and ontological links, which make them machine-understandable and cross-system reusable.</jats:p>","the development of electronic health records, wearable devices, health applications and internet of things (iot)-empowered smart homes is promoting various applications. it also makes health self-management much more feasible, which can partially mitigate one of the challenges that the current healthcare system is facing. effective and convenient self-management of health requires the collaborative use of health data and home environment data from different services, devices, and even open data on the web. although health data interoperability standards including hl7 fast healthcare interoperability resources (fhir) and iot ontology including semantic sensor network (ssn) have been developed and promoted, it is impossible for all the different categories of services to adopt the same standard in the near future. this study presents a method that applies semantic web technologies to integrate the health data and home environment data from heterogeneously built services and devices. we propose a web ontology language (owl)-based integration ontology that models health data from hl7 fhir standard implemented services, normal web services and web of things (wot) services and linked data together with home environment data from formal ontology-described wot services. it works on the resource integration layer of the layered integration architecture. an example use case with a prototype implementation shows that the proposed method successfully integrates the health data and home environment data into a resource graph. the integrated data are annotated with semantics and ontological links, which make them machine-understandable and cross-system reusable."
http://orkg.org/orkg/resource/R75872,Atomic layer deposition of TiO2 and Al2O3 thin films for the electrochemical study of corrosion protection in aluminum alloy cans used in beverage,10.1088/2053-1591/aba557,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Titanium dioxide (TiO<jats:sub>2</jats:sub>) and aluminum oxide (Al<jats:sub>2</jats:sub>O<jats:sub>3</jats:sub>) thin films, with thicknesses around 100 nm, were grown on commercial pure- and resin-coated Al substrates using the atomic layer deposition (ALD). A comprehensive and comparative study of corrosion protection was carried out by linear sweep voltammetry (LSV) and electrochemical impedance spectroscopy (EIS) measurements for a set of six samples: two reference samples (Al-bare and Al-resin), and four ALD coated samples ( Al-TiO<jats:sub>2</jats:sub>, Al-Al<jats:sub>2</jats:sub>O<jats:sub>3</jats:sub>, Al-resin-TiO<jats:sub>2</jats:sub>, and Al-resin-Al<jats:sub>2</jats:sub>O<jats:sub>3</jats:sub>). The LSV and EIS results display good mutual agreement, indicating a higher protection efficiency of all ALD-coated samples after immersion in NaCl. When compared to Al-bare, all ALD coated samples (TiO<jats:sub>2</jats:sub> or Al<jats:sub>2</jats:sub>O<jats:sub>3</jats:sub>) showed a corrosion inhibition enhancement factor of 99%. Besides, our results demonstrated that Al-resin+Al<jats:sub>2</jats:sub>O<jats:sub>3</jats:sub> has 24.95% and 33.40% more corrosion inhibition than Al-Al<jats:sub>2</jats:sub>O<jats:sub>3</jats:sub> and Al-resin, respectively. EIS data were fitted by an equivalent electric circuit (EEC). The Nyquist and Bode plots from the experiments showed that ALD films are a potential candidate for altering/improving commercial resin-coated Al cans.</jats:p>","abstract \n titanium dioxide (tio 2 ) and aluminum oxide (al 2 o 3 ) thin films, with thicknesses around 100 nm, were grown on commercial pure- and resin-coated al substrates using the atomic layer deposition (ald). a comprehensive and comparative study of corrosion protection was carried out by linear sweep voltammetry (lsv) and electrochemical impedance spectroscopy (eis) measurements for a set of six samples: two reference samples (al-bare and al-resin), and four ald coated samples ( al-tio 2 , al-al 2 o 3 , al-resin-tio 2 , and al-resin-al 2 o 3 ). the lsv and eis results display good mutual agreement, indicating a higher protection efficiency of all ald-coated samples after immersion in nacl. when compared to al-bare, all ald coated samples (tio 2 or al 2 o 3 ) showed a corrosion inhibition enhancement factor of 99%. besides, our results demonstrated that al-resin+al 2 o 3 has 24.95% and 33.40% more corrosion inhibition than al-al 2 o 3 and al-resin, respectively. eis data were fitted by an equivalent electric circuit (eec). the nyquist and bode plots from the experiments showed that ald films are a potential candidate for altering/improving commercial resin-coated al cans."
http://orkg.org/orkg/resource/R75426,Security and Cryptographic Challenges for Authentication Based on Biometrics Data,10.3390/cryptography2040039,crossref,"<jats:p>Authentication systems based on biometrics characteristics and data represents one of the most important trend in the evolution of the society, e.g., Smart City, Internet-of-Things (IoT), Cloud Computing, Big Data. In the near future, biometrics systems will be everywhere in the society, such as government, education, smart cities, banks etc. Due to its uniqueness, characteristic, biometrics systems will become more and more vulnerable, privacy being one of the most important challenges. The classic cryptographic primitives are not sufficient to assure a strong level of secureness for privacy. The current paper has several objectives. The main objective consists in creating a framework based on cryptographic modules which can be applied in systems with biometric authentication methods. The technologies used in creating the framework are: C#, Java, C++, Python, and Haskell. The wide range of technologies for developing the algorithms give the readers the possibility and not only, to choose the proper modules for their own research or business direction. The cryptographic modules contain algorithms based on machine learning and modern cryptographic algorithms: AES (Advanced Encryption System), SHA-256, RC4, RC5, RC6, MARS, BLOWFISH, TWOFISH, THREEFISH, RSA (Rivest-Shamir-Adleman), Elliptic Curve, and Diffie Hellman. As methods for implementing with success the cryptographic modules, we will propose a methodology which can be used as a how-to guide. The article will focus only on the first category, machine learning, and data clustering, algorithms with applicability in the cloud computing environment. For tests we have used a virtual machine (Virtual Box) with Apache Hadoop and a Biometric Analysis Tool. The weakness of the algorithms and methods implemented within the framework will be evaluated and presented in order for the reader to acknowledge the latest status of the security analysis and the vulnerabilities founded in the mentioned algorithms. Another important result of the authors consists in creating a scheme for biometric enrollment (in Results). The purpose of the scheme is to give a big overview on how to use it, step by step, in real life, and how to use the algorithms. In the end, as a conclusion, the current work paper gives a comprehensive background on the most important and challenging aspects on how to design and implement an authentication system based on biometrics characteristics.</jats:p>","authentication systems based on biometrics characteristics and data represents one of the most important trend in the evolution of the society, e.g., smart city, internet-of-things (iot), cloud computing, big data. in the near future, biometrics systems will be everywhere in the society, such as government, education, smart cities, banks etc. due to its uniqueness, characteristic, biometrics systems will become more and more vulnerable, privacy being one of the most important challenges. the classic cryptographic primitives are not sufficient to assure a strong level of secureness for privacy. the current paper has several objectives. the main objective consists in creating a framework based on cryptographic modules which can be applied in systems with biometric authentication methods. the technologies used in creating the framework are: c#, java, c++, python, and haskell. the wide range of technologies for developing the algorithms give the readers the possibility and not only, to choose the proper modules for their own research or business direction. the cryptographic modules contain algorithms based on machine learning and modern cryptographic algorithms: aes (advanced encryption system), sha-256, rc4, rc5, rc6, mars, blowfish, twofish, threefish, rsa (rivest-shamir-adleman), elliptic curve, and diffie hellman. as methods for implementing with success the cryptographic modules, we will propose a methodology which can be used as a how-to guide. the article will focus only on the first category, machine learning, and data clustering, algorithms with applicability in the cloud computing environment. for tests we have used a virtual machine (virtual box) with apache hadoop and a biometric analysis tool. the weakness of the algorithms and methods implemented within the framework will be evaluated and presented in order for the reader to acknowledge the latest status of the security analysis and the vulnerabilities founded in the mentioned algorithms. another important result of the authors consists in creating a scheme for biometric enrollment (in results). the purpose of the scheme is to give a big overview on how to use it, step by step, in real life, and how to use the algorithms. in the end, as a conclusion, the current work paper gives a comprehensive background on the most important and challenging aspects on how to design and implement an authentication system based on biometrics characteristics."
http://orkg.org/orkg/resource/R75435,MapReduce: simplified data processing on large clusters,10.1145/1327452.1327492,crossref,"""<jats:p>\n            MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a\n            <jats:italic>map</jats:italic>\n            and a\n            <jats:italic>reduce</jats:italic>\n            function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.\n          </jats:p>""",""" \n mapreduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. users specify the computation in terms of a\n map \n and a\n reduce \n function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. programmers find the system easy to use: more than ten thousand distinct mapreduce programs have been implemented internally at google over the past four years, and an average of one hundred thousand mapreduce jobs are executed on google's clusters every day, processing a total of more than twenty petabytes of data per day.\n """
http://orkg.org/orkg/resource/R75877,MOS Capacitance Measurements for PEALD TiO2 Dielectric Films Grown under Different Conditions and the Impact of Al2O3 Partial-Monolayer Insertion,10.3390/nano10020338,crossref,"<jats:p>In this paper, we report the plasma-enhanced atomic layer deposition (PEALD) of TiO2 and TiO2/Al2O3 nanolaminate films on p-Si(100) to fabricate metal-oxide-semiconductor (MOS) capacitors. In the PEALD process, we used titanium tetraisopropoxide (TTIP) as a titanium precursor, trimethyl aluminum (TMA) as an aluminum precursor and O2 plasma as an oxidant, keeping the process temperature at 250 °C. The effects of PEALD process parameters, such as RF power, substrate exposure mode (direct or remote plasma exposure) and Al2O3 partial-monolayer insertion (generating a nanolaminate structure) on the physical and chemical properties of the TiO2 films were investigated by Rutherford backscattering spectroscopy (RBS), Raman spectroscopy, grazing incidence X-ray diffraction (GIXRD), and field emission scanning electron microscopy (FESEM) techniques. The MOS capacitor structures were fabricated by evaporation of Al gates through mechanical mask on PEALD TiO2 thin film, followed by evaporation of an Al layer on the back side of the Si substrate. The capacitors were characterized by current density-voltage (J-V), capacitance-voltage (C-V) and conductance-voltage (G-V) measurements. Our results indicate that RF power and exposure mode promoted significant modifications on the characteristics of the PEALD TiO2 films, while the insertion of Al2O3 partial monolayers allows the synthesis of TiO2/Al2O3 nanolaminate with well-spaced crystalline TiO2 grains in an amorphous structure. The electrical characterization of the MOS structures evidenced a significant leakage current in the accumulation region in the PEALD TiO2 films, which could be reduced by the addition of partial-monolayers of Al2O3 in the bulk of TiO2 films or by reducing RF power.</jats:p>","in this paper, we report the plasma-enhanced atomic layer deposition (peald) of tio2 and tio2/al2o3 nanolaminate films on p-si(100) to fabricate metal-oxide-semiconductor (mos) capacitors. in the peald process, we used titanium tetraisopropoxide (ttip) as a titanium precursor, trimethyl aluminum (tma) as an aluminum precursor and o2 plasma as an oxidant, keeping the process temperature at 250 °c. the effects of peald process parameters, such as rf power, substrate exposure mode (direct or remote plasma exposure) and al2o3 partial-monolayer insertion (generating a nanolaminate structure) on the physical and chemical properties of the tio2 films were investigated by rutherford backscattering spectroscopy (rbs), raman spectroscopy, grazing incidence x-ray diffraction (gixrd), and field emission scanning electron microscopy (fesem) techniques. the mos capacitor structures were fabricated by evaporation of al gates through mechanical mask on peald tio2 thin film, followed by evaporation of an al layer on the back side of the si substrate. the capacitors were characterized by current density-voltage (j-v), capacitance-voltage (c-v) and conductance-voltage (g-v) measurements. our results indicate that rf power and exposure mode promoted significant modifications on the characteristics of the peald tio2 films, while the insertion of al2o3 partial monolayers allows the synthesis of tio2/al2o3 nanolaminate with well-spaced crystalline tio2 grains in an amorphous structure. the electrical characterization of the mos structures evidenced a significant leakage current in the accumulation region in the peald tio2 films, which could be reduced by the addition of partial-monolayers of al2o3 in the bulk of tio2 films or by reducing rf power."
http://orkg.org/orkg/resource/R75371,Isolating SARS-CoV-2 Strains From Countries in the Same Meridian: Genome Evolutionary Analysis,10.2196/25995,crossref,"<jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>COVID-19, caused by the novel SARS-CoV-2, is considered the most threatening respiratory infection in the world, with over 40 million people infected and over 0.934 million related deaths reported worldwide. It is speculated that epidemiological and clinical features of COVID-19 may differ across countries or continents. Genomic comparison of 48,635 SARS-CoV-2 genomes has shown that the average number of mutations per sample was 7.23, and most SARS-CoV-2 strains belong to one of 3 clades characterized by geographic and genomic specificity: Europe, Asia, and North America.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Objective</jats:title>\n            <jats:p>The aim of this study was to compare the genomes of SARS-CoV-2 strains isolated from Italy, Sweden, and Congo, that is, 3 different countries in the same meridian (longitude) but with different climate conditions, and from Brazil (as an outgroup country), to analyze similarities or differences in patterns of possible evolutionary pressure signatures in their genomes.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Methods</jats:title>\n            <jats:p>We obtained data from the Global Initiative on Sharing All Influenza Data repository by sampling all genomes available on that date. Using HyPhy, we achieved the recombination analysis by genetic algorithm recombination detection method, trimming, removal of the stop codons, and phylogenetic tree and mixed effects model of evolution analyses. We also performed secondary structure prediction analysis for both sequences (mutated and wild-type) and “disorder” and “transmembrane” analyses of the protein. We analyzed both protein structures with an ab initio approach to predict their ontologies and 3D structures.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>Evolutionary analysis revealed that codon 9628 is under episodic selective pressure for all SARS-CoV-2 strains isolated from the 4 countries, suggesting it is a key site for virus evolution. Codon 9628 encodes the P0DTD3 (Y14_SARS2) uncharacterized protein 14. Further investigation showed that the codon mutation was responsible for helical modification in the secondary structure. The codon was positioned in the more ordered region of the gene (41-59) and near to the area acting as the transmembrane (54-67), suggesting its involvement in the attachment phase of the virus. The predicted protein structures of both wild-type and mutated P0DTD3 confirmed the importance of the codon to define the protein structure. Moreover, ontological analysis of the protein emphasized that the mutation enhances the binding probability.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>Our results suggest that RNA secondary structure may be affected and, consequently, the protein product changes T (threonine) to G (glycine) in position 50 of the protein. This position is located close to the predicted transmembrane region. Mutation analysis revealed that the change from G (glycine) to D (aspartic acid) may confer a new function to the protein—binding activity, which in turn may be responsible for attaching the virus to human eukaryotic cells. These findings can help design in vitro experiments and possibly facilitate a vaccine design and successful antiviral strategies.</jats:p>\n          </jats:sec>","\n background \n covid-19, caused by the novel sars-cov-2, is considered the most threatening respiratory infection in the world, with over 40 million people infected and over 0.934 million related deaths reported worldwide. it is speculated that epidemiological and clinical features of covid-19 may differ across countries or continents. genomic comparison of 48,635 sars-cov-2 genomes has shown that the average number of mutations per sample was 7.23, and most sars-cov-2 strains belong to one of 3 clades characterized by geographic and genomic specificity: europe, asia, and north america. \n \n \n objective \n the aim of this study was to compare the genomes of sars-cov-2 strains isolated from italy, sweden, and congo, that is, 3 different countries in the same meridian (longitude) but with different climate conditions, and from brazil (as an outgroup country), to analyze similarities or differences in patterns of possible evolutionary pressure signatures in their genomes. \n \n \n methods \n we obtained data from the global initiative on sharing all influenza data repository by sampling all genomes available on that date. using hyphy, we achieved the recombination analysis by genetic algorithm recombination detection method, trimming, removal of the stop codons, and phylogenetic tree and mixed effects model of evolution analyses. we also performed secondary structure prediction analysis for both sequences (mutated and wild-type) and “disorder” and “transmembrane” analyses of the protein. we analyzed both protein structures with an ab initio approach to predict their ontologies and 3d structures. \n \n \n results \n evolutionary analysis revealed that codon 9628 is under episodic selective pressure for all sars-cov-2 strains isolated from the 4 countries, suggesting it is a key site for virus evolution. codon 9628 encodes the p0dtd3 (y14_sars2) uncharacterized protein 14. further investigation showed that the codon mutation was responsible for helical modification in the secondary structure. the codon was positioned in the more ordered region of the gene (41-59) and near to the area acting as the transmembrane (54-67), suggesting its involvement in the attachment phase of the virus. the predicted protein structures of both wild-type and mutated p0dtd3 confirmed the importance of the codon to define the protein structure. moreover, ontological analysis of the protein emphasized that the mutation enhances the binding probability. \n \n \n conclusions \n our results suggest that rna secondary structure may be affected and, consequently, the protein product changes t (threonine) to g (glycine) in position 50 of the protein. this position is located close to the predicted transmembrane region. mutation analysis revealed that the change from g (glycine) to d (aspartic acid) may confer a new function to the protein—binding activity, which in turn may be responsible for attaching the virus to human eukaryotic cells. these findings can help design in vitro experiments and possibly facilitate a vaccine design and successful antiviral strategies. \n"
http://orkg.org/orkg/resource/R76393,A Gamification–Motivation Design Framework for Educational Software Developers,10.1177/0047239518783153,crossref,"<jats:p> Gamification is the use of game design elements in nongame contexts and has been shown to be effective in motivating behavior change. By seeing game elements as “motivational affordances,” and formalizing the relationship between these elements and motivational affordances, it is the position of this article that gamification can be effectively applied to improve software systems across many different application domains. The research reported here aims to formalize the relationship between game elements and motivation, toward making gamification’s use more systematic. The focus is on the development of a framework linking commonly occurring game elements with the components of a psychological motivational model known as the self-determination theory, coupled with a proposed framework of commonly occurring game elements. The goal is to inform system designers who would like to leverage gamification of the game elements they would need to employ as motivational affordances. </jats:p>","gamification is the use of game design elements in nongame contexts and has been shown to be effective in motivating behavior change. by seeing game elements as “motivational affordances,” and formalizing the relationship between these elements and motivational affordances, it is the position of this article that gamification can be effectively applied to improve software systems across many different application domains. the research reported here aims to formalize the relationship between game elements and motivation, toward making gamification’s use more systematic. the focus is on the development of a framework linking commonly occurring game elements with the components of a psychological motivational model known as the self-determination theory, coupled with a proposed framework of commonly occurring game elements. the goal is to inform system designers who would like to leverage gamification of the game elements they would need to employ as motivational affordances."
http://orkg.org/orkg/resource/R76370,The Shift to Gamification in Education: A Review on Dominant Issues,10.1177/0047239520917629,crossref,"<jats:p> This article examines gamification literature on education since 2011. Using highlighted themes from Kirriemuir and McFarlane’s review on games and education as a starting point, the study identified 32 published papers. Furthermore, the study evaluated and identified previous conceptual and methodological approaches for evaluating gamification in education research. Using the identifying themes, the study discusses the development and use of gamification in education (Theme I), the application of gamification in education (Theme II), and the impact of gamification in education (Theme III) and propose that there is increased gamification and game elements research activities bridging the idea of gamified information systems in education and offering interesting opportunities for future research. The study concludes with future research directions for gamification in education. </jats:p>","this article examines gamification literature on education since 2011. using highlighted themes from kirriemuir and mcfarlane’s review on games and education as a starting point, the study identified 32 published papers. furthermore, the study evaluated and identified previous conceptual and methodological approaches for evaluating gamification in education research. using the identifying themes, the study discusses the development and use of gamification in education (theme i), the application of gamification in education (theme ii), and the impact of gamification in education (theme iii) and propose that there is increased gamification and game elements research activities bridging the idea of gamified information systems in education and offering interesting opportunities for future research. the study concludes with future research directions for gamification in education."
http://orkg.org/orkg/resource/R75706,Natural Plant Extracts as Acid-Base Indicator and Determination of Their pKa Value,10.1155/2019/2031342,crossref,"<jats:p>Commonly used indicators for acid-base titrations are synthetic, and this work was focused to identify the eco-friendly natural indicators and to determine their pKa values. The analytical potential of the flower extracts is very promising as seen in its application in acid-base titrimetry. These selected flower extracts were found to perform well in titrating strong acid-strong base than in weak acid-strong base. We have obtained a sharp and clear colour change from red to brownish yellow for the <jats:italic>Bougainvillea glabra</jats:italic> extract, from red to yellow for the <jats:italic>Bauhinia purpurea</jats:italic> extract, and from red to brownish yellow for the <jats:italic>Impatiens balsamina</jats:italic> extract. All the three flower extracts gave clear colour change with acids and bases, and the colour change was maintained with different acids and bases. The sharp contrast between their colours in acid and base made the pigment suitable for use as acid-base indicators. As these flower extracts have very simple,cost-effective, environment friendly extraction procedure and excellent performance with sharp colour change in end points of the titrations, it would be possible to replace the standard indicators being used in conventional laboratories with natural flower indicators.</jats:p>","commonly used indicators for acid-base titrations are synthetic, and this work was focused to identify the eco-friendly natural indicators and to determine their pka values. the analytical potential of the flower extracts is very promising as seen in its application in acid-base titrimetry. these selected flower extracts were found to perform well in titrating strong acid-strong base than in weak acid-strong base. we have obtained a sharp and clear colour change from red to brownish yellow for the bougainvillea glabra extract, from red to yellow for the bauhinia purpurea extract, and from red to brownish yellow for the impatiens balsamina extract. all the three flower extracts gave clear colour change with acids and bases, and the colour change was maintained with different acids and bases. the sharp contrast between their colours in acid and base made the pigment suitable for use as acid-base indicators. as these flower extracts have very simple,cost-effective, environment friendly extraction procedure and excellent performance with sharp colour change in end points of the titrations, it would be possible to replace the standard indicators being used in conventional laboratories with natural flower indicators."
http://orkg.org/orkg/resource/R75682,Association between dietary patterns and overweight risk among Malaysian adults: evidence from nationally representative surveys,10.1017/s1368980019001861,crossref,"<jats:title>Abstract</jats:title><jats:sec id=""S1368980019001861_as1""><jats:title>Objective:</jats:title><jats:p>To investigate the association between dietary patterns (DP) and overweight risk in the Malaysian Adult Nutrition Surveys (MANS) of 2003 and 2014.</jats:p></jats:sec><jats:sec id=""S1368980019001861_as2""><jats:title>Design:</jats:title><jats:p>DP were derived from the MANS FFQ using principal component analysis. The cross-sectional association of the derived DP with prevalence of overweight was analysed.</jats:p></jats:sec><jats:sec id=""S1368980019001861_as3""><jats:title>Setting:</jats:title><jats:p>Malaysia.</jats:p></jats:sec><jats:sec id=""S1368980019001861_as4""><jats:title>Participants:</jats:title><jats:p>Nationally representative sample of Malaysian adults from MANS (2003, <jats:italic>n</jats:italic> 6928; 2014, <jats:italic>n</jats:italic> 3000).</jats:p></jats:sec><jats:sec id=""S1368980019001861_as5""><jats:title>Results:</jats:title><jats:p>Three major DP were identified for both years. These were ‘Traditional’ (fish, eggs, local cakes), ‘Western’ (fast foods, meat, carbonated beverages) and ‘Mixed’ (ready-to-eat cereals, bread, vegetables). A fourth DP was generated in 2003, ‘Flatbread &amp; Beverages’ (flatbread, creamer, malted beverages), and 2014, ‘Noodles &amp; Meat’ (noodles, meat, eggs). These DP accounted for 25·6 and 26·6 % of DP variations in 2003 and 2014, respectively. For both years, Traditional DP was significantly associated with rural households, lower income, men and Malay ethnicity, while Western DP was associated with younger age and higher income. Mixed DP was positively associated with women and higher income. None of the DP showed positive association with overweight risk, except for reduced adjusted odds of overweight with adherence to Traditional DP in 2003.</jats:p></jats:sec><jats:sec id=""S1368980019001861_as6""><jats:title>Conclusions:</jats:title><jats:p>Overweight could not be attributed to adherence to a single dietary pattern among Malaysian adults. This may be due to the constantly morphing dietary landscape in Malaysia, especially in urban areas, given the ease of availability and relative affordability of multi-ethnic and international foods. Timely surveys are recommended to monitor implications of these changes.</jats:p></jats:sec>","abstract objective: to investigate the association between dietary patterns (dp) and overweight risk in the malaysian adult nutrition surveys (mans) of 2003 and 2014. design: dp were derived from the mans ffq using principal component analysis. the cross-sectional association of the derived dp with prevalence of overweight was analysed. setting: malaysia. participants: nationally representative sample of malaysian adults from mans (2003, n 6928; 2014, n 3000). results: three major dp were identified for both years. these were ‘traditional’ (fish, eggs, local cakes), ‘western’ (fast foods, meat, carbonated beverages) and ‘mixed’ (ready-to-eat cereals, bread, vegetables). a fourth dp was generated in 2003, ‘flatbread &amp; beverages’ (flatbread, creamer, malted beverages), and 2014, ‘noodles &amp; meat’ (noodles, meat, eggs). these dp accounted for 25·6 and 26·6 % of dp variations in 2003 and 2014, respectively. for both years, traditional dp was significantly associated with rural households, lower income, men and malay ethnicity, while western dp was associated with younger age and higher income. mixed dp was positively associated with women and higher income. none of the dp showed positive association with overweight risk, except for reduced adjusted odds of overweight with adherence to traditional dp in 2003. conclusions: overweight could not be attributed to adherence to a single dietary pattern among malaysian adults. this may be due to the constantly morphing dietary landscape in malaysia, especially in urban areas, given the ease of availability and relative affordability of multi-ethnic and international foods. timely surveys are recommended to monitor implications of these changes."
http://orkg.org/orkg/resource/R75689,Diet Quality of Malaysians across Lifespan: A Scoping Review of Evidence in a Multi-Ethnic Population,10.3390/nu13041380,crossref,"<jats:p>Malaysia is a rapidly developing economy experiencing a nutrition transition. It suffers from a double burden of over- and undernutrition, making it essential to understand diet quality in the population. In this scoping review, we have collated the existing literature on Malaysian diet quality, including factors that influence it, and the association between diet quality and health outcomes across the lifespan of Malaysians. Overall, diet quality was poor in all age groups studied. The Healthy Eating Index (HEI) and its iterations were predominantly used in urban and clinical settings to evaluate diet-chronic disease relationships. These indices were significantly associated with cardio-metabolic and disease risks in adults. The Diet Diversity Score (DDS) and Food Variety Score (FVS) were used to gauge diet quality in maternal and child nutrition studies and were associated with appropriate growth and caloric intake. Deficiencies were found in fruit, vegetable, legumes, and dairy intake. Meat, salt, and sugar intake were found to be excessive in many studies. The findings can inform policies to improve diet quality in this population. The review also identified knowledge gaps that require further investigation.</jats:p>","malaysia is a rapidly developing economy experiencing a nutrition transition. it suffers from a double burden of over- and undernutrition, making it essential to understand diet quality in the population. in this scoping review, we have collated the existing literature on malaysian diet quality, including factors that influence it, and the association between diet quality and health outcomes across the lifespan of malaysians. overall, diet quality was poor in all age groups studied. the healthy eating index (hei) and its iterations were predominantly used in urban and clinical settings to evaluate diet-chronic disease relationships. these indices were significantly associated with cardio-metabolic and disease risks in adults. the diet diversity score (dds) and food variety score (fvs) were used to gauge diet quality in maternal and child nutrition studies and were associated with appropriate growth and caloric intake. deficiencies were found in fruit, vegetable, legumes, and dairy intake. meat, salt, and sugar intake were found to be excessive in many studies. the findings can inform policies to improve diet quality in this population. the review also identified knowledge gaps that require further investigation."
http://orkg.org/orkg/resource/R75710,"Comparative Analysis of the Properties of Acid-Base Indicator of Rose (Rosa setigera), Allamanda (Allamanda cathartica), and Hibiscus (Hibiscus rosa-sinensis) Flowers",10.1155/2015/381721,crossref,"<jats:p>The need to develop effective alternative for synthetic indicators is the demand of present-day chemistry. The acid-base indicator properties of Rose (<jats:italic>Rosa setigera</jats:italic>), Allamanda (<jats:italic>Allamanda cathartica</jats:italic>), and Hibiscus (<jats:italic>Hibiscus rosa-sinensis</jats:italic>) flowers were examined. Colour pigments were extracted from the flowers via cold and solvent extraction using soxhlet extractor. The pH value of the extracts with wavelengths of absorption was determined using ultraviolet spectrophotometer. From the results obtained, all the extracts exhibited sharp contrast between their colours in acid and base. Their pH was found to be 5.5 for cold extract of Rose and 5.6 for solvent extraction, 5.24 for cold extract of a Hibiscus and 6.52 for solvent extraction, 5.35 for cold extract of Allamanda, and 5.45 for solvent extraction. The maximum wavelengths of absorption obtained for all the extract fall within the visible region of electromagnetic spectrum. These values are almost similar to that obtained from synthetic indicators. It is on these bases that we concluded that natural indicators could be an excellent replacement for synthetic indicators since they are cheap, readily available, simple to extract, not toxic, user and environmentally friendly.</jats:p>","the need to develop effective alternative for synthetic indicators is the demand of present-day chemistry. the acid-base indicator properties of rose ( rosa setigera ), allamanda ( allamanda cathartica ), and hibiscus ( hibiscus rosa-sinensis ) flowers were examined. colour pigments were extracted from the flowers via cold and solvent extraction using soxhlet extractor. the ph value of the extracts with wavelengths of absorption was determined using ultraviolet spectrophotometer. from the results obtained, all the extracts exhibited sharp contrast between their colours in acid and base. their ph was found to be 5.5 for cold extract of rose and 5.6 for solvent extraction, 5.24 for cold extract of a hibiscus and 6.52 for solvent extraction, 5.35 for cold extract of allamanda, and 5.45 for solvent extraction. the maximum wavelengths of absorption obtained for all the extract fall within the visible region of electromagnetic spectrum. these values are almost similar to that obtained from synthetic indicators. it is on these bases that we concluded that natural indicators could be an excellent replacement for synthetic indicators since they are cheap, readily available, simple to extract, not toxic, user and environmentally friendly."
http://orkg.org/orkg/resource/R75942,Parental well-being in times of Covid-19 in Germany,10.1007/s11150-020-09529-4,crossref,"<jats:title>Abstract</jats:title><jats:p>We examine the effects of Covid-19 and related restrictions on individuals with dependent children in Germany. We specifically focus on the role of day care center and school closures, which may be regarded as a “disruptive exogenous shock” to family life. We make use of a novel representative survey of parental well-being collected in May and June 2020 in Germany, when schools and day care centers were closed but while other measures had been relaxed and new infections were low. In our descriptive analysis, we compare well-being during this period with a pre-crisis period for different groups. In a difference-in-differences design, we compare the change for individuals with children to the change for individuals without children, accounting for unrelated trends as well as potential survey mode and context effects. We find that the crisis lowered the relative well-being of individuals with children, especially for individuals with young children, for women, and for persons with lower secondary schooling qualifications. Our results suggest that public policy measures taken to contain Covid-19 can have large effects on family well-being, with implications for child development and parental labor market outcomes.</jats:p>","abstract we examine the effects of covid-19 and related restrictions on individuals with dependent children in germany. we specifically focus on the role of day care center and school closures, which may be regarded as a “disruptive exogenous shock” to family life. we make use of a novel representative survey of parental well-being collected in may and june 2020 in germany, when schools and day care centers were closed but while other measures had been relaxed and new infections were low. in our descriptive analysis, we compare well-being during this period with a pre-crisis period for different groups. in a difference-in-differences design, we compare the change for individuals with children to the change for individuals without children, accounting for unrelated trends as well as potential survey mode and context effects. we find that the crisis lowered the relative well-being of individuals with children, especially for individuals with young children, for women, and for persons with lower secondary schooling qualifications. our results suggest that public policy measures taken to contain covid-19 can have large effects on family well-being, with implications for child development and parental labor market outcomes."
http://orkg.org/orkg/resource/R75828,Decision-making at the sharp end: a survey of literature related to decision-making in humanitarian contexts,10.1186/s41018-020-00068-2,crossref,"<jats:title>Abstract</jats:title><jats:p>In a humanitarian response, leaders are often tasked with making large numbers of decisions, many of which have significant consequences, in situations of urgency and uncertainty. These conditions have an impact on the decision-maker (causing stress, for example) and subsequently on how decisions get made.</jats:p><jats:p>Evaluations of humanitarian action suggest that decision-making is an area of weakness in many operations. There are examples of important decisions being missed and of decision-making processes that are slow and ad hoc. As part of a research process to address these challenges, this article considers literature from the humanitarian and emergency management sectors that relates to decision-making. It outlines what the literature tells us about the nature of the decisions that leaders at the country level are taking during humanitarian operations, and the circumstances under which these decisions are taken. It then considers the potential application of two different types of decision-making process in these contexts: rational/analytical decision-making and naturalistic decision-making. The article concludes with broad hypotheses that can be drawn from the literature and with the recommendation that these be further tested by academics with an interest in the topic.</jats:p>","abstract in a humanitarian response, leaders are often tasked with making large numbers of decisions, many of which have significant consequences, in situations of urgency and uncertainty. these conditions have an impact on the decision-maker (causing stress, for example) and subsequently on how decisions get made. evaluations of humanitarian action suggest that decision-making is an area of weakness in many operations. there are examples of important decisions being missed and of decision-making processes that are slow and ad hoc. as part of a research process to address these challenges, this article considers literature from the humanitarian and emergency management sectors that relates to decision-making. it outlines what the literature tells us about the nature of the decisions that leaders at the country level are taking during humanitarian operations, and the circumstances under which these decisions are taken. it then considers the potential application of two different types of decision-making process in these contexts: rational/analytical decision-making and naturalistic decision-making. the article concludes with broad hypotheses that can be drawn from the literature and with the recommendation that these be further tested by academics with an interest in the topic."
http://orkg.org/orkg/resource/R75825,Evidence-Based Decision-Making (Part II): Applications in Disaster Relief Operations,10.1017/s1049023x0000738x,crossref,"<jats:title>Abstract</jats:title><jats:p>Recognized limitations to data in disaster management have led to dozens of initiatives to strengthen data gathering and decision-making during disasters. These initiatives are complicated by fundamental problems of definitions of terms, ambiguity of concepts, lack of standardization in methods of data collection, and inadequate attempts to strengthen the analytic capability of field organizations. Cross-cutting issues in needs assessment, coordination, and evaluation illustrate additional recurring challenges in dealing with evidence in humanitarian assistance. These challenges include lack of agency expertise, dyscoordination at the field level, inappropriate reliance on indicators that measure process rather than outcome, flawed scientific inference, and erosion of the concept of minimum standards.</jats:p><jats:p>Decision-making in disaster management currently places a premium on expert or eminence-based decisions. By contrast, scientific advances in disaster medicine call for evidence-based decisions whose strength of evidence is established by the methods of data acquisition. At present, disaster relief operations may be data driven, but that does not mean that they are soundly evidence-based.</jats:p><jats:p>Options for strengthening evidence-based activities include rigorously adhering to evidenced-based interventions, using evidence-based tools to identify new approaches to problems of concern, studying model programs as well as failed ones to identify approaches that deserve replication, and improving standards for evidence of effectiveness in disaster science and services.</jats:p>","abstract recognized limitations to data in disaster management have led to dozens of initiatives to strengthen data gathering and decision-making during disasters. these initiatives are complicated by fundamental problems of definitions of terms, ambiguity of concepts, lack of standardization in methods of data collection, and inadequate attempts to strengthen the analytic capability of field organizations. cross-cutting issues in needs assessment, coordination, and evaluation illustrate additional recurring challenges in dealing with evidence in humanitarian assistance. these challenges include lack of agency expertise, dyscoordination at the field level, inappropriate reliance on indicators that measure process rather than outcome, flawed scientific inference, and erosion of the concept of minimum standards. decision-making in disaster management currently places a premium on expert or eminence-based decisions. by contrast, scientific advances in disaster medicine call for evidence-based decisions whose strength of evidence is established by the methods of data acquisition. at present, disaster relief operations may be data driven, but that does not mean that they are soundly evidence-based. options for strengthening evidence-based activities include rigorously adhering to evidenced-based interventions, using evidence-based tools to identify new approaches to problems of concern, studying model programs as well as failed ones to identify approaches that deserve replication, and improving standards for evidence of effectiveness in disaster science and services."
http://orkg.org/orkg/resource/R75864,Progresses in Synthesis and Application of SiC Films: From CVD to ALD and from MEMS to NEMS,10.3390/mi11090799,crossref,"<jats:p>A search of the recent literature reveals that there is a continuous growth of scientific publications on the development of chemical vapor deposition (CVD) processes for silicon carbide (SiC) films and their promising applications in micro- and nanoelectromechanical systems (MEMS/NEMS) devices. In recent years, considerable effort has been devoted to deposit high-quality SiC films on large areas enabling the low-cost fabrication methods of MEMS/NEMS sensors. The relatively high temperatures involved in CVD SiC growth are a drawback and studies have been made to develop low-temperature CVD processes. In this respect, atomic layer deposition (ALD), a modified CVD process promising for nanotechnology fabrication techniques, has attracted attention due to the deposition of thin films at low temperatures and additional benefits, such as excellent uniformity, conformability, good reproducibility, large area, and batch capability. This review article focuses on the recent advances in the strategies for the CVD of SiC films, with a special emphasis on low-temperature processes, as well as ALD. In addition, we summarize the applications of CVD SiC films in MEMS/NEMS devices and prospects for advancement of the CVD SiC technology.</jats:p>","a search of the recent literature reveals that there is a continuous growth of scientific publications on the development of chemical vapor deposition (cvd) processes for silicon carbide (sic) films and their promising applications in micro- and nanoelectromechanical systems (mems/nems) devices. in recent years, considerable effort has been devoted to deposit high-quality sic films on large areas enabling the low-cost fabrication methods of mems/nems sensors. the relatively high temperatures involved in cvd sic growth are a drawback and studies have been made to develop low-temperature cvd processes. in this respect, atomic layer deposition (ald), a modified cvd process promising for nanotechnology fabrication techniques, has attracted attention due to the deposition of thin films at low temperatures and additional benefits, such as excellent uniformity, conformability, good reproducibility, large area, and batch capability. this review article focuses on the recent advances in the strategies for the cvd of sic films, with a special emphasis on low-temperature processes, as well as ald. in addition, we summarize the applications of cvd sic films in mems/nems devices and prospects for advancement of the cvd sic technology."
http://orkg.org/orkg/resource/R75871,The Influence of AlN Intermediate Layer on the Structural and Chemical Properties of SiC Thin Films Produced by High-Power Impulse Magnetron Sputtering,10.3390/mi10030202,crossref,"<jats:p>Many strategies have been developed for the synthesis of silicon carbide (SiC) thin films on silicon (Si) substrates by plasma-based deposition techniques, especially plasma enhanced chemical vapor deposition (PECVD) and magnetron sputtering, due to the importance of these materials for microelectronics and related fields. A drawback is the large lattice mismatch between SiC and Si. The insertion of an aluminum nitride (AlN) intermediate layer between them has been shown useful to overcome this problem. Herein, the high-power impulse magnetron sputtering (HiPIMS) technique was used to grow SiC thin films on AlN/Si substrates. Furthermore, SiC films were also grown on Si substrates. A comparison of the structural and chemical properties of SiC thin films grown on the two types of substrate allowed us to evaluate the influence of the AlN layer on such properties. The chemical composition and stoichiometry of the samples were investigated by Rutherford backscattering spectrometry (RBS) and Raman spectroscopy, while the crystallinity was characterized by grazing incidence X-ray diffraction (GIXRD). Our set of results evidenced the versatility of the HiPIMS technique to produce polycrystalline SiC thin films at near-room temperature by only varying the discharge power. In addition, this study opens up a feasible route for the deposition of crystalline SiC films with good structural quality using an AlN intermediate layer.</jats:p>","many strategies have been developed for the synthesis of silicon carbide (sic) thin films on silicon (si) substrates by plasma-based deposition techniques, especially plasma enhanced chemical vapor deposition (pecvd) and magnetron sputtering, due to the importance of these materials for microelectronics and related fields. a drawback is the large lattice mismatch between sic and si. the insertion of an aluminum nitride (aln) intermediate layer between them has been shown useful to overcome this problem. herein, the high-power impulse magnetron sputtering (hipims) technique was used to grow sic thin films on aln/si substrates. furthermore, sic films were also grown on si substrates. a comparison of the structural and chemical properties of sic thin films grown on the two types of substrate allowed us to evaluate the influence of the aln layer on such properties. the chemical composition and stoichiometry of the samples were investigated by rutherford backscattering spectrometry (rbs) and raman spectroscopy, while the crystallinity was characterized by grazing incidence x-ray diffraction (gixrd). our set of results evidenced the versatility of the hipims technique to produce polycrystalline sic thin films at near-room temperature by only varying the discharge power. in addition, this study opens up a feasible route for the deposition of crystalline sic films with good structural quality using an aln intermediate layer."
http://orkg.org/orkg/resource/R75874,"Black TiO2 Thin Films Production Using Hollow Cathode Hydrogen Plasma Treatment: Synthesis, Material Characteristics and Photocatalytic Activity",10.3390/catal10030282,crossref,"<jats:p>Black TiO2 materials have been quite widely explored due to their large solar absorption and superior photocatalytic activity. In this paper, the blackening process of titanium dioxide (TiO2) thin film using the hollow cathode hydrogen plasma (HCHP) technique is reported. First, pristine anatase TiO2 films were grown by magnetron sputtering onto silicon and cover glass substrates and then annealed at 450 °C for 2 h. Then, the as-grown TiO2 films were treated with HCHP for 15 min. The physical, chemical and morphological properties of the films were analyzed by profilometry, X-ray diffraction (XRD), UV-Vis spectrophotometry, Raman spectroscopy, X-ray photoelectron spectroscopy (XPS) and atomic force microscopy (AFM) techniques. Electrical and photocatalytic measurements were performed by four-point probe and methylene blue UV degradation assays, respectively. The results showed that the black TiO2 film is highly absorbent in the UV-visible region, has low electrical resistance and greater surface area compared to the non-treated TiO2 film. These properties of black TiO2 film, as well as its performance as a photocatalytic agent, were investigated, indicating the superior quality of this material in thin film form and the promising potential of the HCHP treatment to produce hydrogenated TiO2 in short process time.</jats:p>","black tio2 materials have been quite widely explored due to their large solar absorption and superior photocatalytic activity. in this paper, the blackening process of titanium dioxide (tio2) thin film using the hollow cathode hydrogen plasma (hchp) technique is reported. first, pristine anatase tio2 films were grown by magnetron sputtering onto silicon and cover glass substrates and then annealed at 450 °c for 2 h. then, the as-grown tio2 films were treated with hchp for 15 min. the physical, chemical and morphological properties of the films were analyzed by profilometry, x-ray diffraction (xrd), uv-vis spectrophotometry, raman spectroscopy, x-ray photoelectron spectroscopy (xps) and atomic force microscopy (afm) techniques. electrical and photocatalytic measurements were performed by four-point probe and methylene blue uv degradation assays, respectively. the results showed that the black tio2 film is highly absorbent in the uv-visible region, has low electrical resistance and greater surface area compared to the non-treated tio2 film. these properties of black tio2 film, as well as its performance as a photocatalytic agent, were investigated, indicating the superior quality of this material in thin film form and the promising potential of the hchp treatment to produce hydrogenated tio2 in short process time."
http://orkg.org/orkg/resource/R76026,Design and Implementation of e-Health System Based on Semantic Sensor Network Using IETF YANG,,crossref,"<jats:p>Recently, healthcare services can be delivered effectively to patients anytime and anywhere using e-Health systems. e-Health systems are developed through Information and Communication Technologies (ICT) that involve sensors, mobiles, and web-based applications for the delivery of healthcare services and information. Remote healthcare is an important purpose of the e-Health system. Usually, the eHealth system includes heterogeneous sensors from diverse manufacturers producing data in different formats. Device interoperability and data normalization is a challenging task that needs research attention. Several solutions are proposed in the literature based on manual interpretation through explicit programming. However, programmatically implementing the interpretation of the data sender and data receiver in the e-Health system for the data transmission is counterproductive as modification will be required for each new device added into the system. In this paper, an e-Health system with the Semantic Sensor Network (SSN) is proposed to address the device interoperability issue. In the proposed system, we have used IETF YANG for modeling the semantic e-Health data to represent the information of e-Health sensors. This modeling scheme helps in provisioning semantic interoperability between devices and expressing the sensing data in a user-friendly manner. For this purpose, we have developed an ontology for e-Health data that supports different styles of data formats. The ontology is defined in YANG for provisioning semantic interpretation of sensing data in the system by constructing meta-models of e-Health sensors. The proposed approach assists in the auto-configuration of eHealth sensors and querying the sensor network with semantic interoperability support for the e-Health system.</jats:p>","recently, healthcare services can be delivered effectively to patients anytime and anywhere using e-health systems. e-health systems are developed through information and communication technologies (ict) that involve sensors, mobiles, and web-based applications for the delivery of healthcare services and information. remote healthcare is an important purpose of the e-health system. usually, the ehealth system includes heterogeneous sensors from diverse manufacturers producing data in different formats. device interoperability and data normalization is a challenging task that needs research attention. several solutions are proposed in the literature based on manual interpretation through explicit programming. however, programmatically implementing the interpretation of the data sender and data receiver in the e-health system for the data transmission is counterproductive as modification will be required for each new device added into the system. in this paper, an e-health system with the semantic sensor network (ssn) is proposed to address the device interoperability issue. in the proposed system, we have used ietf yang for modeling the semantic e-health data to represent the information of e-health sensors. this modeling scheme helps in provisioning semantic interoperability between devices and expressing the sensing data in a user-friendly manner. for this purpose, we have developed an ontology for e-health data that supports different styles of data formats. the ontology is defined in yang for provisioning semantic interpretation of sensing data in the system by constructing meta-models of e-health sensors. the proposed approach assists in the auto-configuration of ehealth sensors and querying the sensor network with semantic interoperability support for the e-health system."
http://orkg.org/orkg/resource/R76130,The role of alkali metal cations in the stabilization of guanine quadruplexes: why K+ is the best,10.1039/c6cp01030j,crossref,<p>The desolvation and size of monovalent alkali metal ions are of equal importance for the cation affinity of guanine quadruplexes.</p>,the desolvation and size of monovalent alkali metal ions are of equal importance for the cation affinity of guanine quadruplexes.
http://orkg.org/orkg/resource/R76138,The role of alkali metal cations in the stabilization of guanine quadruplexes: why K+ is the best,10.1039/c6cp01030j,crossref,<p>The desolvation and size of monovalent alkali metal ions are of equal importance for the cation affinity of guanine quadruplexes.</p>,the desolvation and size of monovalent alkali metal ions are of equal importance for the cation affinity of guanine quadruplexes.
http://orkg.org/orkg/resource/R76141,Bullying Victimization among In-School Adolescents in Ghana: Analysis of Prevalence and Correlates from the Global School-Based Health Survey,10.3390/healthcare9030292,crossref,"<jats:p>(1) Background: Although bullying victimization is a phenomenon that is increasingly being recognized as a public health and mental health concern in many countries, research attention on this aspect of youth violence in low- and middle-income countries, especially sub-Saharan Africa, is minimal. The current study examined the national prevalence of bullying victimization and its correlates among in-school adolescents in Ghana. (2) Methods: A sample of 1342 in-school adolescents in Ghana (55.2% males; 44.8% females) aged 12–18 was drawn from the 2012 Global School-based Health Survey (GSHS) for the analysis. Self-reported bullying victimization “during the last 30 days, on how many days were you bullied?” was used as the central criterion variable. Three-level analyses using descriptive, Pearson chi-square, and binary logistic regression were performed. Results of the regression analysis were presented as adjusted odds ratios (aOR) at 95% confidence intervals (CIs), with a statistical significance pegged at p &lt; 0.05. (3) Results: Bullying victimization was prevalent among 41.3% of the in-school adolescents. Pattern of results indicates that adolescents in SHS 3 [aOR = 0.34, 95% CI = 0.25, 0.47] and SHS 4 [aOR = 0.30, 95% CI = 0.21, 0.44] were less likely to be victims of bullying. Adolescents who had sustained injury [aOR = 2.11, 95% CI = 1.63, 2.73] were more likely to be bullied compared to those who had not sustained any injury. The odds of bullying victimization were higher among adolescents who had engaged in physical fight [aOR = 1.90, 95% CI = 1.42, 2.25] and those who had been physically attacked [aOR = 1.73, 95% CI = 1.32, 2.27]. Similarly, adolescents who felt lonely were more likely to report being bullied [aOR = 1.50, 95% CI = 1.08, 2.08] as against those who did not feel lonely. Additionally, adolescents with a history of suicide attempts were more likely to be bullied [aOR = 1.63, 95% CI = 1.11, 2.38] and those who used marijuana had higher odds of bullying victimization [aOR = 3.36, 95% CI = 1.10, 10.24]. (4) Conclusions: Current findings require the need for policy makers and school authorities in Ghana to design and implement policies and anti-bullying interventions (e.g., Social Emotional Learning (SEL), Emotive Behavioral Education (REBE), Marijuana Cessation Therapy (MCT)) focused on addressing behavioral issues, mental health and substance abuse among in-school adolescents.</jats:p>","(1) background: although bullying victimization is a phenomenon that is increasingly being recognized as a public health and mental health concern in many countries, research attention on this aspect of youth violence in low- and middle-income countries, especially sub-saharan africa, is minimal. the current study examined the national prevalence of bullying victimization and its correlates among in-school adolescents in ghana. (2) methods: a sample of 1342 in-school adolescents in ghana (55.2% males; 44.8% females) aged 12–18 was drawn from the 2012 global school-based health survey (gshs) for the analysis. self-reported bullying victimization “during the last 30 days, on how many days were you bullied?” was used as the central criterion variable. three-level analyses using descriptive, pearson chi-square, and binary logistic regression were performed. results of the regression analysis were presented as adjusted odds ratios (aor) at 95% confidence intervals (cis), with a statistical significance pegged at p &lt; 0.05. (3) results: bullying victimization was prevalent among 41.3% of the in-school adolescents. pattern of results indicates that adolescents in shs 3 [aor = 0.34, 95% ci = 0.25, 0.47] and shs 4 [aor = 0.30, 95% ci = 0.21, 0.44] were less likely to be victims of bullying. adolescents who had sustained injury [aor = 2.11, 95% ci = 1.63, 2.73] were more likely to be bullied compared to those who had not sustained any injury. the odds of bullying victimization were higher among adolescents who had engaged in physical fight [aor = 1.90, 95% ci = 1.42, 2.25] and those who had been physically attacked [aor = 1.73, 95% ci = 1.32, 2.27]. similarly, adolescents who felt lonely were more likely to report being bullied [aor = 1.50, 95% ci = 1.08, 2.08] as against those who did not feel lonely. additionally, adolescents with a history of suicide attempts were more likely to be bullied [aor = 1.63, 95% ci = 1.11, 2.38] and those who used marijuana had higher odds of bullying victimization [aor = 3.36, 95% ci = 1.10, 10.24]. (4) conclusions: current findings require the need for policy makers and school authorities in ghana to design and implement policies and anti-bullying interventions (e.g., social emotional learning (sel), emotive behavioral education (rebe), marijuana cessation therapy (mct)) focused on addressing behavioral issues, mental health and substance abuse among in-school adolescents."
http://orkg.org/orkg/resource/R76379,Designing for Game-Based Learning: The Effective Integration of Technology to Support Learning,10.1177/0047239515588164,crossref,"<jats:p> The use of games and game structures in educational contexts is growing in popularity. An increasing number of technologies have been developed to meet the needs of designing a course as a game. This article discussed the design process in game-based learning and reviewed the research on structuring a course with a focus on feedback, goals, and interaction. In addition, we presented the best practices and technologies to support the integration of badges and leaderboards into game-based learning. With the intentional and systematic design of game-based learning, instructors and designers will increase the impact of game attributes and elements on student achievement and motivation. Further investigation of game-based learning attributes and elements is needed to provide detailed knowledge on the compatibility with current technological tools. </jats:p>","the use of games and game structures in educational contexts is growing in popularity. an increasing number of technologies have been developed to meet the needs of designing a course as a game. this article discussed the design process in game-based learning and reviewed the research on structuring a course with a focus on feedback, goals, and interaction. in addition, we presented the best practices and technologies to support the integration of badges and leaderboards into game-based learning. with the intentional and systematic design of game-based learning, instructors and designers will increase the impact of game attributes and elements on student achievement and motivation. further investigation of game-based learning attributes and elements is needed to provide detailed knowledge on the compatibility with current technological tools."
http://orkg.org/orkg/resource/R76542,Up and About: Older Adults’ Well-being During the COVID-19 Pandemic in a Swedish Longitudinal Study,10.1093/geronb/gbaa084,crossref,"<jats:title>Abstract</jats:title>\n               <jats:sec>\n                  <jats:title>Objectives</jats:title>\n                  <jats:p>To investigate early effects of the COVID-19 pandemic related to (a) levels of worry, risk perception, and social distancing; (b) longitudinal effects on well-being; and (c) effects of worry, risk perception, and social distancing on well-being.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Methods</jats:title>\n                  <jats:p>We analyzed annual changes in four aspects of well-being over 5 years (2015–2020): life satisfaction, financial satisfaction, self-rated health, and loneliness in a subsample (n = 1,071, aged 65–71) from a larger survey of Swedish older adults. The 2020 wave, collected March 26–April 2, included measures of worry, risk perception, and social distancing in response to COVID-19.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Results</jats:title>\n                  <jats:p>(a) In relation to COVID-19: 44.9% worried about health, 69.5% about societal consequences, 25.1% about financial consequences; 86.4% perceived a high societal risk, 42.3% a high risk of infection, and 71.2% reported high levels of social distancing. (b) Well-being remained stable (life satisfaction and loneliness) or even increased (self-rated health and financial satisfaction) in 2020 compared to previous years. (c) More worry about health and financial consequences was related to lower scores in all four well-being measures. Higher societal worry and more social distancing were related to higher well-being.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Discussion</jats:title>\n                  <jats:p>In the early stage of the pandemic, Swedish older adults on average rated their well-being as high as, or even higher than, previous years. However, those who worried more reported lower well-being. Our findings speak to the resilience, but also heterogeneity, among older adults during the pandemic. Further research, on a broad range of health factors and long-term psychological consequences, is needed.</jats:p>\n               </jats:sec>","abstract \n \n objectives \n to investigate early effects of the covid-19 pandemic related to (a) levels of worry, risk perception, and social distancing; (b) longitudinal effects on well-being; and (c) effects of worry, risk perception, and social distancing on well-being. \n \n \n methods \n we analyzed annual changes in four aspects of well-being over 5 years (2015–2020): life satisfaction, financial satisfaction, self-rated health, and loneliness in a subsample (n = 1,071, aged 65–71) from a larger survey of swedish older adults. the 2020 wave, collected march 26–april 2, included measures of worry, risk perception, and social distancing in response to covid-19. \n \n \n results \n (a) in relation to covid-19: 44.9% worried about health, 69.5% about societal consequences, 25.1% about financial consequences; 86.4% perceived a high societal risk, 42.3% a high risk of infection, and 71.2% reported high levels of social distancing. (b) well-being remained stable (life satisfaction and loneliness) or even increased (self-rated health and financial satisfaction) in 2020 compared to previous years. (c) more worry about health and financial consequences was related to lower scores in all four well-being measures. higher societal worry and more social distancing were related to higher well-being. \n \n \n discussion \n in the early stage of the pandemic, swedish older adults on average rated their well-being as high as, or even higher than, previous years. however, those who worried more reported lower well-being. our findings speak to the resilience, but also heterogeneity, among older adults during the pandemic. further research, on a broad range of health factors and long-term psychological consequences, is needed. \n"
http://orkg.org/orkg/resource/R76644,Earthquake Hazard Safety Assessment of Existing Buildings Using Optimized Multi-Layer Perceptron Neural Network,,crossref,"<jats:p>The latest earthquakes have proven that several existing buildings, particularly in developing countries, are not secured from damages of earthquake. A variety of statistical and machine-learning approaches have been proposed to identify vulnerable buildings for the prioritization of retrofitting. The present work aims to investigate earthquake susceptibility through the combination of six building performance variables that can be used to obtain an optimal prediction of the damage state of reinforced concrete buildings using artificial neural network (ANN). In this regard, a multi-layer perceptron network is trained and optimized using a database of 484 damaged buildings from the Düzce earthquake in Turkey. The results demonstrate the feasibility and effectiveness of the selected ANN approach to classify concrete structural damage that can be used as a preliminary assessment technique to identify vulnerable buildings in disaster risk-management programs.</jats:p>","the latest earthquakes have proven that several existing buildings, particularly in developing countries, are not secured from damages of earthquake. a variety of statistical and machine-learning approaches have been proposed to identify vulnerable buildings for the prioritization of retrofitting. the present work aims to investigate earthquake susceptibility through the combination of six building performance variables that can be used to obtain an optimal prediction of the damage state of reinforced concrete buildings using artificial neural network (ann). in this regard, a multi-layer perceptron network is trained and optimized using a database of 484 damaged buildings from the düzce earthquake in turkey. the results demonstrate the feasibility and effectiveness of the selected ann approach to classify concrete structural damage that can be used as a preliminary assessment technique to identify vulnerable buildings in disaster risk-management programs."
http://orkg.org/orkg/resource/R77163,Economic Sustainability in a Wider Context: Case Study of Considerable ICT Sector Sub-Divisions,10.3390/su10072511,crossref,"<jats:p>This paper situates the process of economic sustainability in the wider context of regional specialization and geographic concentration. The main object is to analyze the implications of increasing importance of sustainable development. In this context, the ICT (Information and Communication Technology) is at the same time a part of the problem and solution. The focus of this paper is also the ICT firms themselves. This research aimed to explore the ICT firms operating in the ICT sector and focused more on the ICT firms’ sustainability in connection with industry geographic concentration and regional specialization. The economic sustainability (evaluated by sustainability sub-index) and geographical and regional analysis were studied for 62 Computer Programming and 63 Information Services sub-divisions of the ICT sector. The results confirm a strong correlation between economic sustainability and firm geographic concentration. Results show that a worsening value of economic sustainability does not always lead to the worsening conditions in the industry and there is a strong relationship between the economic sustainability and regional analysis.</jats:p>","this paper situates the process of economic sustainability in the wider context of regional specialization and geographic concentration. the main object is to analyze the implications of increasing importance of sustainable development. in this context, the ict (information and communication technology) is at the same time a part of the problem and solution. the focus of this paper is also the ict firms themselves. this research aimed to explore the ict firms operating in the ict sector and focused more on the ict firms’ sustainability in connection with industry geographic concentration and regional specialization. the economic sustainability (evaluated by sustainability sub-index) and geographical and regional analysis were studied for 62 computer programming and 63 information services sub-divisions of the ict sector. the results confirm a strong correlation between economic sustainability and firm geographic concentration. results show that a worsening value of economic sustainability does not always lead to the worsening conditions in the industry and there is a strong relationship between the economic sustainability and regional analysis."
http://orkg.org/orkg/resource/R78251,An Observational Process Ontology-Based Modeling Approach for Water Quality Monitoring,,crossref,"<jats:p>The increasing deterioration of aquatic environments has attracted more attention to water quality monitoring techniques, with most researchers focusing on the acquisition and assessment of water quality data, but seldom on the discovery and tracing of pollution sources. In this study, a semantic-enhanced modeling method for ontology modeling and rules building is proposed, which can be used for river water quality monitoring and relevant data observation processing. The observational process ontology (OPO) method can describe the semantic properties of water resources and observation data. In addition, it can provide the semantic relevance among the different concepts involved in the observational process of water quality monitoring. A pollution alert can be achieved using the reasoning rules for the water quality monitoring stations. In this study, a case is made for the usability testing of the OPO models and reasoning rules by utilizing a water quality monitoring system. The system contributes to the water quality observational monitoring process and traces the source of pollutants using sensors, observation data, process models, and observation products that users can access in a timely manner.</jats:p>","the increasing deterioration of aquatic environments has attracted more attention to water quality monitoring techniques, with most researchers focusing on the acquisition and assessment of water quality data, but seldom on the discovery and tracing of pollution sources. in this study, a semantic-enhanced modeling method for ontology modeling and rules building is proposed, which can be used for river water quality monitoring and relevant data observation processing. the observational process ontology (opo) method can describe the semantic properties of water resources and observation data. in addition, it can provide the semantic relevance among the different concepts involved in the observational process of water quality monitoring. a pollution alert can be achieved using the reasoning rules for the water quality monitoring stations. in this study, a case is made for the usability testing of the opo models and reasoning rules by utilizing a water quality monitoring system. the system contributes to the water quality observational monitoring process and traces the source of pollutants using sensors, observation data, process models, and observation products that users can access in a timely manner."
http://orkg.org/orkg/resource/R78130,Estimation of the asymptomatic ratio of novel coronavirus infections (COVID-19),,crossref,"<jats:p>A total of 565 Japanese citizens were evacuated from Wuhan, China to Japan. All passengers were screened for symptoms and also undertook reverse transcription polymerase chain reaction testing, identifying 5 asymptomatic and 7 symptomatic passengers testing positive for 2019-nCoV. We show that the screening result is suggestive of the asymptomatic ratio at 41.6%.</jats:p>","a total of 565 japanese citizens were evacuated from wuhan, china to japan. all passengers were screened for symptoms and also undertook reverse transcription polymerase chain reaction testing, identifying 5 asymptomatic and 7 symptomatic passengers testing positive for 2019-ncov. we show that the screening result is suggestive of the asymptomatic ratio at 41.6%."
http://orkg.org/orkg/resource/R78124,Serial interval of novel coronavirus (COVID-19) infections,,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Objective</jats:title><jats:p>To estimate the serial interval of novel coronavirus (COVID-19) from information on 28 infector-infectee pairs.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>We collected dates of illness onset for primary cases (infectors) and secondary cases (infectees) from published research articles and case investigation reports. We subjectively ranked the credibility of the data and performed analyses on both the full dataset (<jats:italic>n</jats:italic>=28) and a subset of pairs with highest certainty in reporting (<jats:italic>n</jats:italic>=18). In addition, we adjusting for right truncation of the data as the epidemic is still in its growth phase.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Accounting for right truncation and analyzing all pairs, we estimated the median serial interval at 4.0 days (95% credible interval [CrI]: 3.1, 4.9). Limiting our data to only the most certain pairs, the median serial interval was estimated at 4.6 days (95% CrI: 3.5, 5.9).</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>The serial interval of COVID-19 is shorter than its median incubation period. This suggests that a substantial proportion of secondary transmission may occur prior to illness onset. The COVID-19 serial interval is also shorter than the serial interval of severe acute respiratory syndrome (SARS), indicating that calculations made using the SARS serial interval may introduce bias.</jats:p></jats:sec><jats:sec><jats:title>Highlights</jats:title><jats:list list-type=""simple""><jats:list-item><jats:label>-</jats:label><jats:p>The serial interval of novel coronavirus (COVID-19) infections was estimated from a total of 28 infector-infectee pairs.</jats:p></jats:list-item><jats:list-item><jats:label>-</jats:label><jats:p>The median serial interval is shorter than the median incubation period, suggesting a substantial proportion of pre-symptomatic transmission.</jats:p></jats:list-item><jats:list-item><jats:label>-</jats:label><jats:p>A short serial interval makes it difficult to trace contacts due to the rapid turnover of case generations.</jats:p></jats:list-item></jats:list></jats:sec>","abstract objective to estimate the serial interval of novel coronavirus (covid-19) from information on 28 infector-infectee pairs. methods we collected dates of illness onset for primary cases (infectors) and secondary cases (infectees) from published research articles and case investigation reports. we subjectively ranked the credibility of the data and performed analyses on both the full dataset ( n =28) and a subset of pairs with highest certainty in reporting ( n =18). in addition, we adjusting for right truncation of the data as the epidemic is still in its growth phase. results accounting for right truncation and analyzing all pairs, we estimated the median serial interval at 4.0 days (95% credible interval [cri]: 3.1, 4.9). limiting our data to only the most certain pairs, the median serial interval was estimated at 4.6 days (95% cri: 3.5, 5.9). conclusions the serial interval of covid-19 is shorter than its median incubation period. this suggests that a substantial proportion of secondary transmission may occur prior to illness onset. the covid-19 serial interval is also shorter than the serial interval of severe acute respiratory syndrome (sars), indicating that calculations made using the sars serial interval may introduce bias. highlights - the serial interval of novel coronavirus (covid-19) infections was estimated from a total of 28 infector-infectee pairs. - the median serial interval is shorter than the median incubation period, suggesting a substantial proportion of pre-symptomatic transmission. - a short serial interval makes it difficult to trace contacts due to the rapid turnover of case generations."
http://orkg.org/orkg/resource/R78160,Preliminary estimation of the novel coronavirus disease (COVID-19) cases in Iran: A modelling analysis based on overseas cases and air travel data,,crossref,"<jats:title>Abstract</jats:title><jats:p>As of 1 March 2020, Iran has reported 987 COVID-19 cases and including 54 associated deaths. At least six neighboring countries (Bahrain, Iraq, Kuwait, Oman, Afghanistan and Pakistan) have reported imported COVID-19 cases from Iran. We used air travel data and the cases from Iran to other Middle East countries and estimated 16533 (95% CI: 5925, 35538) COVID-19 cases in Iran by 25 February, before UAE and other Gulf Cooperation Council countries suspended inbound and outbound flights from Iran.</jats:p>","abstract as of 1 march 2020, iran has reported 987 covid-19 cases and including 54 associated deaths. at least six neighboring countries (bahrain, iraq, kuwait, oman, afghanistan and pakistan) have reported imported covid-19 cases from iran. we used air travel data and the cases from iran to other middle east countries and estimated 16533 (95% ci: 5925, 35538) covid-19 cases in iran by 25 february, before uae and other gulf cooperation council countries suspended inbound and outbound flights from iran."
http://orkg.org/orkg/resource/R78247,An Ontology-Based Framework for Publishing and Exploiting Linked Open Data: A Use Case on Water Resources Management,,crossref,"<jats:p>Nowadays, the increasing demand of water for electricity production, agricultural and industrial uses are directly affecting the reduction of available quality water for human consumption in the world. Efficient and sustainable maintenance of water reservoirs and supply networks implies a holistic strategy that takes into account, as much as possible, information from the stages of water usage. Next,-generation decision-making software tools, for supporting water management, require the integration of multiple and heterogeneous data sources of different knowledge domains. In this regard, Linked Data and Semantic Web technologies enable harmonization of different data sources, as well as the efficient querying for feeding upper-level Business Intelligence processes. This work investigates the design, implementation and usage of a semantic approach driven by ontology to capture, store, integrate and exploit real-world data concerning water supply networks management. As a main contribution, the proposal helps with obtaining semantically enriched linked data, enhancing the analysis of water network performance. For validation purposes, in the use case, a series of data sources from different measures have been considered, in the scope of an actual water management system of the Mediterranean region of Valencia (Spain), throughout several years of activity. The obtained experience shows the benefits of using the proposed approach to identify possible correlations between the measures such as the supplied water, the water leaks or the population.</jats:p>","nowadays, the increasing demand of water for electricity production, agricultural and industrial uses are directly affecting the reduction of available quality water for human consumption in the world. efficient and sustainable maintenance of water reservoirs and supply networks implies a holistic strategy that takes into account, as much as possible, information from the stages of water usage. next,-generation decision-making software tools, for supporting water management, require the integration of multiple and heterogeneous data sources of different knowledge domains. in this regard, linked data and semantic web technologies enable harmonization of different data sources, as well as the efficient querying for feeding upper-level business intelligence processes. this work investigates the design, implementation and usage of a semantic approach driven by ontology to capture, store, integrate and exploit real-world data concerning water supply networks management. as a main contribution, the proposal helps with obtaining semantically enriched linked data, enhancing the analysis of water network performance. for validation purposes, in the use case, a series of data sources from different measures have been considered, in the scope of an actual water management system of the mediterranean region of valencia (spain), throughout several years of activity. the obtained experience shows the benefits of using the proposed approach to identify possible correlations between the measures such as the supplied water, the water leaks or the population."
http://orkg.org/orkg/resource/R78423,Regional Competitiveness as an Aspect Promoting Sustainability of Latvia,10.14207/ejsd.2021.v10n1p650,crossref,"<jats:p>Providing of sustainability is one of the main priorities in normative documents in various countries. Factors affecting regional competitiveness is seen as close to them determining sustainability in many researches. The aim of this research was to identify and evaluate main factors of competitiveness for statistical regions of Latvia to promote sustainable development of the country, applying the complex regional competitiveness assessment system developed by the author. The analysis of the Regional Competitiveness Index (RCI) and its sub-indexes showed that each statistical region has both: factors promoting and hindering competitiveness. Overall the most competitive is Riga statistical region, but the last place takes Latgale statistical region. It is possible to promote equal regional development and sustainability of Latvia by implementing well-developed regional development strategy and National Action Plan. To develop such strategies, it is necessary to understand the concept of sustainable competitiveness. To evaluate sustainable competitiveness of Latvia and its regions it is necessary to develop further the methodology of regional competitiveness evaluation.</jats:p>","providing of sustainability is one of the main priorities in normative documents in various countries. factors affecting regional competitiveness is seen as close to them determining sustainability in many researches. the aim of this research was to identify and evaluate main factors of competitiveness for statistical regions of latvia to promote sustainable development of the country, applying the complex regional competitiveness assessment system developed by the author. the analysis of the regional competitiveness index (rci) and its sub-indexes showed that each statistical region has both: factors promoting and hindering competitiveness. overall the most competitive is riga statistical region, but the last place takes latgale statistical region. it is possible to promote equal regional development and sustainability of latvia by implementing well-developed regional development strategy and national action plan. to develop such strategies, it is necessary to understand the concept of sustainable competitiveness. to evaluate sustainable competitiveness of latvia and its regions it is necessary to develop further the methodology of regional competitiveness evaluation."
http://orkg.org/orkg/resource/R78420,"Evaluation of Developmental Progress in Some Cities of Punjab, Pakistan, Using Urban Sustainability Indicators",10.3390/su9081473,crossref,"<jats:p>Sustainable urbanization is a challenge to human beings in the modern era of technology. Cities all over the world are facing several problems due to urbanization and industrialization. Urban problems could be assessed through development of indices of urban sustainability on the basis of its three dimensions: environment, economics and social. The present study was conducted to identify indicators to develop indices for assessment of sustainability in some populated cities of Punjab. The study focused on the indicators based on environmental, economic and social development to develop a rational indicator system on the basis of secondary data collected from 2004 to 2014. A total of 40 indicators were identified to assess the urban sustainability progress in Lahore, Rawalpindi, Multan, Gujranwala and Faisalabad cities. The result of the sub-indices indicated that poor performance was evident in the environmental sector, rather than in the economic and social sectors. The cities scored between 0.27 and 0.58 in environmental dimensions, showing a decreasing trend from 2004 to 2014. The declining trend of indices was due to population influx, rapid urbanization, reduction in green areas, industrialization, high level of atmospheric and water pollutants. In case of the economic sector, an increasing trend was observed which indicates the gradual improvement in living standards of people. In the social dimensions of the indicator system, less variations were observed among the cities and ranged between 0.49 and 0.58. Overall, the results of the urban sustainability index showed score was ranges between 0.41 to 0.52. None of the Punjab cities attained the position as a sustainable city (0.75) Lahore (0.52) and Faisalabad (0.52) were ranked as moderately sustainable cities, whereas, Rawalpindi (0.48), Gujranwala (0.47) and Multan (0.41) were ranked as weak sustainable cities. The study highlighted that the urban sustainability indicator system could be useful to determine the existing sustainability in cities of the developing countries for better resource management practices.</jats:p>","sustainable urbanization is a challenge to human beings in the modern era of technology. cities all over the world are facing several problems due to urbanization and industrialization. urban problems could be assessed through development of indices of urban sustainability on the basis of its three dimensions: environment, economics and social. the present study was conducted to identify indicators to develop indices for assessment of sustainability in some populated cities of punjab. the study focused on the indicators based on environmental, economic and social development to develop a rational indicator system on the basis of secondary data collected from 2004 to 2014. a total of 40 indicators were identified to assess the urban sustainability progress in lahore, rawalpindi, multan, gujranwala and faisalabad cities. the result of the sub-indices indicated that poor performance was evident in the environmental sector, rather than in the economic and social sectors. the cities scored between 0.27 and 0.58 in environmental dimensions, showing a decreasing trend from 2004 to 2014. the declining trend of indices was due to population influx, rapid urbanization, reduction in green areas, industrialization, high level of atmospheric and water pollutants. in case of the economic sector, an increasing trend was observed which indicates the gradual improvement in living standards of people. in the social dimensions of the indicator system, less variations were observed among the cities and ranged between 0.49 and 0.58. overall, the results of the urban sustainability index showed score was ranges between 0.41 to 0.52. none of the punjab cities attained the position as a sustainable city (0.75) lahore (0.52) and faisalabad (0.52) were ranked as moderately sustainable cities, whereas, rawalpindi (0.48), gujranwala (0.47) and multan (0.41) were ranked as weak sustainable cities. the study highlighted that the urban sustainability indicator system could be useful to determine the existing sustainability in cities of the developing countries for better resource management practices."
http://orkg.org/orkg/resource/R166689,A Fuzzy Cognitive Mapping Approach to the Conference Selection Problem,10.1142/S0219622020500352,crossref,"<jats:p> Academic conferences are popular platforms for academicians to share their research with colleagues, get feedback, and stay up to date on recent academic studies. Conferences also provide opportunities for the participants to express themselves, expand their network, and become socialized. However, academicians are forced to choose a limited number of conferences to participate due to several different factors such as time required for preparing a research, traveling and lodging expenses, and conference fees. At this multi-criteria decision problem, relevant factors can be used to evaluate the alternatives (i.e., academic conferences to participate) and prioritization of these factors would be necessary in advance. To address this issue, this study suggests an improved fuzzy cognitive mapping (FCM) approach to analyze factors affecting the choice of academic conferences to participate. The classical FCM allows to observe the dynamic behavior of complex systems during time. While the approach is widely used in different areas, it has considerable drawbacks: (i) producing same steady state values under different initial conditions and (ii) yielding completely different steady state values when different threshold functions are used. The new approach provides a mathematical formulation that produces steady state values sensitive to initial conditions. Since the selection of the threshold function in classical FCM is a highly subjective choice, the proposed approach offers an alternative way to obtain comparable values. </jats:p>","academic conferences are popular platforms for academicians to share their research with colleagues, get feedback, and stay up to date on recent academic studies. conferences also provide opportunities for the participants to express themselves, expand their network, and become socialized. however, academicians are forced to choose a limited number of conferences to participate due to several different factors such as time required for preparing a research, traveling and lodging expenses, and conference fees. at this multi-criteria decision problem, relevant factors can be used to evaluate the alternatives (i.e., academic conferences to participate) and prioritization of these factors would be necessary in advance. to address this issue, this study suggests an improved fuzzy cognitive mapping (fcm) approach to analyze factors affecting the choice of academic conferences to participate. the classical fcm allows to observe the dynamic behavior of complex systems during time. while the approach is widely used in different areas, it has considerable drawbacks: (i) producing same steady state values under different initial conditions and (ii) yielding completely different steady state values when different threshold functions are used. the new approach provides a mathematical formulation that produces steady state values sensitive to initial conditions. since the selection of the threshold function in classical fcm is a highly subjective choice, the proposed approach offers an alternative way to obtain comparable values."
http://orkg.org/orkg/resource/R175056,Attracting new users or business as usual? A case study of converting academic subscription-based journals to open access,10.1162/qss_a_00126,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>This paper studies a selection of 11 Norwegian journals in the humanities and social sciences and their conversion from subscription to open access, a move heavily incentivized by governmental mandates and open access policies. By investigating the journals’ visiting logs in the period 2014–2019, the study finds that a conversion to open access induces higher visiting numbers; all journals in the study had a significant increase, which can be attributed to the conversion. Converting a journal had no spillover in terms of increased visits to previously published articles still behind the paywall in the same journals. Visits from previously subscribing Norwegian higher education institutions did not account for the increase in visits, indicating that the increase must be accounted for by visitors from other sectors. The results could be relevant for policymakers concerning the effects of strict policies targeting economically vulnerable national journals, and could further inform journal owners and editors on the effects of converting to open access.</jats:p>","abstract \n this paper studies a selection of 11 norwegian journals in the humanities and social sciences and their conversion from subscription to open access, a move heavily incentivized by governmental mandates and open access policies. by investigating the journals’ visiting logs in the period 2014–2019, the study finds that a conversion to open access induces higher visiting numbers; all journals in the study had a significant increase, which can be attributed to the conversion. converting a journal had no spillover in terms of increased visits to previously published articles still behind the paywall in the same journals. visits from previously subscribing norwegian higher education institutions did not account for the increase in visits, indicating that the increase must be accounted for by visitors from other sectors. the results could be relevant for policymakers concerning the effects of strict policies targeting economically vulnerable national journals, and could further inform journal owners and editors on the effects of converting to open access."
http://orkg.org/orkg/resource/R175037,"Social Preferences, Beliefs, and the Dynamics of Free Riding in Public Goods Experiments",10.1257/aer.100.1.541,crossref,"""<jats:p> One lingering puzzle is why voluntary contributions to public goods decline over time in experimental and real-world settings. We show that the decline of cooperation is driven by individual preferences for imperfect conditional cooperation. Many people's desire to contribute less than others, rather than changing beliefs of what others will contribute over time or people's heterogeneity in preferences makes voluntary cooperation fragile. Universal free riding thus eventually emerges, despite the fact that most people are not selfish. (D12, D 83, H41, Z13) </jats:p>""",""" one lingering puzzle is why voluntary contributions to public goods decline over time in experimental and real-world settings. we show that the decline of cooperation is driven by individual preferences for imperfect conditional cooperation. many people's desire to contribute less than others, rather than changing beliefs of what others will contribute over time or people's heterogeneity in preferences makes voluntary cooperation fragile. universal free riding thus eventually emerges, despite the fact that most people are not selfish. (d12, d 83, h41, z13) """
http://orkg.org/orkg/resource/R166335,Overview of BioCreAtIvE task 1B: normalized gene lists,10.1186/1471-2105-6-s1-s11,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Our goal in BioCreAtIve has been to assess the state of the art in text mining, with emphasis on applications that reflect real biological applications, e.g., the curation process for model organism databases. This paper summarizes the BioCreAtIvE task 1B, the ""Normalized Gene List"" task, which was inspired by the gene list supplied for each curated paper in a model organism database. The task was to produce the correct list of unique gene identifiers for the genes and gene products mentioned in sets of abstracts from three model organisms (Yeast, Fly, and Mouse).</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>Eight groups fielded systems for three data sets (Yeast, Fly, and Mouse). For Yeast, the top scoring system (out of 15) achieved 0.92 F-measure (harmonic mean of precision and recall); for Mouse and Fly, the task was more difficult, due to larger numbers of genes, more ambiguity in the gene naming conventions (particularly for Fly), and complex gene names (for Mouse). For Fly, the top F-measure was 0.82 out of 11 systems and for Mouse, it was 0.79 out of 16 systems.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusion</jats:title>\n            <jats:p>This assessment demonstrates that multiple groups were able to perform a real biological task across a range of organisms. The performance was dependent on the organism, and specifically on the naming conventions associated with each organism. These results hold out promise that the technology can provide partial automation of the curation process in the near future.</jats:p>\n          </jats:sec>","abstract \n \n background \n our goal in biocreative has been to assess the state of the art in text mining, with emphasis on applications that reflect real biological applications, e.g., the curation process for model organism databases. this paper summarizes the biocreative task 1b, the ""normalized gene list"" task, which was inspired by the gene list supplied for each curated paper in a model organism database. the task was to produce the correct list of unique gene identifiers for the genes and gene products mentioned in sets of abstracts from three model organisms (yeast, fly, and mouse). \n \n \n results \n eight groups fielded systems for three data sets (yeast, fly, and mouse). for yeast, the top scoring system (out of 15) achieved 0.92 f-measure (harmonic mean of precision and recall); for mouse and fly, the task was more difficult, due to larger numbers of genes, more ambiguity in the gene naming conventions (particularly for fly), and complex gene names (for mouse). for fly, the top f-measure was 0.82 out of 11 systems and for mouse, it was 0.79 out of 16 systems. \n \n \n conclusion \n this assessment demonstrates that multiple groups were able to perform a real biological task across a range of organisms. the performance was dependent on the organism, and specifically on the naming conventions associated with each organism. these results hold out promise that the technology can provide partial automation of the curation process in the near future. \n"
http://orkg.org/orkg/resource/R166784,Automation and New Tasks: How Technology Displaces and Reinstates Labor,10.1257/jep.33.2.3,crossref,"<jats:p> We present a framework for understanding the effects of automation and other types of technological changes on labor demand, and use it to interpret changes in US employment over the recent past. At the center of our framework is the allocation of tasks to capital and labor—the task content of production. Automation, which enables capital to replace labor in tasks it was previously engaged in, shifts the task content of production against labor because of a displacement effect. As a result, automation always reduces the labor share in value added and may reduce labor demand even as it raises productivity. The effects of automation are counterbalanced by the creation of new tasks in which labor has a comparative advantage. The introduction of new tasks changes the task content of production in favor of labor because of a reinstatement effect, and always raises the labor share and labor demand. We show how the role of changes in the task content of production—due to automation and new tasks—can be inferred from industry-level data. Our empirical decomposition suggests that the slower growth of employment over the last three decades is accounted for by an acceleration in the displacement effect, especially in manufacturing, a weaker reinstatement effect, and slower growth of productivity than in previous decades. </jats:p>","we present a framework for understanding the effects of automation and other types of technological changes on labor demand, and use it to interpret changes in us employment over the recent past. at the center of our framework is the allocation of tasks to capital and labor—the task content of production. automation, which enables capital to replace labor in tasks it was previously engaged in, shifts the task content of production against labor because of a displacement effect. as a result, automation always reduces the labor share in value added and may reduce labor demand even as it raises productivity. the effects of automation are counterbalanced by the creation of new tasks in which labor has a comparative advantage. the introduction of new tasks changes the task content of production in favor of labor because of a reinstatement effect, and always raises the labor share and labor demand. we show how the role of changes in the task content of production—due to automation and new tasks—can be inferred from industry-level data. our empirical decomposition suggests that the slower growth of employment over the last three decades is accounted for by an acceleration in the displacement effect, especially in manufacturing, a weaker reinstatement effect, and slower growth of productivity than in previous decades."
http://orkg.org/orkg/resource/R166576,KM Cyberary is a Gateway to Knowledge Resources: ,10.4018/978-1-59140-989-2.ch088,crossref,"<jats:p>The literature about portals or gateways exists in huge volume. However, only a limited number of articles have addressed the topic of knowledge portals applying various concepts of arrangement to organizing the information on the portals. Likewise, not many authors have written about knowledge portals. So, this study helps to understand one such knowledge portal and its features, coverage of subjects, and so forth.</jats:p>","the literature about portals or gateways exists in huge volume. however, only a limited number of articles have addressed the topic of knowledge portals applying various concepts of arrangement to organizing the information on the portals. likewise, not many authors have written about knowledge portals. so, this study helps to understand one such knowledge portal and its features, coverage of subjects, and so forth."
http://orkg.org/orkg/resource/R166564,Managing Knowledge and Scholarly Assets in Academic Libraries: Issues and Challenges,10.4018/978-1-5225-1741-2.CH013,crossref,"<jats:p>Knowledge Management (KM) aspects have prominent role in corporate sectors since many years. But there is an opportunity in higher education sector (i.e. in academia) especially to adapt the strategy in libraries to manage intellectual or scholarly assets of an organization. This chapter is intended for library professionals, knowledge managers, students and other communities planning to implement the knowledge management aspects in libraries. The objective of this chapter is to provide insight on strategic approach for successful implementation of knowledge management in libraries. It caters to library and KM professionals who want to improve their understanding of the vital role and implementation of KM aspects in libraries. In this direction, this chapter provides ideas to its readers about the approaches about strategy and innovative measures, practical applications, tools and technologies, platforms, challenges, and issues, change management and other related aspects required for Library and Information Science (LIS) and Knowledge Management (KM) professionals. </jats:p>","knowledge management (km) aspects have prominent role in corporate sectors since many years. but there is an opportunity in higher education sector (i.e. in academia) especially to adapt the strategy in libraries to manage intellectual or scholarly assets of an organization. this chapter is intended for library professionals, knowledge managers, students and other communities planning to implement the knowledge management aspects in libraries. the objective of this chapter is to provide insight on strategic approach for successful implementation of knowledge management in libraries. it caters to library and km professionals who want to improve their understanding of the vital role and implementation of km aspects in libraries. in this direction, this chapter provides ideas to its readers about the approaches about strategy and innovative measures, practical applications, tools and technologies, platforms, challenges, and issues, change management and other related aspects required for library and information science (lis) and knowledge management (km) professionals."
http://orkg.org/orkg/resource/R166573,Intranets: A New Dimension for Library Services,10.14429/dbit.24.1.3617,crossref,"<p>This article is based on some practical experiences of Indian Rubber Manufacturing Research Association (IRMRA). It gives a brief introduction about the intranet, controversies of an intranet, and its library applications. It also gives information about accessing management information, improving internal communications, e-mail, intra mail, collaborative working, communities of interests, discussion groups, electronic forms, internal newsletters, search facilities, training materials, and library applications/access. The use of intranet in providing library services like SDI, CAS and other information management is explained with a schematic diagram.</p>","this article is based on some practical experiences of indian rubber manufacturing research association (irmra). it gives a brief introduction about the intranet, controversies of an intranet, and its library applications. it also gives information about accessing management information, improving internal communications, e-mail, intra mail, collaborative working, communities of interests, discussion groups, electronic forms, internal newsletters, search facilities, training materials, and library applications/access. the use of intranet in providing library services like sdi, cas and other information management is explained with a schematic diagram."
http://orkg.org/orkg/resource/R166596,Ontologies in Portal Design,10.4018/978-1-59140-989-2.CH109,crossref,"<jats:p>Portals are becoming more and more ubiquitous on the Internet and that is why their architecture is a topic of concern among domain stakeholders. In order to ensure a solid architecture in portal design, ontologies must be considered as a necessary agent of design. An ontology provides a classification system for all the data and metadata in a domain. Ontologies supply metadata in order to bring about a streamlined delivery of information to users. While portals exist in order to assist users gain access to information, ontologies enhance portals by providing access to relevant information.</jats:p>","portals are becoming more and more ubiquitous on the internet and that is why their architecture is a topic of concern among domain stakeholders. in order to ensure a solid architecture in portal design, ontologies must be considered as a necessary agent of design. an ontology provides a classification system for all the data and metadata in a domain. ontologies supply metadata in order to bring about a streamlined delivery of information to users. while portals exist in order to assist users gain access to information, ontologies enhance portals by providing access to relevant information."
http://orkg.org/orkg/resource/R166605,Library Consortia as Cooperation and Collaboration Initiative for Libraries: A Proposed Model,10.4018/978-1-7998-0043-9.ch012,crossref,"<jats:p>This chapter discusses the different aspects of cooperation and collaboration initiatives for libraries that includes associated opportunities, issues, and challenges. A proposed model for library consortia is depicted to illustrate the benefits that can help institutes to overcome these issues and challenges faced by the libraries for smooth functioning. The proposed cross-resource sharing model may provide an appropriate platform for both institutes and publishers that can enhance the library cooperation and collaboration in a much more effective way by overcoming the fund issues and to enable a suitable platform in improvising the library services. By achieving a successful and appropriate library consortia model, the cooperation and collaboration initiatives for the libraries gives an effective result and better services to its users. </jats:p>","this chapter discusses the different aspects of cooperation and collaboration initiatives for libraries that includes associated opportunities, issues, and challenges. a proposed model for library consortia is depicted to illustrate the benefits that can help institutes to overcome these issues and challenges faced by the libraries for smooth functioning. the proposed cross-resource sharing model may provide an appropriate platform for both institutes and publishers that can enhance the library cooperation and collaboration in a much more effective way by overcoming the fund issues and to enable a suitable platform in improvising the library services. by achieving a successful and appropriate library consortia model, the cooperation and collaboration initiatives for the libraries gives an effective result and better services to its users."
http://orkg.org/orkg/resource/R166561,Research Data Management: A proposed framework to boost research in Higher Educational Institutes.,10.29173/iq12,crossref,"<jats:p>This paper attempts to present a brief overview of several Research Data Management (RDM) issues and a detailed literature review regarding the RDM aspects adopted in libraries globally. Furthermore, it will describe several tendencies concerning the management of repository tools for research data, as well as the challenges in implementing the RDM. The proper planned training and skill development for all stakeholders by mentors to train both staff and users are some of the issues that need to be considered to enhance the RDM process. An effort will be also made to present the suitable policies and workflows along with the adoption of best practices in RDM, so as to boost the research process in an organisation. This study will showcase the implementation of RDM processes in the Higher Educational Institute of India, referring particularly to the Central Library @ NIT Rourkela in Odisha, India with a proposed framework. Finally, this study will also propose an area of opportunities that can boost research activities in the Institute.</jats:p>","this paper attempts to present a brief overview of several research data management (rdm) issues and a detailed literature review regarding the rdm aspects adopted in libraries globally. furthermore, it will describe several tendencies concerning the management of repository tools for research data, as well as the challenges in implementing the rdm. the proper planned training and skill development for all stakeholders by mentors to train both staff and users are some of the issues that need to be considered to enhance the rdm process. an effort will be also made to present the suitable policies and workflows along with the adoption of best practices in rdm, so as to boost the research process in an organisation. this study will showcase the implementation of rdm processes in the higher educational institute of india, referring particularly to the central library @ nit rourkela in odisha, india with a proposed framework. finally, this study will also propose an area of opportunities that can boost research activities in the institute."
http://orkg.org/orkg/resource/R175444,Efficient synthesis of physically valid human motion,10.1145/882262.882286,crossref,"<jats:p>\n            Optimization is a promising way to generate new animations from a minimal amount of input data. Physically based optimization techniques, however, are difficult to scale to complex animated characters, in part because evaluating and differentiating physical quantities becomes prohibitively slow. Traditional approaches often require optimizing or constraining parameters involving joint torques; obtaining first derivatives for these parameters is generally an\n            <jats:italic>O</jats:italic>\n            (\n            <jats:italic>D</jats:italic>\n            <jats:sup>2</jats:sup>\n            ) process, where\n            <jats:italic>D</jats:italic>\n            is the number of degrees of freedom of the character. In this paper, we describe a set of objective functions and constraints that lead to linear time analytical first derivatives. The surprising finding is that this set includes constraints on physical validity, such as ground contact constraints. Considering only constraints and objective functions that lead to linear time first derivatives results in fast per-iteration computation times and an optimization problem that appears to scale well to more complex characters. We show that qualities such as squash-and-stretch that are expected from physically based optimization result from our approach. Our animation system is particularly useful for synthesizing highly dynamic motions, and we show examples of swinging and leaping motions for characters having from 7 to 22 degrees of freedom.\n          </jats:p>","\n optimization is a promising way to generate new animations from a minimal amount of input data. physically based optimization techniques, however, are difficult to scale to complex animated characters, in part because evaluating and differentiating physical quantities becomes prohibitively slow. traditional approaches often require optimizing or constraining parameters involving joint torques; obtaining first derivatives for these parameters is generally an\n o \n (\n d \n 2 \n ) process, where\n d \n is the number of degrees of freedom of the character. in this paper, we describe a set of objective functions and constraints that lead to linear time analytical first derivatives. the surprising finding is that this set includes constraints on physical validity, such as ground contact constraints. considering only constraints and objective functions that lead to linear time first derivatives results in fast per-iteration computation times and an optimization problem that appears to scale well to more complex characters. we show that qualities such as squash-and-stretch that are expected from physically based optimization result from our approach. our animation system is particularly useful for synthesizing highly dynamic motions, and we show examples of swinging and leaping motions for characters having from 7 to 22 degrees of freedom.\n"
http://orkg.org/orkg/resource/R166504,bioNerDS: exploring bioinformatics’ database and software use through literature mining,10.1186/1471-2105-14-194,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Biology-focused databases and software define bioinformatics and their use is central to computational biology. In such a complex and dynamic field, it is of interest to understand what resources are available, which are used, how much they are used, and for what they are used. While scholarly literature surveys can provide some insights, large-scale computer-based approaches to identify mentions of bioinformatics databases and software from primary literature would automate systematic cataloguing, facilitate the monitoring of usage, and provide the foundations for the recovery of computational methods for analysing biological data, with the long-term aim of identifying best/common practice in different areas of biology.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>We have developed bioNerDS, a named entity recogniser for the recovery of bioinformatics databases and software from primary literature. We identify such entities with an F-measure ranging from 63% to 91% at the mention level and 63-78% at the document level, depending on corpus. Not attaining a higher F-measure is mostly due to high ambiguity in resource naming, which is compounded by the on-going introduction of new resources. To demonstrate the software, we applied bioNerDS to full-text articles from BMC Bioinformatics and Genome Biology. General mention patterns reflect the remit of these journals, highlighting BMC Bioinformatics’s emphasis on new tools and Genome Biology’s greater emphasis on data analysis. The data also illustrates some shifts in resource usage: for example, the past decade has seen R and the Gene Ontology join BLAST and GenBank as the main components in bioinformatics processing.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>We demonstrate the feasibility of automatically identifying resource names on a large-scale from the scientific literature and show that the generated data can be used for exploration of bioinformatics database and software usage. For example, our results help to investigate the rate of change in resource usage and corroborate the suspicion that a vast majority of resources are created, but rarely (if ever) used thereafter. bioNerDS is available at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""http://bionerds.sourceforge.net/"" ext-link-type=""uri"">http://bionerds.sourceforge.net/</jats:ext-link>.</jats:p>\n          </jats:sec>","abstract \n \n background \n biology-focused databases and software define bioinformatics and their use is central to computational biology. in such a complex and dynamic field, it is of interest to understand what resources are available, which are used, how much they are used, and for what they are used. while scholarly literature surveys can provide some insights, large-scale computer-based approaches to identify mentions of bioinformatics databases and software from primary literature would automate systematic cataloguing, facilitate the monitoring of usage, and provide the foundations for the recovery of computational methods for analysing biological data, with the long-term aim of identifying best/common practice in different areas of biology. \n \n \n results \n we have developed bionerds, a named entity recogniser for the recovery of bioinformatics databases and software from primary literature. we identify such entities with an f-measure ranging from 63% to 91% at the mention level and 63-78% at the document level, depending on corpus. not attaining a higher f-measure is mostly due to high ambiguity in resource naming, which is compounded by the on-going introduction of new resources. to demonstrate the software, we applied bionerds to full-text articles from bmc bioinformatics and genome biology. general mention patterns reflect the remit of these journals, highlighting bmc bioinformatics’s emphasis on new tools and genome biology’s greater emphasis on data analysis. the data also illustrates some shifts in resource usage: for example, the past decade has seen r and the gene ontology join blast and genbank as the main components in bioinformatics processing. \n \n \n conclusions \n we demonstrate the feasibility of automatically identifying resource names on a large-scale from the scientific literature and show that the generated data can be used for exploration of bioinformatics database and software usage. for example, our results help to investigate the rate of change in resource usage and corroborate the suspicion that a vast majority of resources are created, but rarely (if ever) used thereafter. bionerds is available at http://bionerds.sourceforge.net/ . \n"
http://orkg.org/orkg/resource/R166589,Social Networks and Knowledge Management: An Explorative Study in Library Systems,10.4018/978-1-61350-195-5.ch004,crossref,"<jats:p>This chapter gives a brief introduction to Knowledge Management (KM) and its components, emphasizing the role Social Networks (SNs) can play on KM. The authors will delineate the benefits of collaboration between the concept of Social Networking and the process of KM. With the advent of Web 2.0 technologies, it is a natural evolutionary outcome that SNs have driven the advancement of KM, and conversely KM has driven the advancement of SNs. In certain instances, SNs and KM have a symbiotic relationship whereby one cannot exist without the other. Moreover, an impact analysis will be performed to show that while SNs are an outcome of KM, both require each other in order to succeed where Social Software fits. This chapter is particularly intended to cater to the needs of librarians in a corporate environment and to show the impact and benefits of SNs and KM in the information world.</jats:p>","this chapter gives a brief introduction to knowledge management (km) and its components, emphasizing the role social networks (sns) can play on km. the authors will delineate the benefits of collaboration between the concept of social networking and the process of km. with the advent of web 2.0 technologies, it is a natural evolutionary outcome that sns have driven the advancement of km, and conversely km has driven the advancement of sns. in certain instances, sns and km have a symbiotic relationship whereby one cannot exist without the other. moreover, an impact analysis will be performed to show that while sns are an outcome of km, both require each other in order to succeed where social software fits. this chapter is particularly intended to cater to the needs of librarians in a corporate environment and to show the impact and benefits of sns and km in the information world."
http://orkg.org/orkg/resource/R165783,ROBOKOP: an abstraction layer and user interface for knowledge graphs to support question answering,10.1093/bioinformatics/btz604,crossref,"<jats:title>Abstract</jats:title>\n               <jats:sec>\n                  <jats:title>Summary</jats:title>\n                  <jats:p>Knowledge graphs (KGs) are quickly becoming a common-place tool for storing relationships between entities from which higher-level reasoning can be conducted. KGs are typically stored in a graph-database format, and graph-database queries can be used to answer questions of interest that have been posed by users such as biomedical researchers. For simple queries, the inclusion of direct connections in the KG and the storage and analysis of query results are straightforward; however, for complex queries, these capabilities become exponentially more challenging with each increase in complexity of the query. For instance, one relatively complex query can yield a KG with hundreds of thousands of query results. Thus, the ability to efficiently query, store, rank and explore sub-graphs of a complex KG represents a major challenge to any effort designed to exploit the use of KGs for applications in biomedical research and other domains. We present Reasoning Over Biomedical Objects linked in Knowledge Oriented Pathways as an abstraction layer and user interface to more easily query KGs and store, rank and explore query results.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Availability and implementation</jats:title>\n                  <jats:p>An instance of the ROBOKOP UI for exploration of the ROBOKOP Knowledge Graph can be found at http://robokop.renci.org. The ROBOKOP Knowledge Graph can be accessed at http://robokopkg.renci.org. Code and instructions for building and deploying ROBOKOP are available under the MIT open software license from https://github.com/NCATS-Gamma/robokop.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Supplementary information</jats:title>\n                  <jats:p>Supplementary data are available at Bioinformatics online.</jats:p>\n               </jats:sec>","abstract \n \n summary \n knowledge graphs (kgs) are quickly becoming a common-place tool for storing relationships between entities from which higher-level reasoning can be conducted. kgs are typically stored in a graph-database format, and graph-database queries can be used to answer questions of interest that have been posed by users such as biomedical researchers. for simple queries, the inclusion of direct connections in the kg and the storage and analysis of query results are straightforward; however, for complex queries, these capabilities become exponentially more challenging with each increase in complexity of the query. for instance, one relatively complex query can yield a kg with hundreds of thousands of query results. thus, the ability to efficiently query, store, rank and explore sub-graphs of a complex kg represents a major challenge to any effort designed to exploit the use of kgs for applications in biomedical research and other domains. we present reasoning over biomedical objects linked in knowledge oriented pathways as an abstraction layer and user interface to more easily query kgs and store, rank and explore query results. \n \n \n availability and implementation \n an instance of the robokop ui for exploration of the robokop knowledge graph can be found at http://robokop.renci.org. the robokop knowledge graph can be accessed at http://robokopkg.renci.org. code and instructions for building and deploying robokop are available under the mit open software license from https://github.com/ncats-gamma/robokop. \n \n \n supplementary information \n supplementary data are available at bioinformatics online. \n"
http://orkg.org/orkg/resource/R163875,The role of software in science: a knowledge graph-based analysis of software mentions in PubMed Central,10.7717/peerj-cs.835,crossref,"<jats:p>Science across all disciplines has become increasingly data-driven, leading to additional needs with respect to software for collecting, processing and analysing data. Thus, transparency about software used as part of the scientific process is crucial to understand provenance of individual research data and insights, is a prerequisite for reproducibility and can enable macro-analysis of the evolution of scientific methods over time. However, missing rigor in software citation practices renders the automated detection and disambiguation of software mentions a challenging problem. In this work, we provide a large-scale analysis of software usage and citation practices facilitated through an unprecedented knowledge graph of software mentions and affiliated metadata generated through supervised information extraction models trained on a unique gold standard <jats:italic>corpus</jats:italic> and applied to more than 3 million scientific articles. Our information extraction approach distinguishes different types of software and mentions, disambiguates mentions and outperforms the state-of-the-art significantly, leading to the most comprehensive <jats:italic>corpus</jats:italic> of 11.8 M software mentions that are described through a knowledge graph consisting of more than 300 M triples. Our analysis provides insights into the evolution of software usage and citation patterns across various fields, ranks of journals, and impact of publications. Whereas, to the best of our knowledge, this is the most comprehensive analysis of software use and citation at the time, all data and models are shared publicly to facilitate further research into scientific use and citation of software.</jats:p>","science across all disciplines has become increasingly data-driven, leading to additional needs with respect to software for collecting, processing and analysing data. thus, transparency about software used as part of the scientific process is crucial to understand provenance of individual research data and insights, is a prerequisite for reproducibility and can enable macro-analysis of the evolution of scientific methods over time. however, missing rigor in software citation practices renders the automated detection and disambiguation of software mentions a challenging problem. in this work, we provide a large-scale analysis of software usage and citation practices facilitated through an unprecedented knowledge graph of software mentions and affiliated metadata generated through supervised information extraction models trained on a unique gold standard corpus and applied to more than 3 million scientific articles. our information extraction approach distinguishes different types of software and mentions, disambiguates mentions and outperforms the state-of-the-art significantly, leading to the most comprehensive corpus of 11.8 m software mentions that are described through a knowledge graph consisting of more than 300 m triples. our analysis provides insights into the evolution of software usage and citation patterns across various fields, ranks of journals, and impact of publications. whereas, to the best of our knowledge, this is the most comprehensive analysis of software use and citation at the time, all data and models are shared publicly to facilitate further research into scientific use and citation of software."
http://orkg.org/orkg/resource/R164130,A RE-EVALUATION OF BIOMEDICAL NAMED ENTITY–TERM RELATIONS,10.1142/s0219720010005014,crossref,"<jats:p> Text mining can support the interpretation of the enormous quantity of textual data produced in biomedical field. Recent developments in biomedical text mining include advances in the reliability of the recognition of named entities (NEs) such as specific genes and proteins, as well as movement toward richer representations of the associations of NEs. We argue that this shift in representation should be accompanied by the adoption of a more detailed model of the relations holding between NEs and other relevant domain terms. As a step toward this goal, we study NE–term relations with the aim of defining a detailed, broadly applicable set of relation types based on accepted domain standard concepts for use in corpus annotation and domain information extraction approaches. </jats:p>","text mining can support the interpretation of the enormous quantity of textual data produced in biomedical field. recent developments in biomedical text mining include advances in the reliability of the recognition of named entities (nes) such as specific genes and proteins, as well as movement toward richer representations of the associations of nes. we argue that this shift in representation should be accompanied by the adoption of a more detailed model of the relations holding between nes and other relevant domain terms. as a step toward this goal, we study ne–term relations with the aim of defining a detailed, broadly applicable set of relation types based on accepted domain standard concepts for use in corpus annotation and domain information extraction approaches."
http://orkg.org/orkg/resource/R164425,Research Data Management: A proposed framework to boost research in Higher Educational Institutes.,10.29173/iq12,crossref,"<jats:p>This paper attempts to present a brief overview of several Research Data Management (RDM) issues and a detailed literature review regarding the RDM aspects adopted in libraries globally. Furthermore, it will describe several tendencies concerning the management of repository tools for research data, as well as the challenges in implementing the RDM. The proper planned training and skill development for all stakeholders by mentors to train both staff and users are some of the issues that need to be considered to enhance the RDM process. An effort will be also made to present the suitable policies and workflows along with the adoption of best practices in RDM, so as to boost the research process in an organisation. This study will showcase the implementation of RDM processes in the Higher Educational Institute of India, referring particularly to the Central Library @ NIT Rourkela in Odisha, India with a proposed framework. Finally, this study will also propose an area of opportunities that can boost research activities in the Institute.</jats:p>","this paper attempts to present a brief overview of several research data management (rdm) issues and a detailed literature review regarding the rdm aspects adopted in libraries globally. furthermore, it will describe several tendencies concerning the management of repository tools for research data, as well as the challenges in implementing the rdm. the proper planned training and skill development for all stakeholders by mentors to train both staff and users are some of the issues that need to be considered to enhance the rdm process. an effort will be also made to present the suitable policies and workflows along with the adoption of best practices in rdm, so as to boost the research process in an organisation. this study will showcase the implementation of rdm processes in the higher educational institute of india, referring particularly to the central library @ nit rourkela in odisha, india with a proposed framework. finally, this study will also propose an area of opportunities that can boost research activities in the institute."
http://orkg.org/orkg/resource/R162349,BioCreAtIvE Task 1A: gene mention finding evaluation,10.1186/1471-2105-6-s1-s2,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>The biological research literature is a major repository of knowledge. As the amount of literature increases, it will get harder to find the information of interest on a particular topic. There has been an increasing amount of work on text mining this literature, but comparing this work is hard because of a lack of standards for making comparisons. To address this, we worked with colleagues at the Protein Design Group, CNB-CSIC, Madrid to develop BioCreAtIvE (Critical Assessment for Information Extraction in Biology), an open common evaluation of systems on a number of biological text mining tasks. We report here on task 1A, which deals with finding mentions of genes and related entities in text. ""Finding mentions"" is a basic task, which can be used as a building block for other text mining tasks. The task makes use of data and evaluation software provided by the (US) National Center for Biotechnology Information (NCBI).</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>15 teams took part in task 1A. A number of teams achieved scores over 80% F-measure (balanced precision and recall). The teams that tried to use their task 1A systems to help on other BioCreAtIvE tasks reported mixed results.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusion</jats:title>\n            <jats:p>The 80% plus F-measure results are good, but still somewhat lag the best scores achieved in some other domains such as newswire, due in part to the complexity and length of gene names, compared to person or organization names in newswire.</jats:p>\n          </jats:sec>","abstract \n \n background \n the biological research literature is a major repository of knowledge. as the amount of literature increases, it will get harder to find the information of interest on a particular topic. there has been an increasing amount of work on text mining this literature, but comparing this work is hard because of a lack of standards for making comparisons. to address this, we worked with colleagues at the protein design group, cnb-csic, madrid to develop biocreative (critical assessment for information extraction in biology), an open common evaluation of systems on a number of biological text mining tasks. we report here on task 1a, which deals with finding mentions of genes and related entities in text. ""finding mentions"" is a basic task, which can be used as a building block for other text mining tasks. the task makes use of data and evaluation software provided by the (us) national center for biotechnology information (ncbi). \n \n \n results \n 15 teams took part in task 1a. a number of teams achieved scores over 80% f-measure (balanced precision and recall). the teams that tried to use their task 1a systems to help on other biocreative tasks reported mixed results. \n \n \n conclusion \n the 80% plus f-measure results are good, but still somewhat lag the best scores achieved in some other domains such as newswire, due in part to the complexity and length of gene names, compared to person or organization names in newswire. \n"
http://orkg.org/orkg/resource/R162342,Overview of BioCreAtIvE: critical assessment of information extraction for biology,10.1186/1471-2105-6-s1-s1,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>The goal of the first BioCreAtIvE challenge (Critical Assessment of Information Extraction in Biology) was to provide a set of common evaluation tasks to assess the state of the art for text mining applied to biological problems. The results were presented in a workshop held in Granada, Spain March 28–31, 2004. The articles collected in this <jats:italic>BMC Bioinformatics</jats:italic> supplement entitled ""A critical assessment of text mining methods in molecular biology"" describe the BioCreAtIvE tasks, systems, results and their independent evaluation.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>BioCreAtIvE focused on two tasks. The first dealt with extraction of gene or protein names from text, and their mapping into standardized gene identifiers for three model organism databases (fly, mouse, yeast). The second task addressed issues of functional annotation, requiring systems to identify specific text passages that supported Gene Ontology annotations for specific proteins, given full text articles.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusion</jats:title>\n            <jats:p>The first BioCreAtIvE assessment achieved a high level of international participation (27 groups from 10 countries). The assessment provided state-of-the-art performance results for a basic task (gene name finding and normalization), where the best systems achieved a balanced 80% precision / recall or better, which potentially makes them suitable for real applications in biology. The results for the advanced task (functional annotation from free text) were significantly lower, demonstrating the current limitations of text-mining approaches where knowledge extrapolation and interpretation are required. In addition, an important contribution of BioCreAtIvE has been the creation and release of training and test data sets for both tasks. There are 22 articles in this special issue, including six that provide analyses of results or data quality for the data sets, including a novel inter-annotator consistency assessment for the test set used in task 2.</jats:p>\n          </jats:sec>","abstract \n \n background \n the goal of the first biocreative challenge (critical assessment of information extraction in biology) was to provide a set of common evaluation tasks to assess the state of the art for text mining applied to biological problems. the results were presented in a workshop held in granada, spain march 28–31, 2004. the articles collected in this bmc bioinformatics supplement entitled ""a critical assessment of text mining methods in molecular biology"" describe the biocreative tasks, systems, results and their independent evaluation. \n \n \n results \n biocreative focused on two tasks. the first dealt with extraction of gene or protein names from text, and their mapping into standardized gene identifiers for three model organism databases (fly, mouse, yeast). the second task addressed issues of functional annotation, requiring systems to identify specific text passages that supported gene ontology annotations for specific proteins, given full text articles. \n \n \n conclusion \n the first biocreative assessment achieved a high level of international participation (27 groups from 10 countries). the assessment provided state-of-the-art performance results for a basic task (gene name finding and normalization), where the best systems achieved a balanced 80% precision / recall or better, which potentially makes them suitable for real applications in biology. the results for the advanced task (functional annotation from free text) were significantly lower, demonstrating the current limitations of text-mining approaches where knowledge extrapolation and interpretation are required. in addition, an important contribution of biocreative has been the creation and release of training and test data sets for both tasks. there are 22 articles in this special issue, including six that provide analyses of results or data quality for the data sets, including a novel inter-annotator consistency assessment for the test set used in task 2. \n"
http://orkg.org/orkg/resource/R162540,The extraction of complex relationships and their conversion to biological expression language (BEL) overview of the BioCreative VI (2017) BEL track,10.1093/database/baz084,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Knowledge of the molecular interactions of biological and chemical entities and their involvement in biological processes or clinical phenotypes is important for data interpretation. Unfortunately, this knowledge is mostly embedded in the literature in such a way that it is unavailable for automated data analysis procedures. Biological expression language (BEL) is a syntax representation allowing for the structured representation of a broad range of biological relationships. It is used in various situations to extract such knowledge and transform it into BEL networks. To support the tedious and time-intensive extraction work of curators with automated methods, we developed the BEL track within the framework of BioCreative Challenges. Within the BEL track, we provide training data and an evaluation environment to encourage the text mining community to tackle the automatic extraction of complex BEL relationships. In 2017 BioCreative VI, the 2015 BEL track was repeated with new test data. Although only minor improvements in text snippet retrieval for given statements were achieved during this second BEL task iteration, a significant increase of BEL statement extraction performance from provided sentences could be seen. The best performing system reached a 32% F-score for the extraction of complete BEL statements and with the given named entities this increased to 49%. This time, besides rule-based systems, new methods involving hierarchical sequence labeling and neural networks were applied for BEL statement extraction.</jats:p>","abstract \n knowledge of the molecular interactions of biological and chemical entities and their involvement in biological processes or clinical phenotypes is important for data interpretation. unfortunately, this knowledge is mostly embedded in the literature in such a way that it is unavailable for automated data analysis procedures. biological expression language (bel) is a syntax representation allowing for the structured representation of a broad range of biological relationships. it is used in various situations to extract such knowledge and transform it into bel networks. to support the tedious and time-intensive extraction work of curators with automated methods, we developed the bel track within the framework of biocreative challenges. within the bel track, we provide training data and an evaluation environment to encourage the text mining community to tackle the automatic extraction of complex bel relationships. in 2017 biocreative vi, the 2015 bel track was repeated with new test data. although only minor improvements in text snippet retrieval for given statements were achieved during this second bel task iteration, a significant increase of bel statement extraction performance from provided sentences could be seen. the best performing system reached a 32% f-score for the extraction of complete bel statements and with the given named entities this increased to 49%. this time, besides rule-based systems, new methods involving hierarchical sequence labeling and neural networks were applied for bel statement extraction."
http://orkg.org/orkg/resource/R161490,Tantalum-Doped TiO2 Prepared by Atomic Layer Deposition and Its Application in Perovskite Solar Cells,10.3390/nano11061504,crossref,"<jats:p>Tantalum (Ta)-doped titanium oxide (TiO2) thin films are grown by plasma enhanced atomic layer deposition (PEALD), and used as both an electron transport layer and hole blocking compact layer of perovskite solar cells. The metal precursors of tantalum ethoxide and titanium isopropoxide are simultaneously injected into the deposition chamber. The Ta content is controlled by the temperature of the metal precursors. The experimental results show that the Ta incorporation introduces oxygen vacancies defects, accompanied by the reduced crystallinity and optical band gap. The PEALD Ta-doped films show a resistivity three orders of magnitude lower than undoped TiO2, even at a low Ta content (0.8–0.95 at.%). The ultraviolet photoelectron spectroscopy spectra reveal that Ta incorporation leads to a down shift of valance band and conduction positions, and this is helpful for the applications involving band alignment engineering. Finally, the perovskite solar cell with Ta-doped TiO2 electron transport layer demonstrates significantly improved fill factor and conversion efficiency as compared to that with the undoped TiO2 layer.</jats:p>","tantalum (ta)-doped titanium oxide (tio2) thin films are grown by plasma enhanced atomic layer deposition (peald), and used as both an electron transport layer and hole blocking compact layer of perovskite solar cells. the metal precursors of tantalum ethoxide and titanium isopropoxide are simultaneously injected into the deposition chamber. the ta content is controlled by the temperature of the metal precursors. the experimental results show that the ta incorporation introduces oxygen vacancies defects, accompanied by the reduced crystallinity and optical band gap. the peald ta-doped films show a resistivity three orders of magnitude lower than undoped tio2, even at a low ta content (0.8–0.95 at.%). the ultraviolet photoelectron spectroscopy spectra reveal that ta incorporation leads to a down shift of valance band and conduction positions, and this is helpful for the applications involving band alignment engineering. finally, the perovskite solar cell with ta-doped tio2 electron transport layer demonstrates significantly improved fill factor and conversion efficiency as compared to that with the undoped tio2 layer."
http://orkg.org/orkg/resource/R161482,Atomic layer deposition of TiO2 blocking layers for dye-sensitized solar cells,10.1108/mi-01-2020-0007,crossref,"<jats:sec>\n<jats:title content-type=""abstract-subheading"">Purpose</jats:title>\n<jats:p>The purpose of this paper is to improve the efficiency of dye-sensitized solar cells (DSSCs) which present promising low-cost alternative to the conventional silicon solar cells mainly due to comparatively low manufacturing cost, ease of fabrication and relatively good efficiency. One of the undesirable factor in DSSCs is the electron recombination process that takes place at the transparent conductive oxide/electrolyte interface, on the side of photoelectrode. To reduce this effect in the structure of the solar cell, a TiO<jats:sub>2</jats:sub> blocking layer (BL) by atomic layer deposition (ALD) was deposited.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>\n<jats:p>Scanning electron microscope, Raman and UV-Vis spectroscopy were used to evaluate the influence of BL on the photovoltaic properties. Electrical parameters of manufactured DSSCs with and without BL were characterized by measurements of current-voltage characteristics under standard AM 1.5 radiation.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Findings</jats:title>\n<jats:p>The TiO<jats:sub>2</jats:sub> BL prevents the physical contact of fluorine-doped tin oxide (FTO) and the electrolyte and leads to increase in the cell’s overall efficiency, from 5.15 to 6.18%. Higher density of the BL, together with larger contact area and improved adherence between the TiO<jats:sub>2</jats:sub> layer and FTO surface provide more electron pathways from TiO<jats:sub>2</jats:sub> to FTO which facilitates electron transfer.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>\n<jats:p>This paper demonstrates that the introduction of a BL into the photovoltaic device structure is an important step in technology of DSSCs to improve its efficiency. Moreover, the ALD is a powerful technique which allows for the highly reproducible growth of pinhole-free thin films with excellent thickness accuracy and conformality at low temperature.</jats:p>\n</jats:sec>","\n purpose \n the purpose of this paper is to improve the efficiency of dye-sensitized solar cells (dsscs) which present promising low-cost alternative to the conventional silicon solar cells mainly due to comparatively low manufacturing cost, ease of fabrication and relatively good efficiency. one of the undesirable factor in dsscs is the electron recombination process that takes place at the transparent conductive oxide/electrolyte interface, on the side of photoelectrode. to reduce this effect in the structure of the solar cell, a tio 2 blocking layer (bl) by atomic layer deposition (ald) was deposited. \n \n \n design/methodology/approach \n scanning electron microscope, raman and uv-vis spectroscopy were used to evaluate the influence of bl on the photovoltaic properties. electrical parameters of manufactured dsscs with and without bl were characterized by measurements of current-voltage characteristics under standard am 1.5 radiation. \n \n \n findings \n the tio 2 bl prevents the physical contact of fluorine-doped tin oxide (fto) and the electrolyte and leads to increase in the cell’s overall efficiency, from 5.15 to 6.18%. higher density of the bl, together with larger contact area and improved adherence between the tio 2 layer and fto surface provide more electron pathways from tio 2 to fto which facilitates electron transfer. \n \n \n originality/value \n this paper demonstrates that the introduction of a bl into the photovoltaic device structure is an important step in technology of dsscs to improve its efficiency. moreover, the ald is a powerful technique which allows for the highly reproducible growth of pinhole-free thin films with excellent thickness accuracy and conformality at low temperature. \n"
http://orkg.org/orkg/resource/R161497,In situ growth of an opal-like TiO2 electron transport layer by atomic layer deposition for perovskite solar cells,10.1039/d0se01558j,crossref,"<p>Here, we use an atomic layer deposition system and different sizes of polystyrene colloidal spheres to <italic>in situ</italic> prepare an opal-like TiO<sub>2</sub> mesoporous electron transport layer for perovskite solar cells.</p>","here, we use an atomic layer deposition system and different sizes of polystyrene colloidal spheres to in situ prepare an opal-like tio 2 mesoporous electron transport layer for perovskite solar cells."
http://orkg.org/orkg/resource/R161568,Current Technologies in Depolymerization Process and the Road Ahead,10.3390/polym13030449,crossref,"<jats:p>Although plastic is considered an indispensable commodity, plastic pollution is a major concern around the world due to its rapid accumulation rate, complexity, and lack of management. Some political policies, such as the Chinese import ban on plastic waste, force us to think about a long-term solution to eliminate plastic wastes. Converting waste plastics into liquid and gaseous fuels is considered a promising technique to eliminate the harm to the environment and decrease the dependence on fossil fuels, and recycling waste plastic by converting it into monomers is another effective solution to the plastic pollution problem. This paper presents the critical situation of plastic pollution, various methods of plastic depolymerization based on different kinds of polymers defined in the Society of the Plastics Industry (SPI) Resin Identification Coding System, and the opportunities and challenges in the future.</jats:p>","although plastic is considered an indispensable commodity, plastic pollution is a major concern around the world due to its rapid accumulation rate, complexity, and lack of management. some political policies, such as the chinese import ban on plastic waste, force us to think about a long-term solution to eliminate plastic wastes. converting waste plastics into liquid and gaseous fuels is considered a promising technique to eliminate the harm to the environment and decrease the dependence on fossil fuels, and recycling waste plastic by converting it into monomers is another effective solution to the plastic pollution problem. this paper presents the critical situation of plastic pollution, various methods of plastic depolymerization based on different kinds of polymers defined in the society of the plastics industry (spi) resin identification coding system, and the opportunities and challenges in the future."
http://orkg.org/orkg/resource/R161104,Our plastic age,10.1098/rstb.2009.0054,crossref,"<jats:p>\n            Within the last few decades, plastics have revolutionized our daily lives. Globally we use in excess of 260 million tonnes of plastic per annum, accounting for approximately 8 per cent of world oil production. In this Theme Issue of\n            <jats:italic>Philosophical Transactions of the Royal Society</jats:italic>\n            , we describe current and future trends in usage, together with the many benefits that plastics bring to society. At the same time, we examine the environmental consequences resulting from the accumulation of waste plastic, the effects of plastic debris on wildlife and concerns for human health that arise from the production, usage and disposal of plastics. Finally, we consider some possible solutions to these problems together with the research and policy priorities necessary for their implementation.\n          </jats:p>","\n within the last few decades, plastics have revolutionized our daily lives. globally we use in excess of 260 million tonnes of plastic per annum, accounting for approximately 8 per cent of world oil production. in this theme issue of\n philosophical transactions of the royal society \n , we describe current and future trends in usage, together with the many benefits that plastics bring to society. at the same time, we examine the environmental consequences resulting from the accumulation of waste plastic, the effects of plastic debris on wildlife and concerns for human health that arise from the production, usage and disposal of plastics. finally, we consider some possible solutions to these problems together with the research and policy priorities necessary for their implementation.\n"
http://orkg.org/orkg/resource/R161598,Hydrolysis and Solvolysis as Benign Routes for the End-of-Life Management of Thermoset Polymer Waste,10.1146/annurev-chembioeng-120919-012253,crossref,"<jats:p> The production of thermoset polymers is increasing globally owing to their advantageous properties, particularly when applied as composite materials. Though these materials are traditionally used in more durable, longer-lasting applications, ultimately, they become waste at the end of their usable lifetimes. Current recycling practices are not applicable to traditional thermoset waste, owing to their network structures and lack of processability. Recently, researchers have been developing thermoset polymers with the right functionalities to be chemically degraded under relatively benign conditions postuse, providing a route to future management of thermoset waste. This review presents thermosets containing hydrolytically or solvolytically cleavable bonds, such as esters and acetals. Hydrolysis and solvolysis mechanisms are discussed, and various factors that influence the degradation rates are examined. Degradable thermosets with impressive mechanical, thermal, and adhesion behavior are discussed, illustrating that the design of material end-of-life need not limit material performance. </jats:p>","the production of thermoset polymers is increasing globally owing to their advantageous properties, particularly when applied as composite materials. though these materials are traditionally used in more durable, longer-lasting applications, ultimately, they become waste at the end of their usable lifetimes. current recycling practices are not applicable to traditional thermoset waste, owing to their network structures and lack of processability. recently, researchers have been developing thermoset polymers with the right functionalities to be chemically degraded under relatively benign conditions postuse, providing a route to future management of thermoset waste. this review presents thermosets containing hydrolytically or solvolytically cleavable bonds, such as esters and acetals. hydrolysis and solvolysis mechanisms are discussed, and various factors that influence the degradation rates are examined. degradable thermosets with impressive mechanical, thermal, and adhesion behavior are discussed, illustrating that the design of material end-of-life need not limit material performance."
http://orkg.org/orkg/resource/R161466,Atomic Layer Deposition of TiO2 ultrathin films on 3D substrates for energy applications,10.1557/opl.2012.913,crossref,"<jats:title>ABSTRACT</jats:title><jats:p>In the present global environmental context, it becomes more and more critical to find efficient solutions to lower our energy consumption on one hand, and to produce energy from clean renewable sources on the other hand. Consequently, research efforts on materials for energy applications are intensifying.</jats:p><jats:p>The present work aims at developing optoelectrical components usable for both energy saving (light emitting diodes) and renewable energy production (solar cells) by fabricating p-n heterojunctions based on a single semiconductor, titanium dioxide. TiO<jats:sub>2</jats:sub> is indeed a very promising candidate: it is chemically and physically stable under irradiation, transparent to visible and near-infrared light (E<jats:sub>g</jats:sub>= 3 – 3.5 eV), presents photocatalytic activity, is non-toxic and low cost, which permits to envisage its large scale use.</jats:p><jats:p>In the present paper, the proposed architecture for both solar cells and LEDs is original as well as common for both applications: a three-dimensional architecture based on an anodic alumina nanoporous membrane which serves as nanomask for TiO<jats:sub>2</jats:sub> growth in order to enlarge the effective surface of the components. TiO<jats:sub>2</jats:sub>is synthesized by Atomic Layer Deposition (ALD), a technique particularly well adapted to the deposition of ultrathin films (from one monolayer to few tens of nanometers) on 3D porous substrates patterned with high aspect ratio nanopores.</jats:p><jats:p>In this work, the capacity of synthesizing 3D nanostructures is demonstrated. TiO<jats:sub>2</jats:sub>ultrathin films (10 to 100 nm) were grown by ALD on flat, micropatterned, microporous and nanoporous anodic alumina membranes (AAM) substrates. The films were highly conformal, as confirmed by SEM and TEM imaging. Both EDS and XPS analyses validated the dioxide film stoichiometry.</jats:p>","abstract in the present global environmental context, it becomes more and more critical to find efficient solutions to lower our energy consumption on one hand, and to produce energy from clean renewable sources on the other hand. consequently, research efforts on materials for energy applications are intensifying. the present work aims at developing optoelectrical components usable for both energy saving (light emitting diodes) and renewable energy production (solar cells) by fabricating p-n heterojunctions based on a single semiconductor, titanium dioxide. tio 2 is indeed a very promising candidate: it is chemically and physically stable under irradiation, transparent to visible and near-infrared light (e g = 3 – 3.5 ev), presents photocatalytic activity, is non-toxic and low cost, which permits to envisage its large scale use. in the present paper, the proposed architecture for both solar cells and leds is original as well as common for both applications: a three-dimensional architecture based on an anodic alumina nanoporous membrane which serves as nanomask for tio 2 growth in order to enlarge the effective surface of the components. tio 2 is synthesized by atomic layer deposition (ald), a technique particularly well adapted to the deposition of ultrathin films (from one monolayer to few tens of nanometers) on 3d porous substrates patterned with high aspect ratio nanopores. in this work, the capacity of synthesizing 3d nanostructures is demonstrated. tio 2 ultrathin films (10 to 100 nm) were grown by ald on flat, micropatterned, microporous and nanoporous anodic alumina membranes (aam) substrates. the films were highly conformal, as confirmed by sem and tem imaging. both eds and xps analyses validated the dioxide film stoichiometry."
http://orkg.org/orkg/resource/R162352,Evaluation of BioCreAtIvE assessment of task 2,10.1186/1471-2105-6-s1-s16,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Molecular Biology accumulated substantial amounts of data concerning functions of genes and proteins. Information relating to functional descriptions is generally extracted manually from textual data and stored in biological databases to build up annotations for large collections of gene products. Those annotation databases are crucial for the interpretation of large scale analysis approaches using bioinformatics or experimental techniques. Due to the growing accumulation of functional descriptions in biomedical literature the need for text mining tools to facilitate the extraction of such annotations is urgent. In order to make text mining tools useable in real world scenarios, for instance to assist database curators during annotation of protein function, comparisons and evaluations of different approaches on full text articles are needed.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>The Critical Assessment for Information Extraction in Biology (BioCreAtIvE) contest consists of a community wide competition aiming to evaluate different strategies for text mining tools, as applied to biomedical literature. We report on task two which addressed the automatic extraction and assignment of Gene Ontology (GO) annotations of human proteins, using full text articles. The predictions of task 2 are based on triplets of <jats:italic>protein – GO term – article passage</jats:italic>. The annotation-relevant text passages were returned by the participants and evaluated by expert curators of the GO annotation (GOA) team at the European Institute of Bioinformatics (EBI). Each participant could submit up to three results for each sub-task comprising task 2. In total more than 15,000 individual results were provided by the participants. The curators evaluated in addition to the annotation itself, whether the protein and the GO term were correctly predicted and traceable through the submitted text fragment.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusion</jats:title>\n            <jats:p>Concepts provided by GO are currently the most extended set of terms used for annotating gene products, thus they were explored to assess how effectively text mining tools are able to extract those annotations automatically. Although the obtained results are promising, they are still far from reaching the required performance demanded by real world applications. Among the principal difficulties encountered to address the proposed task, were the complex nature of the GO terms and protein names (the large range of variants which are used to express proteins and especially GO terms in free text), and the lack of a standard training set. A range of very different strategies were used to tackle this task. The dataset generated in line with the BioCreative challenge is publicly available and will allow new possibilities for training information extraction methods in the domain of molecular biology.</jats:p>\n          </jats:sec>","abstract \n \n background \n molecular biology accumulated substantial amounts of data concerning functions of genes and proteins. information relating to functional descriptions is generally extracted manually from textual data and stored in biological databases to build up annotations for large collections of gene products. those annotation databases are crucial for the interpretation of large scale analysis approaches using bioinformatics or experimental techniques. due to the growing accumulation of functional descriptions in biomedical literature the need for text mining tools to facilitate the extraction of such annotations is urgent. in order to make text mining tools useable in real world scenarios, for instance to assist database curators during annotation of protein function, comparisons and evaluations of different approaches on full text articles are needed. \n \n \n results \n the critical assessment for information extraction in biology (biocreative) contest consists of a community wide competition aiming to evaluate different strategies for text mining tools, as applied to biomedical literature. we report on task two which addressed the automatic extraction and assignment of gene ontology (go) annotations of human proteins, using full text articles. the predictions of task 2 are based on triplets of protein – go term – article passage . the annotation-relevant text passages were returned by the participants and evaluated by expert curators of the go annotation (goa) team at the european institute of bioinformatics (ebi). each participant could submit up to three results for each sub-task comprising task 2. in total more than 15,000 individual results were provided by the participants. the curators evaluated in addition to the annotation itself, whether the protein and the go term were correctly predicted and traceable through the submitted text fragment. \n \n \n conclusion \n concepts provided by go are currently the most extended set of terms used for annotating gene products, thus they were explored to assess how effectively text mining tools are able to extract those annotations automatically. although the obtained results are promising, they are still far from reaching the required performance demanded by real world applications. among the principal difficulties encountered to address the proposed task, were the complex nature of the go terms and protein names (the large range of variants which are used to express proteins and especially go terms in free text), and the lack of a standard training set. a range of very different strategies were used to tackle this task. the dataset generated in line with the biocreative challenge is publicly available and will allow new possibilities for training information extraction methods in the domain of molecular biology. \n"
http://orkg.org/orkg/resource/R160712,Sea–air CO&lt;sub&gt;2&lt;/sub&gt; fluxes in the Indian Ocean between 1990 and 2009,10.5194/bg-10-7035-2013,crossref,"<jats:p>Abstract. The Indian Ocean (44° S–30° N) plays an important role in the global carbon cycle, yet it remains one of the most poorly sampled ocean regions. Several approaches have been used to estimate net sea–air CO2 fluxes in this region: interpolated observations, ocean biogeochemical models, atmospheric and ocean inversions. As part of the RECCAP (REgional Carbon Cycle Assessment and Processes) project, we combine these different approaches to quantify and assess the magnitude and variability in Indian Ocean sea–air CO2 fluxes between 1990 and 2009. Using all of the models and inversions, the median annual mean sea–air CO2 uptake of −0.37 ± 0.06 PgC yr−1 is consistent with the −0.24 ± 0.12 PgC yr−1 calculated from observations. The fluxes from the southern Indian Ocean (18–44° S; −0.43 ± 0.07 PgC yr−1 are similar in magnitude to the annual uptake for the entire Indian Ocean. All models capture the observed pattern of fluxes in the Indian Ocean with the following exceptions: underestimation of upwelling fluxes in the northwestern region (off Oman and Somalia), overestimation in the northeastern region (Bay of Bengal) and underestimation of the CO2 sink in the subtropical convergence zone. These differences were mainly driven by lack of atmospheric CO2 data in atmospheric inversions, and poor simulation of monsoonal currents and freshwater discharge in ocean biogeochemical models. Overall, the models and inversions do capture the phase of the observed seasonality for the entire Indian Ocean but overestimate the magnitude. The predicted sea–air CO2 fluxes by ocean biogeochemical models (OBGMs) respond to seasonal variability with strong phase lags with reference to climatological CO2 flux, whereas the atmospheric inversions predicted an order of magnitude higher seasonal flux than OBGMs. The simulated interannual variability by the OBGMs is weaker than that found by atmospheric inversions. Prediction of such weak interannual variability in CO2 fluxes by atmospheric inversions was mainly caused by a lack of atmospheric data in the Indian Ocean. The OBGM models suggest a small strengthening of the sink over the period 1990–2009 of −0.01 PgC decade−1. This is inconsistent with the observations in the southwestern Indian Ocean that shows the growth rate of oceanic pCO2 was faster than the observed atmospheric CO2 growth, a finding attributed to the trend of the Southern Annular Mode (SAM) during the 1990s.\n                    </jats:p>","abstract. the indian ocean (44° s–30° n) plays an important role in the global carbon cycle, yet it remains one of the most poorly sampled ocean regions. several approaches have been used to estimate net sea–air co2 fluxes in this region: interpolated observations, ocean biogeochemical models, atmospheric and ocean inversions. as part of the reccap (regional carbon cycle assessment and processes) project, we combine these different approaches to quantify and assess the magnitude and variability in indian ocean sea–air co2 fluxes between 1990 and 2009. using all of the models and inversions, the median annual mean sea–air co2 uptake of −0.37 ± 0.06 pgc yr−1 is consistent with the −0.24 ± 0.12 pgc yr−1 calculated from observations. the fluxes from the southern indian ocean (18–44° s; −0.43 ± 0.07 pgc yr−1 are similar in magnitude to the annual uptake for the entire indian ocean. all models capture the observed pattern of fluxes in the indian ocean with the following exceptions: underestimation of upwelling fluxes in the northwestern region (off oman and somalia), overestimation in the northeastern region (bay of bengal) and underestimation of the co2 sink in the subtropical convergence zone. these differences were mainly driven by lack of atmospheric co2 data in atmospheric inversions, and poor simulation of monsoonal currents and freshwater discharge in ocean biogeochemical models. overall, the models and inversions do capture the phase of the observed seasonality for the entire indian ocean but overestimate the magnitude. the predicted sea–air co2 fluxes by ocean biogeochemical models (obgms) respond to seasonal variability with strong phase lags with reference to climatological co2 flux, whereas the atmospheric inversions predicted an order of magnitude higher seasonal flux than obgms. the simulated interannual variability by the obgms is weaker than that found by atmospheric inversions. prediction of such weak interannual variability in co2 fluxes by atmospheric inversions was mainly caused by a lack of atmospheric data in the indian ocean. the obgm models suggest a small strengthening of the sink over the period 1990–2009 of −0.01 pgc decade−1. this is inconsistent with the observations in the southwestern indian ocean that shows the growth rate of oceanic pco2 was faster than the observed atmospheric co2 growth, a finding attributed to the trend of the southern annular mode (sam) during the 1990s.\n"
http://orkg.org/orkg/resource/R160755,Chemolithoautotrophic production mediating the cycling of the greenhouses gases N&lt;sub&gt;2&lt;/sub&gt;O and CH&lt;sub&gt;4&lt;/sub&gt; in an upwelling ecosystem,10.5194/bgd-6-6205-2009,crossref,"<jats:p>Abstract. Coastal upwelling ecosystems with marked oxyclines (redoxclines) present high availability of electron donors that favour chemoautotrophy, leading in turn to high N2O and CH4 cycling associated with aerobic NH4+ (AAO) and CH4 oxidation (AMO). This is the case of the highly productive coastal upwelling area off Central Chile (36° S), where we evaluated the importance of total chemolithoautotrophic vs. photoautotrophic production, the specific contributions of AAO and AMO to chemosynthesis and their role in gas cycling. Chemoautotrophy (involving bacteria and archaea) was studied at a time-series station during monthly (2002–2009) and seasonal cruises (January 2008, September 2008, January 2009) and was assessed in terms of dark carbon assimilation (CA), N2O and CH4 cycling, and the natural C isotopic ratio of particulate organic carbon (δ13POC). Total Integrated dark CA fluctuated between 19.4 and 2.924 mg C m−2 d−1. It was higher during active upwelling and represented on average 27% of the integrated photoautotrophic production (from 135 to 7.626 mg C m−2d−1). At the oxycline, δ13POC averaged -22.209‰ this was significantly lighter compared to the surface (-19.674‰) and bottom layers (-20.716‰). This pattern, along with low NH4+ content and high accumulations of N2O, NO2- and NO3- within the oxycline indicates that chemolithoautotrophs and specifically AA oxydisers were active. Dark CA was reduced from 27 to 48% after addition of a specific AAO inhibitor (ATU) and from 24 to 76% with GC7, a specific archaea inhibitor, indicating that AAO and maybe AMO microbes (most of them archaea) were performing dark CA through oxidation of NH4+ and CH4. AAO produced N2O at rates from 8.88 to 43 nM d−1 and a fraction of it was effluxed into the atmosphere (up to 42.85 μmol m−2 d−1). AMO on the other hand consumed CH4 at rates between 0.41 and 26.8 nM d−1 therefore preventing its efflux to the atmosphere (up to 18.69 μmol m−2 d−1). These findings show that chemically driven chemoautotrophy (with NH4+ and CH4 acting as electron donors) could be more important than previously thought in upwelling ecosystems and open new questions concerning its future relevance.</jats:p>","abstract. coastal upwelling ecosystems with marked oxyclines (redoxclines) present high availability of electron donors that favour chemoautotrophy, leading in turn to high n2o and ch4 cycling associated with aerobic nh4+ (aao) and ch4 oxidation (amo). this is the case of the highly productive coastal upwelling area off central chile (36° s), where we evaluated the importance of total chemolithoautotrophic vs. photoautotrophic production, the specific contributions of aao and amo to chemosynthesis and their role in gas cycling. chemoautotrophy (involving bacteria and archaea) was studied at a time-series station during monthly (2002–2009) and seasonal cruises (january 2008, september 2008, january 2009) and was assessed in terms of dark carbon assimilation (ca), n2o and ch4 cycling, and the natural c isotopic ratio of particulate organic carbon (δ13poc). total integrated dark ca fluctuated between 19.4 and 2.924 mg c m−2 d−1. it was higher during active upwelling and represented on average 27% of the integrated photoautotrophic production (from 135 to 7.626 mg c m−2d−1). at the oxycline, δ13poc averaged -22.209‰ this was significantly lighter compared to the surface (-19.674‰) and bottom layers (-20.716‰). this pattern, along with low nh4+ content and high accumulations of n2o, no2- and no3- within the oxycline indicates that chemolithoautotrophs and specifically aa oxydisers were active. dark ca was reduced from 27 to 48% after addition of a specific aao inhibitor (atu) and from 24 to 76% with gc7, a specific archaea inhibitor, indicating that aao and maybe amo microbes (most of them archaea) were performing dark ca through oxidation of nh4+ and ch4. aao produced n2o at rates from 8.88 to 43 nm d−1 and a fraction of it was effluxed into the atmosphere (up to 42.85 μmol m−2 d−1). amo on the other hand consumed ch4 at rates between 0.41 and 26.8 nm d−1 therefore preventing its efflux to the atmosphere (up to 18.69 μmol m−2 d−1). these findings show that chemically driven chemoautotrophy (with nh4+ and ch4 acting as electron donors) could be more important than previously thought in upwelling ecosystems and open new questions concerning its future relevance."
http://orkg.org/orkg/resource/R160340,Integrating Virtual Reality and Digital Twin in Circular Economy Practices: A Laboratory Application Case,10.3390/su12062286,crossref,"<jats:p>The increasing awareness of customers toward climate change effects, the high demand instability affecting several industrial sectors, and the fast automation and digitalization of production systems are forcing companies to re-think their business strategies and models in view of both the Circular Economy (CE) and Industry 4.0 (I4.0) paradigms. Some studies have already assessed the relations between CE and I4.0, their benefits, and barriers. However, a practical demonstration of their potential impact in real contexts is still lacking. The aim of this paper is to present a laboratory application case showing how I4.0-based technologies can support CE practices by virtually testing a waste from electrical and electronic equipment (WEEE) disassembly plant configuration through a set of dedicated simulation tools. Our results highlight that service-oriented, event-driven processing and information models can support the integration of smart and digital solutions in current CE practices at the factory level.</jats:p>","the increasing awareness of customers toward climate change effects, the high demand instability affecting several industrial sectors, and the fast automation and digitalization of production systems are forcing companies to re-think their business strategies and models in view of both the circular economy (ce) and industry 4.0 (i4.0) paradigms. some studies have already assessed the relations between ce and i4.0, their benefits, and barriers. however, a practical demonstration of their potential impact in real contexts is still lacking. the aim of this paper is to present a laboratory application case showing how i4.0-based technologies can support ce practices by virtually testing a waste from electrical and electronic equipment (weee) disassembly plant configuration through a set of dedicated simulation tools. our results highlight that service-oriented, event-driven processing and information models can support the integration of smart and digital solutions in current ce practices at the factory level."
http://orkg.org/orkg/resource/R160363,Beyond the State of the Art of Electric Vehicles: A Fact-Based Paper of the Current and Prospective Electric Vehicle Technologies,10.3390/wevj12010020,crossref,"<jats:p>Today, there are many recent developments that focus on improving the electric vehicles and their components, particularly regarding advances in batteries, energy management systems, autonomous features and charging infrastructure. This plays an important role in developing next electric vehicle generations, and encourages more efficient and sustainable eco-system. This paper not only provides insights in the latest knowledge and developments of electric vehicles (EVs), but also the new promising and novel EV technologies based on scientific facts and figures—which could be from a technological point of view feasible by 2030. In this paper, potential design and modelling tools, such as digital twin with connected Internet-of-Things (IoT), are addressed. Furthermore, the potential technological challenges and research gaps in all EV aspects from hard-core battery material sciences, power electronics and powertrain engineering up to environmental assessments and market considerations are addressed. The paper is based on the knowledge of the 140+ FTE counting multidisciplinary research centre MOBI-VUB, that has a 40-year track record in the field of electric vehicles and e-mobility.</jats:p>","today, there are many recent developments that focus on improving the electric vehicles and their components, particularly regarding advances in batteries, energy management systems, autonomous features and charging infrastructure. this plays an important role in developing next electric vehicle generations, and encourages more efficient and sustainable eco-system. this paper not only provides insights in the latest knowledge and developments of electric vehicles (evs), but also the new promising and novel ev technologies based on scientific facts and figures—which could be from a technological point of view feasible by 2030. in this paper, potential design and modelling tools, such as digital twin with connected internet-of-things (iot), are addressed. furthermore, the potential technological challenges and research gaps in all ev aspects from hard-core battery material sciences, power electronics and powertrain engineering up to environmental assessments and market considerations are addressed. the paper is based on the knowledge of the 140+ fte counting multidisciplinary research centre mobi-vub, that has a 40-year track record in the field of electric vehicles and e-mobility."
http://orkg.org/orkg/resource/R160377,Time series behavior modeling with digital twin for Internet of Vehicles,10.1186/s13638-019-1589-8,crossref,"<jats:title>Abstract</jats:title><jats:p>Electric vehicle (EV) is considered eco-friendly with low carbon emission and maintenance costs. Given the current battery and charging technology, driving experience of EVs relies heavily on the availability and reachability of EV charging infrastructure. As the number of charging piles increases, carefully designed arrangement of resources and efficient utilization of the infrastructure is essential to the future development of EV industry. The mobility and distribution of EVs determine the charging demand and the load of power distribution grid. Then, dynamic traffic pattern of numerous interconnected EVs poses great impact on charging plans and charging infrastructure.</jats:p><jats:p>In this paper, we introduce the digital twin of a real-world EV by modeling the mobility based on a time series behaviors of EVs to evaluate the charging algorithm and pile arrangement policy. The introduced digital twin EV is a virtually simulated equivalence with same traffic behaviors and charging activities as the EV in real world. The behavior and route choice of EVs is dynamically simulated base on the time-varying driving operations, travel intent, and charging plan in a simulated large-scale charging scenario composed of concurrently moving EVs and correspondingly equipped charging piles. Different EV navigation algorithms and charging algorithms of Internet of Vehicle can be exactly evaluated in the dynamic simulation of the digital twins of the moving EVs and charging infrastructure. Then we analyze the collected data such as energy consumption, charging capacity, charging frequency, and waiting time in queue on both the EV side and the charging pile side to evaluate the charging efficiency. The simulation is used to study the relations between the scheduled charging operation of EVs and the deployment of piles. The proposed model helps evaluate and validate the design of the charging recommendation and the deployment plan regarding to the arrangement and distribution of charging piles.</jats:p>","abstract electric vehicle (ev) is considered eco-friendly with low carbon emission and maintenance costs. given the current battery and charging technology, driving experience of evs relies heavily on the availability and reachability of ev charging infrastructure. as the number of charging piles increases, carefully designed arrangement of resources and efficient utilization of the infrastructure is essential to the future development of ev industry. the mobility and distribution of evs determine the charging demand and the load of power distribution grid. then, dynamic traffic pattern of numerous interconnected evs poses great impact on charging plans and charging infrastructure. in this paper, we introduce the digital twin of a real-world ev by modeling the mobility based on a time series behaviors of evs to evaluate the charging algorithm and pile arrangement policy. the introduced digital twin ev is a virtually simulated equivalence with same traffic behaviors and charging activities as the ev in real world. the behavior and route choice of evs is dynamically simulated base on the time-varying driving operations, travel intent, and charging plan in a simulated large-scale charging scenario composed of concurrently moving evs and correspondingly equipped charging piles. different ev navigation algorithms and charging algorithms of internet of vehicle can be exactly evaluated in the dynamic simulation of the digital twins of the moving evs and charging infrastructure. then we analyze the collected data such as energy consumption, charging capacity, charging frequency, and waiting time in queue on both the ev side and the charging pile side to evaluate the charging efficiency. the simulation is used to study the relations between the scheduled charging operation of evs and the deployment of piles. the proposed model helps evaluate and validate the design of the charging recommendation and the deployment plan regarding to the arrangement and distribution of charging piles."
http://orkg.org/orkg/resource/R160415,Digital Twins: From Personalised Medicine to Precision Public Health,10.3390/jpm11080745,crossref,"<jats:p>A digital twin is a virtual model of a physical entity, with dynamic, bi-directional links between the physical entity and its corresponding twin in the digital domain. Digital twins are increasingly used today in different industry sectors. Applied to medicine and public health, digital twin technology can drive a much-needed radical transformation of traditional electronic health/medical records (focusing on individuals) and their aggregates (covering populations) to make them ready for a new era of precision (and accuracy) medicine and public health. Digital twins enable learning and discovering new knowledge, new hypothesis generation and testing, and in silico experiments and comparisons. They are poised to play a key role in formulating highly personalised treatments and interventions in the future. This paper provides an overview of the technology’s history and main concepts. A number of application examples of digital twins for personalised medicine, public health, and smart healthy cities are presented, followed by a brief discussion of the key technical and other challenges involved in such applications, including ethical issues that arise when digital twins are applied to model humans.</jats:p>","a digital twin is a virtual model of a physical entity, with dynamic, bi-directional links between the physical entity and its corresponding twin in the digital domain. digital twins are increasingly used today in different industry sectors. applied to medicine and public health, digital twin technology can drive a much-needed radical transformation of traditional electronic health/medical records (focusing on individuals) and their aggregates (covering populations) to make them ready for a new era of precision (and accuracy) medicine and public health. digital twins enable learning and discovering new knowledge, new hypothesis generation and testing, and in silico experiments and comparisons. they are poised to play a key role in formulating highly personalised treatments and interventions in the future. this paper provides an overview of the technology’s history and main concepts. a number of application examples of digital twins for personalised medicine, public health, and smart healthy cities are presented, followed by a brief discussion of the key technical and other challenges involved in such applications, including ethical issues that arise when digital twins are applied to model humans."
http://orkg.org/orkg/resource/R160474,Diverse rock types detected in the lunar South Pole–Aitken Basin by the Chang’E-4 lunar mission,10.1130/G47280.1,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>The South Pole–Aitken (SPA) basin, located between the South Pole and Aitken crater on the far side of the Moon, is the largest confirmed lunar impact structure. The pre-Nectarian SPA basin is a 2400 × 2050 km elliptical structure centered at 53°S, 191°E, which should have exposed lower crust and upper mantle due to the enormous excavation depth. Olivine, the dominant mineral in Earth’s mantle, has only been identified in small and localized exposures in the margins of the SPA basin, and the dominant mafic component is, instead, pyroxene. These mineralogical characteristics could be explained by the recent hypothesis that the lunar upper mantle is dominated by low-calcium pyroxene, not olivine. Here, we present observations from imaging and spectral data from China’s Chang’E-4 (CE-4) lunar mission in the first 4 synodic days, especially the first in situ visible/near-infrared spectrometer observations of an exposed boulder. We identified a variety of rock types, but not the recently reported olivine-rich materials in the landing region. The results are consistent with orbital observations. The obtained mineralogical information provides a better understanding of the nature and origin of SPA materials.</jats:p>","abstract \n the south pole–aitken (spa) basin, located between the south pole and aitken crater on the far side of the moon, is the largest confirmed lunar impact structure. the pre-nectarian spa basin is a 2400 × 2050 km elliptical structure centered at 53°s, 191°e, which should have exposed lower crust and upper mantle due to the enormous excavation depth. olivine, the dominant mineral in earth’s mantle, has only been identified in small and localized exposures in the margins of the spa basin, and the dominant mafic component is, instead, pyroxene. these mineralogical characteristics could be explained by the recent hypothesis that the lunar upper mantle is dominated by low-calcium pyroxene, not olivine. here, we present observations from imaging and spectral data from china’s chang’e-4 (ce-4) lunar mission in the first 4 synodic days, especially the first in situ visible/near-infrared spectrometer observations of an exposed boulder. we identified a variety of rock types, but not the recently reported olivine-rich materials in the landing region. the results are consistent with orbital observations. the obtained mineralogical information provides a better understanding of the nature and origin of spa materials."
http://orkg.org/orkg/resource/R160158,Nitrous oxide emissions from the Arabian Sea: A synthesis,10.5194/acp-1-61-2001,crossref,"<jats:p>Abstract. We computed high-resolution (1º latitude x\xa0 1º longitude) seasonal and annual nitrous oxide (N2O) concentration fields for the Arabian Sea surface layer using a database containing more than 2400 values measured between December 1977 and July 1997. N2O concentrations are highest during the southwest (SW) monsoon along the southern Indian continental shelf. Annual emissions range from 0.33 to 0.70 Tg N2O and are dominated by fluxes from coastal regions during the SW and northeast monsoons. Our revised estimate for the annual N2O flux from the Arabian Sea is much more tightly constrained than the previous consensus derived using averaged in-situ data from a smaller number of studies. However, the tendency to focus on measurements in locally restricted features in combination with insufficient seasonal data coverage leads to considerable uncertainties of the concentration fields and thus in the flux estimates, especially in the coastal zones of the northern and eastern Arabian Sea. The overall mean relative error of the annual N2O emissions from the Arabian Sea was estimated to be at least 65%.\n                    </jats:p>","abstract. we computed high-resolution (1º latitude x\xa0 1º longitude) seasonal and annual nitrous oxide (n2o) concentration fields for the arabian sea surface layer using a database containing more than 2400 values measured between december 1977 and july 1997. n2o concentrations are highest during the southwest (sw) monsoon along the southern indian continental shelf. annual emissions range from 0.33 to 0.70 tg n2o and are dominated by fluxes from coastal regions during the sw and northeast monsoons. our revised estimate for the annual n2o flux from the arabian sea is much more tightly constrained than the previous consensus derived using averaged in-situ data from a smaller number of studies. however, the tendency to focus on measurements in locally restricted features in combination with insufficient seasonal data coverage leads to considerable uncertainties of the concentration fields and thus in the flux estimates, especially in the coastal zones of the northern and eastern arabian sea. the overall mean relative error of the annual n2o emissions from the arabian sea was estimated to be at least 65%.\n"
http://orkg.org/orkg/resource/R160241,AUTOMATIC 3D BUILDINGS COMPACT RECONSTRUCTION FROM LIDAR POINT CLOUDS,10.5194/isprs-archives-xliii-b2-2020-473-2020,crossref,"<jats:p>Abstract. Point clouds generated from aerial LiDAR and photogrammetric techniques are great ways to obtain valuable spatial insights over large scale. However, their nature hinders the direct extraction and sharing of underlying information. The generation of consistent large-scale 3D city models from this real-world data is a major challenge. Specifically, the integration in workflows usable by decision-making scenarios demands that the data is structured, rich and exchangeable. CityGML permits new advances in terms of interoperable endeavour to use city models in a collaborative way. Efforts have led to render good-looking digital twins of cities but few of them take into account their potential use in finite elements simulations (wind, floods, heat radiation model, etc.). In this paper, we target the automatic reconstruction of consistent 3D city buildings highlighting closed solids, coherent surface junctions, perfect snapping of vertices, etc. It specifically investigates the topological and geometrical consistency of generated models from aerial LiDAR point cloud, formatted following the CityJSON specifications. These models are then usable to store relevant information and provides geometries usable within complex computations such as computational fluid dynamics, free of local inconsistencies (e.g. holes and unclosed solids).\n                    </jats:p>","abstract. point clouds generated from aerial lidar and photogrammetric techniques are great ways to obtain valuable spatial insights over large scale. however, their nature hinders the direct extraction and sharing of underlying information. the generation of consistent large-scale 3d city models from this real-world data is a major challenge. specifically, the integration in workflows usable by decision-making scenarios demands that the data is structured, rich and exchangeable. citygml permits new advances in terms of interoperable endeavour to use city models in a collaborative way. efforts have led to render good-looking digital twins of cities but few of them take into account their potential use in finite elements simulations (wind, floods, heat radiation model, etc.). in this paper, we target the automatic reconstruction of consistent 3d city buildings highlighting closed solids, coherent surface junctions, perfect snapping of vertices, etc. it specifically investigates the topological and geometrical consistency of generated models from aerial lidar point cloud, formatted following the cityjson specifications. these models are then usable to store relevant information and provides geometries usable within complex computations such as computational fluid dynamics, free of local inconsistencies (e.g. holes and unclosed solids).\n"
http://orkg.org/orkg/resource/R160402,BIM and IoT: A Synopsis from GIS Perspective,10.5194/isprsarchives-xl-2-w4-33-2015,crossref,"<jats:p>Abstract. Internet-of-Things (IoT) focuses on enabling communication between all devices, things that are existent in real life or that are virtual. Building Information Models (BIMs) and Building Information Modelling is a hype that has been the buzzword of the construction industry for last 15 years. BIMs emerged as a result of a push by the software companies, to tackle the problems of inefficient information exchange between different software and to enable true interoperability. In BIM approach most up-to-date an accurate models of a building are stored in shared central databases during the design and the construction of a project and at post-construction stages. GIS based city monitoring / city management applications require the fusion of information acquired from multiple resources, BIMs, City Models and Sensors. This paper focuses on providing a method for facilitating the GIS based fusion of information residing in digital building “Models” and information acquired from the city objects i.e. “Things”. Once this information fusion is accomplished, many fields ranging from Emergency Response, Urban Surveillance, Urban Monitoring to Smart Buildings will have potential benefits.\n                    </jats:p>","abstract. internet-of-things (iot) focuses on enabling communication between all devices, things that are existent in real life or that are virtual. building information models (bims) and building information modelling is a hype that has been the buzzword of the construction industry for last 15 years. bims emerged as a result of a push by the software companies, to tackle the problems of inefficient information exchange between different software and to enable true interoperability. in bim approach most up-to-date an accurate models of a building are stored in shared central databases during the design and the construction of a project and at post-construction stages. gis based city monitoring / city management applications require the fusion of information acquired from multiple resources, bims, city models and sensors. this paper focuses on providing a method for facilitating the gis based fusion of information residing in digital building “models” and information acquired from the city objects i.e. “things”. once this information fusion is accomplished, many fields ranging from emergency response, urban surveillance, urban monitoring to smart buildings will have potential benefits.\n"
http://orkg.org/orkg/resource/R160263,"Evaluation of Urban-Scale Building Energy-Use Models and Tools—Application for the City of Fribourg, Switzerland",10.3390/su13041595,crossref,"<jats:p>Building energy-use models and tools can simulate and represent the distribution of energy consumption of buildings located in an urban area. The aim of these models is to simulate the energy performance of buildings at multiple temporal and spatial scales, taking into account both the building shape and the surrounding urban context. This paper investigates existing models by simulating the hourly space heating consumption of residential buildings in an urban environment. Existing bottom-up urban-energy models were applied to the city of Fribourg in order to evaluate the accuracy and flexibility of energy simulations. Two common energy-use models—a machine learning model and a GIS-based engineering model—were compared and evaluated against anonymized monitoring data. The study shows that the simulations were quite precise with an annual mean absolute percentage error of 12.8 and 19.3% for the machine learning and the GIS-based engineering model, respectively, on residential buildings built in different periods of construction. Moreover, a sensitivity analysis using the Morris method was carried out on the GIS-based engineering model in order to assess the impact of input variables on space heating consumption and to identify possible optimization opportunities of the existing model.</jats:p>","building energy-use models and tools can simulate and represent the distribution of energy consumption of buildings located in an urban area. the aim of these models is to simulate the energy performance of buildings at multiple temporal and spatial scales, taking into account both the building shape and the surrounding urban context. this paper investigates existing models by simulating the hourly space heating consumption of residential buildings in an urban environment. existing bottom-up urban-energy models were applied to the city of fribourg in order to evaluate the accuracy and flexibility of energy simulations. two common energy-use models—a machine learning model and a gis-based engineering model—were compared and evaluated against anonymized monitoring data. the study shows that the simulations were quite precise with an annual mean absolute percentage error of 12.8 and 19.3% for the machine learning and the gis-based engineering model, respectively, on residential buildings built in different periods of construction. moreover, a sensitivity analysis using the morris method was carried out on the gis-based engineering model in order to assess the impact of input variables on space heating consumption and to identify possible optimization opportunities of the existing model."
http://orkg.org/orkg/resource/R160311,Digital Twin and CyberGIS for Improving Connectivity and Measuring the Impact of Infrastructure Construction Planning in Smart Cities,10.3390/ijgi9040240,crossref,"<jats:p>Smart technologies are advancing, and smart cities can be made smarter by increasing the connectivity and interactions of humans, the environment, and smart devices. This paper discusses selective technologies that can potentially contribute to developing an intelligent environment and smarter cities. While the connectivity and efficiency of smart cities is important, the analysis of the impact of construction development and large projects in the city is crucial to decision and policy makers, before the project is approved. This raises the question of assessing the impact of a new infrastructure project on the community prior to its commencement—what type of technologies can potentially be used for creating a virtual representation of the city? How can a smart city be improved by utilizing these technologies? There are a wide range of technologies and applications available but understanding their function, interoperability, and compatibility with the community requires more discussion around system designs and architecture. These questions can be the basis of developing an agenda for further investigations. In particular, the need for advanced tools such as mobile scanners, Geospatial Artificial Intelligence, Unmanned Aerial Vehicles, Geospatial Augmented Reality apps, Light Detection, and Ranging in smart cities is discussed. In line with smart city technology development, this Special Issue includes eight accepted articles covering trending topics, which are briefly reviewed.</jats:p>","smart technologies are advancing, and smart cities can be made smarter by increasing the connectivity and interactions of humans, the environment, and smart devices. this paper discusses selective technologies that can potentially contribute to developing an intelligent environment and smarter cities. while the connectivity and efficiency of smart cities is important, the analysis of the impact of construction development and large projects in the city is crucial to decision and policy makers, before the project is approved. this raises the question of assessing the impact of a new infrastructure project on the community prior to its commencement—what type of technologies can potentially be used for creating a virtual representation of the city? how can a smart city be improved by utilizing these technologies? there are a wide range of technologies and applications available but understanding their function, interoperability, and compatibility with the community requires more discussion around system designs and architecture. these questions can be the basis of developing an agenda for further investigations. in particular, the need for advanced tools such as mobile scanners, geospatial artificial intelligence, unmanned aerial vehicles, geospatial augmented reality apps, light detection, and ranging in smart cities is discussed. in line with smart city technology development, this special issue includes eight accepted articles covering trending topics, which are briefly reviewed."
http://orkg.org/orkg/resource/R160301,Digital Twin Aided Vulnerability Assessment and Risk-Based Maintenance Planning of Bridge Infrastructures Exposed to Extreme Conditions,10.3390/su13042051,crossref,"<jats:p>Over the past centuries, millions of bridge infrastructures have been constructed globally. Many of those bridges are ageing and exhibit significant potential risks. Frequent risk-based inspection and maintenance management of highway bridges is particularly essential for public safety. At present, most bridges rely on manual inspection methods for management. The efficiency is extremely low, causing the risk of bridge deterioration and defects to increase day by day, reducing the load-bearing capacity of bridges, and restricting the normal and safe use of them. At present, the applications of digital twins in the construction industry have gained significant momentum and the industry has gradually entered the information age. In order to obtain and share relevant information, engineers and decision makers have adopted digital twins over the entire life cycle of a project, but their applications are still limited to data sharing and visualization. This study has further demonstrated the unprecedented applications of digital twins to sustainability and vulnerability assessments, which can enable the next generation risk-based inspection and maintenance framework. This study adopts the data obtained from a constructor of Zhongcheng Village Bridge in Zhejiang Province, China as a case study. The applications of digital twins to bridge model establishment, information collection and sharing, data processing, inspection and maintenance planning have been highlighted. Then, the integration of “digital twins (or Building Information Modelling, BIM) + bridge risk inspection model” has been established, which will become a more effective information platform for all stakeholders to mitigate risks and uncertainties of exposure to extreme weather conditions over the entire life cycle.</jats:p>","over the past centuries, millions of bridge infrastructures have been constructed globally. many of those bridges are ageing and exhibit significant potential risks. frequent risk-based inspection and maintenance management of highway bridges is particularly essential for public safety. at present, most bridges rely on manual inspection methods for management. the efficiency is extremely low, causing the risk of bridge deterioration and defects to increase day by day, reducing the load-bearing capacity of bridges, and restricting the normal and safe use of them. at present, the applications of digital twins in the construction industry have gained significant momentum and the industry has gradually entered the information age. in order to obtain and share relevant information, engineers and decision makers have adopted digital twins over the entire life cycle of a project, but their applications are still limited to data sharing and visualization. this study has further demonstrated the unprecedented applications of digital twins to sustainability and vulnerability assessments, which can enable the next generation risk-based inspection and maintenance framework. this study adopts the data obtained from a constructor of zhongcheng village bridge in zhejiang province, china as a case study. the applications of digital twins to bridge model establishment, information collection and sharing, data processing, inspection and maintenance planning have been highlighted. then, the integration of “digital twins (or building information modelling, bim) + bridge risk inspection model” has been established, which will become a more effective information platform for all stakeholders to mitigate risks and uncertainties of exposure to extreme weather conditions over the entire life cycle."
http://orkg.org/orkg/resource/R160337,Digital Twin in Circular Economy: Remanufacturing in Construction,10.1088/1755-1315/588/3/032014,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Global warming attracts increasing public attention. However, in the past few decades, the contribution of construction to greenhouse gas emissions is around 40% of total emissions. The promotion of construction waste remanufacturing faces challenges. The application of digital twins in the remanufacturing of construction waste contributes to the tracking, recycling and management of construction waste. This article reviews the current research on construction waste remanufacturing and the application of Digital Twin in construction and remanufacturing, aiming at finding the current challenge of construction waste remanufacturing and the opportunity of Digital Twin to solve it. Then, the Digital Twin platform concept for construction waste remanufacturing is provided as a solution for the current challenges. Theoretically, this paper points out the shortcomings of the current research in construction waste remanufacturing based on literature review. Meanwhile, this article proposes the application of Digital Twin in construction waste remanufacturing, which expands the research scope of circular economy in construction. In fact, this research has driven the Digital Twin application in more industries. Besides, this research proposes a concept of potential solutions for the current challenges of construction waste in circular economy.</jats:p>","abstract \n global warming attracts increasing public attention. however, in the past few decades, the contribution of construction to greenhouse gas emissions is around 40% of total emissions. the promotion of construction waste remanufacturing faces challenges. the application of digital twins in the remanufacturing of construction waste contributes to the tracking, recycling and management of construction waste. this article reviews the current research on construction waste remanufacturing and the application of digital twin in construction and remanufacturing, aiming at finding the current challenge of construction waste remanufacturing and the opportunity of digital twin to solve it. then, the digital twin platform concept for construction waste remanufacturing is provided as a solution for the current challenges. theoretically, this paper points out the shortcomings of the current research in construction waste remanufacturing based on literature review. meanwhile, this article proposes the application of digital twin in construction waste remanufacturing, which expands the research scope of circular economy in construction. in fact, this research has driven the digital twin application in more industries. besides, this research proposes a concept of potential solutions for the current challenges of construction waste in circular economy."
http://orkg.org/orkg/resource/R160349,Leveraging Digital Twin for Sustainability Assessment of an Educational Building,10.3390/su13020480,crossref,"<jats:p>The EU Green Deal, beginning in 2019, promoted a roadmap for operating the transition to a sustainable EU economy by turning climate issues and environmental challenges into opportunities in all policy areas and making the transition fair and inclusive for all. Focusing on the built environment, the voluntary adoption of rating systems for sustainability assessment is growing, with an increasing market value, and is perceived as a social responsibility both by public administration and by private companies. This paper proposes a framework for shifting from a static sustainability assessment to a digital twin (DT)-based and Internet of Things (IoT)-enabled dynamic approach. This new approach allows for a real-time evaluation and control of a wide range of sustainability criteria with a user-centered point of view. A pilot building, namely, the eLUX lab cognitive building in the University of Brescia, was used to test the framework with some sample applications. The educational building accommodates the daily activities of the engineering students by constantly interacting with the sensorized asset monitoring indoor comfort and air quality conditions as well as the energy behavior of the building in order to optimize the trade-off with renewable energy production. The framework is the cornerstone of a methodology exploiting the digital twin approach to support the decision processes related to sustainability through the whole building’s life cycle.</jats:p>","the eu green deal, beginning in 2019, promoted a roadmap for operating the transition to a sustainable eu economy by turning climate issues and environmental challenges into opportunities in all policy areas and making the transition fair and inclusive for all. focusing on the built environment, the voluntary adoption of rating systems for sustainability assessment is growing, with an increasing market value, and is perceived as a social responsibility both by public administration and by private companies. this paper proposes a framework for shifting from a static sustainability assessment to a digital twin (dt)-based and internet of things (iot)-enabled dynamic approach. this new approach allows for a real-time evaluation and control of a wide range of sustainability criteria with a user-centered point of view. a pilot building, namely, the elux lab cognitive building in the university of brescia, was used to test the framework with some sample applications. the educational building accommodates the daily activities of the engineering students by constantly interacting with the sensorized asset monitoring indoor comfort and air quality conditions as well as the energy behavior of the building in order to optimize the trade-off with renewable energy production. the framework is the cornerstone of a methodology exploiting the digital twin approach to support the decision processes related to sustainability through the whole building’s life cycle."
http://orkg.org/orkg/resource/R160323,Circular cities,10.1177/0042098018806133,crossref,"<jats:p> A circular approach to the way in which we manage the resources consumed and produced in cities – materials, energy, water and land – will significantly reduce the consumption of finite resources globally. It will also help to address urban problems including resource security, waste disposal, greenhouse gas emissions, pollution, heating, drought and flooding. Taking a circular approach can also tackle many other socio-economic problems afflicting cities, for example, providing access to affordable accommodation, expanding and diversifying the economic base, building more engaged and collaborative communities in cities. Thus it has great potential to improve our urban living environments. To date, the industrial ecologists and economists have tended to dominate the circularity debate, focusing on closed-loop industrial systems and circular economy (circular businesses and systems of provision). In this paper I investigate why the current state-of-the-art conceptualisation for circular economy (RESOLVE) is inadequate when applied to a city. Through this critique and a broader review of the literature I identify the principles and components which are lacking from the circular economy (CE) conceptualisation when applied to a city. I then use this to develop my own definition and conceptualisation of a circular approach to urban resource management. </jats:p>","a circular approach to the way in which we manage the resources consumed and produced in cities – materials, energy, water and land – will significantly reduce the consumption of finite resources globally. it will also help to address urban problems including resource security, waste disposal, greenhouse gas emissions, pollution, heating, drought and flooding. taking a circular approach can also tackle many other socio-economic problems afflicting cities, for example, providing access to affordable accommodation, expanding and diversifying the economic base, building more engaged and collaborative communities in cities. thus it has great potential to improve our urban living environments. to date, the industrial ecologists and economists have tended to dominate the circularity debate, focusing on closed-loop industrial systems and circular economy (circular businesses and systems of provision). in this paper i investigate why the current state-of-the-art conceptualisation for circular economy (resolve) is inadequate when applied to a city. through this critique and a broader review of the literature i identify the principles and components which are lacking from the circular economy (ce) conceptualisation when applied to a city. i then use this to develop my own definition and conceptualisation of a circular approach to urban resource management."
http://orkg.org/orkg/resource/R160015,Demonstration of SiC Pressure Sensors at 750 °C,10.4071/hitec-ta21,crossref,"<jats:p>We report the first demonstration of MEMS-based 4H-SiC piezoresistive pressure sensors tested at 750 °C and in the process confirmed the existence of strain sensitivity recovery with increasing temperature above 400 °C, eventually achieving near or up to 100 % of the room temperature values at 750 °C. This strain sensitivity recovery phenomenon in 4H-SiC is uncharacteristic of the well-known monotonic decrease in strain sensitivity with increasing temperature in silicon piezoresistors. For the three sensors tested, the room temperature full-scale output (FSO) at 200 psig ranged between 29 and 36 mV. Although the FSO at 400 °C dropped by about 60 %, full recovery was achieved at 750 °C. This result will allow the operation of SiC pressure sensors at higher temperatures, thereby permitting deeper insertion into the engine combustion chamber to improve the accurate quantification of combustor dynamics.</jats:p>","we report the first demonstration of mems-based 4h-sic piezoresistive pressure sensors tested at 750 °c and in the process confirmed the existence of strain sensitivity recovery with increasing temperature above 400 °c, eventually achieving near or up to 100 % of the room temperature values at 750 °c. this strain sensitivity recovery phenomenon in 4h-sic is uncharacteristic of the well-known monotonic decrease in strain sensitivity with increasing temperature in silicon piezoresistors. for the three sensors tested, the room temperature full-scale output (fso) at 200 psig ranged between 29 and 36 mv. although the fso at 400 °c dropped by about 60 %, full recovery was achieved at 750 °c. this result will allow the operation of sic pressure sensors at higher temperatures, thereby permitting deeper insertion into the engine combustion chamber to improve the accurate quantification of combustor dynamics."
http://orkg.org/orkg/resource/R160019,Design of SiC-Doped Piezoresistive Pressure Sensor for High-Temperature Applications,10.3390/s21186066,crossref,"<jats:p>Within these studies the piezoresistive effect was analyzed for 6H-SiC and 4H-SiC material doped with various elements: N, B, and Sc. Bulk SiC crystals with a specific concentration of dopants were fabricated by the Physical Vapor Transport (PVT) technique. For such materials, the structures and properties were analyzed using X-ray diffraction, SEM, and Hall measurements. The samples in the form of a beam were also prepared and strained (bent) to measure the resistance change (Gauge Factor). Based on the results obtained for bulk materials, piezoresistive thin films on 6H-SiC and 4H-SiC substrate were fabricated by Chemical Vapor Deposition (CVD). Such materials were shaped by Focus Ion Beam (FIB) into pressure sensors with a specific geometry. The characteristics of the sensors made from different materials under a range of pressures and temperatures were obtained and are presented herewith.</jats:p>","within these studies the piezoresistive effect was analyzed for 6h-sic and 4h-sic material doped with various elements: n, b, and sc. bulk sic crystals with a specific concentration of dopants were fabricated by the physical vapor transport (pvt) technique. for such materials, the structures and properties were analyzed using x-ray diffraction, sem, and hall measurements. the samples in the form of a beam were also prepared and strained (bent) to measure the resistance change (gauge factor). based on the results obtained for bulk materials, piezoresistive thin films on 6h-sic and 4h-sic substrate were fabricated by chemical vapor deposition (cvd). such materials were shaped by focus ion beam (fib) into pressure sensors with a specific geometry. the characteristics of the sensors made from different materials under a range of pressures and temperatures were obtained and are presented herewith."
http://orkg.org/orkg/resource/R160032,Fabrication of SiC Sealing Cavity Structure for All-SiC Piezoresistive Pressure Sensor Applications,10.3390/ma14010128,crossref,"<jats:p>High hardness and corrosion resistance of SiC (silicon carbide) bulk materials have always been a difficult problem in the processing of an all-SiC piezoresistive pressure sensor. In this work, we demonstrated a SiC sealing cavity structure utilizing SiC shallow plasma-etched process (≤20 μm) and SiC–SiC room temperature bonding technology. The SiC bonding interface was closely connected, and its average tensile strength could reach 6.71 MPa. In addition, through a rapid thermal annealing (RTA) experiment of 1 min and 10 mins in N2 atmosphere of 1000 °C, it was found that Si, C and O elements at the bonding interface were diffused, while the width of the intermediate interface layer was narrowed, and the tensile strength could remain stable. This SiC sealing cavity structure has important application value in the realization of an all-SiC piezoresistive pressure sensor.</jats:p>","high hardness and corrosion resistance of sic (silicon carbide) bulk materials have always been a difficult problem in the processing of an all-sic piezoresistive pressure sensor. in this work, we demonstrated a sic sealing cavity structure utilizing sic shallow plasma-etched process (≤20 μm) and sic–sic room temperature bonding technology. the sic bonding interface was closely connected, and its average tensile strength could reach 6.71 mpa. in addition, through a rapid thermal annealing (rta) experiment of 1 min and 10 mins in n2 atmosphere of 1000 °c, it was found that si, c and o elements at the bonding interface were diffused, while the width of the intermediate interface layer was narrowed, and the tensile strength could remain stable. this sic sealing cavity structure has important application value in the realization of an all-sic piezoresistive pressure sensor."
http://orkg.org/orkg/resource/R160037,Design and Fabrication of Bulk Micromachined 4H-SiC Piezoresistive Pressure Chips Based on Femtosecond Laser Technology,10.3390/mi12010056,crossref,"<jats:p>Silicon carbide (SiC) has promising potential for pressure sensing in a high temperature and harsh environment due to its outstanding material properties. In this work, a 4H-SiC piezoresistive pressure chip fabricated based on femtosecond laser technology was proposed. A 1030 nm, 200 fs Yb: KGW laser with laser average powers of 1.5, 3 and 5 W was used to drill blind micro holes for achieving circular sensor diaphragms. An accurate per lap feed of 16.2 μm was obtained under laser average power of 1.5 W. After serialized laser processing, the machining depth error of no more than 2% and the surface roughness as low as 153 nm of the blind hole were measured. The homoepitaxial piezoresistors with a doping concentration of 1019 cm−3 were connected by a closed-loop Wheatstone bridge after a rapid thermal annealing process, with a specific contact resistivity of 9.7 × 10−5 Ω cm2. Our research paved the way for the integration of femtosecond laser micromachining and SiC pressure sensor chips manufacturing.</jats:p>","silicon carbide (sic) has promising potential for pressure sensing in a high temperature and harsh environment due to its outstanding material properties. in this work, a 4h-sic piezoresistive pressure chip fabricated based on femtosecond laser technology was proposed. a 1030 nm, 200 fs yb: kgw laser with laser average powers of 1.5, 3 and 5 w was used to drill blind micro holes for achieving circular sensor diaphragms. an accurate per lap feed of 16.2 μm was obtained under laser average power of 1.5 w. after serialized laser processing, the machining depth error of no more than 2% and the surface roughness as low as 153 nm of the blind hole were measured. the homoepitaxial piezoresistors with a doping concentration of 1019 cm−3 were connected by a closed-loop wheatstone bridge after a rapid thermal annealing process, with a specific contact resistivity of 9.7 × 10−5 ω cm2. our research paved the way for the integration of femtosecond laser micromachining and sic pressure sensor chips manufacturing."
http://orkg.org/orkg/resource/R159450,"Urban Digital Twins for Smart Cities and Citizens: The Case Study of Herrenberg, Germany",10.3390/su12062307,crossref,"<jats:p>Cities are complex systems connected to economic, ecological, and demographic conditions and change. They are also characterized by diverging perceptions and interests of citizens and stakeholders. Thus, in the arena of urban planning, we are in need of approaches that are able to cope not only with urban complexity but also allow for participatory and collaborative processes to empower citizens. This to create democratic cities. Connected to the field of smart cities and citizens, we present in this paper, the prototype of an urban digital twin for the 30,000-people town of Herrenberg in Germany. Urban digital twins are sophisticated data models allowing for collaborative processes. The herein presented prototype comprises (1) a 3D model of the built environment, (2) a street network model using the theory and method of space syntax, (3) an urban mobility simulation, (4) a wind flow simulation, and (5) a number of empirical quantitative and qualitative data using volunteered geographic information (VGI). In addition, the urban digital twin was implemented in a visualization platform for virtual reality and was presented to the general public during diverse public participatory processes, as well as in the framework of the “Morgenstadt Werkstatt” (Tomorrow’s Cities Workshop). The results of a survey indicated that this method and technology could significantly aid in participatory and collaborative processes. Further understanding of how urban digital twins support urban planners, urban designers, and the general public as a collaboration and communication tool and for decision support allows us to be more intentional when creating smart cities and sustainable cities with the help of digital twins. We conclude the paper with a discussion of the presented results and further research directions.</jats:p>","cities are complex systems connected to economic, ecological, and demographic conditions and change. they are also characterized by diverging perceptions and interests of citizens and stakeholders. thus, in the arena of urban planning, we are in need of approaches that are able to cope not only with urban complexity but also allow for participatory and collaborative processes to empower citizens. this to create democratic cities. connected to the field of smart cities and citizens, we present in this paper, the prototype of an urban digital twin for the 30,000-people town of herrenberg in germany. urban digital twins are sophisticated data models allowing for collaborative processes. the herein presented prototype comprises (1) a 3d model of the built environment, (2) a street network model using the theory and method of space syntax, (3) an urban mobility simulation, (4) a wind flow simulation, and (5) a number of empirical quantitative and qualitative data using volunteered geographic information (vgi). in addition, the urban digital twin was implemented in a visualization platform for virtual reality and was presented to the general public during diverse public participatory processes, as well as in the framework of the “morgenstadt werkstatt” (tomorrow’s cities workshop). the results of a survey indicated that this method and technology could significantly aid in participatory and collaborative processes. further understanding of how urban digital twins support urban planners, urban designers, and the general public as a collaboration and communication tool and for decision support allows us to be more intentional when creating smart cities and sustainable cities with the help of digital twins. we conclude the paper with a discussion of the presented results and further research directions."
http://orkg.org/orkg/resource/R159459,RESEARCH ON CONSTRUCTION OF SPATIO-TEMPORAL DATA VISUALIZATION PLATFORM FOR GIS AND BIM FUSION,10.5194/isprs-archives-xlii-3-w10-555-2020,crossref,"<jats:p>Abstract. The visualization model of GIS and BIM fusion can provide data bearing platform and main technical support for future urban operation centers, digital twin cities, and smart cities. Based on the analysis of the features and advantages of GIS and BIM Fusion, this paper proposes a construction method of the spatio-temporal data visualization platform for GIS and BIM Fusion. It expounds and analyzes the overall architecture design of platform, multi-dimensional and multi-spatial scales visualization, space analysis for GIS and BIM fusion, and platform applications and so on. The urban virtual simulation spatio-temporal data platform project of Teda New District in Tianjin has verified and demonstrated that the effect of application is good. This provides a feasible solution for the construction of spatio-temporal Data Visualization Platform.\n                    </jats:p>","abstract. the visualization model of gis and bim fusion can provide data bearing platform and main technical support for future urban operation centers, digital twin cities, and smart cities. based on the analysis of the features and advantages of gis and bim fusion, this paper proposes a construction method of the spatio-temporal data visualization platform for gis and bim fusion. it expounds and analyzes the overall architecture design of platform, multi-dimensional and multi-spatial scales visualization, space analysis for gis and bim fusion, and platform applications and so on. the urban virtual simulation spatio-temporal data platform project of teda new district in tianjin has verified and demonstrated that the effect of application is good. this provides a feasible solution for the construction of spatio-temporal data visualization platform.\n"
http://orkg.org/orkg/resource/R161707,Just Add Functions: A Neural-Symbolic Language Model,10.1609/aaai.v34i05.6264,crossref,"""<jats:p>Neural network language models (NNLMs) have achieved ever-improving accuracy due to more sophisticated architectures and increasing amounts of training data. However, the inductive bias of these models (formed by the distributional hypothesis of language), while ideally suited to modeling most running text, results in key limitations for today's models. In particular, the models often struggle to learn certain spatial, temporal, or quantitative relationships, which are commonplace in text and are second-nature for human readers. Yet, in many cases, these relationships can be encoded with simple mathematical or logical expressions. How can we augment today's neural models with such encodings?In this paper, we propose a general methodology to enhance the inductive bias of NNLMs by incorporating simple functions into a neural architecture to form a hierarchical neural-symbolic language model (NSLM). These functions explicitly encode symbolic deterministic relationships to form probability distributions over words. We explore the effectiveness of this approach on numbers and geographic locations, and show that NSLMs significantly reduce perplexity in small-corpus language modeling, and that the performance improvement persists for rare tokens even on much larger corpora. The approach is simple and general, and we discuss how it can be applied to other word classes beyond numbers and geography.</jats:p>""",""" neural network language models (nnlms) have achieved ever-improving accuracy due to more sophisticated architectures and increasing amounts of training data. however, the inductive bias of these models (formed by the distributional hypothesis of language), while ideally suited to modeling most running text, results in key limitations for today's models. in particular, the models often struggle to learn certain spatial, temporal, or quantitative relationships, which are commonplace in text and are second-nature for human readers. yet, in many cases, these relationships can be encoded with simple mathematical or logical expressions. how can we augment today's neural models with such encodings?in this paper, we propose a general methodology to enhance the inductive bias of nnlms by incorporating simple functions into a neural architecture to form a hierarchical neural-symbolic language model (nslm). these functions explicitly encode symbolic deterministic relationships to form probability distributions over words. we explore the effectiveness of this approach on numbers and geographic locations, and show that nslms significantly reduce perplexity in small-corpus language modeling, and that the performance improvement persists for rare tokens even on much larger corpora. the approach is simple and general, and we discuss how it can be applied to other word classes beyond numbers and geography. """
http://orkg.org/orkg/resource/R159456,Geospatial Artificial Intelligence: Potentials of Machine Learning for 3D Point Clouds and Geospatial Digital Twins,10.1007/s41064-020-00102-3,crossref,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence (AI) is changing fundamentally the way how IT solutions are implemented and operated across all application domains, including the geospatial domain. This contribution outlines AI-based techniques for 3D point clouds and geospatial digital twins as generic components of geospatial AI. First, we briefly reflect on the term “AI” and outline technology developments needed to apply AI to IT solutions, seen from a software engineering perspective. Next, we characterize 3D point clouds as key category of geodata and their role for creating the basis for geospatial digital twins; we explain the feasibility of machine learning (ML) and deep learning (DL) approaches for 3D point clouds. In particular, we argue that 3D point clouds can be seen as a corpus with similar properties as natural language corpora and formulate a “Naturalness Hypothesis” for 3D point clouds. In the main part, we introduce a workflow for interpreting 3D point clouds based on ML/DL approaches that derive domain-specific and application-specific semantics for 3D point clouds without having to create explicit spatial 3D models or explicit rule sets. Finally, examples are shown how ML/DL enables us to efficiently build and maintain base data for geospatial digital twins such as virtual 3D city models, indoor models, or building information models.</jats:p>","abstract artificial intelligence (ai) is changing fundamentally the way how it solutions are implemented and operated across all application domains, including the geospatial domain. this contribution outlines ai-based techniques for 3d point clouds and geospatial digital twins as generic components of geospatial ai. first, we briefly reflect on the term “ai” and outline technology developments needed to apply ai to it solutions, seen from a software engineering perspective. next, we characterize 3d point clouds as key category of geodata and their role for creating the basis for geospatial digital twins; we explain the feasibility of machine learning (ml) and deep learning (dl) approaches for 3d point clouds. in particular, we argue that 3d point clouds can be seen as a corpus with similar properties as natural language corpora and formulate a “naturalness hypothesis” for 3d point clouds. in the main part, we introduce a workflow for interpreting 3d point clouds based on ml/dl approaches that derive domain-specific and application-specific semantics for 3d point clouds without having to create explicit spatial 3d models or explicit rule sets. finally, examples are shown how ml/dl enables us to efficiently build and maintain base data for geospatial digital twins such as virtual 3d city models, indoor models, or building information models."
http://orkg.org/orkg/resource/R159481,The Digital Twin of the City of Zurich for Urban Planning,10.1007/s41064-020-00092-2,crossref,"<jats:title>Abstract</jats:title><jats:p>Population growth will confront the City of Zurich with a variety of challenges in the coming years, as the increase in the number of inhabitants and jobs will lead to densification and competing land uses. The tasks for the city administration have become more complex, whereas tools and methods are often based on traditional, static approaches while involving a limited number of citizens and stakeholders in relevant decisions. The digital transformation of more and more\xa0pieces of the planning and decision-making process will make both increasingly more illustrative, easier to understand and more comprehensible. An important data basis for these processes is the digital twin of the City of Zurich. 3D spatial data and their models transform themes of the city, such as buildings, bridges, vegetation, etc., to the digital world, are being updated when required, and create advantages in digital space. These benefits need to be highlighted and published. An important step in public awareness is the release of 3D spatial data under Open Government Data. This allows the development of applications, the promotion of understanding, and the simplification of the creation of different collaborative platforms. By\xa0visualization and analysis of digital prototypes and the demonstration of interactions with the built environment, scenarios can be digitally developed and discussed in decision-making bodies. Questions about the urban climate can be simulated with the help of the digital twin and results can be linked to the existing 3D spatial data. Thus, the 3D spatial data set, the models and their descriptions through\xa0metadata become the reference and must be updated according to the requirements. Depending on requirements and questions, further 3D spatial data must be added. The description of the 3D spatial data and their models or the lifecycle management of the digital twin must be carried out with great care. Only in this way, decision processes can be supported in a comprehensible way.</jats:p>","abstract population growth will confront the city of zurich with a variety of challenges in the coming years, as the increase in the number of inhabitants and jobs will lead to densification and competing land uses. the tasks for the city administration have become more complex, whereas tools and methods are often based on traditional, static approaches while involving a limited number of citizens and stakeholders in relevant decisions. the digital transformation of more and more\xa0pieces of the planning and decision-making process will make both increasingly more illustrative, easier to understand and more comprehensible. an important data basis for these processes is the digital twin of the city of zurich. 3d spatial data and their models transform themes of the city, such as buildings, bridges, vegetation, etc., to the digital world, are being updated when required, and create advantages in digital space. these benefits need to be highlighted and published. an important step in public awareness is the release of 3d spatial data under open government data. this allows the development of applications, the promotion of understanding, and the simplification of the creation of different collaborative platforms. by\xa0visualization and analysis of digital prototypes and the demonstration of interactions with the built environment, scenarios can be digitally developed and discussed in decision-making bodies. questions about the urban climate can be simulated with the help of the digital twin and results can be linked to the existing 3d spatial data. thus, the 3d spatial data set, the models and their descriptions through\xa0metadata become the reference and must be updated according to the requirements. depending on requirements and questions, further 3d spatial data must be added. the description of the 3d spatial data and their models or the lifecycle management of the digital twin must be carried out with great care. only in this way, decision processes can be supported in a comprehensible way."
http://orkg.org/orkg/resource/R159484,Smart city dvelopment with digital twin technology,10.18690/978-961-286-362-3.20,crossref,"<jats:p>Growing urban areas are major consumers of natural resources, energy and raw materials. Understanding cities´ urban metabolism is salient when developing sustainable and resilient cities. This paper addresses concepts of smart city and digital twin technology as means to foster more sustainable urban development. Smart city has globally been well adopted concept in urban development. With smart city development cities aim to optimize overall performance of the city, its infrastructures, processes and services, but also to improve socio-economic wellbeing. Dynamic digital twins are constituted to form real-time connectivity between virtual and physical objects. Digital twin combines virtual objects to its physical counterparts. This conceptual paper provides additionally examples from dynamic digital twin platforms and digital twin of Helsinki, Finland.</jats:p>","growing urban areas are major consumers of natural resources, energy and raw materials. understanding cities´ urban metabolism is salient when developing sustainable and resilient cities. this paper addresses concepts of smart city and digital twin technology as means to foster more sustainable urban development. smart city has globally been well adopted concept in urban development. with smart city development cities aim to optimize overall performance of the city, its infrastructures, processes and services, but also to improve socio-economic wellbeing. dynamic digital twins are constituted to form real-time connectivity between virtual and physical objects. digital twin combines virtual objects to its physical counterparts. this conceptual paper provides additionally examples from dynamic digital twin platforms and digital twin of helsinki, finland."
http://orkg.org/orkg/resource/R160291,A Smart Campus’ Digital Twin for Sustainable Comfort Monitoring,10.3390/su12219196,crossref,"<jats:p>Interdisciplinary cross-cultural and cross-organizational research offers great opportunities for innovative breakthroughs in the field of smart cities, yet it also presents organizational and knowledge development hurdles. Smart cities must be large towns able to sustain the needs of their citizens while promoting environmental sustainability. Smart cities foment the widespread use of novel information and communication technologies (ICTs); however, experimenting with these technologies in such a large geographical area is unfeasible. Consequently, smart campuses (SCs), which are universities where technological devices and applications create new experiences or services and facilitate operational efficiency, allow experimentation on a smaller scale, the concept of SCs as a testbed for a smart city is gaining momentum in the research community. Nevertheless, while universities acknowledge the academic role of a smart and sustainable approach to higher education, campus life and other student activities remain a mystery, which have never been universally solved. This paper proposes a SC concept to investigate the integration of building information modeling tools with Internet of Things- (IoT)-based wireless sensor networks in the fields of environmental monitoring and emotion detection to provide insights into the level of comfort. Additionally, it explores the ability of universities to contribute to local sustainability projects by sharing knowledge and experience across a multi-disciplinary team. Preliminary results highlight the significance of monitoring workspaces because productivity has been proven to be directly influenced by environment parameters. The comfort-monitoring infrastructure could also be reused to monitor physical parameters from educational premises to increase energy efficiency.</jats:p>","interdisciplinary cross-cultural and cross-organizational research offers great opportunities for innovative breakthroughs in the field of smart cities, yet it also presents organizational and knowledge development hurdles. smart cities must be large towns able to sustain the needs of their citizens while promoting environmental sustainability. smart cities foment the widespread use of novel information and communication technologies (icts); however, experimenting with these technologies in such a large geographical area is unfeasible. consequently, smart campuses (scs), which are universities where technological devices and applications create new experiences or services and facilitate operational efficiency, allow experimentation on a smaller scale, the concept of scs as a testbed for a smart city is gaining momentum in the research community. nevertheless, while universities acknowledge the academic role of a smart and sustainable approach to higher education, campus life and other student activities remain a mystery, which have never been universally solved. this paper proposes a sc concept to investigate the integration of building information modeling tools with internet of things- (iot)-based wireless sensor networks in the fields of environmental monitoring and emotion detection to provide insights into the level of comfort. additionally, it explores the ability of universities to contribute to local sustainability projects by sharing knowledge and experience across a multi-disciplinary team. preliminary results highlight the significance of monitoring workspaces because productivity has been proven to be directly influenced by environment parameters. the comfort-monitoring infrastructure could also be reused to monitor physical parameters from educational premises to increase energy efficiency."
http://orkg.org/orkg/resource/R159637,Using video to re-present the user,10.1145/203356.203368,crossref,"<jats:p>Advocates of user-centered design and participatory design, also referred to as “work practice practitioners” include computer scientists, systems designers, software engineers, social scientists, industrial and graphic designers, marketing, sales, and service personnel. Working singly or in teams, we have been identifying and combining effective techniques and methods of: gathering data, interacting with user participants, representing activities and observations, and integrating findings with the design and construction of new technologies.</jats:p>","advocates of user-centered design and participatory design, also referred to as “work practice practitioners” include computer scientists, systems designers, software engineers, social scientists, industrial and graphic designers, marketing, sales, and service personnel. working singly or in teams, we have been identifying and combining effective techniques and methods of: gathering data, interacting with user participants, representing activities and observations, and integrating findings with the design and construction of new technologies."
http://orkg.org/orkg/resource/R158016,Ontological Representation of Smart City Data: From Devices to Cities,10.3390/app9010032,crossref,"<jats:p>Existing smart city ontologies allow representing different types of city-related data from cities. They have been developed according to different ontological commitments and hence do not share a minimum core model that would facilitate interoperability among smart city information systems. In this work, a survey has been carried out in order to study available smart city ontologies and to identify the domains they are representing. Taking into account the findings of the survey and a set of ontological requirements for smart city data, a list of ontology design patterns is proposed. These patterns aim to be easily replicated and provide a minimum set of core concepts in order to guide the development of smart city ontologies.</jats:p>","existing smart city ontologies allow representing different types of city-related data from cities. they have been developed according to different ontological commitments and hence do not share a minimum core model that would facilitate interoperability among smart city information systems. in this work, a survey has been carried out in order to study available smart city ontologies and to identify the domains they are representing. taking into account the findings of the survey and a set of ontological requirements for smart city data, a list of ontology design patterns is proposed. these patterns aim to be easily replicated and provide a minimum set of core concepts in order to guide the development of smart city ontologies."
http://orkg.org/orkg/resource/R156819,A Saturated X-ray Laser Beam at 7 Nanometers,10.1126/science.276.5315.1097,crossref,"<jats:p>A saturated nickel-like samarium x-ray laser beam at 7 nanometers has been demonstrated with an output energy of 0.3 millijoule in 50-picosecond pulses, demonstrating that saturated operation of a laser at wavelengths shorter than 10 nanometers can be achieved. The narrow divergence, short wavelength, short pulse duration, high efficiency, and high brightness of this samarium laser make it an ideal candidate for many x-ray laser applications.</jats:p>","a saturated nickel-like samarium x-ray laser beam at 7 nanometers has been demonstrated with an output energy of 0.3 millijoule in 50-picosecond pulses, demonstrating that saturated operation of a laser at wavelengths shorter than 10 nanometers can be achieved. the narrow divergence, short wavelength, short pulse duration, high efficiency, and high brightness of this samarium laser make it an ideal candidate for many x-ray laser applications."
http://orkg.org/orkg/resource/R157039,DNA barcode library for European Gelechiidae (Lepidoptera) suggests greatly underestimated species diversity,10.3897/zookeys.921.49199,crossref,"<jats:p>For the first time, a nearly complete barcode library for European Gelechiidae is provided. DNA barcode sequences (COI gene – cytochrome <jats:italic>c</jats:italic> oxidase 1) from 751 out of 865 nominal species, belonging to 105 genera, were successfully recovered. A total of 741 species represented by specimens with sequences ≥ 500bp and an additional ten species represented by specimens with shorter sequences were used to produce 53 NJ trees. Intraspecific barcode divergence averaged only 0.54% whereas distance to the Nearest-Neighbour species averaged 5.58%. Of these, 710 species possessed unique DNA barcodes, but 31 species could not be reliably discriminated because of barcode sharing or partial barcode overlap. Species discrimination based on the Barcode Index System (BIN) was successful for 668 out of 723 species which clustered from minimum one to maximum 22 unique BINs. Fifty-five species shared a BIN with up to four species and identification from DNA barcode data is uncertain. Finally, 65 clusters with a unique BIN remained unidentified to species level. These putative taxa, as well as 114 nominal species with more than one BIN, suggest the presence of considerable cryptic diversity, cases which should be examined in future revisionary studies.</jats:p>","for the first time, a nearly complete barcode library for european gelechiidae is provided. dna barcode sequences (coi gene – cytochrome c oxidase 1) from 751 out of 865 nominal species, belonging to 105 genera, were successfully recovered. a total of 741 species represented by specimens with sequences ≥ 500bp and an additional ten species represented by specimens with shorter sequences were used to produce 53 nj trees. intraspecific barcode divergence averaged only 0.54% whereas distance to the nearest-neighbour species averaged 5.58%. of these, 710 species possessed unique dna barcodes, but 31 species could not be reliably discriminated because of barcode sharing or partial barcode overlap. species discrimination based on the barcode index system (bin) was successful for 668 out of 723 species which clustered from minimum one to maximum 22 unique bins. fifty-five species shared a bin with up to four species and identification from dna barcode data is uncertain. finally, 65 clusters with a unique bin remained unidentified to species level. these putative taxa, as well as 114 nominal species with more than one bin, suggest the presence of considerable cryptic diversity, cases which should be examined in future revisionary studies."
http://orkg.org/orkg/resource/R155867,Psychological distance towards COVID-19: Geographical and hypothetical distance predict attitudes and mediate knowledge,10.1007/s12144-021-02415-x,crossref,"<jats:title>Abstract</jats:title><jats:p>While different antecedents have been examined to explain peoples’ reactions towards COVID-19, there is only scarce understanding about the role of the subjective closeness and distance to the pandemic. Within the current study, we applied the concept of psychological distance to understand the distance towards COVID-19 and investigated its (1) connection with preventive attitudes and proactive behaviors, (2) context-specific antecedents, and its (3) mediating effect of knowledge on attitudes. Using an online sample from a German quantitative cross-sectional study (<jats:italic>N</jats:italic>\u2009=\u2009395, <jats:italic>M</jats:italic>\u2009=\u200932.2\xa0years, <jats:italic>SD</jats:italic>\u2009=\u200913.9\xa0years, 64.3% female) in July 2020, a time with a general low incidence of people infected with Sars-CoV2, we measured relevant socio-psychological constructs addressing COVID-19 and included further information from external sources. Based on a path model, we found geographical distance as a significant predictor of cognitive attitudes towards COVID-19. Furthermore, hypothetical distance (i.e., feeling to be likely affected by COVID-19) predicted not only participants’ affective, cognitive, and behavioral attitudes, but also the installation of a corona warning-app. While several variables affected the different dimensions of psychological distance, hypothetical and geographical distance mediated the effect of knowledge on attitudes. These results underline the role of geographical and hypothetical distance for health-related behaviors and education. For example, people will only comply with preventive measures if they feel geographically concerned by the disease, which is particularly challenging for fast-spreading global diseases such as COVID-19. Therefore, there is a need to clearly communicate the personal risks of diseases and address peoples’ hypothetical distance.</jats:p>","abstract while different antecedents have been examined to explain peoples’ reactions towards covid-19, there is only scarce understanding about the role of the subjective closeness and distance to the pandemic. within the current study, we applied the concept of psychological distance to understand the distance towards covid-19 and investigated its (1) connection with preventive attitudes and proactive behaviors, (2) context-specific antecedents, and its (3) mediating effect of knowledge on attitudes. using an online sample from a german quantitative cross-sectional study ( n \u2009=\u2009395, m \u2009=\u200932.2\xa0years, sd \u2009=\u200913.9\xa0years, 64.3% female) in july 2020, a time with a general low incidence of people infected with sars-cov2, we measured relevant socio-psychological constructs addressing covid-19 and included further information from external sources. based on a path model, we found geographical distance as a significant predictor of cognitive attitudes towards covid-19. furthermore, hypothetical distance (i.e., feeling to be likely affected by covid-19) predicted not only participants’ affective, cognitive, and behavioral attitudes, but also the installation of a corona warning-app. while several variables affected the different dimensions of psychological distance, hypothetical and geographical distance mediated the effect of knowledge on attitudes. these results underline the role of geographical and hypothetical distance for health-related behaviors and education. for example, people will only comply with preventive measures if they feel geographically concerned by the disease, which is particularly challenging for fast-spreading global diseases such as covid-19. therefore, there is a need to clearly communicate the personal risks of diseases and address peoples’ hypothetical distance."
http://orkg.org/orkg/resource/R155615,Inhaled Voriconazole for Prevention of Invasive Pulmonary Aspergillosis,10.1128/aac.01657-08,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            Targeted airway delivery of antifungals as prophylaxis against invasive aspergillosis may lead to high lung drug concentrations while avoiding toxicities associated with systemically administered agents. We evaluated the effectiveness of aerosolizing the intravenous formulation of voriconazole as prophylaxis against invasive pulmonary aspergillosis caused by\n            <jats:italic>Aspergillus fumigatus</jats:italic>\n            in an established murine model. Inhaled voriconazole significantly improved survival and limited the extent of invasive disease, as assessed by histopathology, compared to control and amphotericin B treatments.\n          </jats:p>","abstract \n \n targeted airway delivery of antifungals as prophylaxis against invasive aspergillosis may lead to high lung drug concentrations while avoiding toxicities associated with systemically administered agents. we evaluated the effectiveness of aerosolizing the intravenous formulation of voriconazole as prophylaxis against invasive pulmonary aspergillosis caused by\n aspergillus fumigatus \n in an established murine model. inhaled voriconazole significantly improved survival and limited the extent of invasive disease, as assessed by histopathology, compared to control and amphotericin b treatments.\n"
http://orkg.org/orkg/resource/R155523,Transfer of diazotroph-derived nitrogen to the planktonic food web across gradients of N&lt;sub&gt;2&lt;/sub&gt; fixation activity and diversity in the western tropical South Pacific Ocean,10.5194/bg-15-3795-2018,crossref,"<jats:p>Abstract. Biological dinitrogen (N2) fixation provides the major source of\nnew nitrogen (N) to the open ocean, contributing more than atmospheric\ndeposition and riverine inputs to the N supply. Yet the fate of the\ndiazotroph-derived N (DDN) in the planktonic food web is poorly understood.\nThe main goals of this study were (i)\xa0to quantify how much of DDN is released\nto the dissolved pool during N2 fixation and how much is\ntransferred to bacteria, phytoplankton and zooplankton, and (ii)\xa0to compare\nthe DDN release and transfer efficiencies under contrasting N2\nfixation activity and diversity in the oligotrophic waters of the western\ntropical South Pacific (WTSP) Ocean. We used nanometre-scale secondary ion\nmass spectrometry (nanoSIMS) coupled with 15N2 isotopic\nlabelling and flow cytometry cell sorting to track the DDN transfer to\nplankton, in regions where the diazotroph community was dominated by either\nTrichodesmium or by UCYN-B. After 48\u2009h, ∼\u200920–40\u2009% of the\nN2 fixed during the experiment was released to the dissolved pool\nwhen Trichodesmium dominated, while the DDN release was not\nquantifiable when UCYN-B dominated; ∼\u20097–15\u2009% of the total fixed N\n(net N2 fixation\u2009+\u2009release) was transferred to non-diazotrophic\nplankton within 48\u2009h, with higher transfer efficiencies (15\u2009±\u20093\u2009%)\nwhen UCYN-B dominated as compared to when Trichodesmium dominated\n(9\u2009±\u20093\u2009%). The pico-cyanobacteria Synechococcus and\nProchlorococcus were the primary beneficiaries of the DDN\ntransferred (∼\u200965–70\u2009%), followed by heterotrophic bacteria\n(∼\u200923–34\u2009%). The DDN transfer in bacteria was higher\n(34\u2009±\u20097\u2009%) in the UCYN-B-dominating experiment compared to the\nTrichodesmium-dominating experiments (24\u2009±\u20095\u2009%). Regarding\nhigher trophic levels, the DDN transfer to the dominant zooplankton species\nwas less efficient when the diazotroph community was dominated by\nTrichodesmium (∼\u20095–9\u2009% of the DDN transfer) than when it\nwas dominated by UCYN-B (∼\u200928\u2009±\u200913\u2009% of the DDN transfer). To\nour knowledge, this study provides the first quantification of DDN release\nand transfer to phytoplankton, bacteria and zooplankton communities in open\nocean waters. It reveals that despite UCYN-B fix N2 at lower rates\ncompared to Trichodesmium in the WTSP, the DDN from UCYN-B is much\nmore available and efficiently transferred to the planktonic food web than\nthe DDN originating from Trichodesmium.\n                    </jats:p>","abstract. biological dinitrogen (n2) fixation provides the major source of\nnew nitrogen (n) to the open ocean, contributing more than atmospheric\ndeposition and riverine inputs to the n supply. yet the fate of the\ndiazotroph-derived n (ddn) in the planktonic food web is poorly understood.\nthe main goals of this study were (i)\xa0to quantify how much of ddn is released\nto the dissolved pool during n2 fixation and how much is\ntransferred to bacteria, phytoplankton and zooplankton, and (ii)\xa0to compare\nthe ddn release and transfer efficiencies under contrasting n2\nfixation activity and diversity in the oligotrophic waters of the western\ntropical south pacific (wtsp) ocean. we used nanometre-scale secondary ion\nmass spectrometry (nanosims) coupled with 15n2 isotopic\nlabelling and flow cytometry cell sorting to track the ddn transfer to\nplankton, in regions where the diazotroph community was dominated by either\ntrichodesmium or by ucyn-b. after 48\u2009h, ∼\u200920–40\u2009% of the\nn2 fixed during the experiment was released to the dissolved pool\nwhen trichodesmium dominated, while the ddn release was not\nquantifiable when ucyn-b dominated; ∼\u20097–15\u2009% of the total fixed n\n(net n2 fixation\u2009+\u2009release) was transferred to non-diazotrophic\nplankton within 48\u2009h, with higher transfer efficiencies (15\u2009±\u20093\u2009%)\nwhen ucyn-b dominated as compared to when trichodesmium dominated\n(9\u2009±\u20093\u2009%). the pico-cyanobacteria synechococcus and\nprochlorococcus were the primary beneficiaries of the ddn\ntransferred (∼\u200965–70\u2009%), followed by heterotrophic bacteria\n(∼\u200923–34\u2009%). the ddn transfer in bacteria was higher\n(34\u2009±\u20097\u2009%) in the ucyn-b-dominating experiment compared to the\ntrichodesmium-dominating experiments (24\u2009±\u20095\u2009%). regarding\nhigher trophic levels, the ddn transfer to the dominant zooplankton species\nwas less efficient when the diazotroph community was dominated by\ntrichodesmium (∼\u20095–9\u2009% of the ddn transfer) than when it\nwas dominated by ucyn-b (∼\u200928\u2009±\u200913\u2009% of the ddn transfer). to\nour knowledge, this study provides the first quantification of ddn release\nand transfer to phytoplankton, bacteria and zooplankton communities in open\nocean waters. it reveals that despite ucyn-b fix n2 at lower rates\ncompared to trichodesmium in the wtsp, the ddn from ucyn-b is much\nmore available and efficiently transferred to the planktonic food web than\nthe ddn originating from trichodesmium.\n"
http://orkg.org/orkg/resource/R155530,Nitrogen budgets following a Lagrangian strategy in the Western Tropical South Pacific Ocean: the prominent role of N&lt;sub&gt;2&lt;/sub&gt; fixation (OUTPACE cruise),10.5194/bg-2017-468,crossref,"<jats:p>Abstract. We performed N budgets at three stations in the western tropical South Pacific (WTSP) Ocean during austral summer conditions (Feb. Mar. 2015) and quantified all major N fluxes both entering the system (N2 fixation, nitrate eddy diffusion, atmospheric deposition) and leaving the system (PN export). Thanks to a Lagrangian strategy, we sampled the same water mass for the entire duration of each long duration (5 days) station, allowing to consider only vertical exchanges. Two stations located at the western end of the transect (Melanesian archipelago (MA) waters, LD A and LD B) were oligotrophic and characterized by a deep chlorophyll maximum (DCM) located at 51\u2009±\u200918\u2009m and 81\u2009±\u20099\u2009m at LD A and LD B. Station LD C was characterized by a DCM located at 132\u2009±\u20097\u2009m, representative of the ultra-oligotrophic waters of the South Pacific gyre (SPG water). N2 fixation rates were extremely high at both LD A (593\u2009±\u200951\u2009µmol\u2009N\u2009m−2\u2009d−1) and LD B (706\u2009±\u2009302\u2009µmol\u2009N\u2009m−2\u2009d−1), and the diazotroph community was dominated by Trichodesmium. N2 fixation rates were lower (59\u2009±\u200916\u2009µmol\u2009N\u2009m−2\u2009d−1) at LD C and the diazotroph community was dominated by unicellular N2-fixing cyanobacteria (UCYN). At all stations, N2 fixation was the major source of new N (&gt;\u200990\u2009%) before atmospheric deposition and upward nitrate fluxes induced by turbulence. N2 fixation contributed circa 8–12\u2009% of primary production in the MA region and 3\u2009% in the SPG water and sustained nearly all new primary production at all stations. The e-ratio (e-ratio\u2009=\u2009PC export/PP) was maximum at LD A (9.7\u2009%) and was higher than the e-ratio in most studied oligotrophic regions (~\u20091\u2009%), indicating a high efficiency of the WTSP to export carbon relative to primary production. The direct export of diazotrophs assessed by qPCR of the nifH gene in sediment traps represented up to 30.6\u2009% of the PC export at LD A, while there contribution was 5 and \n                        </jats:p>","abstract. we performed n budgets at three stations in the western tropical south pacific (wtsp) ocean during austral summer conditions (feb. mar. 2015) and quantified all major n fluxes both entering the system (n2 fixation, nitrate eddy diffusion, atmospheric deposition) and leaving the system (pn export). thanks to a lagrangian strategy, we sampled the same water mass for the entire duration of each long duration (5 days) station, allowing to consider only vertical exchanges. two stations located at the western end of the transect (melanesian archipelago (ma) waters, ld a and ld b) were oligotrophic and characterized by a deep chlorophyll maximum (dcm) located at 51\u2009±\u200918\u2009m and 81\u2009±\u20099\u2009m at ld a and ld b. station ld c was characterized by a dcm located at 132\u2009±\u20097\u2009m, representative of the ultra-oligotrophic waters of the south pacific gyre (spg water). n2 fixation rates were extremely high at both ld a (593\u2009±\u200951\u2009µmol\u2009n\u2009m−2\u2009d−1) and ld b (706\u2009±\u2009302\u2009µmol\u2009n\u2009m−2\u2009d−1), and the diazotroph community was dominated by trichodesmium. n2 fixation rates were lower (59\u2009±\u200916\u2009µmol\u2009n\u2009m−2\u2009d−1) at ld c and the diazotroph community was dominated by unicellular n2-fixing cyanobacteria (ucyn). at all stations, n2 fixation was the major source of new n (&gt;\u200990\u2009%) before atmospheric deposition and upward nitrate fluxes induced by turbulence. n2 fixation contributed circa 8–12\u2009% of primary production in the ma region and 3\u2009% in the spg water and sustained nearly all new primary production at all stations. the e-ratio (e-ratio\u2009=\u2009pc export/pp) was maximum at ld a (9.7\u2009%) and was higher than the e-ratio in most studied oligotrophic regions (~\u20091\u2009%), indicating a high efficiency of the wtsp to export carbon relative to primary production. the direct export of diazotrophs assessed by qpcr of the nifh gene in sediment traps represented up to 30.6\u2009% of the pc export at ld a, while there contribution was 5 and \n"
http://orkg.org/orkg/resource/R154620,Production Rate Analysis of Fractured Horizontal Well considering Multitransport Mechanisms in Shale Gas Reservoir,10.1155/2018/3148298,crossref,"<jats:p>Shale gas reservoir has been aggressively exploited around the world, which has complex pore structure with multiple transport mechanisms according to the reservoir characteristics. In this paper, a new comprehensive mathematical model is established to analyze the production performance of multiple fractured horizontal well (MFHW) in box-shaped shale gas reservoir considering multiscaled flow mechanisms (ad/desorption and Fick diffusion). In the model, the adsorbed gas is assumed not directly diffused into the natural macrofractures but into the macropores of matrix first and then flows into the natural fractures. The ad/desorption phenomenon of shale gas on the matrix particles is described by a combination of the Langmuir’s isothermal adsorption equation, continuity equation, gas state equation, and the motion equation in matrix system. On the basis of the Green’s function theory, the point source solution is derived under the assumption that gas flow from macropores into natural fractures follows transient interporosity and absorbed gas diffused into macropores from nanopores follows unsteady-state diffusion. The production rate expression of a MFHW producing at constant bottomhole pressure is obtained by using Duhamel’s principle. Moreover, the curves of well production rate and cumulative production <jats:italic>vs.</jats:italic> time are plotted by Stehfest numerical inversion algorithm and also the effects of influential factors on well production performance are analyzed. The results derived in this paper have significance to the guidance of shale gas reservoir development.</jats:p>","shale gas reservoir has been aggressively exploited around the world, which has complex pore structure with multiple transport mechanisms according to the reservoir characteristics. in this paper, a new comprehensive mathematical model is established to analyze the production performance of multiple fractured horizontal well (mfhw) in box-shaped shale gas reservoir considering multiscaled flow mechanisms (ad/desorption and fick diffusion). in the model, the adsorbed gas is assumed not directly diffused into the natural macrofractures but into the macropores of matrix first and then flows into the natural fractures. the ad/desorption phenomenon of shale gas on the matrix particles is described by a combination of the langmuir’s isothermal adsorption equation, continuity equation, gas state equation, and the motion equation in matrix system. on the basis of the green’s function theory, the point source solution is derived under the assumption that gas flow from macropores into natural fractures follows transient interporosity and absorbed gas diffused into macropores from nanopores follows unsteady-state diffusion. the production rate expression of a mfhw producing at constant bottomhole pressure is obtained by using duhamel’s principle. moreover, the curves of well production rate and cumulative production vs. time are plotted by stehfest numerical inversion algorithm and also the effects of influential factors on well production performance are analyzed. the results derived in this paper have significance to the guidance of shale gas reservoir development."
http://orkg.org/orkg/resource/R154460,A highly stable Ru/LaCO3OH catalyst consisting of support-coated Ru nanoparticles in aqueous-phase hydrogenolysis reactions,10.1039/c7gc02414b,crossref,<p>A hydrothermally stable Ru/LaCO<sub>3</sub>OH catalyst consisting of Ru nanoparticles partially encapsulated by the support with a strong metal–support interaction is developed.</p>,a hydrothermally stable ru/laco 3 oh catalyst consisting of ru nanoparticles partially encapsulated by the support with a strong metal–support interaction is developed.
http://orkg.org/orkg/resource/R154399,Selective catalytic conversion of guaiacol to phenols over a molybdenum carbide catalyst,10.1039/c5cc01900a,crossref,<p>An activated carbon supported α-molybdenum carbide catalyst (α-MoC<sub>1−x</sub>/AC) showed remarkable activity in the selective deoxygenation of guaiacol to substituted mono-phenols in low carbon number alcohol solvents.</p>,an activated carbon supported α-molybdenum carbide catalyst (α-moc 1−x /ac) showed remarkable activity in the selective deoxygenation of guaiacol to substituted mono-phenols in low carbon number alcohol solvents.
http://orkg.org/orkg/resource/R153502,"THIS IS NOT A DRILL: Mobile Telephony, Information Verification, and Expressive Communication During Hawaii’s False Missile Alert",,crossref,"<jats:p> On Saturday, 13 January 2018, residents of Hawaii received a chilling message through their smartphones. It read, in all caps, BALLISTIC MISSILE THREAT INBOUND TO HAWAII. SEEK IMMEDIATE SHELTER. THIS IS NOT A DRILL. The message was mistakenly sent, but many residents lived in a threatened state of mind for the 38\u2009minutes it took before a retraction was made. This study is based on a survey of 418 people who experienced the alert, recollecting their immediate responses, including how they attempted to verify the alert and how they used their mobile devices and social media for expressive interactions during the alert period. With the ongoing testing in the United States of nationwide Wireless Emergency Alerts, along with similar expansions of these systems in other countries, the event in Hawaii serves to illuminate how people understand and respond to mobile-based alerts. It shows the extreme speed that information—including misinformation—can flow in an emergency, and, for many, expressive communication affects people’s reactions. </jats:p>","on saturday, 13 january 2018, residents of hawaii received a chilling message through their smartphones. it read, in all caps, ballistic missile threat inbound to hawaii. seek immediate shelter. this is not a drill. the message was mistakenly sent, but many residents lived in a threatened state of mind for the 38\u2009minutes it took before a retraction was made. this study is based on a survey of 418 people who experienced the alert, recollecting their immediate responses, including how they attempted to verify the alert and how they used their mobile devices and social media for expressive interactions during the alert period. with the ongoing testing in the united states of nationwide wireless emergency alerts, along with similar expansions of these systems in other countries, the event in hawaii serves to illuminate how people understand and respond to mobile-based alerts. it shows the extreme speed that information—including misinformation—can flow in an emergency, and, for many, expressive communication affects people’s reactions."
http://orkg.org/orkg/resource/R153391,Neuro-Symbolic Probabilistic Argumentation Machines,10.24963/kr.2020/90,crossref,"<jats:p>Neural-symbolic systems combine the strengths of neural networks and symbolic formalisms. In this paper, we introduce a neural-symbolic system which combines restricted Boltzmann machines and probabilistic semi-abstract argumentation. We propose to train networks on argument labellings explaining the data, so that any sampled data outcome is associated with an argument labelling. Argument labellings are integrated as constraints within restricted Boltzmann machines, so that the neural networks are used to learn probabilistic dependencies amongst argument labels. Given a dataset and an argumentation graph as prior knowledge, for every example/case K in the dataset, we use a so-called K-maxconsistent labelling of the graph, and an explanation of case K refers to a K-maxconsistent labelling of the given argumentation graph. The abilities of the proposed system to predict correct labellings were evaluated and compared with standard machine learning techniques. Experiments revealed that such argumentation Boltzmann machines can outperform other classification models, especially in noisy settings.</jats:p>","neural-symbolic systems combine the strengths of neural networks and symbolic formalisms. in this paper, we introduce a neural-symbolic system which combines restricted boltzmann machines and probabilistic semi-abstract argumentation. we propose to train networks on argument labellings explaining the data, so that any sampled data outcome is associated with an argument labelling. argument labellings are integrated as constraints within restricted boltzmann machines, so that the neural networks are used to learn probabilistic dependencies amongst argument labels. given a dataset and an argumentation graph as prior knowledge, for every example/case k in the dataset, we use a so-called k-maxconsistent labelling of the graph, and an explanation of case k refers to a k-maxconsistent labelling of the given argumentation graph. the abilities of the proposed system to predict correct labellings were evaluated and compared with standard machine learning techniques. experiments revealed that such argumentation boltzmann machines can outperform other classification models, especially in noisy settings."
http://orkg.org/orkg/resource/R152995,Communicating with the Workforce during Emergencies: Developing an Employee Text Messaging Program in a Local Public Health Setting,,crossref,"<jats:p> Short message service (SMS) text messaging can be useful for communicating information to public health employees and improving workforce situational awareness during emergencies. We sought to understand how the 1,500 employees at Public Health – Seattle &amp; King County, Washington, perceived barriers to and benefits of participation in a voluntary, employer-based SMS program. Based on employee feedback, we developed the system, marketed it, and invited employees to opt in. The system was tested during an ice storm in January 2012. Employee concerns about opting into an SMS program included possible work encroachment during non-work time and receiving excessive irrelevant messages. Employees who received messages during the weather event reported high levels of satisfaction and perceived utility from the program. We conclude that text messaging is a feasible form of communication with employees during emergencies. Care should be taken to design and deploy a program that maximizes employee satisfaction. </jats:p>","short message service (sms) text messaging can be useful for communicating information to public health employees and improving workforce situational awareness during emergencies. we sought to understand how the 1,500 employees at public health – seattle &amp; king county, washington, perceived barriers to and benefits of participation in a voluntary, employer-based sms program. based on employee feedback, we developed the system, marketed it, and invited employees to opt in. the system was tested during an ice storm in january 2012. employee concerns about opting into an sms program included possible work encroachment during non-work time and receiving excessive irrelevant messages. employees who received messages during the weather event reported high levels of satisfaction and perceived utility from the program. we conclude that text messaging is a feasible form of communication with employees during emergencies. care should be taken to design and deploy a program that maximizes employee satisfaction."
http://orkg.org/orkg/resource/R153001,Milling and Public Warnings,,crossref,"<jats:p> Given the potential of modern warning technology to save lives, discovering whether it is possible to craft mobile alerts for imminent events in a way that reduces people’s tendency to seek and confirm information before initiating protective action is essential. The purpose of this study was to examine the possibility of designing messages for mobile devices, such as Wireless Emergency Alert (WEA) messages, to minimize action delay. The impact of messages with varied amounts of information on respondents’ understanding, believing, personalizing, deciding, and intended milling was used to test Emergent Norm Theory, using quantitative and qualitative methods. Relative to shorter messages, longer public warning messages reduced people’s inclination to search for and confirm information, thereby shortening warning response delay. The Emergent Norm Theory used herein is broader in application than the context-specific models provided by leading warning scholars to date and yields deeper understanding about how people respond to warnings. </jats:p>","given the potential of modern warning technology to save lives, discovering whether it is possible to craft mobile alerts for imminent events in a way that reduces people’s tendency to seek and confirm information before initiating protective action is essential. the purpose of this study was to examine the possibility of designing messages for mobile devices, such as wireless emergency alert (wea) messages, to minimize action delay. the impact of messages with varied amounts of information on respondents’ understanding, believing, personalizing, deciding, and intended milling was used to test emergent norm theory, using quantitative and qualitative methods. relative to shorter messages, longer public warning messages reduced people’s inclination to search for and confirm information, thereby shortening warning response delay. the emergent norm theory used herein is broader in application than the context-specific models provided by leading warning scholars to date and yields deeper understanding about how people respond to warnings."
http://orkg.org/orkg/resource/R151278,Do ICTs Help To Maintain Social Capital In The Disaster Recovery Phase? A Case Study Of The L'aquila Earthquake,,crossref,"<jats:p>The use of information and communication technologies (ICTs), especially of the social networking sites (SNSs), in emergency situations is constantly on the rise. With this study, we have investigated the use of Information and Communication Technologies (ICTs) after the massive changes that occurred in the physical environment following the L’Aquila earthquake (central Italy) in 2009. Two years after the disaster, thirteen key individuals affected by the earthquake were interviewed through semi-structured interviews. Results suggest that new media can serve, to some extent, similar functions of sustaining the creation and maintenance of social relationships as the ones previously fulfilled by physical spaces. Although limited, this research may have the potential to open up an interesting debate on the web-mediated construction of the concept of “place” in the wake of a disaster.\xa0</jats:p>","the use of information and communication technologies (icts), especially of the social networking sites (snss), in emergency situations is constantly on the rise. with this study, we have investigated the use of information and communication technologies (icts) after the massive changes that occurred in the physical environment following the l’aquila earthquake (central italy) in 2009. two years after the disaster, thirteen key individuals affected by the earthquake were interviewed through semi-structured interviews. results suggest that new media can serve, to some extent, similar functions of sustaining the creation and maintenance of social relationships as the ones previously fulfilled by physical spaces. although limited, this research may have the potential to open up an interesting debate on the web-mediated construction of the concept of “place” in the wake of a disaster.\xa0"
http://orkg.org/orkg/resource/R151268,Social Media in Crisis: When Professional Responders Meet Digital Volunteers,,crossref,"<jats:title>Abstract</jats:title><jats:p>In this paper, we examine the socio-technical impact that social media has had on coordination between professional emergency responders and digital volunteers. Drawing from the research literature, we outline the problem space and explore ways to improve coordination and collaboration between these two groups. Possible improvements include mediators, revisiting trust, emergency policy and process changes, a bounded social environment, digital volunteer data as context, and computational solutions. As the space matures and collaboration improves, we predict that professional responders will begin to rely on the data and products produced by digital volunteers. Volunteer groups will be challenged to mature as well, to develop volunteer management systems, permanent staff, data management practices, and training for new volunteers to ensure consistent response to professional responders as needed.</jats:p>","abstract in this paper, we examine the socio-technical impact that social media has had on coordination between professional emergency responders and digital volunteers. drawing from the research literature, we outline the problem space and explore ways to improve coordination and collaboration between these two groups. possible improvements include mediators, revisiting trust, emergency policy and process changes, a bounded social environment, digital volunteer data as context, and computational solutions. as the space matures and collaboration improves, we predict that professional responders will begin to rely on the data and products produced by digital volunteers. volunteer groups will be challenged to mature as well, to develop volunteer management systems, permanent staff, data management practices, and training for new volunteers to ensure consistent response to professional responders as needed."
http://orkg.org/orkg/resource/R151506,Topically effective ocular hypotensive acetazolamide and ethoxyzolamide formulations in rabbits,10.1111/j.2042-7158.1994.tb03835.x,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>The effect of topically active 2-hydroxypropyl-β-cyclodextrin (HP-β-CyD) eye-drop formulations containing solutions of acetazolamide, ethoxyzolamide or timolol on the intra-ocular pressure (IOP) was investigated in normotensive conscious rabbits. Both acetazolamide and ethoxyzolamide were active but their IOP-lowering effect was less than that of timolol. The IOP-lowering effects of acetazolamide and ethoxyzolamide and that of timolol appeared to be to some extent additive. Combination of acetazolamide and timolol or ethoxyzolamide and timolol in one HP-β-CyD formulation resulted in a significant increase in the duration of activity compared with HP-β-CyD formulations containing only acetazolamide, ethoxyzolamide or timolol. Also, it was possible to increase the IOP-lowering effect of acetazolamide by formulating the drug as a suspension in an aqueous HP-β-CyD vehicle.</jats:p>","abstract \n the effect of topically active 2-hydroxypropyl-β-cyclodextrin (hp-β-cyd) eye-drop formulations containing solutions of acetazolamide, ethoxyzolamide or timolol on the intra-ocular pressure (iop) was investigated in normotensive conscious rabbits. both acetazolamide and ethoxyzolamide were active but their iop-lowering effect was less than that of timolol. the iop-lowering effects of acetazolamide and ethoxyzolamide and that of timolol appeared to be to some extent additive. combination of acetazolamide and timolol or ethoxyzolamide and timolol in one hp-β-cyd formulation resulted in a significant increase in the duration of activity compared with hp-β-cyd formulations containing only acetazolamide, ethoxyzolamide or timolol. also, it was possible to increase the iop-lowering effect of acetazolamide by formulating the drug as a suspension in an aqueous hp-β-cyd vehicle."
http://orkg.org/orkg/resource/R150261,Uses of Electronic Health Records for Public Health Surveillance to Advance Public Health,10.1146/annurev-publhealth-031914-122747,crossref,"<jats:p> Public health surveillance conducted by health departments in the United States has improved in completeness and timeliness owing to electronic laboratory reporting. However, the collection of detailed clinical information about reported cases, which is necessary to confirm the diagnosis, to understand transmission, or to determine disease-related risk factors, is still heavily dependent on manual processes. The increasing prevalence and functionality of electronic health record (EHR) systems in the United States present important opportunities to advance public health surveillance. EHR data have the potential to further increase the breadth, detail, timeliness, and completeness of public health surveillance and thereby provide better data to guide public health interventions. EHRs also provide a unique opportunity to expand the role and vision of current surveillance efforts and to help bridge the gap between public health practice and clinical medicine. </jats:p>","public health surveillance conducted by health departments in the united states has improved in completeness and timeliness owing to electronic laboratory reporting. however, the collection of detailed clinical information about reported cases, which is necessary to confirm the diagnosis, to understand transmission, or to determine disease-related risk factors, is still heavily dependent on manual processes. the increasing prevalence and functionality of electronic health record (ehr) systems in the united states present important opportunities to advance public health surveillance. ehr data have the potential to further increase the breadth, detail, timeliness, and completeness of public health surveillance and thereby provide better data to guide public health interventions. ehrs also provide a unique opportunity to expand the role and vision of current surveillance efforts and to help bridge the gap between public health practice and clinical medicine."
http://orkg.org/orkg/resource/R150258,Mobile technologies for disease surveillance in humans and animals,10.4102/ojvr.v81i2.737,crossref,"<jats:p>A paper-based disease reporting system has been associated with a number of challenges.\xa0These include difficulties to submit hard copies of the disease surveillance forms because\xa0of poor road infrastructure, weather conditions or challenging terrain, particularly in the\xa0developing countries. The system demands re-entry of the data at data processing and\xa0analysis points, thus making it prone to introduction of errors during this process. All these\xa0challenges contribute to delayed acquisition, processing and response to disease events\xa0occurring in remote hard to reach areas. Our study piloted the use of mobile phones in\xa0order to transmit near to real-time data from remote districts in Tanzania (Ngorongoro and\xa0Ngara), Burundi (Muyinga) and Zambia (Kazungula and Sesheke). Two technologies namely,\xa0digital and short messaging services were used to capture and transmit disease event data\xa0in the animal and human health sectors in the study areas based on a server–client model.\xa0Smart phones running the Android operating system (minimum required version: Android\xa01.6), and which supported open source application, Epicollect, as well as the Open Data Kit\xa0application, were used in the study. These phones allowed collection of geo-tagged data, with\xa0the opportunity of including static and moving images related to disease events. The project\xa0supported routine disease surveillance systems in the ministries responsible for animal and\xa0human health in Burundi, Tanzania and Zambia, as well as data collection for researchers at\xa0the Sokoine University of Agriculture, Tanzania. During the project implementation period\xa0between 2011 and 2013, a total number of 1651 diseases event-related forms were submitted,\xa0which allowed reporters to include GPS coordinates and photographs related to the events\xa0captured. It was concluded that the new technology-based surveillance system is useful in\xa0providing near to real-time data, with potential for enhancing timely response in rural remote\xa0areas of Africa. We recommended adoption of the proven technologies to improve disease\xa0surveillance, particularly in the developing countries.</jats:p>","a paper-based disease reporting system has been associated with a number of challenges.\xa0these include difficulties to submit hard copies of the disease surveillance forms because\xa0of poor road infrastructure, weather conditions or challenging terrain, particularly in the\xa0developing countries. the system demands re-entry of the data at data processing and\xa0analysis points, thus making it prone to introduction of errors during this process. all these\xa0challenges contribute to delayed acquisition, processing and response to disease events\xa0occurring in remote hard to reach areas. our study piloted the use of mobile phones in\xa0order to transmit near to real-time data from remote districts in tanzania (ngorongoro and\xa0ngara), burundi (muyinga) and zambia (kazungula and sesheke). two technologies namely,\xa0digital and short messaging services were used to capture and transmit disease event data\xa0in the animal and human health sectors in the study areas based on a server–client model.\xa0smart phones running the android operating system (minimum required version: android\xa01.6), and which supported open source application, epicollect, as well as the open data kit\xa0application, were used in the study. these phones allowed collection of geo-tagged data, with\xa0the opportunity of including static and moving images related to disease events. the project\xa0supported routine disease surveillance systems in the ministries responsible for animal and\xa0human health in burundi, tanzania and zambia, as well as data collection for researchers at\xa0the sokoine university of agriculture, tanzania. during the project implementation period\xa0between 2011 and 2013, a total number of 1651 diseases event-related forms were submitted,\xa0which allowed reporters to include gps coordinates and photographs related to the events\xa0captured. it was concluded that the new technology-based surveillance system is useful in\xa0providing near to real-time data, with potential for enhancing timely response in rural remote\xa0areas of africa. we recommended adoption of the proven technologies to improve disease\xa0surveillance, particularly in the developing countries."
http://orkg.org/orkg/resource/R150475,Biomedical named entity recognition and linking datasets: survey and our recent development,10.1093/bib/bbaa054,crossref,"<jats:title>Abstract</jats:title><jats:p>Natural language processing (NLP) is widely applied in biological domains to retrieve information from publications. Systems to address numerous applications exist, such as biomedical named entity recognition (BNER), named entity normalization (NEN) and protein–protein interaction extraction (PPIE). High-quality datasets can assist the development of robust and reliable systems; however, due to the endless applications and evolving techniques, the annotations of benchmark datasets may become outdated and inappropriate. In this study, we first review commonlyused BNER datasets and their potential annotation problems such as inconsistency and low portability. Then, we introduce a revised version of the JNLPBA dataset that solves potential problems in the original and use state-of-the-art named entity recognition systems to evaluate its portability to different kinds of biomedical literature, including protein–protein interaction and biology events. Lastly, we introduce an ensembled biomedical entity dataset (EBED) by extending the revised JNLPBA dataset with PubMed Central full-text paragraphs, figure captions and patent abstracts. This EBED is a multi-task dataset that covers annotations including gene, disease and chemical entities. In total, it contains 85000 entity mentions, 25000 entity mentions with database identifiers and 5000 attribute tags. To demonstrate the usage of the EBED, we review the BNER track from the AI CUP Biomedical Paper Analysis challenge. Availability: The revised JNLPBA dataset is available at https://iasl-btm.iis.sinica.edu.tw/BNER/Content/Re vised_JNLPBA.zip. The EBED dataset is available at https://iasl-btm.iis.sinica.edu.tw/BNER/Content/AICUP _EBED_dataset.rar. Contact: Email: thtsai@g.ncu.edu.tw, Tel. 886-3-4227151 ext. 35203, Fax: 886-3-422-2681 Email: hsu@iis.sinica.edu.tw, Tel. 886-2-2788-3799 ext. 2211, Fax: 886-2-2782-4814 Supplementary information: Supplementary data are available at Briefings in Bioinformatics online.</jats:p>","abstract natural language processing (nlp) is widely applied in biological domains to retrieve information from publications. systems to address numerous applications exist, such as biomedical named entity recognition (bner), named entity normalization (nen) and protein–protein interaction extraction (ppie). high-quality datasets can assist the development of robust and reliable systems; however, due to the endless applications and evolving techniques, the annotations of benchmark datasets may become outdated and inappropriate. in this study, we first review commonlyused bner datasets and their potential annotation problems such as inconsistency and low portability. then, we introduce a revised version of the jnlpba dataset that solves potential problems in the original and use state-of-the-art named entity recognition systems to evaluate its portability to different kinds of biomedical literature, including protein–protein interaction and biology events. lastly, we introduce an ensembled biomedical entity dataset (ebed) by extending the revised jnlpba dataset with pubmed central full-text paragraphs, figure captions and patent abstracts. this ebed is a multi-task dataset that covers annotations including gene, disease and chemical entities. in total, it contains 85000 entity mentions, 25000 entity mentions with database identifiers and 5000 attribute tags. to demonstrate the usage of the ebed, we review the bner track from the ai cup biomedical paper analysis challenge. availability: the revised jnlpba dataset is available at https://iasl-btm.iis.sinica.edu.tw/bner/content/re vised_jnlpba.zip. the ebed dataset is available at https://iasl-btm.iis.sinica.edu.tw/bner/content/aicup _ebed_dataset.rar. contact: email: thtsai@g.ncu.edu.tw, tel. 886-3-4227151 ext. 35203, fax: 886-3-422-2681 email: hsu@iis.sinica.edu.tw, tel. 886-2-2788-3799 ext. 2211, fax: 886-2-2782-4814 supplementary information: supplementary data are available at briefings in bioinformatics online."
http://orkg.org/orkg/resource/R150537,LINNAEUS: A species name identification system for biomedical literature,10.1186/1471-2105-11-85,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>The task of recognizing and identifying species names in biomedical literature has recently been regarded as critical for a number of applications in text and data mining, including gene name recognition, species-specific document retrieval, and semantic enrichment of biomedical articles.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>In this paper we describe an open-source species name recognition and normalization software system, LINNAEUS, and evaluate its performance relative to several automatically generated biomedical corpora, as well as a novel corpus of full-text documents manually annotated for species mentions. LINNAEUS uses a dictionary-based approach (implemented as an efficient deterministic finite-state automaton) to identify species names and a set of heuristics to resolve ambiguous mentions. When compared against our manually annotated corpus, LINNAEUS performs with 94% recall and 97% precision at the mention level, and 98% recall and 90% precision at the document level. Our system successfully solves the problem of disambiguating uncertain species mentions, with 97% of all mentions in PubMed Central full-text documents resolved to unambiguous NCBI taxonomy identifiers.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>LINNAEUS is an open source, stand-alone software system capable of recognizing and normalizing species name mentions with speed and accuracy, and can therefore be integrated into a range of bioinformatics and text-mining applications. The software and manually annotated corpus can be downloaded freely at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""http://linnaeus.sourceforge.net/"" ext-link-type=""uri"">http://linnaeus.sourceforge.net/</jats:ext-link>.</jats:p>\n          </jats:sec>","abstract \n \n background \n the task of recognizing and identifying species names in biomedical literature has recently been regarded as critical for a number of applications in text and data mining, including gene name recognition, species-specific document retrieval, and semantic enrichment of biomedical articles. \n \n \n results \n in this paper we describe an open-source species name recognition and normalization software system, linnaeus, and evaluate its performance relative to several automatically generated biomedical corpora, as well as a novel corpus of full-text documents manually annotated for species mentions. linnaeus uses a dictionary-based approach (implemented as an efficient deterministic finite-state automaton) to identify species names and a set of heuristics to resolve ambiguous mentions. when compared against our manually annotated corpus, linnaeus performs with 94% recall and 97% precision at the mention level, and 98% recall and 90% precision at the document level. our system successfully solves the problem of disambiguating uncertain species mentions, with 97% of all mentions in pubmed central full-text documents resolved to unambiguous ncbi taxonomy identifiers. \n \n \n conclusions \n linnaeus is an open source, stand-alone software system capable of recognizing and normalizing species name mentions with speed and accuracy, and can therefore be integrated into a range of bioinformatics and text-mining applications. the software and manually annotated corpus can be downloaded freely at http://linnaeus.sourceforge.net/ . \n"
http://orkg.org/orkg/resource/R150566,Supplementation with vitamin D in the COVID-19 pandemic?,10.1093/nutrit/nuaa081,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>The coronavirus disease 2019 (COVID-19) pandemic was declared a public health emergency of international concern by the World Health Organization. COVID-19 has high transmissibility and could result in acute lung injury in a fraction of patients. By counterbalancing the activity of the renin-angiotensin system, angiotensin-converting enzyme 2, which is the fusion receptor of the virus, plays a protective role against the development of complications of this viral infection. Vitamin\u2009D can induce the expression of angiotensin-converting enzyme 2 and regulate the immune system through different mechanisms. Epidemiologic studies of the relationship between vitamin\u2009D and various respiratory infections were reviewed and, here, the postulated mechanisms and clinical data supporting the protective role of vitamin\u2009D against COVID-19–mediated complications are discussed.</jats:p>","abstract \n the coronavirus disease 2019 (covid-19) pandemic was declared a public health emergency of international concern by the world health organization. covid-19 has high transmissibility and could result in acute lung injury in a fraction of patients. by counterbalancing the activity of the renin-angiotensin system, angiotensin-converting enzyme 2, which is the fusion receptor of the virus, plays a protective role against the development of complications of this viral infection. vitamin\u2009d can induce the expression of angiotensin-converting enzyme 2 and regulate the immune system through different mechanisms. epidemiologic studies of the relationship between vitamin\u2009d and various respiratory infections were reviewed and, here, the postulated mechanisms and clinical data supporting the protective role of vitamin\u2009d against covid-19–mediated complications are discussed."
http://orkg.org/orkg/resource/R151036,A hybrid AI approach for supporting clinical diagnosis of attention deficit hyperactivity disorder (ADHD) in adults,10.1007/s13755-020-00123-7,crossref,"<jats:title>Abstract</jats:title><jats:p>Attention deficit hyperactivity disorder (ADHD) is a neurodevelopmental disorder that includes symptoms such as inattentiveness, hyperactivity and impulsiveness. It is considered as an important public health issue and prevalence of, as well as demand for diagnosis, has increased as awareness of the disease grew over the past years. Supply of specialist medical experts has not kept pace with the increasing demand for assessment, both due to financial pressures on health systems and the difficulty to train new experts, resulting in growing waiting lists. Patients are not being treated quickly enough causing problems in other areas of health systems (e.g. increased GP visits, increased risk of self-harm and accidents) and more broadly (e.g. time off work, relationship problems). Advances in AI make it possible to support the clinical diagnosis of ADHD based on the analysis of relevant data. This paper reports on findings related to the mental health services of a specialist Trust within the UK’s National Health Service (NHS). The analysis studied data of adult patients who underwent diagnosis over the past few years, and developed a hybrid approach, consisting of two different models: a machine learning model obtained by training on data of past cases; and a knowledge model capturing the expertise of medical experts through knowledge engineering. The resulting algorithm has an accuracy of 95% on data currently available, and is currently being tested in a clinical environment.</jats:p>","abstract attention deficit hyperactivity disorder (adhd) is a neurodevelopmental disorder that includes symptoms such as inattentiveness, hyperactivity and impulsiveness. it is considered as an important public health issue and prevalence of, as well as demand for diagnosis, has increased as awareness of the disease grew over the past years. supply of specialist medical experts has not kept pace with the increasing demand for assessment, both due to financial pressures on health systems and the difficulty to train new experts, resulting in growing waiting lists. patients are not being treated quickly enough causing problems in other areas of health systems (e.g. increased gp visits, increased risk of self-harm and accidents) and more broadly (e.g. time off work, relationship problems). advances in ai make it possible to support the clinical diagnosis of adhd based on the analysis of relevant data. this paper reports on findings related to the mental health services of a specialist trust within the uk’s national health service (nhs). the analysis studied data of adult patients who underwent diagnosis over the past few years, and developed a hybrid approach, consisting of two different models: a machine learning model obtained by training on data of past cases; and a knowledge model capturing the expertise of medical experts through knowledge engineering. the resulting algorithm has an accuracy of 95% on data currently available, and is currently being tested in a clinical environment."
http://orkg.org/orkg/resource/R150063,Selected Attractiveness Factors of Academic Conferences as a Product on the International Tourism Market,10.18778/0867-5856.30.1.13,crossref,"<jats:p>The paper identifies selected determinants of attractiveness of international academic conferences as products on the contemporary international tourism market, especially on the business tourism market. To achieve this aim, methods of analysing the literature, reports, synthesis, along with passive and active observation were used. In addition, a direct survey using a research questionnaire via a website and addressed to participants on an Erasmus International Week in Kaunas was made. Some of the most important conditions of the attractiveness of the conference for participants, including transport and information accessibility as well as to the originality of the destination and the leisure program for participants, were indicated. Contemporary international academic conferences lasting a few days epitomize multiple products on tourism market. The article is empirical, but it also presents ideas for the development of conferences as attractive products in the contemporary economy of the European Union.</jats:p>","the paper identifies selected determinants of attractiveness of international academic conferences as products on the contemporary international tourism market, especially on the business tourism market. to achieve this aim, methods of analysing the literature, reports, synthesis, along with passive and active observation were used. in addition, a direct survey using a research questionnaire via a website and addressed to participants on an erasmus international week in kaunas was made. some of the most important conditions of the attractiveness of the conference for participants, including transport and information accessibility as well as to the originality of the destination and the leisure program for participants, were indicated. contemporary international academic conferences lasting a few days epitomize multiple products on tourism market. the article is empirical, but it also presents ideas for the development of conferences as attractive products in the contemporary economy of the european union."
http://orkg.org/orkg/resource/R149792,Changing competences of public managers: tensions in commitment,10.1108/09513550010350300,crossref,"<jats:p>The literature on managerial competences has not sufficiently addressed the value contents of competences and the generic features of public managers. This article presents a model of five competence areas: task competence, professional competence in substantive policy field, professional competence in administration, political competence and ethical competence. Each competence area includes both value and instrumental competences. Relatively permanent value competences are understood as commitments. The assumptions of new public management question not only the instrumental competences but also the commitments of traditional public service. The efficacy of human resource development is limited in learning new commitments. Apart from structural reforms that speed up the process, the friction in the change of commitments is seen as slow cultural change in many public organisations. This is expressed by transitional tensions in task commitment, professional commitment, political commitment, and ethical commitment of public managers.</jats:p>","the literature on managerial competences has not sufficiently addressed the value contents of competences and the generic features of public managers. this article presents a model of five competence areas: task competence, professional competence in substantive policy field, professional competence in administration, political competence and ethical competence. each competence area includes both value and instrumental competences. relatively permanent value competences are understood as commitments. the assumptions of new public management question not only the instrumental competences but also the commitments of traditional public service. the efficacy of human resource development is limited in learning new commitments. apart from structural reforms that speed up the process, the friction in the change of commitments is seen as slow cultural change in many public organisations. this is expressed by transitional tensions in task commitment, professional commitment, political commitment, and ethical commitment of public managers."
http://orkg.org/orkg/resource/R149787,Identifying Collaborative Competencies,10.1177/0734371x08315434,crossref,"<jats:p> Increasingly, federal organizations must work together with other organizations to jointly produce public value. Thus, it is important for public employees to develop critical collaborative skills. The National Academy of Public Administration affirmed this by calling for a focus on collaborative competencies, but the question remained: What are collaborative competencies? Many skills are theoretically connected to collaboration, but these links have not been tested empirically. Following the methodology developed by McClelland and furthered by Spencer and Spencer, this article presents the results of a collaborative competency study. This investigation involved the use of matched criterion samples (superior versus average collaborators) from the federal government. Individuals in the criterion samples were interviewed using the behavioral event interview design to identify differentiating competencies and create a competency model for future validation. </jats:p>","increasingly, federal organizations must work together with other organizations to jointly produce public value. thus, it is important for public employees to develop critical collaborative skills. the national academy of public administration affirmed this by calling for a focus on collaborative competencies, but the question remained: what are collaborative competencies? many skills are theoretically connected to collaboration, but these links have not been tested empirically. following the methodology developed by mcclelland and furthered by spencer and spencer, this article presents the results of a collaborative competency study. this investigation involved the use of matched criterion samples (superior versus average collaborators) from the federal government. individuals in the criterion samples were interviewed using the behavioral event interview design to identify differentiating competencies and create a competency model for future validation."
http://orkg.org/orkg/resource/R149244,Building new competencies for government administrators and managers in an era of public sector reforms: the case of Mozambique,10.1177/0020852310381803,crossref,"<jats:p> African public administration today is mixed with elements of the old bureaucratic model continuing alongside the new public management (NPM). The increasing application of the NPM approach has placed public administration and management systems in the spotlight and raised a number of challenges. Among them are the relevance of policy importation and the availability of civil servants with the requisite competence to perform the very critical responsibilities of government that reforms introduce. Using the case of Mozambique, this article shows that implementation of public sector reforms has brought in its trail considerable gaps between reform strategies and the competences needed to execute them. Convinced that competences of public administrators are a vital prerequisite for the success of reforms, the government of Mozambique has instituted a series of training programmes to provide the kind of competences that would reflect the new demands and realities facing the public sector. Though it is too early to expect results, the article concludes that the technical, managerial and leadership skills of public administrators and managers are being improved through better training curricula than were provided in the past. </jats:p><jats:p> Points for practitioners </jats:p><jats:p> The structure, functions, and processes of public administration and management have undergone remarkable changes as a result of NPM approaches. But as new approaches are being introduced, government managers have found themselves trying very hard to manage using old skills. In almost every profession, new circumstances require the development of new, or a redefinition of existing, skills. Institutionalizing new training curricula to provide technical, managerial and leadership competence for government administrators has become imperative now more than ever before. The caveat is that building new competencies will not necessarily fix all the problems unless other structural problems such as remuneration, promotion and utilization of ex-trainees are also addressed. </jats:p>","african public administration today is mixed with elements of the old bureaucratic model continuing alongside the new public management (npm). the increasing application of the npm approach has placed public administration and management systems in the spotlight and raised a number of challenges. among them are the relevance of policy importation and the availability of civil servants with the requisite competence to perform the very critical responsibilities of government that reforms introduce. using the case of mozambique, this article shows that implementation of public sector reforms has brought in its trail considerable gaps between reform strategies and the competences needed to execute them. convinced that competences of public administrators are a vital prerequisite for the success of reforms, the government of mozambique has instituted a series of training programmes to provide the kind of competences that would reflect the new demands and realities facing the public sector. though it is too early to expect results, the article concludes that the technical, managerial and leadership skills of public administrators and managers are being improved through better training curricula than were provided in the past. points for practitioners the structure, functions, and processes of public administration and management have undergone remarkable changes as a result of npm approaches. but as new approaches are being introduced, government managers have found themselves trying very hard to manage using old skills. in almost every profession, new circumstances require the development of new, or a redefinition of existing, skills. institutionalizing new training curricula to provide technical, managerial and leadership competence for government administrators has become imperative now more than ever before. the caveat is that building new competencies will not necessarily fix all the problems unless other structural problems such as remuneration, promotion and utilization of ex-trainees are also addressed."
http://orkg.org/orkg/resource/R149178,Correlation between Ferumoxytol Uptake in Tumor Lesions by MRI and Response to Nanoliposomal Irinotecan in Patients with Advanced Solid Tumors: A Pilot Study,10.1158/1078-0432.ccr-16-1990,crossref,"""<jats:title>Abstract</jats:title><jats:p>Purpose: To determine whether deposition characteristics of ferumoxytol (FMX) iron nanoparticles in tumors, identified by quantitative MRI, may predict tumor lesion response to nanoliposomal irinotecan (nal-IRI).</jats:p><jats:p>Experimental Design: Eligible patients with previously treated solid tumors had FMX-MRI scans before and following (1, 24, and 72 hours) FMX injection. After MRI acquisition, R2* signal was used to calculate FMX levels in plasma, reference tissue, and tumor lesions by comparison with a phantom-based standard curve. Patients then received nal-IRI (70 mg/m2 free base strength) biweekly until progression. Two percutaneous core biopsies were collected from selected tumor lesions 72 hours after FMX or nal-IRI.</jats:p><jats:p>Results: Iron particle levels were quantified by FMX-MRI in plasma, reference tissues, and tumor lesions in 13 of 15 eligible patients. On the basis of a mechanistic pharmacokinetic model, tissue permeability to FMX correlated with early FMX-MRI signals at 1 and 24 hours, while FMX tissue binding contributed at 72 hours. Higher FMX levels (ranked relative to median value of multiple evaluable lesions from 9 patients) were significantly associated with reduction in lesion size by RECIST v1.1 at early time points (P &amp;lt; 0.001 at 1 hour and P &amp;lt; 0.003 at 24 hours FMX-MRI, one-way ANOVA). No association was observed with post-FMX levels at 72 hours. Irinotecan drug levels in lesions correlated with patient's time on treatment (Spearman ρ = 0.7824; P = 0.0016).</jats:p><jats:p>Conclusions: Correlation between FMX levels in tumor lesions and nal-IRI activity suggests that lesion permeability to FMX and subsequent tumor uptake may be a useful noninvasive and predictive biomarker for nal-IRI response in patients with solid tumors. Clin Cancer Res; 23(14); 3638–48. ©2017 AACR.</jats:p>""",""" abstract purpose: to determine whether deposition characteristics of ferumoxytol (fmx) iron nanoparticles in tumors, identified by quantitative mri, may predict tumor lesion response to nanoliposomal irinotecan (nal-iri). experimental design: eligible patients with previously treated solid tumors had fmx-mri scans before and following (1, 24, and 72 hours) fmx injection. after mri acquisition, r2* signal was used to calculate fmx levels in plasma, reference tissue, and tumor lesions by comparison with a phantom-based standard curve. patients then received nal-iri (70 mg/m2 free base strength) biweekly until progression. two percutaneous core biopsies were collected from selected tumor lesions 72 hours after fmx or nal-iri. results: iron particle levels were quantified by fmx-mri in plasma, reference tissues, and tumor lesions in 13 of 15 eligible patients. on the basis of a mechanistic pharmacokinetic model, tissue permeability to fmx correlated with early fmx-mri signals at 1 and 24 hours, while fmx tissue binding contributed at 72 hours. higher fmx levels (ranked relative to median value of multiple evaluable lesions from 9 patients) were significantly associated with reduction in lesion size by recist v1.1 at early time points (p &amp;lt; 0.001 at 1 hour and p &amp;lt; 0.003 at 24 hours fmx-mri, one-way anova). no association was observed with post-fmx levels at 72 hours. irinotecan drug levels in lesions correlated with patient's time on treatment (spearman ρ = 0.7824; p = 0.0016). conclusions: correlation between fmx levels in tumor lesions and nal-iri activity suggests that lesion permeability to fmx and subsequent tumor uptake may be a useful noninvasive and predictive biomarker for nal-iri response in patients with solid tumors. clin cancer res; 23(14); 3638–48. ©2017 aacr. """
http://orkg.org/orkg/resource/R149031,Developing E-Government Coursework through the NASPAA Competencies Framework,10.1177/0144739417690582,crossref,"<jats:p> Information technology (IT) is often less emphasized in coursework related to public administration education, despite the growing need for technological capabilities in those joining the public sector workforce. This coupled with a lesser emphasis on e-government/IT skills by accreditation standards adds to the widening gap between theory and practice in the field. This study examines the emphasis placed on e-government/IT concepts in Master of Public Administration (MPA) and Master of Public Policy (MPP) programs, either through complete course offerings or through related courses such as public management, strategic planning, performance measurement and organization theory. Based on a content analysis of their syllabi, the paper analyzes the extent to which the IT/e-government courses in MPA/Master of Public Policy programs address the Network of Schools of Public Policy, Affairs, and Administration competency standards, and further discuss the orientation of the courses with two of the competencies: management and policy. Specifically, are e-government/IT courses more management-oriented or policy-oriented? Do public management, strategic planning, performance measurement, and organization theory courses address IT concerns? </jats:p>","information technology (it) is often less emphasized in coursework related to public administration education, despite the growing need for technological capabilities in those joining the public sector workforce. this coupled with a lesser emphasis on e-government/it skills by accreditation standards adds to the widening gap between theory and practice in the field. this study examines the emphasis placed on e-government/it concepts in master of public administration (mpa) and master of public policy (mpp) programs, either through complete course offerings or through related courses such as public management, strategic planning, performance measurement and organization theory. based on a content analysis of their syllabi, the paper analyzes the extent to which the it/e-government courses in mpa/master of public policy programs address the network of schools of public policy, affairs, and administration competency standards, and further discuss the orientation of the courses with two of the competencies: management and policy. specifically, are e-government/it courses more management-oriented or policy-oriented? do public management, strategic planning, performance measurement, and organization theory courses address it concerns?"
http://orkg.org/orkg/resource/R148112,"2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text",10.1136/amiajnl-2011-000203,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the VA provided an annotated reference standard corpus for the three tasks. Using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification.</jats:p>\n               <jats:p>These systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. Depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. Ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate.</jats:p>","abstract \n the 2010 i2b2/va workshop on natural language processing challenges for clinical records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the va provided an annotated reference standard corpus for the three tasks. using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification. \n these systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate."
http://orkg.org/orkg/resource/R148246,"Design, synthesis, and structural characterization of the first dithienocyclopentacarbazole-based n-type organic semiconductor and its application in non-fullerene polymer solar cells",10.1039/c7ta01143a,crossref,<p>A novel dithienocyclopentacarbazole-containing n-type organic semiconductor (DTCC–IC) was designed and synthesized as the acceptor for non-fullerene solar cells.</p>,a novel dithienocyclopentacarbazole-containing n-type organic semiconductor (dtcc–ic) was designed and synthesized as the acceptor for non-fullerene solar cells.
http://orkg.org/orkg/resource/R148131,Construction of an annotated corpus to support biomedical information extraction,10.1186/1471-2105-10-349,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Information Extraction (IE) is a component of text mining that facilitates knowledge discovery by automatically locating instances of interesting biomedical events from huge document collections. As events are usually centred on verbs and nominalised verbs, understanding the syntactic and semantic behaviour of these words is highly important. Corpora annotated with information concerning this behaviour can constitute a valuable resource in the training of IE components and resources.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>We have defined a new scheme for annotating sentence-bound gene regulation events, centred on both verbs and nominalised verbs. For each event instance, all participants (<jats:italic>arguments</jats:italic>) in the same sentence are identified and assigned a semantic role from a rich set of 13 roles tailored to biomedical research articles, together with a biological concept type linked to the Gene Regulation Ontology. To our knowledge, our scheme is unique within the biomedical field in terms of the range of event arguments identified. Using the scheme, we have created the Gene Regulation Event Corpus (GREC), consisting of 240 MEDLINE abstracts, in which events relating to gene regulation and expression have been annotated by biologists. A novel method of evaluating various different facets of the annotation task showed that average inter-annotator agreement rates fall within the range of 66% - 90%.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusion</jats:title>\n            <jats:p>The GREC is a unique resource within the biomedical field, in that it annotates not only core relationships between entities, but also a range of other important details about these relationships, e.g., location, temporal, manner and environmental conditions. As such, it is specifically designed to support bio-specific tool and resource development. It has already been used to acquire semantic frames for inclusion within the <jats:italic>BioLexicon</jats:italic> (a lexical, terminological resource to aid biomedical text mining). Initial experiments have also shown that the corpus may viably be used to train IE components, such as semantic role labellers. The corpus and annotation guidelines are freely available for academic purposes.</jats:p>\n          </jats:sec>","abstract \n \n background \n information extraction (ie) is a component of text mining that facilitates knowledge discovery by automatically locating instances of interesting biomedical events from huge document collections. as events are usually centred on verbs and nominalised verbs, understanding the syntactic and semantic behaviour of these words is highly important. corpora annotated with information concerning this behaviour can constitute a valuable resource in the training of ie components and resources. \n \n \n results \n we have defined a new scheme for annotating sentence-bound gene regulation events, centred on both verbs and nominalised verbs. for each event instance, all participants ( arguments ) in the same sentence are identified and assigned a semantic role from a rich set of 13 roles tailored to biomedical research articles, together with a biological concept type linked to the gene regulation ontology. to our knowledge, our scheme is unique within the biomedical field in terms of the range of event arguments identified. using the scheme, we have created the gene regulation event corpus (grec), consisting of 240 medline abstracts, in which events relating to gene regulation and expression have been annotated by biologists. a novel method of evaluating various different facets of the annotation task showed that average inter-annotator agreement rates fall within the range of 66% - 90%. \n \n \n conclusion \n the grec is a unique resource within the biomedical field, in that it annotates not only core relationships between entities, but also a range of other important details about these relationships, e.g., location, temporal, manner and environmental conditions. as such, it is specifically designed to support bio-specific tool and resource development. it has already been used to acquire semantic frames for inclusion within the biolexicon (a lexical, terminological resource to aid biomedical text mining). initial experiments have also shown that the corpus may viably be used to train ie components, such as semantic role labellers. the corpus and annotation guidelines are freely available for academic purposes. \n"
http://orkg.org/orkg/resource/R148204,Halogenated conjugated molecules for ambipolar field-effect transistors and non-fullerene organic solar cells,10.1039/c7qm00025a,crossref,"<p>Halogenated conjugated molecules containing F, Cl, Br and I with high crystallinity were developed to show high electron mobilities of 1.3 cm<sup>2</sup> V<sup>−1</sup> s<sup>−1</sup> in field-effect transistors and power conversion efficiencies above 9% in non-fullerene solar cells.</p>","halogenated conjugated molecules containing f, cl, br and i with high crystallinity were developed to show high electron mobilities of 1.3 cm 2 v −1 s −1 in field-effect transistors and power conversion efficiencies above 9% in non-fullerene solar cells."
http://orkg.org/orkg/resource/R147944,A near-infrared non-fullerene electron acceptor for high performance polymer solar cells,10.1039/c7ee00844a,crossref,"<p>Low-bandgap polymers/molecules are an interesting family of semiconductor materials, and have enabled many recent exciting breakthroughs in the field of organic electronics, especially for organic photovoltaics (OPVs).</p>","low-bandgap polymers/molecules are an interesting family of semiconductor materials, and have enabled many recent exciting breakthroughs in the field of organic electronics, especially for organic photovoltaics (opvs)."
http://orkg.org/orkg/resource/R150061,Informing the Public About a Pandemic,10.1287/mnsc.2021.4016,crossref,"<jats:p> This paper explores how governments may efficiently inform the public about an epidemic to induce compliance with their confinement measures. Using an information design framework, we find the government has an incentive to either downplay or exaggerate the severity of the epidemic if it heavily prioritizes the economy over population health or vice versa. Importantly, we find that the level of economic inequality in the population has an effect on these distortions. The more unequal the disease’s economic impact on the population, the less the government exaggerates and the more it downplays the severity of the epidemic. When the government weighs the economy and population health sufficiently equally, however, the government should always be fully transparent about the severity of the epidemic. </jats:p><jats:p> This paper was accepted by Stefan Scholtes, healthcare management. </jats:p>","this paper explores how governments may efficiently inform the public about an epidemic to induce compliance with their confinement measures. using an information design framework, we find the government has an incentive to either downplay or exaggerate the severity of the epidemic if it heavily prioritizes the economy over population health or vice versa. importantly, we find that the level of economic inequality in the population has an effect on these distortions. the more unequal the disease’s economic impact on the population, the less the government exaggerates and the more it downplays the severity of the epidemic. when the government weighs the economy and population health sufficiently equally, however, the government should always be fully transparent about the severity of the epidemic. this paper was accepted by stefan scholtes, healthcare management."
http://orkg.org/orkg/resource/R147159,Evidence of high N&lt;sub&gt;2&lt;/sub&gt; fixation rates in the temperate northeast Atlantic,10.5194/bg-16-999-2019,crossref,"<jats:p>Abstract. Diazotrophic activity and primary\nproduction (PP) were investigated along two transects (Belgica BG2014/14 and\nGEOVIDE cruises) off the western Iberian Margin and the Bay of Biscay in\nMay\xa02014. Substantial N2 fixation activity was observed at 8 of the\n10 stations sampled, ranging overall from 81 to\n384\u2009µmol\u2009N\u2009m−2\u2009d−1 (0.7 to\n8.2\u2009nmol\u2009N\u2009L−1\u2009d−1), with two sites close to the Iberian Margin\nsituated between 38.8 and 40.7∘\u2009N yielding rates reaching up to 1355\nand 1533\u2009µmol\u2009N\u2009m−2\u2009d−1. Primary production was\nrelatively lower along the Iberian Margin, with rates ranging from 33 to\n59\u2009mmol\u2009C\u2009m−2\u2009d−1, while it increased towards the northwest\naway from the peninsula, reaching as high as\n135\u2009mmol\u2009C\u2009m−2\u2009d−1. In agreement with the area-averaged Chl\xa0a\nsatellite data contemporaneous with our study period, our results revealed\nthat post-bloom conditions prevailed at most sites, while at the\nnorthwesternmost station the bloom was still ongoing. When converted to\ncarbon uptake using Redfield stoichiometry, N2 fixation could\nsupport 1\u2009% to 3\u2009% of daily PP in the euphotic layer at most sites,\nexcept at the two most active sites where this contribution to daily PP could\nreach up to 25\u2009%. At the two sites where N2 fixation activity\nwas the highest, the prymnesiophyte–symbiont Candidatus\nAtelocyanobacterium thalassa (UCYN-A) dominated the nifH sequence\npool, while the remaining recovered sequences belonged to non-cyanobacterial\nphylotypes. At all the other sites, however, the recovered nifH\nsequences were exclusively assigned phylogenetically to non-cyanobacterial\nphylotypes. The intense N2 fixation activities recorded at the time\nof our study were likely promoted by the availability of\nphytoplankton-derived organic matter produced during the spring bloom, as\nevidenced by the significant surface particulate organic carbon\nconcentrations. Also, the presence of excess phosphorus signature in surface\nwaters seemed to contribute to sustaining N2 fixation, particularly\nat the sites with extreme activities. These results provide a mechanistic\nunderstanding of the unexpectedly high N2 fixation in productive\nwaters of the temperate North Atlantic and highlight the importance of\nN2 fixation for future assessment of the global N inventory.\n                    </jats:p>","abstract. diazotrophic activity and primary\nproduction (pp) were investigated along two transects (belgica bg2014/14 and\ngeovide cruises) off the western iberian margin and the bay of biscay in\nmay\xa02014. substantial n2 fixation activity was observed at 8 of the\n10 stations sampled, ranging overall from 81 to\n384\u2009µmol\u2009n\u2009m−2\u2009d−1 (0.7 to\n8.2\u2009nmol\u2009n\u2009l−1\u2009d−1), with two sites close to the iberian margin\nsituated between 38.8 and 40.7∘\u2009n yielding rates reaching up to 1355\nand 1533\u2009µmol\u2009n\u2009m−2\u2009d−1. primary production was\nrelatively lower along the iberian margin, with rates ranging from 33 to\n59\u2009mmol\u2009c\u2009m−2\u2009d−1, while it increased towards the northwest\naway from the peninsula, reaching as high as\n135\u2009mmol\u2009c\u2009m−2\u2009d−1. in agreement with the area-averaged chl\xa0a\nsatellite data contemporaneous with our study period, our results revealed\nthat post-bloom conditions prevailed at most sites, while at the\nnorthwesternmost station the bloom was still ongoing. when converted to\ncarbon uptake using redfield stoichiometry, n2 fixation could\nsupport 1\u2009% to 3\u2009% of daily pp in the euphotic layer at most sites,\nexcept at the two most active sites where this contribution to daily pp could\nreach up to 25\u2009%. at the two sites where n2 fixation activity\nwas the highest, the prymnesiophyte–symbiont candidatus\natelocyanobacterium thalassa (ucyn-a) dominated the nifh sequence\npool, while the remaining recovered sequences belonged to non-cyanobacterial\nphylotypes. at all the other sites, however, the recovered nifh\nsequences were exclusively assigned phylogenetically to non-cyanobacterial\nphylotypes. the intense n2 fixation activities recorded at the time\nof our study were likely promoted by the availability of\nphytoplankton-derived organic matter produced during the spring bloom, as\nevidenced by the significant surface particulate organic carbon\nconcentrations. also, the presence of excess phosphorus signature in surface\nwaters seemed to contribute to sustaining n2 fixation, particularly\nat the sites with extreme activities. these results provide a mechanistic\nunderstanding of the unexpectedly high n2 fixation in productive\nwaters of the temperate north atlantic and highlight the importance of\nn2 fixation for future assessment of the global n inventory.\n"
http://orkg.org/orkg/resource/R147175,Niche partitioning by photosynthetic plankton as a driver of CO2-fixation across the oligotrophic South Pacific Subtropical Ocean,10.1038/s41396-021-01072-z,crossref,"<jats:title>Abstract</jats:title><jats:p>Oligotrophic ocean gyre ecosystems may be expanding due to rising global temperatures [1–5]. Models predicting carbon flow through these changing ecosystems require accurate descriptions of phytoplankton communities and their metabolic activities [6]. We therefore measured distributions and activities of cyanobacteria and small photosynthetic eukaryotes throughout the euphotic zone on a zonal transect through the South Pacific Ocean, focusing on the ultraoligotrophic waters of the South Pacific Gyre (SPG). Bulk rates of CO<jats:sub>2</jats:sub> fixation were low (0.1\u2009µmol\u2009C\u2009l<jats:sup>−1</jats:sup>\u2009d<jats:sup>−1</jats:sup>) but pervasive throughout both the surface mixed-layer (upper 150\u2009m), as well as the deep chlorophyll <jats:italic>a</jats:italic> maximum of the core SPG. Chloroplast 16S rRNA metabarcoding, and single-cell <jats:sup>13</jats:sup>CO<jats:sub>2</jats:sub> uptake experiments demonstrated niche differentiation among the small eukaryotes and picocyanobacteria. <jats:italic>Prochlorococcus</jats:italic> abundances, activity, and growth were more closely associated with the rims of the gyre. Small, fast-growing, photosynthetic eukaryotes, likely related to the Pelagophyceae, characterized the deep chlorophyll <jats:italic>a</jats:italic> maximum. In contrast, a slower growing population of photosynthetic eukaryotes, likely comprised of Dictyochophyceae and Chrysophyceae, dominated the mixed layer that contributed 65–88% of the areal CO<jats:sub>2</jats:sub> fixation within the core SPG. Small photosynthetic eukaryotes may thus play an underappreciated role in CO<jats:sub>2</jats:sub> fixation in the surface mixed-layer waters of ultraoligotrophic ecosystems.</jats:p>","abstract oligotrophic ocean gyre ecosystems may be expanding due to rising global temperatures [1–5]. models predicting carbon flow through these changing ecosystems require accurate descriptions of phytoplankton communities and their metabolic activities [6]. we therefore measured distributions and activities of cyanobacteria and small photosynthetic eukaryotes throughout the euphotic zone on a zonal transect through the south pacific ocean, focusing on the ultraoligotrophic waters of the south pacific gyre (spg). bulk rates of co 2 fixation were low (0.1\u2009µmol\u2009c\u2009l −1 \u2009d −1 ) but pervasive throughout both the surface mixed-layer (upper 150\u2009m), as well as the deep chlorophyll a maximum of the core spg. chloroplast 16s rrna metabarcoding, and single-cell 13 co 2 uptake experiments demonstrated niche differentiation among the small eukaryotes and picocyanobacteria. prochlorococcus abundances, activity, and growth were more closely associated with the rims of the gyre. small, fast-growing, photosynthetic eukaryotes, likely related to the pelagophyceae, characterized the deep chlorophyll a maximum. in contrast, a slower growing population of photosynthetic eukaryotes, likely comprised of dictyochophyceae and chrysophyceae, dominated the mixed layer that contributed 65–88% of the areal co 2 fixation within the core spg. small photosynthetic eukaryotes may thus play an underappreciated role in co 2 fixation in the surface mixed-layer waters of ultraoligotrophic ecosystems."
http://orkg.org/orkg/resource/R147181,Contribution of picoplankton to the total particulate organic carbon (POC) concentration in the eastern South Pacific,10.5194/bgd-4-1461-2007,crossref,"<jats:p>Abstract. Prochlorococcus, Synechococcus, picophytoeukaryotes and bacterioplankton abundances and contributions to the total particulate organic carbon concentration (POC), derived from the total particle beam attenuation coefficient (cp), were determined across the eastern South Pacific between the Marquesas Islands and the coast of Chile. All flow cytometrically derived abundances decreased towards the hyper-oligotrophic centre of the gyre and were highest at the coast, except for Prochlorococcus, which is not detected under eutrophic conditions. Temperature and nutrient availability appeared important in modulating picophytoplankton abundance, according to the prevailing trophic conditions. Although the non-vegetal particles tended to dominate the cp signal everywhere along the transect (50 to 83%), this dominance seemed to weaken from oligo- to eutrophic conditions, the contributions by vegetal and non-vegetal particles being about equal under mature upwelling conditions. Spatial variability in the vegetal compartment was more important than the non-vegetal one in shaping the water column particulate attenuation coefficient. Spatial variability in picophytoplankton biomass could be traced by changes in both Tchla and cp. Finally, picophytoeukaryotes contributed with ~38% on average to the total integrated phytoplankton carbon biomass or vegetal attenuation signal along the transect, as determined by direct size measurements on cells sorted by flow cytometry and optical theory. The role of picophytoeukaryotes in carbon and energy flow would therefore be very important, even under hyper-oligotrophic conditions.\n                        </jats:p>","abstract. prochlorococcus, synechococcus, picophytoeukaryotes and bacterioplankton abundances and contributions to the total particulate organic carbon concentration (poc), derived from the total particle beam attenuation coefficient (cp), were determined across the eastern south pacific between the marquesas islands and the coast of chile. all flow cytometrically derived abundances decreased towards the hyper-oligotrophic centre of the gyre and were highest at the coast, except for prochlorococcus, which is not detected under eutrophic conditions. temperature and nutrient availability appeared important in modulating picophytoplankton abundance, according to the prevailing trophic conditions. although the non-vegetal particles tended to dominate the cp signal everywhere along the transect (50 to 83%), this dominance seemed to weaken from oligo- to eutrophic conditions, the contributions by vegetal and non-vegetal particles being about equal under mature upwelling conditions. spatial variability in the vegetal compartment was more important than the non-vegetal one in shaping the water column particulate attenuation coefficient. spatial variability in picophytoplankton biomass could be traced by changes in both tchla and cp. finally, picophytoeukaryotes contributed with ~38% on average to the total integrated phytoplankton carbon biomass or vegetal attenuation signal along the transect, as determined by direct size measurements on cells sorted by flow cytometry and optical theory. the role of picophytoeukaryotes in carbon and energy flow would therefore be very important, even under hyper-oligotrophic conditions.\n"
http://orkg.org/orkg/resource/R147188,Phytoplankton Growth and Productivity in the Western North Atlantic: Observations of Regional Variability From the NAAMES Field Campaigns,10.3389/fmars.2020.00024,crossref,"<jats:p>The ability to quantify spatio-temporal variability in phytoplankton growth and productivity is essential to improving our understanding of global carbon dynamics and trophic energy flow. Satellite-based observations offered the first opportunity to estimate depth-integrated net primary production (NPP) at a global scale, but early modeling approaches could not effectively address variability in algal physiology, particularly the effects of photoacclimation on changes in cellular chlorophyll. Here, a previously developed photoacclimation model was used to derive depth-resolved estimates of phytoplankton division rate (μ) and NPP. The new approach predicts NPP values that closely match discrete measurements of <jats:sup>14</jats:sup>C-based NPP and effectively captured both spatial and temporal variability observed during the four field campaigns of the North Atlantic Aerosols and Marine Ecosystems Study (NAAMES). We observed favorable growth conditions for phytoplankton throughout the annual cycle in the subtropical western North Atlantic. As a result, high rates of μ are sustained year-round resulting in a strong coupling between growth and loss processes and a more moderate spring bloom compared to the high-latitude subarctic region. Considerable light limitation was observed in the subarctic province during the winter, which resulted in divergent growth dynamics, stronger decoupling from grazing pressure and a taxonomically distinct phytoplankton community. This study demonstrates how detailed knowledge of phytoplankton division rate furthers our understanding of global carbon cycling by providing insight into the resulting influence on phytoplankton taxonomy and the loss processes that dictate the fate of fixed carbon.</jats:p>","the ability to quantify spatio-temporal variability in phytoplankton growth and productivity is essential to improving our understanding of global carbon dynamics and trophic energy flow. satellite-based observations offered the first opportunity to estimate depth-integrated net primary production (npp) at a global scale, but early modeling approaches could not effectively address variability in algal physiology, particularly the effects of photoacclimation on changes in cellular chlorophyll. here, a previously developed photoacclimation model was used to derive depth-resolved estimates of phytoplankton division rate (μ) and npp. the new approach predicts npp values that closely match discrete measurements of 14 c-based npp and effectively captured both spatial and temporal variability observed during the four field campaigns of the north atlantic aerosols and marine ecosystems study (naames). we observed favorable growth conditions for phytoplankton throughout the annual cycle in the subtropical western north atlantic. as a result, high rates of μ are sustained year-round resulting in a strong coupling between growth and loss processes and a more moderate spring bloom compared to the high-latitude subarctic region. considerable light limitation was observed in the subarctic province during the winter, which resulted in divergent growth dynamics, stronger decoupling from grazing pressure and a taxonomically distinct phytoplankton community. this study demonstrates how detailed knowledge of phytoplankton division rate furthers our understanding of global carbon cycling by providing insight into the resulting influence on phytoplankton taxonomy and the loss processes that dictate the fate of fixed carbon."
http://orkg.org/orkg/resource/R147246,PEG-g-chitosan nanoparticles functionalized with the monoclonal antibody OX26 for brain drug targeting,10.2217/nnm.15.29,crossref,"<jats:p> Aim: Drug targeting to the CNS is challenging due to the presence of blood–brain barrier. We investigated chitosan (Cs) nanoparticles (NPs) as drug transporter system across the blood–brain barrier, based on mAb OX26 modified Cs. Materials &amp; methods: Cs NPs functionalized with PEG, modified and unmodified with OX26 (Cs-PEG-OX26) were prepared and chemico-physically characterized. These NPs were administered (intraperitoneal) in mice to define their ability to reach the brain. Results: Brain uptake of OX26-conjugated NPs is much higher than of unmodified NPs, because: long-circulating abilities (conferred by PEG), interaction between cationic Cs and brain endothelium negative charges and OX26 TfR receptor affinity. Conclusion: Cs-PEG-OX26 NPs are promising drug delivery system to the CNS. </jats:p>","aim: drug targeting to the cns is challenging due to the presence of blood–brain barrier. we investigated chitosan (cs) nanoparticles (nps) as drug transporter system across the blood–brain barrier, based on mab ox26 modified cs. materials &amp; methods: cs nps functionalized with peg, modified and unmodified with ox26 (cs-peg-ox26) were prepared and chemico-physically characterized. these nps were administered (intraperitoneal) in mice to define their ability to reach the brain. results: brain uptake of ox26-conjugated nps is much higher than of unmodified nps, because: long-circulating abilities (conferred by peg), interaction between cationic cs and brain endothelium negative charges and ox26 tfr receptor affinity. conclusion: cs-peg-ox26 nps are promising drug delivery system to the cns."
http://orkg.org/orkg/resource/R149004,Multi-frequency radiation of dissipative solitons in optical fiber cavities,10.1038/s41598-020-65426-x,crossref,<jats:title>Abstract</jats:title><jats:p>New resonant emission of dispersive waves by oscillating solitary structures in optical fiber cavities is considered analytically and numerically. The pulse propagation is described in the framework of the Lugiato-Lefever equation when a Hopf-bifurcation can result in the formation of oscillating dissipative solitons. The resonance condition for the radiation of the dissipative oscillating solitons is derived and it is demonstrated that the predicted resonances match the spectral lines observed in numerical simulations perfectly. The complex recoil of the radiation on the soliton dynamics is discussed. The reported effect can have importance for the generation of frequency combs in nonlinear microring resonators.</jats:p>,abstract new resonant emission of dispersive waves by oscillating solitary structures in optical fiber cavities is considered analytically and numerically. the pulse propagation is described in the framework of the lugiato-lefever equation when a hopf-bifurcation can result in the formation of oscillating dissipative solitons. the resonance condition for the radiation of the dissipative oscillating solitons is derived and it is demonstrated that the predicted resonances match the spectral lines observed in numerical simulations perfectly. the complex recoil of the radiation on the soliton dynamics is discussed. the reported effect can have importance for the generation of frequency combs in nonlinear microring resonators.
http://orkg.org/orkg/resource/R146945,Design and In Silico Evaluation of a Closed-Loop Hemorrhage Resuscitation Algorithm With Blood Pressure as Controlled Variable,10.1115/1.4052312,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>This paper concerns the design and rigorous in silico evaluation of a closed-loop hemorrhage resuscitation algorithm with blood pressure (BP) as controlled variable. A lumped-parameter control design model relating volume resuscitation input to blood volume (BV) and BP responses was developed and experimentally validated. Then, three alternative adaptive control algorithms were developed using the control design model: (i) model reference adaptive control (MRAC) with BP feedback, (ii) composite adaptive control (CAC) with BP feedback, and (iii) CAC with BV and BP feedback. To the best of our knowledge, this is the first work to demonstrate model-based control design for hemorrhage resuscitation with readily available BP as feedback. The efficacy of these closed-loop control algorithms was comparatively evaluated as well as compared with an empiric expert knowledge-based algorithm based on 100 realistic virtual patients created using a well-established physiological model of cardiovascular (CV) hemodynamics. The in silico evaluation results suggested that the adaptive control algorithms outperformed the knowledge-based algorithm in terms of both accuracy and robustness in BP set point tracking: the average median performance error (MDPE) and median absolute performance error (MDAPE) were significantly smaller by &amp;gt;99% and &amp;gt;91%, and as well, their interindividual variability was significantly smaller by &amp;gt;88% and &amp;gt;94%. Pending in vivo evaluation, model-based control design may advance the medical autonomy in closed-loop hemorrhage resuscitation.</jats:p>","abstract \n this paper concerns the design and rigorous in silico evaluation of a closed-loop hemorrhage resuscitation algorithm with blood pressure (bp) as controlled variable. a lumped-parameter control design model relating volume resuscitation input to blood volume (bv) and bp responses was developed and experimentally validated. then, three alternative adaptive control algorithms were developed using the control design model: (i) model reference adaptive control (mrac) with bp feedback, (ii) composite adaptive control (cac) with bp feedback, and (iii) cac with bv and bp feedback. to the best of our knowledge, this is the first work to demonstrate model-based control design for hemorrhage resuscitation with readily available bp as feedback. the efficacy of these closed-loop control algorithms was comparatively evaluated as well as compared with an empiric expert knowledge-based algorithm based on 100 realistic virtual patients created using a well-established physiological model of cardiovascular (cv) hemodynamics. the in silico evaluation results suggested that the adaptive control algorithms outperformed the knowledge-based algorithm in terms of both accuracy and robustness in bp set point tracking: the average median performance error (mdpe) and median absolute performance error (mdape) were significantly smaller by &amp;gt;99% and &amp;gt;91%, and as well, their interindividual variability was significantly smaller by &amp;gt;88% and &amp;gt;94%. pending in vivo evaluation, model-based control design may advance the medical autonomy in closed-loop hemorrhage resuscitation."
http://orkg.org/orkg/resource/R148039,GENETAG: a tagged corpus for gene/protein named entity recognition,10.1186/1471-2105-6-s1-s3,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Named entity recognition (NER) is an important first step for text mining the biomedical literature. Evaluating the performance of biomedical NER systems is impossible without a standardized test corpus. The annotation of such a corpus for gene/protein name NER is a difficult process due to the complexity of gene/protein names. We describe the construction and annotation of GENETAG, a corpus of 20K MEDLINE<jats:sup>®</jats:sup> sentences for gene/protein NER. 15K GENETAG sentences were used for the BioCreAtIvE Task 1A Competition.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>To ensure heterogeneity of the corpus, MEDLINE sentences were first scored for term similarity to documents with known gene names, and 10K high- and 10K low-scoring sentences were chosen at random. The original 20K sentences were run through a gene/protein name tagger, and the results were modified manually to reflect a wide definition of gene/protein names subject to a specificity constraint, a rule that required the tagged entities to refer to specific entities. Each sentence in GENETAG was annotated with acceptable alternatives to the gene/protein names it contained, allowing for partial matching with semantic constraints. Semantic constraints are rules requiring the tagged entity to contain its true meaning in the sentence context. Application of these constraints results in a more meaningful measure of the performance of an NER system than unrestricted partial matching.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusion</jats:title>\n            <jats:p>The annotation of GENETAG required intricate manual judgments by annotators which hindered tagging consistency. The data were pre-segmented into words, to provide indices supporting comparison of system responses to the ""gold standard"". However, character-based indices would have been more robust than word-based indices. GENETAG Train, Test and Round1 data and ancillary programs are freely available at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""ftp://ftp.ncbi.nlm.nih.gov/pub/tanabe/GENETAG.tar.gz"" ext-link-type=""uri"">ftp://ftp.ncbi.nlm.nih.gov/pub/tanabe/GENETAG.tar.gz</jats:ext-link>. A newer version of GENETAG-05, will be released later this year.</jats:p>\n          </jats:sec>","abstract \n \n background \n named entity recognition (ner) is an important first step for text mining the biomedical literature. evaluating the performance of biomedical ner systems is impossible without a standardized test corpus. the annotation of such a corpus for gene/protein name ner is a difficult process due to the complexity of gene/protein names. we describe the construction and annotation of genetag, a corpus of 20k medline ® sentences for gene/protein ner. 15k genetag sentences were used for the biocreative task 1a competition. \n \n \n results \n to ensure heterogeneity of the corpus, medline sentences were first scored for term similarity to documents with known gene names, and 10k high- and 10k low-scoring sentences were chosen at random. the original 20k sentences were run through a gene/protein name tagger, and the results were modified manually to reflect a wide definition of gene/protein names subject to a specificity constraint, a rule that required the tagged entities to refer to specific entities. each sentence in genetag was annotated with acceptable alternatives to the gene/protein names it contained, allowing for partial matching with semantic constraints. semantic constraints are rules requiring the tagged entity to contain its true meaning in the sentence context. application of these constraints results in a more meaningful measure of the performance of an ner system than unrestricted partial matching. \n \n \n conclusion \n the annotation of genetag required intricate manual judgments by annotators which hindered tagging consistency. the data were pre-segmented into words, to provide indices supporting comparison of system responses to the ""gold standard"". however, character-based indices would have been more robust than word-based indices. genetag train, test and round1 data and ancillary programs are freely available at ftp://ftp.ncbi.nlm.nih.gov/pub/tanabe/genetag.tar.gz . a newer version of genetag-05, will be released later this year. \n"
http://orkg.org/orkg/resource/R147153,Phosphate Depletion in the Western North Atlantic Ocean,10.1126/science.289.5480.759,crossref,"<jats:p>\n            Surface waters of the subtropical Sargasso Sea contain dissolved inorganic phosphate (DIP) concentrations of 0.2 to 1.0 nanomolar, which are sufficiently low to result in phosphorus control of primary production. The DIP concentrations in this area (which receives high inputs of iron-rich dust from arid regions of North Africa) are one to two orders of magnitude lower than surface levels in the North Pacific (where eolian iron inputs are much lower and water column denitrification is much more substantial). These data indicate a severe relative phosphorus depletion in the Atlantic. We hypothesize that nitrogen versus phosphorus limitation of primary production in the present-day ocean may be closely linked to iron supply through control of dinitrogen (N\n            <jats:sub>2</jats:sub>\n            ) fixation, an iron-intensive metabolic process. Although the oceanic phosphorus inventory may set the upper limit for the total amount of organic matter produced in the ocean over geological time scales, at any instant in geological time, oceanic primary production may fall below this limit because of a persistent insufficient iron supply. By controlling N\n            <jats:sub>2</jats:sub>\n            fixation, iron may control not only nitrogen versus phosphorus limitation but also carbon fixation and export stoichiometry and hence biological sequestration of atmospheric carbon dioxide.\n          </jats:p>","\n surface waters of the subtropical sargasso sea contain dissolved inorganic phosphate (dip) concentrations of 0.2 to 1.0 nanomolar, which are sufficiently low to result in phosphorus control of primary production. the dip concentrations in this area (which receives high inputs of iron-rich dust from arid regions of north africa) are one to two orders of magnitude lower than surface levels in the north pacific (where eolian iron inputs are much lower and water column denitrification is much more substantial). these data indicate a severe relative phosphorus depletion in the atlantic. we hypothesize that nitrogen versus phosphorus limitation of primary production in the present-day ocean may be closely linked to iron supply through control of dinitrogen (n\n 2 \n ) fixation, an iron-intensive metabolic process. although the oceanic phosphorus inventory may set the upper limit for the total amount of organic matter produced in the ocean over geological time scales, at any instant in geological time, oceanic primary production may fall below this limit because of a persistent insufficient iron supply. by controlling n\n 2 \n fixation, iron may control not only nitrogen versus phosphorus limitation but also carbon fixation and export stoichiometry and hence biological sequestration of atmospheric carbon dioxide.\n"
http://orkg.org/orkg/resource/R147149,Evidence for efficient regenerated production and dinitrogen fixation in nitrogen-deficient waters of the South Pacific Ocean: impact on new and export production estimates,10.5194/bg-5-323-2008,crossref,"<jats:p>Abstract. One of the major objectives of the BIOSOPE cruise, carried out on the R/V Atalante from October-November 2004 in the South Pacific Ocean, was to establish productivity rates along a zonal section traversing the oligotrophic South Pacific Gyre (SPG). These results were then compared to measurements obtained from the nutrient – replete waters in the Chilean upwelling and around the Marquesas Islands. A dual 13C/15N isotope technique was used to estimate the carbon fixation rates, inorganic nitrogen uptake (including dinitrogen fixation), ammonium (NH4) and nitrate (NO3) regeneration and release of dissolved organic nitrogen (DON). The SPG exhibited the lowest primary production rates (0.15 g C m−2 d−1), while rates were 7 to 20 times higher around the Marquesas Islands and in the Chilean upwelling, respectively. In the very low productive area of the SPG, most of the primary production was sustained by active regeneration processes that fuelled up to 95% of the biological nitrogen demand. Nitrification was active in the surface layer and often balanced the biological demand for nitrate, especially in the SPG. The percentage of nitrogen released as DON represented a large proportion of the inorganic nitrogen uptake (13–15% in average), reaching 26–41% in the SPG, where DON production played a major role in nitrogen cycling. Dinitrogen fixation was detectable over the whole study area; even in the Chilean upwelling, where rates as high as 3 nmoles l−1 d−1 were measured. In these nutrient-replete waters new production was very high (0.69±0.49 g C m−2 d−1) and essentially sustained by nitrate levels. In the SPG, dinitrogen fixation, although occurring at much lower daily rates (≈1–2 nmoles l−1 d−1), sustained up to 100% of the new production (0.008±0.007 g C m−2 d−1) which was two orders of magnitude lower than that measured in the upwelling. The annual N2-fixation of the South Pacific is estimated to 21×1012g, of which 1.34×1012g is for the SPG only. Even if our ""snapshot"" estimates of N2-fixation rates were lower than that expected from a recent ocean circulation model, these data confirm that the N-deficiency South Pacific Ocean would provide an ideal ecological niche for the proliferation of N2-fixers which are not yet identified.\n                    </jats:p>","abstract. one of the major objectives of the biosope cruise, carried out on the r/v atalante from october-november 2004 in the south pacific ocean, was to establish productivity rates along a zonal section traversing the oligotrophic south pacific gyre (spg). these results were then compared to measurements obtained from the nutrient – replete waters in the chilean upwelling and around the marquesas islands. a dual 13c/15n isotope technique was used to estimate the carbon fixation rates, inorganic nitrogen uptake (including dinitrogen fixation), ammonium (nh4) and nitrate (no3) regeneration and release of dissolved organic nitrogen (don). the spg exhibited the lowest primary production rates (0.15 g c m−2 d−1), while rates were 7 to 20 times higher around the marquesas islands and in the chilean upwelling, respectively. in the very low productive area of the spg, most of the primary production was sustained by active regeneration processes that fuelled up to 95% of the biological nitrogen demand. nitrification was active in the surface layer and often balanced the biological demand for nitrate, especially in the spg. the percentage of nitrogen released as don represented a large proportion of the inorganic nitrogen uptake (13–15% in average), reaching 26–41% in the spg, where don production played a major role in nitrogen cycling. dinitrogen fixation was detectable over the whole study area; even in the chilean upwelling, where rates as high as 3 nmoles l−1 d−1 were measured. in these nutrient-replete waters new production was very high (0.69±0.49 g c m−2 d−1) and essentially sustained by nitrate levels. in the spg, dinitrogen fixation, although occurring at much lower daily rates (≈1–2 nmoles l−1 d−1), sustained up to 100% of the new production (0.008±0.007 g c m−2 d−1) which was two orders of magnitude lower than that measured in the upwelling. the annual n2-fixation of the south pacific is estimated to 21×1012g, of which 1.34×1012g is for the spg only. even if our ""snapshot"" estimates of n2-fixation rates were lower than that expected from a recent ocean circulation model, these data confirm that the n-deficiency south pacific ocean would provide an ideal ecological niche for the proliferation of n2-fixers which are not yet identified.\n"
http://orkg.org/orkg/resource/R146646,Comprehensive evaluation of DNA barcoding for the molecular species identification of forensically important Australian Sarcophagidae (Diptera),10.1071/is12008,crossref,"<jats:p>Carrion-breeding Sarcophagidae (Diptera) can be used to estimate the post-mortem interval in forensic cases. Difficulties with accurate morphological identifications at any life stage and a lack of documented thermobiological profiles have limited their current usefulness. The molecular-based approach of DNA barcoding, which utilises a 648-bp fragment of the mitochondrial cytochrome oxidase subunitI gene, was evaluated in a pilot study for discrimination between 16 Australian sarcophagids. The current study comprehensively evaluated barcoding for a larger taxon set of 588 Australian sarcophagids. In total, 39 of the 84 known Australian species were represented by 580 specimens, which includes 92% of potentially forensically important species. A further eight specimens could not be identified, but were included nonetheless as six unidentifiable taxa. A neighbour-joining tree was generated and nucleotide sequence divergences were calculated. All species except Sarcophaga (Fergusonimyia) bancroftorum, known for high morphological variability, were resolved as monophyletic (99.2% of cases), with bootstrap support of 100. Excluding S. bancroftorum, the mean intraspecific and interspecific variation ranged from 1.12% and 2.81–11.23%, respectively, allowing for species discrimination. DNA barcoding was therefore validated as a suitable method for molecular identification of Australian Sarcophagidae, which will aid in the implementation of this fauna in forensic entomology.</jats:p>","carrion-breeding sarcophagidae (diptera) can be used to estimate the post-mortem interval in forensic cases. difficulties with accurate morphological identifications at any life stage and a lack of documented thermobiological profiles have limited their current usefulness. the molecular-based approach of dna barcoding, which utilises a 648-bp fragment of the mitochondrial cytochrome oxidase subuniti gene, was evaluated in a pilot study for discrimination between 16 australian sarcophagids. the current study comprehensively evaluated barcoding for a larger taxon set of 588 australian sarcophagids. in total, 39 of the 84 known australian species were represented by 580 specimens, which includes 92% of potentially forensically important species. a further eight specimens could not be identified, but were included nonetheless as six unidentifiable taxa. a neighbour-joining tree was generated and nucleotide sequence divergences were calculated. all species except sarcophaga (fergusonimyia) bancroftorum, known for high morphological variability, were resolved as monophyletic (99.2% of cases), with bootstrap support of 100. excluding s. bancroftorum, the mean intraspecific and interspecific variation ranged from 1.12% and 2.81–11.23%, respectively, allowing for species discrimination. dna barcoding was therefore validated as a suitable method for molecular identification of australian sarcophagidae, which will aid in the implementation of this fauna in forensic entomology."
http://orkg.org/orkg/resource/R148596,Crossover from two-frequency pulse compounds to escaping solitons,10.1038/s41598-021-90705-6,crossref,"<jats:title>Abstract</jats:title><jats:p>The nonlinear interaction of copropagating optical solitons enables a large variety of intriguing bound-states of light. We here investigate the interaction dynamics of two initially superimposed fundamental solitons at distinctly different frequencies. Both pulses are located in distinct domains of anomalous dispersion, separated by an interjacent domain of normal dispersion, so that group velocity matching can be achieved despite a vast frequency gap. We demonstrate the existence of two regions with different dynamical behavior. For small velocity mismatch we observe a domain in which a single heteronuclear pulse compound is formed, which is distinct from the usual concept of soliton molecules. The binding mechanism is realized by the mutual cross phase modulation of the interacting pulses. For large velocity mismatch both pulses escape their mutual binding and move away from each other. The crossover phase between these two cases exhibits two localized states with different velocity, consisting of a strong trapping pulse and weak trapped pulse. We detail a simplified theoretical approach which accurately estimates the parameter range in which compound states are formed. This trapping-to-escape transition allows to study the limits of pulse-bonding as a fundamental phenomenon in nonlinear optics, opening up new perspectives for the all-optical manipulation of light by light.</jats:p>","abstract the nonlinear interaction of copropagating optical solitons enables a large variety of intriguing bound-states of light. we here investigate the interaction dynamics of two initially superimposed fundamental solitons at distinctly different frequencies. both pulses are located in distinct domains of anomalous dispersion, separated by an interjacent domain of normal dispersion, so that group velocity matching can be achieved despite a vast frequency gap. we demonstrate the existence of two regions with different dynamical behavior. for small velocity mismatch we observe a domain in which a single heteronuclear pulse compound is formed, which is distinct from the usual concept of soliton molecules. the binding mechanism is realized by the mutual cross phase modulation of the interacting pulses. for large velocity mismatch both pulses escape their mutual binding and move away from each other. the crossover phase between these two cases exhibits two localized states with different velocity, consisting of a strong trapping pulse and weak trapped pulse. we detail a simplified theoretical approach which accurately estimates the parameter range in which compound states are formed. this trapping-to-escape transition allows to study the limits of pulse-bonding as a fundamental phenomenon in nonlinear optics, opening up new perspectives for the all-optical manipulation of light by light."
http://orkg.org/orkg/resource/R146865,A simple small molecule as an acceptor for fullerene-free organic solar cells with efficiency near 8%,10.1039/c6ta04358e,crossref,<p>A small molecule named DICTF was synthesized and an organic solar cell based on PTB7-Th:DICTF exhibited a high PCE of 7.93%.</p>,a small molecule named dictf was synthesized and an organic solar cell based on ptb7-th:dictf exhibited a high pce of 7.93%.
http://orkg.org/orkg/resource/R146686,Automatic Detection of Heartbeats in Heart Sound Signals Using Deep Convolutional Neural Networks,10.5755/j01.eie.25.3.23680,crossref,"<jats:p>The analysis of non-stationary signals commonly includes the signal segmentation process, dividing such signals into smaller time series, which are considered stationary and thus easier to process. Most commonly, the methods for signal segmentation utilize complex filtering, transformation and feature extraction techniques together with various kinds of classifiers, which especially in the field of biomedical signals, do not perform very well and are generally prone to poor performance when dealing with signals obtained in highly variable environments. In order to address these problems, we designed a new method for the segmentation of heart sound signals using deep convolutional neural networks, which works in a straightforward automatic manner and does not require any complex pre-processing. The proposed method was tested on a set of heartbeat sound clips, collected by non-experts with mobile devices in highly variable environments with excessive background noise. The obtained results show that the proposed method outperforms other methods, which are taking advantage of using domain knowledge for the analysis of the signals. Based on the encouraging experimental results, we believe that the proposed method can be considered as a solid basis for the further development of the automatic segmentation of highly variable signals using deep neural networks.</jats:p>","the analysis of non-stationary signals commonly includes the signal segmentation process, dividing such signals into smaller time series, which are considered stationary and thus easier to process. most commonly, the methods for signal segmentation utilize complex filtering, transformation and feature extraction techniques together with various kinds of classifiers, which especially in the field of biomedical signals, do not perform very well and are generally prone to poor performance when dealing with signals obtained in highly variable environments. in order to address these problems, we designed a new method for the segmentation of heart sound signals using deep convolutional neural networks, which works in a straightforward automatic manner and does not require any complex pre-processing. the proposed method was tested on a set of heartbeat sound clips, collected by non-experts with mobile devices in highly variable environments with excessive background noise. the obtained results show that the proposed method outperforms other methods, which are taking advantage of using domain knowledge for the analysis of the signals. based on the encouraging experimental results, we believe that the proposed method can be considered as a solid basis for the further development of the automatic segmentation of highly variable signals using deep neural networks."
http://orkg.org/orkg/resource/R146689,A Novel Method for Measuring the Timing of Heart Sound Components through Digital Phonocardiography,10.3390/s19081868,crossref,"<jats:p>The auscultation of heart sounds has been for decades a fundamental diagnostic tool in clinical practice. Higher effectiveness can be achieved by recording the corresponding biomedical signal, namely the phonocardiographic signal, and processing it by means of traditional signal processing techniques. An unavoidable processing step is the heart sound segmentation, which is still a challenging task from a technical viewpoint—a limitation of state-of-the-art approaches is the unavailability of trustworthy techniques for the detection of heart sound components. The aim of this work is to design a reliable algorithm for the identification and the classification of heart sounds’ main components. The proposed methodology was tested on a sample population of 24 healthy subjects over 10-min-long simultaneous electrocardiographic and phonocardiographic recordings and it was found capable of correctly detecting and classifying an average of 99.2% of the heart sounds along with their components. Moreover, the delay of each component with respect to the corresponding R-wave peak and the delay among the components of the same heart sound were computed: the resulting experimental values are coherent with what is expected from the literature and what was obtained by other studies.</jats:p>","the auscultation of heart sounds has been for decades a fundamental diagnostic tool in clinical practice. higher effectiveness can be achieved by recording the corresponding biomedical signal, namely the phonocardiographic signal, and processing it by means of traditional signal processing techniques. an unavoidable processing step is the heart sound segmentation, which is still a challenging task from a technical viewpoint—a limitation of state-of-the-art approaches is the unavailability of trustworthy techniques for the detection of heart sound components. the aim of this work is to design a reliable algorithm for the identification and the classification of heart sounds’ main components. the proposed methodology was tested on a sample population of 24 healthy subjects over 10-min-long simultaneous electrocardiographic and phonocardiographic recordings and it was found capable of correctly detecting and classifying an average of 99.2% of the heart sounds along with their components. moreover, the delay of each component with respect to the corresponding r-wave peak and the delay among the components of the same heart sound were computed: the resulting experimental values are coherent with what is expected from the literature and what was obtained by other studies."
http://orkg.org/orkg/resource/R147006,Exendin-4-Loaded PLGA Microspheres Relieve Cerebral Ischemia/Reperfusion Injury and Neurologic Deficits through Long-Lasting Bioactivity-Mediated Phosphorylated Akt/eNOS Signaling in Rats,10.1038/jcbfm.2015.126,crossref,"<jats:p> Glucagon-like peptide-1 (GLP-1) receptor activation in the brain provides neuroprotection. Exendin-4 (Ex-4), a GLP-1 analog, has seen limited clinical usage because of its short half-life. We developed long-lasting Ex-4-loaded poly(D,L-lactide-co-glycolide) microspheres (PEx-4) and explored its neuroprotective potential against cerebral ischemia in diabetic rats. Compared with Ex-4, PEx-4 in the gradually degraded microspheres sustained higher Ex-4 levels in the plasma and cerebrospinal fluid for at least 2 weeks and improved diabetes-induced glycemia after a single subcutaneous administration (20 μg/day). Ten minutes of bilateral carotid artery occlusion (CAO) combined with hemorrhage-induced hypotension (around 30 mm Hg) significantly decreased cerebral blood flow and microcirculation in male Wistar rats subjected to streptozotocin-induced diabetes. CAO increased cortical O<jats:sub>2</jats:sub><jats:sup>–</jats:sup> levels by chemiluminescence amplification and prefrontal cortex edema by T2-weighted magnetic resonance imaging analysis. CAO significantly increased aquaporin 4 and glial fibrillary acidic protein expression and led to cognition deficits. CAO downregulated phosphorylated Akt/endothelial nitric oxide synthase (p-Akt/p-eNOS) signaling and enhanced nuclear factor (NF)-κBp65/ intercellular adhesion molecule-1 (ICAM-1) expression, endoplasmic reticulum (ER) stress, and apoptosis in the cerebral cortex. PEx-4 was more effective than Ex-4 to improve CAO-induced oxidative injury and cognitive deficits. The neuroprotection provided by PEx-4 was through p-Akt/p-eNOS pathways, which suppressed CAO-enhanced NF- κB/ICAM-1 signaling, ER stress, and apoptosis. </jats:p>","glucagon-like peptide-1 (glp-1) receptor activation in the brain provides neuroprotection. exendin-4 (ex-4), a glp-1 analog, has seen limited clinical usage because of its short half-life. we developed long-lasting ex-4-loaded poly(d,l-lactide-co-glycolide) microspheres (pex-4) and explored its neuroprotective potential against cerebral ischemia in diabetic rats. compared with ex-4, pex-4 in the gradually degraded microspheres sustained higher ex-4 levels in the plasma and cerebrospinal fluid for at least 2 weeks and improved diabetes-induced glycemia after a single subcutaneous administration (20 μg/day). ten minutes of bilateral carotid artery occlusion (cao) combined with hemorrhage-induced hypotension (around 30 mm hg) significantly decreased cerebral blood flow and microcirculation in male wistar rats subjected to streptozotocin-induced diabetes. cao increased cortical o 2 – levels by chemiluminescence amplification and prefrontal cortex edema by t2-weighted magnetic resonance imaging analysis. cao significantly increased aquaporin 4 and glial fibrillary acidic protein expression and led to cognition deficits. cao downregulated phosphorylated akt/endothelial nitric oxide synthase (p-akt/p-enos) signaling and enhanced nuclear factor (nf)-κbp65/ intercellular adhesion molecule-1 (icam-1) expression, endoplasmic reticulum (er) stress, and apoptosis in the cerebral cortex. pex-4 was more effective than ex-4 to improve cao-induced oxidative injury and cognitive deficits. the neuroprotection provided by pex-4 was through p-akt/p-enos pathways, which suppressed cao-enhanced nf- κb/icam-1 signaling, er stress, and apoptosis."
http://orkg.org/orkg/resource/R146888,High-performance fullerene-free polymer solar cells with 6.31% efficiency,10.1039/c4ee03424d,crossref,"<p>A nonfullerene electron acceptor (IEIC) based on indaceno[1,2-<italic>b</italic>:5,6-<italic>b</italic>′]dithiophene and 2-(3-oxo-2,3-dihydroinden-1-ylidene)malononitrile was designed and synthesized, and fullerene-free polymer solar cells based on the IEIC acceptor showed power conversion efficiencies of up to 6.31%.</p>","a nonfullerene electron acceptor (ieic) based on indaceno[1,2- b :5,6- b ′]dithiophene and 2-(3-oxo-2,3-dihydroinden-1-ylidene)malononitrile was designed and synthesized, and fullerene-free polymer solar cells based on the ieic acceptor showed power conversion efficiencies of up to 6.31%."
http://orkg.org/orkg/resource/R146907,Non-fullerene polymer solar cells based on a selenophene-containing fused-ring acceptor with photovoltaic performance of 8.6%,10.1039/c6ee00315j,crossref,"<p>In this work, we present a non-fullerene electron acceptor bearing a fused five-heterocyclic ring containing selenium atoms, denoted as IDSe-T-IC, for fullerene-free polymer solar cells (PSCs).</p>","in this work, we present a non-fullerene electron acceptor bearing a fused five-heterocyclic ring containing selenium atoms, denoted as idse-t-ic, for fullerene-free polymer solar cells (pscs)."
http://orkg.org/orkg/resource/R146321,Introduction of software tools for epidemiological surveillance in infection control in Colombia,10.25100/cm.v46i2.1548,crossref,"<jats:p>Introduction:\n\nHealthcare-Associated Infections (HAI) are a challenge for patient safety in the hospitals. Infection control committees (ICC) should follow CDC definitions when monitoring HAI. The handmade method of epidemiological surveillance (ES) may affect the sensitivity and specificity of the monitoring system, while electronic surveillance can improve the performance, quality and traceability of recorded information.\nObjective:\n\nTo assess the implementation of a strategy for electronic surveillance of HAI, Bacterial Resistance and Antimicrobial Consumption by the ICC of 23 high-complexity clinics and hospitals in Colombia, during the period 2012-2013.\nMethods:\n\nAn observational study evaluating the introduction of electronic tools in the ICC was performed; we evaluated the structure and operation of the ICC, the degree of incorporation of the software HAI Solutions and the adherence to record the required information.\nResults:\n\nThirty-eight percent of hospitals (8/23) had active surveillance strategies with standard criteria of the CDC, and 87% of institutions adhered to the module of identification of cases using the HAI Solutions software. In contrast, compliance with the diligence of the risk factors for device-associated HAIs was 33%.\nConclusions:\n\nThe introduction of ES could achieve greater adherence to a model of active surveillance, standardized and prospective, helping to improve the validity and quality of the recorded information.</jats:p>","introduction:\n\nhealthcare-associated infections (hai) are a challenge for patient safety in the hospitals. infection control committees (icc) should follow cdc definitions when monitoring hai. the handmade method of epidemiological surveillance (es) may affect the sensitivity and specificity of the monitoring system, while electronic surveillance can improve the performance, quality and traceability of recorded information.\nobjective:\n\nto assess the implementation of a strategy for electronic surveillance of hai, bacterial resistance and antimicrobial consumption by the icc of 23 high-complexity clinics and hospitals in colombia, during the period 2012-2013.\nmethods:\n\nan observational study evaluating the introduction of electronic tools in the icc was performed; we evaluated the structure and operation of the icc, the degree of incorporation of the software hai solutions and the adherence to record the required information.\nresults:\n\nthirty-eight percent of hospitals (8/23) had active surveillance strategies with standard criteria of the cdc, and 87% of institutions adhered to the module of identification of cases using the hai solutions software. in contrast, compliance with the diligence of the risk factors for device-associated hais was 33%.\nconclusions:\n\nthe introduction of es could achieve greater adherence to a model of active surveillance, standardized and prospective, helping to improve the validity and quality of the recorded information."
http://orkg.org/orkg/resource/R146932,DNA barcodes reveal cryptic genetic diversity within the blackfly subgenus Trichodagmia Enderlein (Diptera: Simuliidae: Simulium) and related taxa in the New World,10.11646/zootaxa.3514.1.3,crossref,"<jats:p>In this paper we investigate the utility of the COI DNA barcoding region for species identification and for revealing hidden diversity within the subgenus Trichodagmia and related taxa in the New World. In total, 24 morphospecies within the current expanded taxonomic concept of Trichodagmia were analyzed. Three species in the subgenus Aspathia and 10 species in the subgenus Simulium s.str. were also included in the analysis because of their putative phylogenetic relationship with Trichodagmia. In the Neighbour Joining analysis tree (NJ) derived from the DNA barcodes most of the specimens grouped together according to species or species groups as recognized by other morphotaxonomic studies. The interspecific genetic divergence averaged 11.2% (range 2.8–19.5%), whereas intraspecific genetic divergence within morphologically distinct species averaged 0.5% (range 0–1.2%). Higher values of genetic divergence (3.2–3.7%) in species complexes suggest the presence of cryptic diversity. The existence of well defined groups within S. piperi, S. duodenicornium, S. canadense and S. rostratum indicate the possible presence of cryptic species within these taxa. Also, the suspected presence of a sibling species in S. tarsatum and S. paynei is supported. DNA barcodes also showed that specimens from species that were taxonomically difficult to delimit such as S. hippovorum, S. rubrithorax, S. paynei, and other related taxa (S. solarii), grouped together in the NJ analysis, confirming the validity of their species status. The recovery of partial barcodes from specimens in collections was time consuming and PCR success was low from specimens more than 10 years old. However, when a sequence was obtained, it provided good resolution for species identification. Larvae preserved in ‘weak’ Carnoy’s solution (9:1 ethanol:acetic acid) provided full DNA barcodes. Adding legs directly to the PCR mix from recently collected and preserved adults was an inexpensive, fast methodology to obtain full barcodes. In summary, DNA barcoding combined with a sound morphotaxonomic framework provides an effective approach for the delineation of species and for the discovery of hidden diversity in the subgenus Trichodagmia.</jats:p>","in this paper we investigate the utility of the coi dna barcoding region for species identification and for revealing hidden diversity within the subgenus trichodagmia and related taxa in the new world. in total, 24 morphospecies within the current expanded taxonomic concept of trichodagmia were analyzed. three species in the subgenus aspathia and 10 species in the subgenus simulium s.str. were also included in the analysis because of their putative phylogenetic relationship with trichodagmia. in the neighbour joining analysis tree (nj) derived from the dna barcodes most of the specimens grouped together according to species or species groups as recognized by other morphotaxonomic studies. the interspecific genetic divergence averaged 11.2% (range 2.8–19.5%), whereas intraspecific genetic divergence within morphologically distinct species averaged 0.5% (range 0–1.2%). higher values of genetic divergence (3.2–3.7%) in species complexes suggest the presence of cryptic diversity. the existence of well defined groups within s. piperi, s. duodenicornium, s. canadense and s. rostratum indicate the possible presence of cryptic species within these taxa. also, the suspected presence of a sibling species in s. tarsatum and s. paynei is supported. dna barcodes also showed that specimens from species that were taxonomically difficult to delimit such as s. hippovorum, s. rubrithorax, s. paynei, and other related taxa (s. solarii), grouped together in the nj analysis, confirming the validity of their species status. the recovery of partial barcodes from specimens in collections was time consuming and pcr success was low from specimens more than 10 years old. however, when a sequence was obtained, it provided good resolution for species identification. larvae preserved in ‘weak’ carnoy’s solution (9:1 ethanol:acetic acid) provided full dna barcodes. adding legs directly to the pcr mix from recently collected and preserved adults was an inexpensive, fast methodology to obtain full barcodes. in summary, dna barcoding combined with a sound morphotaxonomic framework provides an effective approach for the delineation of species and for the discovery of hidden diversity in the subgenus trichodagmia."
http://orkg.org/orkg/resource/R146576,Comparative evaluation of three surveillance systems for infectious equine diseases in France and implications for future synergies,10.1017/s0950268815000217,crossref,"<jats:title>SUMMARY</jats:title><jats:p>It is necessary to assess surveillance systems for infectious animal diseases to ensure they meet their objectives and provide high-quality health information. Each system is generally dedicated to one disease and often comprises various components. In many animal industries, several surveillance systems are implemented separately even if they are based on similar components. This lack of synergy may prevent optimal surveillance. The purpose of this study was to assess several surveillance systems within the same industry using the semi-quantitative OASIS method and to compare the results of the assessments in order to propose improvements, including future synergies. We have focused on the surveillance of three major equine diseases in France. We have identified the mutual and specific strengths and weaknesses of each surveillance system. Furthermore, the comparative assessment has highlighted many possible synergies that could improve the effectiveness and efficiency of surveillance as a whole, including the implementation of new joint tools or the pooling of existing teams, tools or skills. Our approach is an original application of the OASIS method, which requires minimal financial resources and is not very time-consuming. Such a comparative evaluation could conceivably be applied to other surveillance systems, other industries and other countries. This approach would be especially relevant to enhance the efficiency of surveillance activities when resources are limited.</jats:p>","summary it is necessary to assess surveillance systems for infectious animal diseases to ensure they meet their objectives and provide high-quality health information. each system is generally dedicated to one disease and often comprises various components. in many animal industries, several surveillance systems are implemented separately even if they are based on similar components. this lack of synergy may prevent optimal surveillance. the purpose of this study was to assess several surveillance systems within the same industry using the semi-quantitative oasis method and to compare the results of the assessments in order to propose improvements, including future synergies. we have focused on the surveillance of three major equine diseases in france. we have identified the mutual and specific strengths and weaknesses of each surveillance system. furthermore, the comparative assessment has highlighted many possible synergies that could improve the effectiveness and efficiency of surveillance as a whole, including the implementation of new joint tools or the pooling of existing teams, tools or skills. our approach is an original application of the oasis method, which requires minimal financial resources and is not very time-consuming. such a comparative evaluation could conceivably be applied to other surveillance systems, other industries and other countries. this approach would be especially relevant to enhance the efficiency of surveillance activities when resources are limited."
http://orkg.org/orkg/resource/R146470,Electronic Surveillance for Healthcare-Associated Central Line—Associated Bloodstream Infections Outside the Intensive Care Unit,10.1086/662181,crossref,"<jats:sec id=""S0195941700030095_abs1""><jats:title>Background.</jats:title><jats:p>Manual surveillance for central line-associated bloodstream infections (CLABSIs) by infection prevention practitioners is time-consuming and often limited to intensive care units (ICUs). An automated surveillance system using existing databases with patient-level variables and microbiology data was investigated.</jats:p></jats:sec><jats:sec id=""S0195941700030095_abs2""><jats:title>Methods.</jats:title><jats:p>Patients with a positive blood culture in 4 non-ICU wards at Barnes-Jewish Hospital between July 1, 2005, and December 31, 2006, were evaluated. CLABSI determination for these patients was made via 2 sources; a manual chart review and an automated review from electronically available data. Agreement between these 2 sources was used to develop the best-fit electronic algorithm that used a set of rules to identify a CLABSI. Sensitivity, specificity, predictive values, and Pearson\'s correlation were calculated for the various rule sets, using manual chart review as the reference standard.</jats:p></jats:sec><jats:sec id=""S0195941700030095_abs3""><jats:title>Results.</jats:title><jats:p>During the study period, 391 positive blood cultures from 331 patients were evaluated. Eighty-five (22%) of these were confirmed to be CLABSI by manual chart review. The best-fit model included presence of a catheter, blood culture positive for known pathogen or blood culture with a common skin contaminant confirmed by a second positive culture and the presence of fever, and no positive cultures with the same organism from another sterile site. The best-performing rule set had an overall sensitivity of 95.2%, specificity of 97.5%, positive predictive value of 90%, and negative predictive value of 99.2% compared with intensive manual surveillance.</jats:p></jats:sec><jats:sec id=""S0195941700030095_abs4""><jats:title>Conclusions.</jats:title><jats:p>Although CLABSIs were slightly overpredicted by electronic surveillance compared with manual chart review, the method offers the possibility of performing acceptably good surveillance in areas where resources do not allow for traditional manual surveillance.</jats:p></jats:sec>","background. manual surveillance for central line-associated bloodstream infections (clabsis) by infection prevention practitioners is time-consuming and often limited to intensive care units (icus). an automated surveillance system using existing databases with patient-level variables and microbiology data was investigated. methods. patients with a positive blood culture in 4 non-icu wards at barnes-jewish hospital between july 1, 2005, and december 31, 2006, were evaluated. clabsi determination for these patients was made via 2 sources; a manual chart review and an automated review from electronically available data. agreement between these 2 sources was used to develop the best-fit electronic algorithm that used a set of rules to identify a clabsi. sensitivity, specificity, predictive values, and pearson\'s correlation were calculated for the various rule sets, using manual chart review as the reference standard. results. during the study period, 391 positive blood cultures from 331 patients were evaluated. eighty-five (22%) of these were confirmed to be clabsi by manual chart review. the best-fit model included presence of a catheter, blood culture positive for known pathogen or blood culture with a common skin contaminant confirmed by a second positive culture and the presence of fever, and no positive cultures with the same organism from another sterile site. the best-performing rule set had an overall sensitivity of 95.2%, specificity of 97.5%, positive predictive value of 90%, and negative predictive value of 99.2% compared with intensive manual surveillance. conclusions. although clabsis were slightly overpredicted by electronic surveillance compared with manual chart review, the method offers the possibility of performing acceptably good surveillance in areas where resources do not allow for traditional manual surveillance."
http://orkg.org/orkg/resource/R146490,Rapid implementation of mobile technology for real-time epidemiology of COVID-19,10.1126/science.abc0473,crossref,"<jats:title>Mobile symptom tracking</jats:title>\n          <jats:p>\n            The rapidity with which severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spreads through a population is defying attempts at tracking it, and quantitative polymerase chain reaction testing so far has been too slow for real-time epidemiology. Taking advantage of existing longitudinal health care and research patient cohorts, Drew\n            <jats:italic>et al.</jats:italic>\n            pushed software updates to participants to encourage reporting of potential coronavirus disease 2019 (COVID-19) symptoms. The authors recruited about 2 million users (including health care workers) to the COVID Symptom Study (previously known as the COVID Symptom Tracker) from across the United Kingdom and the United States. The prevalence of combinations of symptoms (three or more), including fatigue and cough, followed by diarrhea, fever, and/or anosmia, was predictive of a positive test verification for SARS-CoV-2. As exemplified by data from Wales, United Kingdom, mathematical modeling predicted geographical hotspots of incidence 5 to 7 days in advance of official public health reports.\n          </jats:p>\n          <jats:p>\n            <jats:italic>Science</jats:italic>\n            , this issue p.\n            <jats:related-article xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""doi"" issue=""6497"" page=""1362"" related-article-type=""in-this-issue"" vol=""368"" xlink:href=""10.1126/science.abc0473"">1362</jats:related-article>\n          </jats:p>","mobile symptom tracking \n \n the rapidity with which severe acute respiratory syndrome coronavirus 2 (sars-cov-2) spreads through a population is defying attempts at tracking it, and quantitative polymerase chain reaction testing so far has been too slow for real-time epidemiology. taking advantage of existing longitudinal health care and research patient cohorts, drew\n et al. \n pushed software updates to participants to encourage reporting of potential coronavirus disease 2019 (covid-19) symptoms. the authors recruited about 2 million users (including health care workers) to the covid symptom study (previously known as the covid symptom tracker) from across the united kingdom and the united states. the prevalence of combinations of symptoms (three or more), including fatigue and cough, followed by diarrhea, fever, and/or anosmia, was predictive of a positive test verification for sars-cov-2. as exemplified by data from wales, united kingdom, mathematical modeling predicted geographical hotspots of incidence 5 to 7 days in advance of official public health reports.\n \n \n science \n , this issue p.\n 1362 \n"
http://orkg.org/orkg/resource/R146594,Evaluation of animal and public health surveillance systems: a systematic review,10.1017/s0950268811002160,crossref,"<jats:title>SUMMARY</jats:title><jats:p>Disease surveillance programmes ought to be evaluated regularly to ensure they provide valuable information in an efficient manner. Evaluation of human and animal health surveillance programmes around the world is currently not standardized and therefore inconsistent. The aim of this systematic review was to review surveillance system attributes and the methods used for their assessment, together with the strengths and weaknesses of existing frameworks for evaluating surveillance in animal health, public health and allied disciplines. Information from 99 articles describing the evaluation of 101 surveillance systems was examined. A wide range of approaches for assessing 23 different system attributes was identified although most evaluations addressed only one or two attributes and comprehensive evaluations were uncommon. Surveillance objectives were often not stated in the articles reviewed and so the reasons for choosing certain attributes for assessment were not always apparent. This has the potential to introduce misleading results in surveillance evaluation. Due to the wide range of system attributes that may be assessed, methods should be explored which collapse these down into a small number of grouped characteristics by focusing on the relationships between attributes and their links to the objectives of the surveillance system and the evaluation. A generic and comprehensive evaluation framework could then be developed consisting of a limited number of common attributes together with several sets of secondary attributes which could be selected depending on the disease or range of diseases under surveillance and the purpose of the surveillance. Economic evaluation should be an integral part of the surveillance evaluation process. This would provide a significant benefit to decision-makers who often need to make choices based on limited or diminishing resources.</jats:p>","summary disease surveillance programmes ought to be evaluated regularly to ensure they provide valuable information in an efficient manner. evaluation of human and animal health surveillance programmes around the world is currently not standardized and therefore inconsistent. the aim of this systematic review was to review surveillance system attributes and the methods used for their assessment, together with the strengths and weaknesses of existing frameworks for evaluating surveillance in animal health, public health and allied disciplines. information from 99 articles describing the evaluation of 101 surveillance systems was examined. a wide range of approaches for assessing 23 different system attributes was identified although most evaluations addressed only one or two attributes and comprehensive evaluations were uncommon. surveillance objectives were often not stated in the articles reviewed and so the reasons for choosing certain attributes for assessment were not always apparent. this has the potential to introduce misleading results in surveillance evaluation. due to the wide range of system attributes that may be assessed, methods should be explored which collapse these down into a small number of grouped characteristics by focusing on the relationships between attributes and their links to the objectives of the surveillance system and the evaluation. a generic and comprehensive evaluation framework could then be developed consisting of a limited number of common attributes together with several sets of secondary attributes which could be selected depending on the disease or range of diseases under surveillance and the purpose of the surveillance. economic evaluation should be an integral part of the surveillance evaluation process. this would provide a significant benefit to decision-makers who often need to make choices based on limited or diminishing resources."
http://orkg.org/orkg/resource/R146624,"Cloud-Based System for Effective Surveillance and Control of COVID-19: Useful Experiences From Hubei, China",10.2196/18948,crossref,"<jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Coronavirus disease (COVID-19) has been an unprecedented challenge to the global health care system. Tools that can improve the focus of surveillance efforts and clinical decision support are of paramount importance.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Objective</jats:title>\n            <jats:p>The aim of this study was to illustrate how new medical informatics technologies may enable effective control of the pandemic through the development and successful 72-hour deployment of the Honghu Hybrid System (HHS) for COVID-19 in the city of Honghu in Hubei, China.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Methods</jats:title>\n            <jats:p>The HHS was designed for the collection, integration, standardization, and analysis of COVID-19-related data from multiple sources, which includes a case reporting system, diagnostic labs, electronic medical records, and social media on mobile devices.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>HHS supports four main features: syndromic surveillance on mobile devices, policy-making decision support, clinical decision support and prioritization of resources, and follow-up of discharged patients. The syndromic surveillance component in HHS covered over 95% of the population of over 900,000 people and provided near real time evidence for the control of epidemic emergencies. The clinical decision support component in HHS was also provided to improve patient care and prioritize the limited medical resources. However, the statistical methods still require further evaluations to confirm clinical effectiveness and appropriateness of disposition assigned in this study, which warrants further investigation.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>The facilitating factors and challenges are discussed to provide useful insights to other cities to build suitable solutions based on cloud technologies. The HHS for COVID-19 was shown to be feasible and effective in this real-world field study, and has the potential to be migrated.</jats:p>\n          </jats:sec>","\n background \n coronavirus disease (covid-19) has been an unprecedented challenge to the global health care system. tools that can improve the focus of surveillance efforts and clinical decision support are of paramount importance. \n \n \n objective \n the aim of this study was to illustrate how new medical informatics technologies may enable effective control of the pandemic through the development and successful 72-hour deployment of the honghu hybrid system (hhs) for covid-19 in the city of honghu in hubei, china. \n \n \n methods \n the hhs was designed for the collection, integration, standardization, and analysis of covid-19-related data from multiple sources, which includes a case reporting system, diagnostic labs, electronic medical records, and social media on mobile devices. \n \n \n results \n hhs supports four main features: syndromic surveillance on mobile devices, policy-making decision support, clinical decision support and prioritization of resources, and follow-up of discharged patients. the syndromic surveillance component in hhs covered over 95% of the population of over 900,000 people and provided near real time evidence for the control of epidemic emergencies. the clinical decision support component in hhs was also provided to improve patient care and prioritize the limited medical resources. however, the statistical methods still require further evaluations to confirm clinical effectiveness and appropriateness of disposition assigned in this study, which warrants further investigation. \n \n \n conclusions \n the facilitating factors and challenges are discussed to provide useful insights to other cities to build suitable solutions based on cloud technologies. the hhs for covid-19 was shown to be feasible and effective in this real-world field study, and has the potential to be migrated. \n"
http://orkg.org/orkg/resource/R146211,Highly Sensitive and Efficient Computer–Assisted System for Routine Surveillance for Surgical Site Infection,10.1086/506393,crossref,"<jats:sec id=""S0899823X00196023_abs1""><jats:title>Objectives.</jats:title><jats:p>Surveillance of surgical site infections (SSIs) is effective in reducing the rates of these complications, but it is extremely time-consuming and, consequently, underused. We determined the sensitivity and specificity of a computer-assisted surveillance system, compared with a conventional method involving review of medical records, and the time saved with the computer-assisted system.</jats:p></jats:sec><jats:sec id=""S0899823X00196023_abs2""><jats:title>Method.</jats:title><jats:p>A prospective study was conducted from January 1 to December 31, 2001. With the computer-assisted method, screening for SSIs relied on identification in the laboratory database of positive results of microbiological tests of surgical-site specimens; confirmation was obtained via computer-generated questionnaires completed by the surgeon in charge of the patient. In the conventional method, SSIs were identified by exhaustive chart review. The time spent on surveillance was recorded for both methods.</jats:p></jats:sec><jats:sec id=""S0899823X00196023_abs3""><jats:title>Setting.</jats:title><jats:p>A 25-bed gastrointestinal surgery unit in a tertiary care hospital.</jats:p></jats:sec><jats:sec id=""S0899823X00196023_abs4""><jats:title>Patients.</jats:title><jats:p>A total of 766 consecutive patients who underwent gastrointestinal surgery.</jats:p></jats:sec><jats:sec id=""S0899823X00196023_abs5""><jats:title>Results.</jats:title><jats:p>The sensitivity of the computer-assisted method was 84.3% (95% confidence interval, 0.66-0.94); the specificity was 99.9%. For the 807 surgical procedures in the study, 197 had an SSI identified by culture of a surgical-site specimen. After elimination of 63 duplicate cultures with positive results, 134 questionnaires were sent to the surgeons, who confirmed 27 SSIs. The conventional method identified 32 SSIs. The computer-assisted method required 60% less time than the conventional method (90 hours vs 223 hours).</jats:p></jats:sec><jats:sec id=""S0899823X00196023_abs6""><jats:title>Conclusion.</jats:title><jats:p>Surveillance for SSIs using computer-assisted, laboratory-based screening and case confirmation by surgeons is as efficient as and far less time-consuming than the conventional method of chart review. This method permits routine surveillance for SSIs with reliable accuracy.</jats:p></jats:sec>","objectives. surveillance of surgical site infections (ssis) is effective in reducing the rates of these complications, but it is extremely time-consuming and, consequently, underused. we determined the sensitivity and specificity of a computer-assisted surveillance system, compared with a conventional method involving review of medical records, and the time saved with the computer-assisted system. method. a prospective study was conducted from january 1 to december 31, 2001. with the computer-assisted method, screening for ssis relied on identification in the laboratory database of positive results of microbiological tests of surgical-site specimens; confirmation was obtained via computer-generated questionnaires completed by the surgeon in charge of the patient. in the conventional method, ssis were identified by exhaustive chart review. the time spent on surveillance was recorded for both methods. setting. a 25-bed gastrointestinal surgery unit in a tertiary care hospital. patients. a total of 766 consecutive patients who underwent gastrointestinal surgery. results. the sensitivity of the computer-assisted method was 84.3% (95% confidence interval, 0.66-0.94); the specificity was 99.9%. for the 807 surgical procedures in the study, 197 had an ssi identified by culture of a surgical-site specimen. after elimination of 63 duplicate cultures with positive results, 134 questionnaires were sent to the surgeons, who confirmed 27 ssis. the conventional method identified 32 ssis. the computer-assisted method required 60% less time than the conventional method (90 hours vs 223 hours). conclusion. surveillance for ssis using computer-assisted, laboratory-based screening and case confirmation by surgeons is as efficient as and far less time-consuming than the conventional method of chart review. this method permits routine surveillance for ssis with reliable accuracy."
http://orkg.org/orkg/resource/R146070,"Smart city initiatives in the context of digital transformation: scope, services and technologies",10.30924/mjcmi.24.1.3,crossref,"<jats:p>Digital transformation is an emerging trend in developing the way how the work is being done, and it is present in the private and public sector, in all industries and fields of work. Smart cities, as one of the concepts related to digital transformation, is usually seen as a matter of local governments, as it is their responsibility to ensure a better quality of life for the citizens. Some cities have already taken advantages of possibilities offered by the concept of smart cities, creating new values to all stakeholders interacting in the living city ecosystems, thus serving as examples of good practice, while others are still developing and growing on their intentions to become smart. This paper provides a structured literature analysis and investigates key scope, services and technologies related to smart cities and digital transformation as concepts of empowering social and collaboration interactions, in order to identify leading factors in most smart city initiatives.</jats:p>","digital transformation is an emerging trend in developing the way how the work is being done, and it is present in the private and public sector, in all industries and fields of work. smart cities, as one of the concepts related to digital transformation, is usually seen as a matter of local governments, as it is their responsibility to ensure a better quality of life for the citizens. some cities have already taken advantages of possibilities offered by the concept of smart cities, creating new values to all stakeholders interacting in the living city ecosystems, thus serving as examples of good practice, while others are still developing and growing on their intentions to become smart. this paper provides a structured literature analysis and investigates key scope, services and technologies related to smart cities and digital transformation as concepts of empowering social and collaboration interactions, in order to identify leading factors in most smart city initiatives."
http://orkg.org/orkg/resource/R146208,Automatic Detection of Patients with Nosocomial Infection by a Computer-Based Surveillance System: A Validation Study in a General Hospital,10.1086/502685,crossref,"<jats:sec id=""S0899823X00195303_abs1""><jats:title>Objective.</jats:title><jats:p>To validate an automated system for the detection of patients with nosocomial infection (NI) in an intensive care unit (ICU).</jats:p></jats:sec><jats:sec id=""S0899823X00195303_abs2""><jats:title>Design.</jats:title><jats:p>Retrospective analysis of data from the hospital information system. We applied 3 different NI suspicion criteria (positive microbiology reports, antibiotic administration, and diagnosis of clinical infection) and compared the results to those of a prospective NI incidence study done in the ICU during the same period (1999-2002).</jats:p></jats:sec><jats:sec id=""S0899823X00195303_abs3""><jats:title>Setting.</jats:title><jats:p>A 250-bed general hospital in Barcelona, Spain.</jats:p></jats:sec><jats:sec id=""S0899823X00195303_abs4""><jats:title>Patients.</jats:title><jats:p>From April 15, 1999, through June 30, 2002, 1380 patients were admitted to the ICU. Of these, 1043 had an ICU stay of more than 24 hours and were included in the study.</jats:p></jats:sec><jats:sec id=""S0899823X00195303_abs5""><jats:title>Results.</jats:title><jats:p>At least one NI suspicion criterion was present for 242 patients (23.2%); 2 criteria were present for 184 patients (17.6%); and all 3 criteria were present for 112 (11.7%). Comparison of hospital information system data to the results of the prospective study indicated that the combination of 2 criteria demonstrated the most satisfactory sensitivity (94.3%; 95% confidence interval [CI], 79.5%-99.0%) and specificity (83.6%; 95% CI, 76.8%-88.9%). The positive predictive value was 55.9% (95% CI, 42.5%-68.6%); the negative predictive value was 98.5% (95% CI, 94.2%-99.7%). The system could assign a site of infection for 90.4% of the NIs detected.</jats:p></jats:sec><jats:sec id=""S0899823X00195303_abs6""><jats:title>Conclusion.</jats:title><jats:p>The hospital information system was a useful tool for retrospectively detecting patients with an NI during the ICU stay. Given its high sensitivity, it may be useful as an alert for the NI team.</jats:p></jats:sec>","objective. to validate an automated system for the detection of patients with nosocomial infection (ni) in an intensive care unit (icu). design. retrospective analysis of data from the hospital information system. we applied 3 different ni suspicion criteria (positive microbiology reports, antibiotic administration, and diagnosis of clinical infection) and compared the results to those of a prospective ni incidence study done in the icu during the same period (1999-2002). setting. a 250-bed general hospital in barcelona, spain. patients. from april 15, 1999, through june 30, 2002, 1380 patients were admitted to the icu. of these, 1043 had an icu stay of more than 24 hours and were included in the study. results. at least one ni suspicion criterion was present for 242 patients (23.2%); 2 criteria were present for 184 patients (17.6%); and all 3 criteria were present for 112 (11.7%). comparison of hospital information system data to the results of the prospective study indicated that the combination of 2 criteria demonstrated the most satisfactory sensitivity (94.3%; 95% confidence interval [ci], 79.5%-99.0%) and specificity (83.6%; 95% ci, 76.8%-88.9%). the positive predictive value was 55.9% (95% ci, 42.5%-68.6%); the negative predictive value was 98.5% (95% ci, 94.2%-99.7%). the system could assign a site of infection for 90.4% of the nis detected. conclusion. the hospital information system was a useful tool for retrospectively detecting patients with an ni during the icu stay. given its high sensitivity, it may be useful as an alert for the ni team."
http://orkg.org/orkg/resource/R146256,Improving national surveillance of Lyme neuroborreliosis in Denmark through electronic reporting of specific antibody index testing from 2010 to 2012,10.2807/1560-7917.es2015.20.28.21184,crossref,"<jats:p>Our aim was to evaluate the results of automated surveillance of Lyme neuroborreliosis (LNB) in Denmark using the national microbiology database (MiBa), and to describe the epidemiology of laboratory-confirmed LNB at a national level. MiBa-based surveillance includes electronic transfer of laboratory results, in contrast to the statutory surveillance based on manually processed notifications. Antibody index (AI) testing is the recommend laboratory test to support the diagnosis of LNB in Denmark. In the period from 2010 to 2012, 217 clinical cases of LNB were notified to the statutory surveillance system, while 533 cases were reported AI positive by the MiBa system. Thirty-five unconfirmed cases (29 AI-negative and 6 not tested) were notified, but not captured by MiBa. Using MiBa, the number of reported cases was increased almost 2.5 times. Furthermore, the reporting was timelier (median lag time: 6 vs 58 days). Average annual incidence of AI-confirmed LNB in Denmark was 3.2/100,000 population and incidences stratified by municipality ranged from none to above 10/100,000. This is the first study reporting nationwide incidence of LNB using objective laboratory criteria. Laboratory-based surveillance with electronic data-transfer was more accurate, complete and timely compared to the surveillance based on manually processed notifications. We propose using AI test results for LNB surveillance instead of clinical reporting.\n</jats:p>","our aim was to evaluate the results of automated surveillance of lyme neuroborreliosis (lnb) in denmark using the national microbiology database (miba), and to describe the epidemiology of laboratory-confirmed lnb at a national level. miba-based surveillance includes electronic transfer of laboratory results, in contrast to the statutory surveillance based on manually processed notifications. antibody index (ai) testing is the recommend laboratory test to support the diagnosis of lnb in denmark. in the period from 2010 to 2012, 217 clinical cases of lnb were notified to the statutory surveillance system, while 533 cases were reported ai positive by the miba system. thirty-five unconfirmed cases (29 ai-negative and 6 not tested) were notified, but not captured by miba. using miba, the number of reported cases was increased almost 2.5 times. furthermore, the reporting was timelier (median lag time: 6 vs 58 days). average annual incidence of ai-confirmed lnb in denmark was 3.2/100,000 population and incidences stratified by municipality ranged from none to above 10/100,000. this is the first study reporting nationwide incidence of lnb using objective laboratory criteria. laboratory-based surveillance with electronic data-transfer was more accurate, complete and timely compared to the surveillance based on manually processed notifications. we propose using ai test results for lnb surveillance instead of clinical reporting.\n"
http://orkg.org/orkg/resource/R146565,Automatic Alerts for Methicillin-Resistant Staphylococcus aureus Surveillance and Control: Role of a Hospital Information System,10.1017/s0195941700004641,crossref,"<jats:title>Abstract</jats:title><jats:p><jats:bold>Background:</jats:bold> Methicillin-resistant <jats:italic>Staphylococcus aureus</jats:italic> (MRSA) is an escalating problem in hospitals worldwide. The hospital reservoir for MRSA includes recognized and unrecognized colonized or infected patients, as well as previously colonized or infected patients readmitted to the hospital. Early and appropriate infection control measures (ICM) are key elements to reduce MRSA transmission and to control the hospital reservoir.</jats:p><jats:p><jats:bold>Objective:</jats:bold> To describe the role of an expert system applied to the control of MRSA at a large medical center (1,600 beds) with high endemic rates.</jats:p><jats:p><jats:bold>Methods:</jats:bold> The University Hospital of Geneva has an extended hospital information system (HIS), DIOGENE, structured with an open distributed architecture. It includes administrative, medical, nursing, and laboratory applications with their relational databases. Among available patient databases, clinical microbiology laboratory and admission-discharge-transfer (ADT) databases are used to generate computer alerts. A laboratory alert (lab alert) is printed daily in the Infection Control Program (ICP) offices, listing all patients with cultures positive for MRSA detected within the preceding 24 hours. Patients might be either newly detected patients colonized or infected with MRSA, or previously recognized MRSA patients having surveillance cultures. The ICP nurses subsequently go to the ward or call the ward personnel to implement ICM. A second alert, the “readmission alert,” detects readmission to the hospital of any patient previously colonized or infected with MRSA by periodic queries (q 1 min) to the ADT database. The readmission alert is printed in the ICP offices, but also forwarded with added guidelines to the emergency room.</jats:p><jats:p><jats:bold>Results:</jats:bold> During the first 12 months of application (July 1994 to June 1995), the lab alert detected an average of 4.6 isolates per day, corresponding to 314 hospital admissions (248 patients); the use of this alert saved time for the ICP nurses by improving work organization. There were 438 readmission alerts (1.2 alerts per day) over the study period; of 347 patients screened immediately upon readmission, 114 (33%) were positive for MRSA carriage. Delayed recognition of readmitted MRSA carriers decreased significantly after the implementation of this alert; the proportion of MRSA patients recognized at the time of admission to the hospital increased from 13% in 1993 to 40% in 1995 (<jats:italic>P</jats:italic>&lt;.001).</jats:p><jats:p><jats:bold>Conclusions:</jats:bold> Hospital information system-based alerts can play an important role in the surveillance and early prevention of MRSA transmission, and it can help to recognize patterns of colonization and transmission.</jats:p>","abstract background: methicillin-resistant staphylococcus aureus (mrsa) is an escalating problem in hospitals worldwide. the hospital reservoir for mrsa includes recognized and unrecognized colonized or infected patients, as well as previously colonized or infected patients readmitted to the hospital. early and appropriate infection control measures (icm) are key elements to reduce mrsa transmission and to control the hospital reservoir. objective: to describe the role of an expert system applied to the control of mrsa at a large medical center (1,600 beds) with high endemic rates. methods: the university hospital of geneva has an extended hospital information system (his), diogene, structured with an open distributed architecture. it includes administrative, medical, nursing, and laboratory applications with their relational databases. among available patient databases, clinical microbiology laboratory and admission-discharge-transfer (adt) databases are used to generate computer alerts. a laboratory alert (lab alert) is printed daily in the infection control program (icp) offices, listing all patients with cultures positive for mrsa detected within the preceding 24 hours. patients might be either newly detected patients colonized or infected with mrsa, or previously recognized mrsa patients having surveillance cultures. the icp nurses subsequently go to the ward or call the ward personnel to implement icm. a second alert, the “readmission alert,” detects readmission to the hospital of any patient previously colonized or infected with mrsa by periodic queries (q 1 min) to the adt database. the readmission alert is printed in the icp offices, but also forwarded with added guidelines to the emergency room. results: during the first 12 months of application (july 1994 to june 1995), the lab alert detected an average of 4.6 isolates per day, corresponding to 314 hospital admissions (248 patients); the use of this alert saved time for the icp nurses by improving work organization. there were 438 readmission alerts (1.2 alerts per day) over the study period; of 347 patients screened immediately upon readmission, 114 (33%) were positive for mrsa carriage. delayed recognition of readmitted mrsa carriers decreased significantly after the implementation of this alert; the proportion of mrsa patients recognized at the time of admission to the hospital increased from 13% in 1993 to 40% in 1995 ( p &lt;.001). conclusions: hospital information system-based alerts can play an important role in the surveillance and early prevention of mrsa transmission, and it can help to recognize patterns of colonization and transmission."
http://orkg.org/orkg/resource/R146694,A Robust and Real-Time Capable Envelope-Based Algorithm for Heart Sound Classification: Validation under Different Physiological Conditions,10.3390/s20040972,crossref,"<jats:p>This paper proposes a robust and real-time capable algorithm for classification of the first and second heart sounds. The classification algorithm is based on the evaluation of the envelope curve of the phonocardiogram. For the evaluation, in contrast to other studies, measurements on 12 probands were conducted in different physiological conditions. Moreover, for each measurement the auscultation point, posture and physical stress were varied. The proposed envelope-based algorithm is tested with two different methods for envelope curve extraction: the Hilbert transform and the short-time Fourier transform. The performance of the classification of the first heart sounds is evaluated by using a reference electrocardiogram. Overall, by using the Hilbert transform, the algorithm has a better performance regarding the F1-score and computational effort. The proposed algorithm achieves for the S1 classification an F1-score up to 95.7% and in average 90.5%. The algorithm is robust against the age, BMI, posture, heart rate and auscultation point (except measurements on the back) of the subjects.</jats:p>","this paper proposes a robust and real-time capable algorithm for classification of the first and second heart sounds. the classification algorithm is based on the evaluation of the envelope curve of the phonocardiogram. for the evaluation, in contrast to other studies, measurements on 12 probands were conducted in different physiological conditions. moreover, for each measurement the auscultation point, posture and physical stress were varied. the proposed envelope-based algorithm is tested with two different methods for envelope curve extraction: the hilbert transform and the short-time fourier transform. the performance of the classification of the first heart sounds is evaluated by using a reference electrocardiogram. overall, by using the hilbert transform, the algorithm has a better performance regarding the f1-score and computational effort. the proposed algorithm achieves for the s1 classification an f1-score up to 95.7% and in average 90.5%. the algorithm is robust against the age, bmi, posture, heart rate and auscultation point (except measurements on the back) of the subjects."
http://orkg.org/orkg/resource/R146051,Innovation Management in the Context of Smart Cities Digital Transformation,10.18775/ijmsba.1849-5664-5419.2014.61.1002,crossref,"<jats:p>The paper introduces important aspects of doctoral research concerning innovation management in the context of business management challenges posed by digital transformation. The research was conducted as part of the Research Centre of Business Administration in The Bucharest University of Economic Studies, Romania. The study aims to identify and display key components of innovation management – with a primary focus on topics spurred by the recent wave of digital evolution. Against this background, the issue of smart city solutions makes for an interesting case – firstly, because it affects a large number of people and businesses around the globe and secondly, the complexity of the topic forces companies to pursue different innovation management approaches to successfully manage its associated challenges as well as opportunities. The paper consists of an overview on the existing literature and a concise outline of our research. Both researches from professional associations as well as recognized publishers were considered. Furthermore, market data were gathered and processed. More than 50 publications were analyzed to better understand trends in digital transformation and its impact on innovation management. Our research revealed that in the light of the fundamental challenges posed by digitization, companies are required to take a structured approach towards their innovation management options. In the context of smart city solutions, the adoption of the “4I Solutions Model” enables businesses to choose the strategic option suitable to their individual case. Concisely, this framework includes four different approaches ranging from initiating groundwork innovation internally to establishing partnerships with selected external parties.</jats:p>","the paper introduces important aspects of doctoral research concerning innovation management in the context of business management challenges posed by digital transformation. the research was conducted as part of the research centre of business administration in the bucharest university of economic studies, romania. the study aims to identify and display key components of innovation management – with a primary focus on topics spurred by the recent wave of digital evolution. against this background, the issue of smart city solutions makes for an interesting case – firstly, because it affects a large number of people and businesses around the globe and secondly, the complexity of the topic forces companies to pursue different innovation management approaches to successfully manage its associated challenges as well as opportunities. the paper consists of an overview on the existing literature and a concise outline of our research. both researches from professional associations as well as recognized publishers were considered. furthermore, market data were gathered and processed. more than 50 publications were analyzed to better understand trends in digital transformation and its impact on innovation management. our research revealed that in the light of the fundamental challenges posed by digitization, companies are required to take a structured approach towards their innovation management options. in the context of smart city solutions, the adoption of the “4i solutions model” enables businesses to choose the strategic option suitable to their individual case. concisely, this framework includes four different approaches ranging from initiating groundwork innovation internally to establishing partnerships with selected external parties."
http://orkg.org/orkg/resource/R150254,"Integrated Disease Surveillance and Response (IDSR) strategy: current status, challenges and perspectives for the future in Africa",10.1136/bmjgh-2019-001427,crossref,"<jats:p>In 1998, the WHO African region adopted a strategy called Integrated Disease Surveillance and Response (IDSR). Here, we present the current status of IDSR implementation; and provide some future perspectives for enhancing the IDSR strategy in Africa.</jats:p><jats:p>In 2017, we used two data sources to compile information on the status of IDSR implementation: a pretested rapid assessment questionnaire sent out biannually to all countries and quarterly compilation of data for two IDSR key performance indicators (KPI). The first KPI measures country IDSR performance and the second KPI tracks the number of countries that the WHO secretariat supports to scale up IDSR. The KPI data for 2017 were compared with a retrospective baseline for 2014.</jats:p><jats:p>By December 2017, 44 of 47 African countries (94%) were implementing IDSR. Of the 44 countries implementing IDSR, 40 (85%) had initiated IDSR training at subnational level; 32 (68%) had commenced community-based surveillance; 35 (74%) had event-based surveillance; 33 (70%) had electronic IDSR; and 32 (68%) had a weekly/monthly bulletin for sharing IDSR data. Thirty-two countries (68%) had achieved the timeliness and completeness threshold of at least 80% of the reporting units. However, only 12 countries (26%) had the desired target of at least 90% IDSR implementation coverage at the peripheral level.</jats:p><jats:p>After 20 years of implementing IDSR, there are major achievements in the indicator-based surveillance systems. However, major gaps were identified in event-based surveillance. All African countries should enhance IDSR everywhere.</jats:p>","in 1998, the who african region adopted a strategy called integrated disease surveillance and response (idsr). here, we present the current status of idsr implementation; and provide some future perspectives for enhancing the idsr strategy in africa. in 2017, we used two data sources to compile information on the status of idsr implementation: a pretested rapid assessment questionnaire sent out biannually to all countries and quarterly compilation of data for two idsr key performance indicators (kpi). the first kpi measures country idsr performance and the second kpi tracks the number of countries that the who secretariat supports to scale up idsr. the kpi data for 2017 were compared with a retrospective baseline for 2014. by december 2017, 44 of 47 african countries (94%) were implementing idsr. of the 44 countries implementing idsr, 40 (85%) had initiated idsr training at subnational level; 32 (68%) had commenced community-based surveillance; 35 (74%) had event-based surveillance; 33 (70%) had electronic idsr; and 32 (68%) had a weekly/monthly bulletin for sharing idsr data. thirty-two countries (68%) had achieved the timeliness and completeness threshold of at least 80% of the reporting units. however, only 12 countries (26%) had the desired target of at least 90% idsr implementation coverage at the peripheral level. after 20 years of implementing idsr, there are major achievements in the indicator-based surveillance systems. however, major gaps were identified in event-based surveillance. all african countries should enhance idsr everywhere."
http://orkg.org/orkg/resource/R145720,Investigations on Tailored Forming of AISI 52100 as Rolling Bearing Raceway,10.3390/met10101363,crossref,"<jats:p>Hybrid cylindrical roller thrust bearing washers of type 81212 were manufactured by tailored forming. An AISI 1022M base material, featuring a sufficient strength for structural loads, was cladded with the bearing steel AISI 52100 by plasma transferred arc welding (PTA). Though AISI 52100 is generally regarded as non-weldable, it could be applied as a cladding material by adjusting PTA parameters. The cladded parts were investigated after each individual process step and subsequently tested under rolling contact load. Welding defects that could not be completely eliminated by the subsequent hot forming were characterized by means of scanning acoustic microscopy and micrographs. Below the surface, pores with a typical size of ten µm were found to a depth of about 0.45 mm. In the material transition zone and between individual weld seams, larger voids were observed. Grinding of the surface after heat treatment caused compressive residual stresses near the surface with a relatively small depth. Fatigue tests were carried out on an FE8 test rig. Eighty-two percent of the calculated rating life for conventional bearings was achieved. A high failure slope of the Weibull regression was determined. A relationship between the weld defects and the fatigue behavior is likely.</jats:p>","hybrid cylindrical roller thrust bearing washers of type 81212 were manufactured by tailored forming. an aisi 1022m base material, featuring a sufficient strength for structural loads, was cladded with the bearing steel aisi 52100 by plasma transferred arc welding (pta). though aisi 52100 is generally regarded as non-weldable, it could be applied as a cladding material by adjusting pta parameters. the cladded parts were investigated after each individual process step and subsequently tested under rolling contact load. welding defects that could not be completely eliminated by the subsequent hot forming were characterized by means of scanning acoustic microscopy and micrographs. below the surface, pores with a typical size of ten µm were found to a depth of about 0.45 mm. in the material transition zone and between individual weld seams, larger voids were observed. grinding of the surface after heat treatment caused compressive residual stresses near the surface with a relatively small depth. fatigue tests were carried out on an fe8 test rig. eighty-two percent of the calculated rating life for conventional bearings was achieved. a high failure slope of the weibull regression was determined. a relationship between the weld defects and the fatigue behavior is likely."
http://orkg.org/orkg/resource/R145729,Manufacturing and Evaluation of Multi-Material Axial-Bearing Washers by Tailored Forming,10.3390/met9020232,crossref,"<jats:p>Components subject to rolling contact fatigue, such as gears and rolling bearings, are among the fundamental machine elements in mechanical and vehicle engineering. Rolling bearings are generally not designed to be fatigue-resistant, as the necessary oversizing is not technically and economically marketable. In order to improve the load-bearing capacity, resource efficiency and application possibilities of rolling bearings and other possible multi-material solid components, a new process chain was developed at Leibniz University Hannover as a part of the Collaborative Research Centre 1153 “Tailored Forming”. Semi-finished products, already joined before the forming process, are used here to allow a further optimisation of joint quality by forming and finishing. In this paper, a plasma-powder-deposition welding process is presented, which enables precise material deposition and control of the welding depth. For this study, bearing washers (serving as rolling bearing raceways) of a cylindrical roller thrust bearing, similar to type 81212 with a multi-layer structure, were manufactured. A previously non-weldable high-performance material, steel AISI 5140, was used as the cladding layer. Depending on the degree of forming, grain-refinement within the welded material was achieved by thermo-mechanical treatment of the joining zone during the forming process. This grain-refinements lead to an improvement of the mechanical properties and thus, to a higher lifetime for washers of an axial cylindrical roller bearing, which were examined as an exemplary component on a fatigue test bench. To evaluate the bearing washers, the results of the bearing tests were compared with industrial bearings and deposition welded axial-bearing washers without subsequent forming. In addition, the bearing washers were analysed micro-tribologically and by scanning acoustic microscopy both after welding and after the forming process. Nano-scratch tests were carried out on the bearing washers to analyse the layer properties. Together with the results of additional microscopic images of the surface and cross-sections, the causes of failure due to fatigue and wear were identified.</jats:p>","components subject to rolling contact fatigue, such as gears and rolling bearings, are among the fundamental machine elements in mechanical and vehicle engineering. rolling bearings are generally not designed to be fatigue-resistant, as the necessary oversizing is not technically and economically marketable. in order to improve the load-bearing capacity, resource efficiency and application possibilities of rolling bearings and other possible multi-material solid components, a new process chain was developed at leibniz university hannover as a part of the collaborative research centre 1153 “tailored forming”. semi-finished products, already joined before the forming process, are used here to allow a further optimisation of joint quality by forming and finishing. in this paper, a plasma-powder-deposition welding process is presented, which enables precise material deposition and control of the welding depth. for this study, bearing washers (serving as rolling bearing raceways) of a cylindrical roller thrust bearing, similar to type 81212 with a multi-layer structure, were manufactured. a previously non-weldable high-performance material, steel aisi 5140, was used as the cladding layer. depending on the degree of forming, grain-refinement within the welded material was achieved by thermo-mechanical treatment of the joining zone during the forming process. this grain-refinements lead to an improvement of the mechanical properties and thus, to a higher lifetime for washers of an axial cylindrical roller bearing, which were examined as an exemplary component on a fatigue test bench. to evaluate the bearing washers, the results of the bearing tests were compared with industrial bearings and deposition welded axial-bearing washers without subsequent forming. in addition, the bearing washers were analysed micro-tribologically and by scanning acoustic microscopy both after welding and after the forming process. nano-scratch tests were carried out on the bearing washers to analyse the layer properties. together with the results of additional microscopic images of the surface and cross-sections, the causes of failure due to fatigue and wear were identified."
http://orkg.org/orkg/resource/R145358,Mass Gathering Surveillance: New ESSENCE Report  and Collaboration Win Gold in OR,10.5210/ojphi.v9i1.7719,crossref,"<jats:p>ObjectiveTo streamline production of a daily epidemiology report includingsyndromic surveillance, notifiable disease, and outbreak data duringa mass gatheringIntroductionThe 2016 U.S. Olympic Track and Field Team Trials were heldJuly 1-10 in Eugene, OR. This mass gathering included over 1,000athletes, 1,500 volunteers, and 175,000 spectators. The Oregon PublicHealth Division (PHD) and Lane County Public Health (LCPH)participated in pre-event planning and collaborated to produce adaily epidemiology report for the Incident Management Team (IMT)during the event. The state and county public health agencies hadcollaborated on surveillance for prior mass gatherings, including the2012 Trials. However, 2016 was the first opportunity to use completestate and county syndromic surveillance data.MethodsPHD staff developed an ESSENCE report, highlighting sevenpriority health outcomes: total emergency department visits; injury,gastrointestinal, respiratory, and fever syndromes; and asthma-like and heat-related illness queries. The report included side-by-side comparisons of county and state time series graphs, a tablesummarizing reportable diseases, and space to narratively describeoutbreaks. PHD staff did a virtual demonstration and in-persontutorial for LCPH staff on how to run the report. ESSENCE accesspermissions had to be modified so that county users could see andproduce state time-series graphs but not data details for non-LaneCounty visits. Emphasis was placed on interpretation of likelyscenarios, i.e., one or two days with a warning that was not indicativeof an incident of public health importance.ResultsDuring the event, LCPH staff were able to run the reportsuccessfully, i.e., there were no technical glitches. For the first fewdays, LCPH staff consulted with PHD staff about epidemiologicalinterpretation. State data were of specific interest since data detailswere suppressed. Additionally, increases were seen in the injurysyndrome in the days preceding the July 4 holiday. Stratification bykey demographic factors and looking at subsyndrome breakdownson warning and alert days provided the needed information withoutrequiring the use of the detail details.ConclusionsAfter the event, there were three main recommendations forimproving the process.LCPH suggested that the side-by-side visualization of countyand state time series graphs was useful to see trends but the relativescale of the number of visits was unclear due to size and placement(see figure 1). Solutions for future reports include additionalexplanatory text, limiting the report to only county data, and alternativevisualizations that highlight the differences in visit magnitude.As part of the IMT process, the LCPH lead felt that her efforts tophysically go to the Emergency Operations Center to run the reporthelped facilitate communication with partners. However, it is notclear if this effort directly translated into IMT use of the report, whichwas posted to the online event management system and not includedin the daily situation status reports. While LCPH leadership and staffreported anecdotally that they found the report to be very useful,no formal evaluation of use was done with either public health orIMT staff. In advance of the next event, state and county staff shouldprepare evaluation metrics.The report feature in ESSENCE is a bit cumbersome to set up, butit allows for easy production of appealing and customizable reports.This template can be modified for future mass gatherings, includingathletic competitions and county fairs. PHD staff will continueto collaborate with LCPH to repurpose and improve the report foruse in Lane and other counties. Fostering local user comfort withinterpreting ESSENCE data and generating summaries for local useis a priority of the OR ESSENCE team.</jats:p>","objectiveto streamline production of a daily epidemiology report includingsyndromic surveillance, notifiable disease, and outbreak data duringa mass gatheringintroductionthe 2016 u.s. olympic track and field team trials were heldjuly 1-10 in eugene, or. this mass gathering included over 1,000athletes, 1,500 volunteers, and 175,000 spectators. the oregon publichealth division (phd) and lane county public health (lcph)participated in pre-event planning and collaborated to produce adaily epidemiology report for the incident management team (imt)during the event. the state and county public health agencies hadcollaborated on surveillance for prior mass gatherings, including the2012 trials. however, 2016 was the first opportunity to use completestate and county syndromic surveillance data.methodsphd staff developed an essence report, highlighting sevenpriority health outcomes: total emergency department visits; injury,gastrointestinal, respiratory, and fever syndromes; and asthma-like and heat-related illness queries. the report included side-by-side comparisons of county and state time series graphs, a tablesummarizing reportable diseases, and space to narratively describeoutbreaks. phd staff did a virtual demonstration and in-persontutorial for lcph staff on how to run the report. essence accesspermissions had to be modified so that county users could see andproduce state time-series graphs but not data details for non-lanecounty visits. emphasis was placed on interpretation of likelyscenarios, i.e., one or two days with a warning that was not indicativeof an incident of public health importance.resultsduring the event, lcph staff were able to run the reportsuccessfully, i.e., there were no technical glitches. for the first fewdays, lcph staff consulted with phd staff about epidemiologicalinterpretation. state data were of specific interest since data detailswere suppressed. additionally, increases were seen in the injurysyndrome in the days preceding the july 4 holiday. stratification bykey demographic factors and looking at subsyndrome breakdownson warning and alert days provided the needed information withoutrequiring the use of the detail details.conclusionsafter the event, there were three main recommendations forimproving the process.lcph suggested that the side-by-side visualization of countyand state time series graphs was useful to see trends but the relativescale of the number of visits was unclear due to size and placement(see figure 1). solutions for future reports include additionalexplanatory text, limiting the report to only county data, and alternativevisualizations that highlight the differences in visit magnitude.as part of the imt process, the lcph lead felt that her efforts tophysically go to the emergency operations center to run the reporthelped facilitate communication with partners. however, it is notclear if this effort directly translated into imt use of the report, whichwas posted to the online event management system and not includedin the daily situation status reports. while lcph leadership and staffreported anecdotally that they found the report to be very useful,no formal evaluation of use was done with either public health orimt staff. in advance of the next event, state and county staff shouldprepare evaluation metrics.the report feature in essence is a bit cumbersome to set up, butit allows for easy production of appealing and customizable reports.this template can be modified for future mass gatherings, includingathletic competitions and county fairs. phd staff will continueto collaborate with lcph to repurpose and improve the report foruse in lane and other counties. fostering local user comfort withinterpreting essence data and generating summaries for local useis a priority of the or essence team."
http://orkg.org/orkg/resource/R145371,"Local Public Health Surveillance of Heroin-Related Morbidity and Mortality, Orange County, Florida, 2010-2014",10.1177/0033354917709783,crossref,"<jats:sec><jats:title>Objectives:</jats:title><jats:p> Heroin-related deaths have increased substantially in the past 10 years in the United States, particularly in Florida. Our objectives were to measure heroin-related morbidity and mortality rates in Orange County, Florida, and to assess trends in those rates during 2010-2014. </jats:p></jats:sec><jats:sec><jats:title>Methods:</jats:title><jats:p> We used 3 heroin surveillance methods, based on data from the Florida Medical Examiner, the Florida Agency for Health Care Administration (AHCA), and the Electronic Surveillance System for the Early Notification of Community-Based Epidemics–Florida (ESSENCE-FL). We conducted descriptive and geographic spatial analyses of all 3 data sets, determined heroin-related mortality and morbidity (emergency department [ED] visit) rates, and compared the timeliness of data availability from the 3 data sources. </jats:p></jats:sec><jats:sec><jats:title>Results:</jats:title><jats:p> Heroin-related deaths in Orange County increased by 590%, from 10 in 2010 to 69 in 2014. Heroin-related ED visits during the same period increased 12-fold (from 13 to 154) and 6-fold (from 49 to 307) when based on AHCA and ESSENCE-FL data, respectively. ESSENCE-FL identified 140% more heroin-related visits than did AHCA. Spatial analysis found geographic clustering of heroin-related morbidity and mortality. Hospitals facing the greatest burden of heroin-related ED visits were close to communities with the highest crude heroin-related ED visit rates. Of the 3 data sources, ESSENCE-FL provided the timeliest data availability. </jats:p></jats:sec><jats:sec><jats:title>Conclusions:</jats:title><jats:p> These 3 data sources can be considered acceptable surveillance systems for monitoring heroin-related events in Orange County. The timely availability of data from ESSENCE-FL makes it the most useful source for obtaining near–real-time data about the heroin epidemic, potentially leading to improved identification of populations most in need of interventions to reduce morbidity and mortality. </jats:p></jats:sec>","objectives: heroin-related deaths have increased substantially in the past 10 years in the united states, particularly in florida. our objectives were to measure heroin-related morbidity and mortality rates in orange county, florida, and to assess trends in those rates during 2010-2014. methods: we used 3 heroin surveillance methods, based on data from the florida medical examiner, the florida agency for health care administration (ahca), and the electronic surveillance system for the early notification of community-based epidemics–florida (essence-fl). we conducted descriptive and geographic spatial analyses of all 3 data sets, determined heroin-related mortality and morbidity (emergency department [ed] visit) rates, and compared the timeliness of data availability from the 3 data sources. results: heroin-related deaths in orange county increased by 590%, from 10 in 2010 to 69 in 2014. heroin-related ed visits during the same period increased 12-fold (from 13 to 154) and 6-fold (from 49 to 307) when based on ahca and essence-fl data, respectively. essence-fl identified 140% more heroin-related visits than did ahca. spatial analysis found geographic clustering of heroin-related morbidity and mortality. hospitals facing the greatest burden of heroin-related ed visits were close to communities with the highest crude heroin-related ed visit rates. of the 3 data sources, essence-fl provided the timeliest data availability. conclusions: these 3 data sources can be considered acceptable surveillance systems for monitoring heroin-related events in orange county. the timely availability of data from essence-fl makes it the most useful source for obtaining near–real-time data about the heroin epidemic, potentially leading to improved identification of populations most in need of interventions to reduce morbidity and mortality."
http://orkg.org/orkg/resource/R145318,"Electronic Surveillance System for the Early Notification of Community-Based Epidemics (ESSENCE): Overview, Components, and Public Health Applications",10.2196/26303,crossref,"<jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>The Electronic Surveillance System for the Early Notification of Community-Based Epidemics (ESSENCE) is a secure web-based tool that enables health care practitioners to monitor health indicators of public health importance for the detection and tracking of disease outbreaks, consequences of severe weather, and other events of concern. The ESSENCE concept began in an internally funded project at the Johns Hopkins University Applied Physics Laboratory, advanced with funding from the State of Maryland, and broadened in 1999 as a collaboration with the Walter Reed Army Institute for Research. Versions of the system have been further developed by Johns Hopkins University Applied Physics Laboratory in multiple military and civilian programs for the timely detection and tracking of health threats.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Objective</jats:title>\n            <jats:p>This study aims to describe the components and development of a biosurveillance system increasingly coordinating all-hazards health surveillance and infectious disease monitoring among large and small health departments, to list the key features and lessons learned in the growth of this system, and to describe the range of initiatives and accomplishments of local epidemiologists using it.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Methods</jats:title>\n            <jats:p>The features of ESSENCE include spatial and temporal statistical alerting, custom querying, user-defined alert notifications, geographical mapping, remote data capture, and event communications. To expedite visualization, configurable and interactive modes of data stratification and filtering, graphical and tabular customization, user preference management, and sharing features allow users to query data and view geographic representations, time series and data details pages, and reports. These features allow ESSENCE users to gather and organize the resulting wealth of information into a coherent view of population health status and communicate findings among users.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>The resulting broad utility, applicability, and adaptability of this system led to the adoption of ESSENCE by the Centers for Disease Control and Prevention, numerous state and local health departments, and the Department of Defense, both nationally and globally. The open-source version of Suite for Automated Global Electronic bioSurveillance is available for global, resource-limited settings. Resourceful users of the US National Syndromic Surveillance Program ESSENCE have applied it to the surveillance of infectious diseases, severe weather and natural disaster events, mass gatherings, chronic diseases and mental health, and injury and substance abuse.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>With emerging high-consequence communicable diseases and other health conditions, the continued user requirement–driven enhancements of ESSENCE demonstrate an adaptable disease surveillance capability focused on the everyday needs of public health. The challenge of a live system for widely distributed users with multiple different data sources and high throughput requirements has driven a novel, evolving architecture design.</jats:p>\n          </jats:sec>","\n background \n the electronic surveillance system for the early notification of community-based epidemics (essence) is a secure web-based tool that enables health care practitioners to monitor health indicators of public health importance for the detection and tracking of disease outbreaks, consequences of severe weather, and other events of concern. the essence concept began in an internally funded project at the johns hopkins university applied physics laboratory, advanced with funding from the state of maryland, and broadened in 1999 as a collaboration with the walter reed army institute for research. versions of the system have been further developed by johns hopkins university applied physics laboratory in multiple military and civilian programs for the timely detection and tracking of health threats. \n \n \n objective \n this study aims to describe the components and development of a biosurveillance system increasingly coordinating all-hazards health surveillance and infectious disease monitoring among large and small health departments, to list the key features and lessons learned in the growth of this system, and to describe the range of initiatives and accomplishments of local epidemiologists using it. \n \n \n methods \n the features of essence include spatial and temporal statistical alerting, custom querying, user-defined alert notifications, geographical mapping, remote data capture, and event communications. to expedite visualization, configurable and interactive modes of data stratification and filtering, graphical and tabular customization, user preference management, and sharing features allow users to query data and view geographic representations, time series and data details pages, and reports. these features allow essence users to gather and organize the resulting wealth of information into a coherent view of population health status and communicate findings among users. \n \n \n results \n the resulting broad utility, applicability, and adaptability of this system led to the adoption of essence by the centers for disease control and prevention, numerous state and local health departments, and the department of defense, both nationally and globally. the open-source version of suite for automated global electronic biosurveillance is available for global, resource-limited settings. resourceful users of the us national syndromic surveillance program essence have applied it to the surveillance of infectious diseases, severe weather and natural disaster events, mass gatherings, chronic diseases and mental health, and injury and substance abuse. \n \n \n conclusions \n with emerging high-consequence communicable diseases and other health conditions, the continued user requirement–driven enhancements of essence demonstrate an adaptable disease surveillance capability focused on the everyday needs of public health. the challenge of a live system for widely distributed users with multiple different data sources and high throughput requirements has driven a novel, evolving architecture design. \n"
http://orkg.org/orkg/resource/R145076,"Survey of Clostridium difficile infection surveillance systems in Europe, 2011",10.2807/1560-7917.es.2016.21.29.30291,crossref,"<jats:p>To develop a European surveillance protocol for <jats:italic>Clostridium difficile</jats:italic> infection (CDI), existing national CDI surveillance systems were assessed in 2011. A web-based electronic form was provided for all national coordinators of the European CDI Surveillance Network (ECDIS-Net). Of 35 national coordinators approached, 33 from 31 European countries replied. Surveillance of CDI was in place in 14 of the 31 countries, comprising 18 different nationwide systems. Three of 14 countries with CDI surveillance used public health notification of cases as the route of reporting, and in another three, reporting was limited to public health notification of cases of severe CDI. The CDI definitions published by the European Society of Clinical Microbiology and Infectious Diseases (ESCMID) and the European Centre for Disease Prevention and Control (ECDC) were widely used, but there were differing definitions to distinguish between community- and healthcare-associated cases. All CDI surveillance systems except one reported annual national CDI rates (calculated as number of cases per patient-days). Only four surveillance systems regularly integrated microbiological data (typing and susceptibility testing results). Surveillance methods varied considerably between countries, which emphasises the need for a harmonised European protocol to allow consistent monitoring of the CDI epidemiology at European level. The results of this survey were used to develop a harmonised EU-wide hospital-based CDI surveillance protocol. </jats:p>","to develop a european surveillance protocol for clostridium difficile infection (cdi), existing national cdi surveillance systems were assessed in 2011. a web-based electronic form was provided for all national coordinators of the european cdi surveillance network (ecdis-net). of 35 national coordinators approached, 33 from 31 european countries replied. surveillance of cdi was in place in 14 of the 31 countries, comprising 18 different nationwide systems. three of 14 countries with cdi surveillance used public health notification of cases as the route of reporting, and in another three, reporting was limited to public health notification of cases of severe cdi. the cdi definitions published by the european society of clinical microbiology and infectious diseases (escmid) and the european centre for disease prevention and control (ecdc) were widely used, but there were differing definitions to distinguish between community- and healthcare-associated cases. all cdi surveillance systems except one reported annual national cdi rates (calculated as number of cases per patient-days). only four surveillance systems regularly integrated microbiological data (typing and susceptibility testing results). surveillance methods varied considerably between countries, which emphasises the need for a harmonised european protocol to allow consistent monitoring of the cdi epidemiology at european level. the results of this survey were used to develop a harmonised eu-wide hospital-based cdi surveillance protocol."
http://orkg.org/orkg/resource/R145065,Description and validation of a new automated surveillance system for Clostridium difficile in Denmark,10.1017/s0950268817001315,crossref,"<jats:title>SUMMARY</jats:title><jats:p>The surveillance of <jats:italic>Clostridium difficile</jats:italic> (CD) in Denmark consists of laboratory based data from Departments of Clinical Microbiology (DCMs) sent to the National Registry of Enteric Pathogens (NREP). We validated a new surveillance system for CD based on the Danish Microbiology Database (MiBa). MiBa automatically collects microbiological test results from all Danish DCMs. We built an algorithm to identify positive test results for CD recorded in MiBa. A CD case was defined as a person with a positive culture for CD or PCR detection of toxin A and/or B and/or binary toxin. We compared CD cases identified through the MiBa-based surveillance with those reported to NREP and locally in five DCMs representing different Danish regions. During 2010–2014, NREP reported 13 896 CD cases, and the MiBa-based surveillance 21 252 CD cases. There was a 99·9% concordance between the local datasets and the MiBa-based surveillance. Surveillance based on MiBa was superior to the current surveillance system, and the findings show that the number of CD cases in Denmark hitherto has been under-reported. There were only minor differences between local data and the MiBa-based surveillance, showing the completeness and validity of CD data in MiBa. This nationwide electronic system can greatly strengthen surveillance and research in various applications.</jats:p>","summary the surveillance of clostridium difficile (cd) in denmark consists of laboratory based data from departments of clinical microbiology (dcms) sent to the national registry of enteric pathogens (nrep). we validated a new surveillance system for cd based on the danish microbiology database (miba). miba automatically collects microbiological test results from all danish dcms. we built an algorithm to identify positive test results for cd recorded in miba. a cd case was defined as a person with a positive culture for cd or pcr detection of toxin a and/or b and/or binary toxin. we compared cd cases identified through the miba-based surveillance with those reported to nrep and locally in five dcms representing different danish regions. during 2010–2014, nrep reported 13 896 cd cases, and the miba-based surveillance 21 252 cd cases. there was a 99·9% concordance between the local datasets and the miba-based surveillance. surveillance based on miba was superior to the current surveillance system, and the findings show that the number of cd cases in denmark hitherto has been under-reported. there were only minor differences between local data and the miba-based surveillance, showing the completeness and validity of cd data in miba. this nationwide electronic system can greatly strengthen surveillance and research in various applications."
http://orkg.org/orkg/resource/R146639,DNA barcodes for species delimitation in Chironomidae (Diptera): a case study on the genus Labrundinia,10.4039/tce.2013.44,crossref,"<jats:title>Abstract</jats:title><jats:p>In this study, we analysed the applicability of DNA barcodes for delimitation of 79 specimens of 13 species of nonbiting midges in the subfamily Tanypodinae (Diptera: Chironomidae) from São Paulo State, Brazil. Our results support DNA barcoding as an excellent tool for species identification and for solving taxonomic conflicts in genus<jats:italic>Labrundinia.</jats:italic>Molecular analysis of cytochrome c oxidase subunit I (COI) gene sequences yielded taxon identification trees, supporting 13 cohesive species clusters, of which three similar groups were subsequently linked to morphological variation at the larval and pupal stage. Additionally, another cluster previously described by means of morphology was linked to molecular markers. We found a distinct barcode gap, and in some species substantial interspecific pairwise divergences (up to 19.3%) were observed, which permitted identification of all analysed species. The results also indicated that barcodes can be used to associate life stages of chironomids since COI was easily amplified and sequenced from different life stages with universal barcode primers.</jats:p>","abstract in this study, we analysed the applicability of dna barcodes for delimitation of 79 specimens of 13 species of nonbiting midges in the subfamily tanypodinae (diptera: chironomidae) from são paulo state, brazil. our results support dna barcoding as an excellent tool for species identification and for solving taxonomic conflicts in genus labrundinia. molecular analysis of cytochrome c oxidase subunit i (coi) gene sequences yielded taxon identification trees, supporting 13 cohesive species clusters, of which three similar groups were subsequently linked to morphological variation at the larval and pupal stage. additionally, another cluster previously described by means of morphology was linked to molecular markers. we found a distinct barcode gap, and in some species substantial interspecific pairwise divergences (up to 19.3%) were observed, which permitted identification of all analysed species. the results also indicated that barcodes can be used to associate life stages of chironomids since coi was easily amplified and sequenced from different life stages with universal barcode primers."
http://orkg.org/orkg/resource/R144353,Exosome-based nanocarriers as bio-inspired and versatile vehicles for drug delivery: recent advances and challenges,10.1039/c9tb00170k,crossref,"<p>Exosomes as drug vehicles have attracted increasing attention due to their ability of shuttling proteins, lipids and genes among cells and natural affinity to target cells.</p>","exosomes as drug vehicles have attracted increasing attention due to their ability of shuttling proteins, lipids and genes among cells and natural affinity to target cells."
http://orkg.org/orkg/resource/R146032,Digital transformation of existing cities,10.1051/e3sconf/201911002027,crossref,"<jats:p>The article focuses on the range of problems arising on the way of innovative technologies implementation in the structure of existing cities. The concept of intellectualization of historic cities, as illustrated by Samara, is offered, which was chosen for the realization of a large Russian project “Smart City. Successful Region” in 2018. One of the problems was to study the experience of information hubs projecting with the purpose of determination of their priority functional directions. The following typology of information hubs was made: scientific and research ones, scientific and technical ones, innovative and cultural ones, cultural and informational ones, scientific and informational ones, technological ones, centres for data processing, scientific centres with experimental and production laboratories. As a result of the conducted research, a suggestion on smart city’s infrastructure is developed, the final levels of innovative technologies implementation in the structure of historic territories are determined. A model suggestion on the formation of a scientific and project centre with experimental and production laboratories branded as named “Park-plant” is developed. Smart (as well as real) city technologies, which are supposed to be placed on the territory of “Park-plant”, are systematized. The organizational structure of the promotion of model projects is offered according to the concept of “triad of development agents”, in which the flagship university – urban community – park-plant interact within the project programme. The effects of the development of the being renovated territory of the historic city centre are enumerated.</jats:p>","the article focuses on the range of problems arising on the way of innovative technologies implementation in the structure of existing cities. the concept of intellectualization of historic cities, as illustrated by samara, is offered, which was chosen for the realization of a large russian project “smart city. successful region” in 2018. one of the problems was to study the experience of information hubs projecting with the purpose of determination of their priority functional directions. the following typology of information hubs was made: scientific and research ones, scientific and technical ones, innovative and cultural ones, cultural and informational ones, scientific and informational ones, technological ones, centres for data processing, scientific centres with experimental and production laboratories. as a result of the conducted research, a suggestion on smart city’s infrastructure is developed, the final levels of innovative technologies implementation in the structure of historic territories are determined. a model suggestion on the formation of a scientific and project centre with experimental and production laboratories branded as named “park-plant” is developed. smart (as well as real) city technologies, which are supposed to be placed on the territory of “park-plant”, are systematized. the organizational structure of the promotion of model projects is offered according to the concept of “triad of development agents”, in which the flagship university – urban community – park-plant interact within the project programme. the effects of the development of the being renovated territory of the historic city centre are enumerated."
http://orkg.org/orkg/resource/R144074,Mitochondria-targeted fluorescent thermometer monitors intracellular temperature gradient,10.1039/c5cc01088h,crossref,"<p>A small molecule fluorescent thermometer targeting mitochondria (Mito thermo yellow) enables us to monitor the intracellular temperature gradient, generated by exogenous heating in various cells.</p>","a small molecule fluorescent thermometer targeting mitochondria (mito thermo yellow) enables us to monitor the intracellular temperature gradient, generated by exogenous heating in various cells."
http://orkg.org/orkg/resource/R144217,Humanitarian applications of machine learning with remote-sensing data: review and case study in refugee settlement mapping,10.1098/rsta.2017.0363,crossref,"<jats:p>The coordination of humanitarian relief, e.g. in a natural disaster or a conflict situation, is often complicated by a scarcity of data to inform planning. Remote sensing imagery, from satellites or drones, can give important insights into conditions on the ground, including in areas which are difficult to access. Applications include situation awareness after natural disasters, structural damage assessment in conflict, monitoring human rights violations or population estimation in settlements. We review machine learning approaches for automating these problems, and discuss their potential and limitations. We also provide a case study of experiments using deep learning methods to count the numbers of structures in multiple refugee settlements in Africa and the Middle East. We find that while high levels of accuracy are possible, there is considerable variation in the characteristics of imagery collected from different sensors and regions. In this, as in the other applications discussed in the paper, critical inferences must be made from a relatively small amount of pixel data. We, therefore, consider that using machine learning systems as an augmentation of human analysts is a reasonable strategy to transition from current fully manual operational pipelines to ones which are both more efficient and have the necessary levels of quality control.</jats:p>\n          <jats:p>This article is part of a discussion meeting issue ‘The growing ubiquity of algorithms in society: implications, impacts and innovations’.</jats:p>","the coordination of humanitarian relief, e.g. in a natural disaster or a conflict situation, is often complicated by a scarcity of data to inform planning. remote sensing imagery, from satellites or drones, can give important insights into conditions on the ground, including in areas which are difficult to access. applications include situation awareness after natural disasters, structural damage assessment in conflict, monitoring human rights violations or population estimation in settlements. we review machine learning approaches for automating these problems, and discuss their potential and limitations. we also provide a case study of experiments using deep learning methods to count the numbers of structures in multiple refugee settlements in africa and the middle east. we find that while high levels of accuracy are possible, there is considerable variation in the characteristics of imagery collected from different sensors and regions. in this, as in the other applications discussed in the paper, critical inferences must be made from a relatively small amount of pixel data. we, therefore, consider that using machine learning systems as an augmentation of human analysts is a reasonable strategy to transition from current fully manual operational pipelines to ones which are both more efficient and have the necessary levels of quality control. \n this article is part of a discussion meeting issue ‘the growing ubiquity of algorithms in society: implications, impacts and innovations’."
http://orkg.org/orkg/resource/R144256,"Nanoemulsions: formation, properties and applications",10.1039/c5sm02958a,crossref,<p>Nanoemulsions are kinetically stable liquid-in-liquid dispersions with droplet sizes on the order of 100 nm.</p>,nanoemulsions are kinetically stable liquid-in-liquid dispersions with droplet sizes on the order of 100 nm.
http://orkg.org/orkg/resource/R144086,A cationic fluorescent polymeric thermometer for the ratiometric sensing of intracellular temperature,10.1039/c5an00420a,crossref,<p>The temperature-dependent fluorescence spectra of a new polymeric thermometer enabled highly sensitive and practical ratiometric temperature sensing inside mammalian cells.</p>,the temperature-dependent fluorescence spectra of a new polymeric thermometer enabled highly sensitive and practical ratiometric temperature sensing inside mammalian cells.
http://orkg.org/orkg/resource/R143919,Enabling Folksonomies for Knowledge Extraction: A Semantic Grounding Approach,10.4018/jswis.2012070102,crossref,"<p>Folksonomies emerge as the result of the free tagging activity of a large number of users over a variety of resources. They can be considered as valuable sources from which it is possible to obtain emerging vocabularies that can be leveraged in knowledge extraction tasks. However, when it comes to understanding the meaning of tags in folksonomies, several problems mainly related to the appearance of synonymous and ambiguous tags arise, specifically in the context of multilinguality. The authors aim to turn folksonomies into knowledge structures where tag meanings are identified, and relations between them are asserted. For such purpose, they use DBpedia as a general knowledge base from which they leverage its multilingual capabilities.</p>","folksonomies emerge as the result of the free tagging activity of a large number of users over a variety of resources. they can be considered as valuable sources from which it is possible to obtain emerging vocabularies that can be leveraged in knowledge extraction tasks. however, when it comes to understanding the meaning of tags in folksonomies, several problems mainly related to the appearance of synonymous and ambiguous tags arise, specifically in the context of multilinguality. the authors aim to turn folksonomies into knowledge structures where tag meanings are identified, and relations between them are asserted. for such purpose, they use dbpedia as a general knowledge base from which they leverage its multilingual capabilities."
http://orkg.org/orkg/resource/R143705,A highly stretchable and sensitive strain sensor based on graphene–elastomer composites with a novel double-interconnected network,10.1039/c6tc01925k,crossref,<p>A facile assembly approach was firstly reported to fabricate a highly stretchable and sensitive strain sensor based on graphene–rubber composites with a novel double-interconnected network.</p>,a facile assembly approach was firstly reported to fabricate a highly stretchable and sensitive strain sensor based on graphene–rubber composites with a novel double-interconnected network.
http://orkg.org/orkg/resource/R142756,Smart City Ontologies: Improving the effectiveness of smart city applications,10.18063/jsc.2015.01.001,crossref,"<jats:p>This paper addresses the problem of low impact of smart city applications observed in the fields of energy and transport, which constitute high-priority domains for the development of smart cities. However, these are not the only fields where the impact of smart cities has been limited. The paper provides an explanation for the low impact of various individual applications of smart cities and discusses ways of improving their effectiveness. We argue that the impact of applications depends primarily on their ontology, and secondarily on smart technology and programming features. Consequently, we start by creating an overall ontology for the smart city, defining the building blocks of this ontology with respect to the most cited definitions of smart cities, and structuring this ontology with the Protégé 5.0 editor, defining entities, class hierarchy, object properties, and data type properties. We then analyze how the ontologies of a sample of smart city applications fit into the overall Smart City Ontology, the consistency between digital spaces, knowledge processes, city domains targeted by the applications, and the types of innovation that determine their impact. In conclusion, we underline the relationships between innovation and ontology, and discuss how we can improve the effectiveness of smart city applications, combining expert and user-driven ontology design with the integration and or-chestration of applications over platforms and larger city entities such as neighborhoods, districts, clusters, and sectors of city activities.</jats:p>","this paper addresses the problem of low impact of smart city applications observed in the fields of energy and transport, which constitute high-priority domains for the development of smart cities. however, these are not the only fields where the impact of smart cities has been limited. the paper provides an explanation for the low impact of various individual applications of smart cities and discusses ways of improving their effectiveness. we argue that the impact of applications depends primarily on their ontology, and secondarily on smart technology and programming features. consequently, we start by creating an overall ontology for the smart city, defining the building blocks of this ontology with respect to the most cited definitions of smart cities, and structuring this ontology with the protégé 5.0 editor, defining entities, class hierarchy, object properties, and data type properties. we then analyze how the ontologies of a sample of smart city applications fit into the overall smart city ontology, the consistency between digital spaces, knowledge processes, city domains targeted by the applications, and the types of innovation that determine their impact. in conclusion, we underline the relationships between innovation and ontology, and discuss how we can improve the effectiveness of smart city applications, combining expert and user-driven ontology design with the integration and or-chestration of applications over platforms and larger city entities such as neighborhoods, districts, clusters, and sectors of city activities."
http://orkg.org/orkg/resource/R143695,Electrically conductive thermoplastic elastomer nanocomposites at ultralow graphene loading levels for strain sensor applications,10.1039/c5tc02751a,crossref,<p>Strain sensors with high sensitivity are reported in the thermoplastic polyurethane nanocomposites with ultralow graphene loading.</p>,strain sensors with high sensitivity are reported in the thermoplastic polyurethane nanocomposites with ultralow graphene loading.
http://orkg.org/orkg/resource/R142560,Products and Services Ontologies: A Methodology for Deriving OWL Ontologies from Industrial Categorization Standards,10.4018/jswis.2006010103,crossref,"<p>Using Semantic Web technologies for e-business tasks, like product search or content integration, requires ontologies for products and services. Their manual creation is problematic due to (1) the high specificity, resulting in a large number of concepts, and (2) the need for timely ontology maintenance due to product innovation; and due to cost, since building such ontologies from scratch requires significant resources. At the same time, industrial categorization standards, like UNSPSC, eCl@ss, eOTD, or the RosettaNet Technical Dictionary, reflect some degree of consensus and contain a wealth of concept definitions plus a hierarchy. They can thus be valuable input for creating domain ontologies. However, the transformation of existing standards, originally developed for some purpose other than ontology engineering, into useful ontologies is not as straightforward as it appears. In this paper, (1) we argue that deriving products and services ontologies from industrial taxonomies is more feasible than manual ontology engineering; (2) show that the representation of the original semantics of the input standard, especially the taxonomic relationship, is an important modeling decision that determines the usefulness of the resulting ontology; (3) illustrate the problem by analyzing existing ontologies derived from UNSPCS and eCl@ss; (4) present a methodology for creating ontologies in OWL based on the reuse of existing standards; and (5) demonstrate this approach by transforming eCl@ss 5.1 into a practically useful products and services ontology.</p>","using semantic web technologies for e-business tasks, like product search or content integration, requires ontologies for products and services. their manual creation is problematic due to (1) the high specificity, resulting in a large number of concepts, and (2) the need for timely ontology maintenance due to product innovation; and due to cost, since building such ontologies from scratch requires significant resources. at the same time, industrial categorization standards, like unspsc, ecl@ss, eotd, or the rosettanet technical dictionary, reflect some degree of consensus and contain a wealth of concept definitions plus a hierarchy. they can thus be valuable input for creating domain ontologies. however, the transformation of existing standards, originally developed for some purpose other than ontology engineering, into useful ontologies is not as straightforward as it appears. in this paper, (1) we argue that deriving products and services ontologies from industrial taxonomies is more feasible than manual ontology engineering; (2) show that the representation of the original semantics of the input standard, especially the taxonomic relationship, is an important modeling decision that determines the usefulness of the resulting ontology; (3) illustrate the problem by analyzing existing ontologies derived from unspcs and ecl@ss; (4) present a methodology for creating ontologies in owl based on the reuse of existing standards; and (5) demonstrate this approach by transforming ecl@ss 5.1 into a practically useful products and services ontology."
http://orkg.org/orkg/resource/R142471,DNA barcoding of Northern Nearctic Muscidae (Diptera) reveals high correspondence between morphological and molecular species limits,10.1186/1472-6785-12-24,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Various methods have been proposed to assign unknown specimens to known species using their DNA barcodes, while others have focused on using genetic divergence thresholds to estimate “species” diversity for a taxon, without a well-developed taxonomy and/or an extensive reference library of DNA barcodes. The major goals of the present work were to: a) conduct the largest species-level barcoding study of the Muscidae to date and characterize the range of genetic divergence values in the northern Nearctic fauna; b) evaluate the correspondence between morphospecies and barcode groupings defined using both clustering-based and threshold-based approaches; and c) use the reference library produced to address taxonomic issues.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>Our data set included 1114 individuals and their COI sequences (951 from Churchill, Manitoba), representing 160 morphologically-determined species from 25 genera, covering 89% of the known fauna of Churchill and 23% of the Nearctic fauna. Following an iterative process through which all specimens belonging to taxa with anomalous divergence values and/or monophyly issues were re-examined, identity was modified for 9 taxa, including the reinstatement of <jats:italic>Phaonia luteva</jats:italic> (Walker) <jats:bold>stat. nov.</jats:bold> as a species distinct from <jats:italic>Phaonia errans</jats:italic> (Meigen). In the post-reassessment data set, no distinct gap was found between maximum pairwise intraspecific distances (range 0.00-3.01%) and minimum interspecific distances (range: 0.77-11.33%). Nevertheless, using a clustering-based approach, all individuals within 98% of species grouped with their conspecifics with high (&gt;95%) bootstrap support; in contrast, a maximum species discrimination rate of 90% was obtained at the optimal threshold of 1.2%. DNA barcoding enabled the determination of females from 5 ambiguous species pairs and confirmed that 16 morphospecies were genetically distinct from named taxa. There were morphological differences among all distinct genetic clusters; thus, no cases of cryptic species were detected.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>Our findings reveal the great utility of building a well-populated, species-level reference barcode database against which to compare unknowns. When such a library is unavailable, it is still possible to obtain a fairly accurate (within ~10%) rapid assessment of species richness based upon a barcode divergence threshold alone, but this approach is most accurate when the threshold is tuned to a particular taxon.</jats:p>\n          </jats:sec>","abstract \n \n background \n various methods have been proposed to assign unknown specimens to known species using their dna barcodes, while others have focused on using genetic divergence thresholds to estimate “species” diversity for a taxon, without a well-developed taxonomy and/or an extensive reference library of dna barcodes. the major goals of the present work were to: a) conduct the largest species-level barcoding study of the muscidae to date and characterize the range of genetic divergence values in the northern nearctic fauna; b) evaluate the correspondence between morphospecies and barcode groupings defined using both clustering-based and threshold-based approaches; and c) use the reference library produced to address taxonomic issues. \n \n \n results \n our data set included 1114 individuals and their coi sequences (951 from churchill, manitoba), representing 160 morphologically-determined species from 25 genera, covering 89% of the known fauna of churchill and 23% of the nearctic fauna. following an iterative process through which all specimens belonging to taxa with anomalous divergence values and/or monophyly issues were re-examined, identity was modified for 9 taxa, including the reinstatement of phaonia luteva (walker) stat. nov. as a species distinct from phaonia errans (meigen). in the post-reassessment data set, no distinct gap was found between maximum pairwise intraspecific distances (range 0.00-3.01%) and minimum interspecific distances (range: 0.77-11.33%). nevertheless, using a clustering-based approach, all individuals within 98% of species grouped with their conspecifics with high (&gt;95%) bootstrap support; in contrast, a maximum species discrimination rate of 90% was obtained at the optimal threshold of 1.2%. dna barcoding enabled the determination of females from 5 ambiguous species pairs and confirmed that 16 morphospecies were genetically distinct from named taxa. there were morphological differences among all distinct genetic clusters; thus, no cases of cryptic species were detected. \n \n \n conclusions \n our findings reveal the great utility of building a well-populated, species-level reference barcode database against which to compare unknowns. when such a library is unavailable, it is still possible to obtain a fairly accurate (within ~10%) rapid assessment of species richness based upon a barcode divergence threshold alone, but this approach is most accurate when the threshold is tuned to a particular taxon. \n"
http://orkg.org/orkg/resource/R142705,Ontology Learning from Thesauri: An Experience in the Urban Domain,10.4018/978-1-61520-859-3.ch011,crossref,"<jats:p>Ontology learning is the term used to encompass methods and techniques employed for the (semi-)automatic processing of knowledge resources that facilitate the acquisition of knowledge during ontology construction. This chapter focuses on ontology learning techniques using thesauri as input sources. Thesauri are one of the most promising sources for the creation of domain ontologies thanks to the richness of term definitions, the existence of a priori relationships between terms, and the consensus provided by their extensive use in the library context. Apart from reviewing the state of the art, this chapter shows how ontology learning techniques can be applied in the urban domain for the development of domain ontologies.</jats:p>","ontology learning is the term used to encompass methods and techniques employed for the (semi-)automatic processing of knowledge resources that facilitate the acquisition of knowledge during ontology construction. this chapter focuses on ontology learning techniques using thesauri as input sources. thesauri are one of the most promising sources for the creation of domain ontologies thanks to the richness of term definitions, the existence of a priori relationships between terms, and the consensus provided by their extensive use in the library context. apart from reviewing the state of the art, this chapter shows how ontology learning techniques can be applied in the urban domain for the development of domain ontologies."
http://orkg.org/orkg/resource/R142278,Self-amplifying RNA SARS-CoV-2 lipid nanoparticle vaccine candidate induces high neutralizing antibody titers in mice,10.1038/s41467-020-17409-9,crossref,"<jats:title>Abstract</jats:title><jats:p>The spread of the SARS-CoV-2 into a global pandemic within a few months of onset motivates the development of a rapidly scalable vaccine. Here, we present a self-amplifying RNA encoding the SARS-CoV-2 spike protein encapsulated within a lipid nanoparticle\xa0(LNP) as a vaccine. We observe remarkably high and dose-dependent SARS-CoV-2 specific antibody titers in mouse sera, as well as robust neutralization of both a pseudo-virus and wild-type virus. Upon further characterization we find that the neutralization is proportional to the quantity of specific IgG and of higher magnitude than recovered COVID-19 patients. saRNA LNP immunizations induce a Th1-biased response in mice, and there is no antibody-dependent enhancement (ADE) observed. Finally, we observe high cellular responses, as characterized by IFN-<jats:italic>γ</jats:italic> production, upon re-stimulation with SARS-CoV-2 peptides. These data provide insight into the vaccine design and evaluation of immunogenicity to enable rapid translation to the clinic.</jats:p>","abstract the spread of the sars-cov-2 into a global pandemic within a few months of onset motivates the development of a rapidly scalable vaccine. here, we present a self-amplifying rna encoding the sars-cov-2 spike protein encapsulated within a lipid nanoparticle\xa0(lnp) as a vaccine. we observe remarkably high and dose-dependent sars-cov-2 specific antibody titers in mouse sera, as well as robust neutralization of both a pseudo-virus and wild-type virus. upon further characterization we find that the neutralization is proportional to the quantity of specific igg and of higher magnitude than recovered covid-19 patients. sarna lnp immunizations induce a th1-biased response in mice, and there is no antibody-dependent enhancement (ade) observed. finally, we observe high cellular responses, as characterized by ifn- γ production, upon re-stimulation with sars-cov-2 peptides. these data provide insight into the vaccine design and evaluation of immunogenicity to enable rapid translation to the clinic."
http://orkg.org/orkg/resource/R142290,A Single Dose of Self-Transcribing and Replicating RNA Based SARS-CoV-2 Vaccine Produces Protective Adaptive Immunity In Mice,10.1101/2020.09.03.280446,crossref,"<jats:title>ABSTRACT</jats:title><jats:p>A self-transcribing and replicating RNA (STARR™) based vaccine (LUNAR<jats:sup>®</jats:sup>-COV19) has been developed to prevent SARS-CoV-2 infection. The vaccine encodes an alphavirus-based replicon and the SARS-CoV-2 full length spike glycoprotein. Translation of the replicon produces a replicase complex that amplifies and prolong SARS-CoV-2 spike glycoprotein expression. A single prime vaccination in mice led to robust antibody responses, with neutralizing antibody titers increasing up to day 60. Activation of cell mediated immunity produced a strong viral antigen specific CD8<jats:sup>+</jats:sup> T lymphocyte response. Assaying for intracellular cytokine staining for IFN-γ and IL-4 positive CD4<jats:sup>+</jats:sup> T helper lymphocytes as well as anti-spike glycoprotein IgG2a/IgG1 ratios supported a strong Th1 dominant immune response. Finally, single LUNAR-COV19 vaccination at both 2 μg and 10 μg doses completely protected human ACE2 transgenic mice from both mortality and even measurable infection following wild-type SARS-CoV-2 challenge. Our findings collectively suggest the potential of Lunar-COV19 as a single dose vaccine.</jats:p>","abstract a self-transcribing and replicating rna (starr™) based vaccine (lunar ® -cov19) has been developed to prevent sars-cov-2 infection. the vaccine encodes an alphavirus-based replicon and the sars-cov-2 full length spike glycoprotein. translation of the replicon produces a replicase complex that amplifies and prolong sars-cov-2 spike glycoprotein expression. a single prime vaccination in mice led to robust antibody responses, with neutralizing antibody titers increasing up to day 60. activation of cell mediated immunity produced a strong viral antigen specific cd8 + t lymphocyte response. assaying for intracellular cytokine staining for ifn-γ and il-4 positive cd4 + t helper lymphocytes as well as anti-spike glycoprotein igg2a/igg1 ratios supported a strong th1 dominant immune response. finally, single lunar-cov19 vaccination at both 2 μg and 10 μg doses completely protected human ace2 transgenic mice from both mortality and even measurable infection following wild-type sars-cov-2 challenge. our findings collectively suggest the potential of lunar-cov19 as a single dose vaccine."
http://orkg.org/orkg/resource/R142295,Phase 1 Assessment of the Safety and Immunogenicity of an mRNA- Lipid Nanoparticle Vaccine Candidate Against SARS-CoV-2 in Human Volunteers,10.1101/2020.11.09.20228551,crossref,"<jats:title>ABSTRACT</jats:title><jats:p>There is an urgent need for vaccines to counter the COVID-19 pandemic due to infections with severe acute respiratory syndrome coronavirus (SARS-CoV-2). Evidence from convalescent sera and preclinical studies has identified the viral Spike (S) protein as a key antigenic target for protective immune responses. We have applied an mRNA-based technology platform, RNActive<jats:sup>®</jats:sup>, to develop CVnCoV which contains sequence optimized mRNA coding for a stabilized form of S protein encapsulated in lipid nanoparticles (LNP). Following demonstration of protective immune responses against SARS-CoV-2 in animal models we performed a dose-escalation phase 1 study in healthy 18-60 year-old volunteers.</jats:p><jats:p>This interim analysis shows that two doses of CVnCoV ranging from 2 μg to 12 μg per dose, administered 28 days apart were safe. No vaccine-related serious adverse events were reported. There were dose-dependent increases in frequency and severity of solicited systemic adverse events, and to a lesser extent of local reactions, but the majority were mild or moderate and transient in duration. Immune responses when measured as IgG antibodies against S protein or its receptor-binding domain (RBD) by ELISA, and SARS-CoV-2-virus neutralizing antibodies measured by micro-neutralization, displayed dose-dependent increases. Median titers measured in these assays two weeks after the second 12 μg dose were comparable to the median titers observed in convalescent sera from COVID-19 patients. Seroconversion (defined as a 4-fold increase over baseline titer) of virus neutralizing antibodies two weeks after the second vaccination occurred in all participants who received 12 μg doses.</jats:p><jats:p>Preliminary results in the subset of subjects who were enrolled with known SARS-CoV-2 seropositivity at baseline show that CVnCoV is also safe and well tolerated in this population, and is able to boost the pre-existing immune response even at low dose levels.</jats:p><jats:p>Based on these results, the 12 μg dose is selected for further clinical investigation, including a phase 2b/3 study that will investigate the efficacy, safety, and immunogenicity of the candidate vaccine CVnCoV.</jats:p>","abstract there is an urgent need for vaccines to counter the covid-19 pandemic due to infections with severe acute respiratory syndrome coronavirus (sars-cov-2). evidence from convalescent sera and preclinical studies has identified the viral spike (s) protein as a key antigenic target for protective immune responses. we have applied an mrna-based technology platform, rnactive ® , to develop cvncov which contains sequence optimized mrna coding for a stabilized form of s protein encapsulated in lipid nanoparticles (lnp). following demonstration of protective immune responses against sars-cov-2 in animal models we performed a dose-escalation phase 1 study in healthy 18-60 year-old volunteers. this interim analysis shows that two doses of cvncov ranging from 2 μg to 12 μg per dose, administered 28 days apart were safe. no vaccine-related serious adverse events were reported. there were dose-dependent increases in frequency and severity of solicited systemic adverse events, and to a lesser extent of local reactions, but the majority were mild or moderate and transient in duration. immune responses when measured as igg antibodies against s protein or its receptor-binding domain (rbd) by elisa, and sars-cov-2-virus neutralizing antibodies measured by micro-neutralization, displayed dose-dependent increases. median titers measured in these assays two weeks after the second 12 μg dose were comparable to the median titers observed in convalescent sera from covid-19 patients. seroconversion (defined as a 4-fold increase over baseline titer) of virus neutralizing antibodies two weeks after the second vaccination occurred in all participants who received 12 μg doses. preliminary results in the subset of subjects who were enrolled with known sars-cov-2 seropositivity at baseline show that cvncov is also safe and well tolerated in this population, and is able to boost the pre-existing immune response even at low dose levels. based on these results, the 12 μg dose is selected for further clinical investigation, including a phase 2b/3 study that will investigate the efficacy, safety, and immunogenicity of the candidate vaccine cvncov."
http://orkg.org/orkg/resource/R141977,Smart citizens for smart cities: participating in the future,10.1680/jener.15.00030,crossref,"<jats:p> This paper discusses smart cities and raises critical questions about the faith being placed in technology to reduce carbon dioxide emissions. Given increasingly challenging carbon reduction targets, the role of information and communication technology and the digital economy are increasingly championed as offering potential to contribute to meeting these targets within cities and buildings. This paper questions the faith being placed in smart or intelligent solutions through asking, what role then for the ordinary citizen? The smart approach often appears to have a narrow view of how technology and user-engagement can sit together, viewing the behaviour of users as a hurdle to overcome rather than a resource to be utilised. This paper suggests lessons can be learnt from other disciplines and wider sustainable development policy that champions the role of citizens and user-engagement to harness the co-creation of knowledge, collaboration and empowerment. Specifically, empirical findings and observations are presented from a case study of citizen engagement around an energy-from-waste infrastructure development. Recommendations are provided for engineers, planners and decision makers in order to help plan more effective engagement strategies for citizens, building users and stakeholders. </jats:p>","this paper discusses smart cities and raises critical questions about the faith being placed in technology to reduce carbon dioxide emissions. given increasingly challenging carbon reduction targets, the role of information and communication technology and the digital economy are increasingly championed as offering potential to contribute to meeting these targets within cities and buildings. this paper questions the faith being placed in smart or intelligent solutions through asking, what role then for the ordinary citizen? the smart approach often appears to have a narrow view of how technology and user-engagement can sit together, viewing the behaviour of users as a hurdle to overcome rather than a resource to be utilised. this paper suggests lessons can be learnt from other disciplines and wider sustainable development policy that champions the role of citizens and user-engagement to harness the co-creation of knowledge, collaboration and empowerment. specifically, empirical findings and observations are presented from a case study of citizen engagement around an energy-from-waste infrastructure development. recommendations are provided for engineers, planners and decision makers in order to help plan more effective engagement strategies for citizens, building users and stakeholders."
http://orkg.org/orkg/resource/R141974,Smart Governance: Using a Literature Review and Empirical Analysis to Build a Research Model,10.1177/0894439315611088,crossref,"<jats:p> The attention for Smart governance, a key aspect of Smart cities, is growing, but our conceptual understanding of it is still limited. This article fills this gap in our understanding by exploring the concept of Smart governance both theoretically and empirically and developing a research model of Smart governance. On the basis of a systematic review of the literature defining elements, aspired outcomes and implementation strategies are identified as key dimensions of Smart governance. Inductively, we identify various categories within these variables. The key dimensions were presented to a sample of representatives of European local governments to investigate the dominant perceptions of practitioners and to refine the categories. Our study results in a model for research into the implementation strategies, Smart governance arrangements, and outcomes of Smart governance. </jats:p>","the attention for smart governance, a key aspect of smart cities, is growing, but our conceptual understanding of it is still limited. this article fills this gap in our understanding by exploring the concept of smart governance both theoretically and empirically and developing a research model of smart governance. on the basis of a systematic review of the literature defining elements, aspired outcomes and implementation strategies are identified as key dimensions of smart governance. inductively, we identify various categories within these variables. the key dimensions were presented to a sample of representatives of european local governments to investigate the dominant perceptions of practitioners and to refine the categories. our study results in a model for research into the implementation strategies, smart governance arrangements, and outcomes of smart governance."
http://orkg.org/orkg/resource/R141983,"Smart City Implementation Through Shared Vision of Social Innovation for Environmental Sustainability: A Case Study of Kitakyushu, Japan",10.1177/0894439315611085,crossref,"<jats:p> Environmental sustainability is a critical global issue that requires comprehensive intervention policies. Viewed as localized intervention policy implementations, smart cities leverage information infrastructures and distributed renewable energy smart micro-grids, smart meters, and home/building energy management systems to reduce city-wide carbon emissions. However, theory-driven smart city implementation research is critically lacking. This theory-building case study identifies antecedent conditions necessary for implementing smart cities. We integrated resource dependence, social embeddedness, and citizen-centric e-governance theories to develop a citizen-centric social governance framework. We apply the framework to a field-based case study of Japan’s Kitakyushu smart community project to examine the validity and utility of the framework’s antecedent conditions: resource-dependent leadership network, cross-sector collaboration based on social ties, and citizen-centric e-governance. We conclude that complex smart community implementation processes require shared vision of social innovation owned by diverse stakeholders with conflicting values and adaptive use of informal social governance mechanisms for effective smart city implementation. </jats:p>","environmental sustainability is a critical global issue that requires comprehensive intervention policies. viewed as localized intervention policy implementations, smart cities leverage information infrastructures and distributed renewable energy smart micro-grids, smart meters, and home/building energy management systems to reduce city-wide carbon emissions. however, theory-driven smart city implementation research is critically lacking. this theory-building case study identifies antecedent conditions necessary for implementing smart cities. we integrated resource dependence, social embeddedness, and citizen-centric e-governance theories to develop a citizen-centric social governance framework. we apply the framework to a field-based case study of japan’s kitakyushu smart community project to examine the validity and utility of the framework’s antecedent conditions: resource-dependent leadership network, cross-sector collaboration based on social ties, and citizen-centric e-governance. we conclude that complex smart community implementation processes require shared vision of social innovation owned by diverse stakeholders with conflicting values and adaptive use of informal social governance mechanisms for effective smart city implementation."
http://orkg.org/orkg/resource/R141980,Smart Cities Governance: The Need for a Holistic Approach to Assessing Urban Participatory Policy Making,10.1177/0894439315611103,crossref,"<jats:p> Most of the definitions of a “smart city” make a direct or indirect reference to improving performance as one of the main objectives of initiatives to make cities “smarter”. Several evaluation approaches and models have been put forward in literature and practice to measure smart cities. However, they are often normative or limited to certain aspects of cities’ “smartness”, and a more comprehensive and holistic approach seems to be lacking. Thus, building on a review of the literature and practice in the field, this paper aims to discuss the importance of adopting a holistic approach to the assessment of smart city governance and policy decision making. It also proposes a performance assessment framework that overcomes the limitations of existing approaches and contributes to filling the current gap in the knowledge base in this domain. One of the innovative elements of the proposed framework is its holistic approach to policy evaluation. It is designed to address a smart city’s specificities and can benefit from the active participation of citizens in assessing the public value of policy decisions and their sustainability over time. We focus our attention on the performance measurement of codesign and coproduction by stakeholders and social innovation processes related to public value generation. More specifically, we are interested in the assessment of both the citizen centricity of smart city decision making and the processes by which public decisions are implemented, monitored, and evaluated as regards their capability to develop truly “blended” value services—that is, simultaneously socially inclusive, environmentally friendly, and economically sustainable. </jats:p>","most of the definitions of a “smart city” make a direct or indirect reference to improving performance as one of the main objectives of initiatives to make cities “smarter”. several evaluation approaches and models have been put forward in literature and practice to measure smart cities. however, they are often normative or limited to certain aspects of cities’ “smartness”, and a more comprehensive and holistic approach seems to be lacking. thus, building on a review of the literature and practice in the field, this paper aims to discuss the importance of adopting a holistic approach to the assessment of smart city governance and policy decision making. it also proposes a performance assessment framework that overcomes the limitations of existing approaches and contributes to filling the current gap in the knowledge base in this domain. one of the innovative elements of the proposed framework is its holistic approach to policy evaluation. it is designed to address a smart city’s specificities and can benefit from the active participation of citizens in assessing the public value of policy decisions and their sustainability over time. we focus our attention on the performance measurement of codesign and coproduction by stakeholders and social innovation processes related to public value generation. more specifically, we are interested in the assessment of both the citizen centricity of smart city decision making and the processes by which public decisions are implemented, monitored, and evaluated as regards their capability to develop truly “blended” value services—that is, simultaneously socially inclusive, environmentally friendly, and economically sustainable."
http://orkg.org/orkg/resource/R141989,Governing Smart Cities: An Empirical Analysis,10.1177/0894439315611093,crossref,"<jats:p>Smart cities (SCs) are a recent but emerging phenomenon, aiming at using high technology and especially information and communications technology (ICT) to implement better living conditions in large metropolises, to involve citizens in city government, and to support sustainable economic development and city attractiveness. The final goal is to improve the quality of city life for all stakeholders. Until now, SCs have been developing as bottom-up projects, bringing together smart initiatives driven by public bodies, enterprises, citizens, and not-for-profit organizations. However, to build a long-term smart strategy capable of producing better returns from investments and deciding priorities regarding each city, a comprehensive SC governance framework is needed. The aim of this paper is to collect empirical evidences regarding government structures implemented in SCs and to outline a framework for the roles of local governments, nongovernmental agencies, and administrative officials. The survey shows that no consolidated standards or best practices for governing SCs are implemented in the examined cities; however, each city applies its own governance framework. Moreover, the study reveals some interesting experiences that may be useful for involving citizens and civil society in SC governance.</jats:p>","smart cities (scs) are a recent but emerging phenomenon, aiming at using high technology and especially information and communications technology (ict) to implement better living conditions in large metropolises, to involve citizens in city government, and to support sustainable economic development and city attractiveness. the final goal is to improve the quality of city life for all stakeholders. until now, scs have been developing as bottom-up projects, bringing together smart initiatives driven by public bodies, enterprises, citizens, and not-for-profit organizations. however, to build a long-term smart strategy capable of producing better returns from investments and deciding priorities regarding each city, a comprehensive sc governance framework is needed. the aim of this paper is to collect empirical evidences regarding government structures implemented in scs and to outline a framework for the roles of local governments, nongovernmental agencies, and administrative officials. the survey shows that no consolidated standards or best practices for governing scs are implemented in the examined cities; however, each city applies its own governance framework. moreover, the study reveals some interesting experiences that may be useful for involving citizens and civil society in sc governance."
http://orkg.org/orkg/resource/R142008,"Speculative futures: Cities, data, and governance beyond smart urbanism",10.1177/0308518x16651445,crossref,"<jats:p> In this paper, I examine the convergence of big data and urban governance beyond the discursive and material contexts of the smart city. I argue that in addition to understanding the intensifying relationship between data, cities, and governance in terms of regimes of automated management and coordination in ‘actually existing’ smart cities, we should further engage with urban algorithmic governance and governmentality as material-discursive projects of future-ing, i.e., of anticipating particular kinds of cities-to-come. As urban big data looks to the future, it does so through the lens of an anticipatory security calculus fixated on identifying and diverting risks of urban anarchy and personal harm against which life in cities must be securitized. I suggest that such modes of algorithmic speculation are discernible at two scales of urban big data praxis: the scale of the body, and that of the city itself. At the level of the urbanite body, I use the selective example of mobile neighborhood safety apps to demonstrate how algorithmic governmentality enacts digital mediations of individual mobilities by routing individuals around ‘unsafe’ parts of the city in the interests of technologically ameliorating the risks of urban encounter. At the scale of the city, amongst other empirical examples, sentiment analytics approaches prefigure ephemeral spatialities of civic strife by aggregating and mapping individual emotions distilled from unstructured real-time content flows (such as Tweets). In both of these instances, the urban futures anticipated by the urban ‘big data security assemblage’ are highly uneven, as data and algorithms cannot divest themselves of urban inequalities and the persistence of their geographies. </jats:p>","in this paper, i examine the convergence of big data and urban governance beyond the discursive and material contexts of the smart city. i argue that in addition to understanding the intensifying relationship between data, cities, and governance in terms of regimes of automated management and coordination in ‘actually existing’ smart cities, we should further engage with urban algorithmic governance and governmentality as material-discursive projects of future-ing, i.e., of anticipating particular kinds of cities-to-come. as urban big data looks to the future, it does so through the lens of an anticipatory security calculus fixated on identifying and diverting risks of urban anarchy and personal harm against which life in cities must be securitized. i suggest that such modes of algorithmic speculation are discernible at two scales of urban big data praxis: the scale of the body, and that of the city itself. at the level of the urbanite body, i use the selective example of mobile neighborhood safety apps to demonstrate how algorithmic governmentality enacts digital mediations of individual mobilities by routing individuals around ‘unsafe’ parts of the city in the interests of technologically ameliorating the risks of urban encounter. at the scale of the city, amongst other empirical examples, sentiment analytics approaches prefigure ephemeral spatialities of civic strife by aggregating and mapping individual emotions distilled from unstructured real-time content flows (such as tweets). in both of these instances, the urban futures anticipated by the urban ‘big data security assemblage’ are highly uneven, as data and algorithms cannot divest themselves of urban inequalities and the persistence of their geographies."
http://orkg.org/orkg/resource/R142017,Governing the smart city: a review of the literature on smart urban governance,10.1177/0020852314564308,crossref,"<jats:p> Academic attention to smart cities and their governance is growing rapidly, but the fragmentation in approaches makes for a confusing debate. This article brings some structure to the debate by analyzing a corpus of 51 publications and mapping their variation. The analysis shows that publications differ in their emphasis on (1) smart technology, smart people or smart collaboration as the defining features of smart cities, (2) a transformative or incremental perspective on changes in urban governance, (3) better outcomes or a more open process as the legitimacy claim for smart city governance. We argue for a comprehensive perspective: smart city governance is about crafting new forms of human collaboration through the use of ICTs to obtain better outcomes and more open governance processes. Research into smart city governance could benefit from previous studies into success and failure factors for e-government and build upon sophisticated theories of socio-technical change. This article highlights that smart city governance is not a technological issue: we should study smart city governance as a complex process of institutional change and acknowledge the political nature of appealing visions of socio-technical governance. </jats:p><jats:sec><jats:title>Points for practitioners</jats:title><jats:p> The study provides practitioners with an in-depth understanding of current debates about smart city governance. The article highlights that governing a smart city is about crafting new forms of human collaboration through the use of information and communication technologies. City managers should realize that technology by itself will not make a city smarter: building a smart city requires a political understanding of technology, a process approach to manage the emerging smart city and a focus on both economic gains and other public values. </jats:p></jats:sec>","academic attention to smart cities and their governance is growing rapidly, but the fragmentation in approaches makes for a confusing debate. this article brings some structure to the debate by analyzing a corpus of 51 publications and mapping their variation. the analysis shows that publications differ in their emphasis on (1) smart technology, smart people or smart collaboration as the defining features of smart cities, (2) a transformative or incremental perspective on changes in urban governance, (3) better outcomes or a more open process as the legitimacy claim for smart city governance. we argue for a comprehensive perspective: smart city governance is about crafting new forms of human collaboration through the use of icts to obtain better outcomes and more open governance processes. research into smart city governance could benefit from previous studies into success and failure factors for e-government and build upon sophisticated theories of socio-technical change. this article highlights that smart city governance is not a technological issue: we should study smart city governance as a complex process of institutional change and acknowledge the political nature of appealing visions of socio-technical governance. points for practitioners the study provides practitioners with an in-depth understanding of current debates about smart city governance. the article highlights that governing a smart city is about crafting new forms of human collaboration through the use of information and communication technologies. city managers should realize that technology by itself will not make a city smarter: building a smart city requires a political understanding of technology, a process approach to manage the emerging smart city and a focus on both economic gains and other public values."
http://orkg.org/orkg/resource/R141873,Preparation of highly c-axis oriented AlN thin films on Hastelloy tapes with Y2O3 buffer layer for flexible SAW sensor applications,10.1142/s1793604716500235,crossref,"<jats:p> Highly c-axis oriented aluminum nitrade (AlN) films were successfully deposited on flexible Hastelloy tapes by middle-frequency magnetron sputtering. The microstructure and piezoelectric properties of the AlN films were investigated. The results show that the AlN films deposited directly on the bare Hastelloy substrate have rough surface with root mean square (RMS) roughness of 32.43[Formula: see text]nm and its full width at half maximum (FWHM) of the AlN (0002) peak is [Formula: see text]. However, the AlN films deposited on the Hastelloy substrate with Y<jats:sub>2</jats:sub>O<jats:sub>3</jats:sub> buffer layer show smooth surface with RMS roughness of 5.46[Formula: see text]nm and its FWHM of the AlN (0002) peak is only [Formula: see text]. The piezoelectric coefficient d[Formula: see text] of the AlN films deposited on the Y<jats:sub>2</jats:sub>O<jats:sub>3</jats:sub>/Hastelloy substrate is larger than three times that of the AlN films deposited on the bare Hastelloy substrate. The prepared highly c-axis oriented AlN films can be used to develop high-temperature flexible SAW sensors. </jats:p>","highly c-axis oriented aluminum nitrade (aln) films were successfully deposited on flexible hastelloy tapes by middle-frequency magnetron sputtering. the microstructure and piezoelectric properties of the aln films were investigated. the results show that the aln films deposited directly on the bare hastelloy substrate have rough surface with root mean square (rms) roughness of 32.43[formula: see text]nm and its full width at half maximum (fwhm) of the aln (0002) peak is [formula: see text]. however, the aln films deposited on the hastelloy substrate with y 2 o 3 buffer layer show smooth surface with rms roughness of 5.46[formula: see text]nm and its fwhm of the aln (0002) peak is only [formula: see text]. the piezoelectric coefficient d[formula: see text] of the aln films deposited on the y 2 o 3 /hastelloy substrate is larger than three times that of the aln films deposited on the bare hastelloy substrate. the prepared highly c-axis oriented aln films can be used to develop high-temperature flexible saw sensors."
http://orkg.org/orkg/resource/R142001,The ethics of smart cities and urban science,10.1098/rsta.2016.0115,crossref,"<jats:p>Software-enabled technologies and urban big data have become essential to the functioning of cities. Consequently, urban operational governance and city services are becoming highly responsive to a form of data-driven urbanism that is the key mode of production for smart cities. At the heart of data-driven urbanism is a computational understanding of city systems that reduces urban life to logic and calculative rules and procedures, which is underpinned by an instrumental rationality and realist epistemology. This rationality and epistemology are informed by and sustains urban science and urban informatics, which seek to make cities more knowable and controllable. This paper examines the forms, practices and ethics of smart cities and urban science, paying particular attention to: instrumental rationality and realist epistemology; privacy, datafication, dataveillance and geosurveillance; and data uses, such as social sorting and anticipatory governance. It argues that smart city initiatives and urban science need to be re-cast in three ways: a re-orientation in how cities are conceived; a reconfiguring of the underlying epistemology to openly recognize the contingent and relational nature of urban systems, processes and science; and the adoption of ethical principles designed to realize benefits of smart cities and urban science while reducing pernicious effects.</jats:p><jats:p>This article is part of the themed issue ‘The ethical impact of data science’.</jats:p>","software-enabled technologies and urban big data have become essential to the functioning of cities. consequently, urban operational governance and city services are becoming highly responsive to a form of data-driven urbanism that is the key mode of production for smart cities. at the heart of data-driven urbanism is a computational understanding of city systems that reduces urban life to logic and calculative rules and procedures, which is underpinned by an instrumental rationality and realist epistemology. this rationality and epistemology are informed by and sustains urban science and urban informatics, which seek to make cities more knowable and controllable. this paper examines the forms, practices and ethics of smart cities and urban science, paying particular attention to: instrumental rationality and realist epistemology; privacy, datafication, dataveillance and geosurveillance; and data uses, such as social sorting and anticipatory governance. it argues that smart city initiatives and urban science need to be re-cast in three ways: a re-orientation in how cities are conceived; a reconfiguring of the underlying epistemology to openly recognize the contingent and relational nature of urban systems, processes and science; and the adoption of ethical principles designed to realize benefits of smart cities and urban science while reducing pernicious effects. this article is part of the themed issue ‘the ethical impact of data science’."
http://orkg.org/orkg/resource/R141940,Smart city or smart citizens? The Barcelona case,10.1108/jsma-03-2015-0030,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>– In recent years, the term “smart city” has attracted a lot of attention from policy makers, business leaders and citizenship in general. Although there is not a unique definition of what a smart city is, it is generally accepted that “smart” urban policies refer to local governments’ initiatives that use information and communication technologies in order to increase the quality of life of their inhabitants while contributing to a sustainable development. So far, “smart city” approaches have generally been related to top-down processes of technology diffusion. The purpose of this paper is to present a broader view on “smart” initiatives to analyze both top-down and bottom-up dynamics in a smart city. The authors argue that these two perspectives are complementary and its combination can reinforce the collaboration between different city stakeholders. Top-down and bottom-up initiatives are not opposed forces but, on the contrary, can have a synergistic effect on the innovation capacity of the city. Both perspectives are illustrated by providing examples of different “smart” aspects in the city of Barcelona: smart districts, open collaborative spaces, infrastructures and open data.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>– To illustrate the arguments, the authors analyze the case of the city of Barcelona providing examples of top-down and bottom-up initiatives in four different smart city aspects: smart districts, open collaborative spaces, infrastructures and open data. The research method is based on a case study (Yin, 1984). The primary data consisted on interviews to city council representatives as well as managers of local public institutions, like economic development offices, and local organizations like for instance coworking spaces. The authors interviewed also specialists on the innovation history of the city in order to validate the data. In addition, the authors used secondary data such as reports on the 22@, and documentation on the Barcelona innovation policies, as well as doing a compilation of press articles and the online content of the institutional webpages. All together, the authors have followed a data triangulation strategy to seek data validation based on the cross-verification of the analyzed data sources.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>– The analysis suggests that the top-down and bottom-up perspectives are complementary and their combination can reinforce the collaboration between different city stakeholders. Top-down and bottom-up initiatives are not opposed forces but, on the contrary, can have a synergistic effect on the innovation capacity of the city. Both perspectives are illustrated by providing examples of different “smart” aspects in the city of Barcelona: smart districts, open collaborative spaces, infrastructures and open data.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>– Nevertheless, the analysis has its limitations. Even if the authors have emphasized the importance of the bottom-up initiatives, citizens do not have often the resources to act without governmental intervention. This is the case of services that require high-cost infrastructures or regulatory changes. Also, as it usually happens in the case of disruptive technology, it is hard for citizens to understand the possibilities of its use. In these cases, firms and institutions must play an important role in the first phases of the diffusion of innovations, by informing and incentivizing its use. It is also important to note that some of the emerging usages of technology are confronted to legal or regulatory issues. For instance, distributed and shared Wi-Fi networks might be in opposition to economic interests of internet providers, that often difficult its expansion. It is also the case of services of the sharing economy that represent a menace to established institutions (like the tensions between Uber and taxi companies, or Airbnb and hotels). In these cases, city halls like it is the case in Barcelona, tend to respond to these emergent uses of technology by regulating to ensure protection to existing corporate services.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>– In conclusion, the transformational process that leads a city to become a smart city has to take in consideration the complexity and the plurality of the urban reality. Beyond considering citizens as being users, testers or consumers of technology, local administrations that are able to identify, nourish and integrate the emerging citizens’ initiatives would contribute to the reinforcement of a smart city reality.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>– The contribution of the paper is to go beyond the generalized technologic discourse around smart cities by adding the layer of the citizens’ initiatives.</jats:p></jats:sec>","purpose – in recent years, the term “smart city” has attracted a lot of attention from policy makers, business leaders and citizenship in general. although there is not a unique definition of what a smart city is, it is generally accepted that “smart” urban policies refer to local governments’ initiatives that use information and communication technologies in order to increase the quality of life of their inhabitants while contributing to a sustainable development. so far, “smart city” approaches have generally been related to top-down processes of technology diffusion. the purpose of this paper is to present a broader view on “smart” initiatives to analyze both top-down and bottom-up dynamics in a smart city. the authors argue that these two perspectives are complementary and its combination can reinforce the collaboration between different city stakeholders. top-down and bottom-up initiatives are not opposed forces but, on the contrary, can have a synergistic effect on the innovation capacity of the city. both perspectives are illustrated by providing examples of different “smart” aspects in the city of barcelona: smart districts, open collaborative spaces, infrastructures and open data. design/methodology/approach – to illustrate the arguments, the authors analyze the case of the city of barcelona providing examples of top-down and bottom-up initiatives in four different smart city aspects: smart districts, open collaborative spaces, infrastructures and open data. the research method is based on a case study (yin, 1984). the primary data consisted on interviews to city council representatives as well as managers of local public institutions, like economic development offices, and local organizations like for instance coworking spaces. the authors interviewed also specialists on the innovation history of the city in order to validate the data. in addition, the authors used secondary data such as reports on the 22@, and documentation on the barcelona innovation policies, as well as doing a compilation of press articles and the online content of the institutional webpages. all together, the authors have followed a data triangulation strategy to seek data validation based on the cross-verification of the analyzed data sources. findings – the analysis suggests that the top-down and bottom-up perspectives are complementary and their combination can reinforce the collaboration between different city stakeholders. top-down and bottom-up initiatives are not opposed forces but, on the contrary, can have a synergistic effect on the innovation capacity of the city. both perspectives are illustrated by providing examples of different “smart” aspects in the city of barcelona: smart districts, open collaborative spaces, infrastructures and open data. research limitations/implications – nevertheless, the analysis has its limitations. even if the authors have emphasized the importance of the bottom-up initiatives, citizens do not have often the resources to act without governmental intervention. this is the case of services that require high-cost infrastructures or regulatory changes. also, as it usually happens in the case of disruptive technology, it is hard for citizens to understand the possibilities of its use. in these cases, firms and institutions must play an important role in the first phases of the diffusion of innovations, by informing and incentivizing its use. it is also important to note that some of the emerging usages of technology are confronted to legal or regulatory issues. for instance, distributed and shared wi-fi networks might be in opposition to economic interests of internet providers, that often difficult its expansion. it is also the case of services of the sharing economy that represent a menace to established institutions (like the tensions between uber and taxi companies, or airbnb and hotels). in these cases, city halls like it is the case in barcelona, tend to respond to these emergent uses of technology by regulating to ensure protection to existing corporate services. practical implications – in conclusion, the transformational process that leads a city to become a smart city has to take in consideration the complexity and the plurality of the urban reality. beyond considering citizens as being users, testers or consumers of technology, local administrations that are able to identify, nourish and integrate the emerging citizens’ initiatives would contribute to the reinforcement of a smart city reality. originality/value – the contribution of the paper is to go beyond the generalized technologic discourse around smart cities by adding the layer of the citizens’ initiatives."
http://orkg.org/orkg/resource/R142180,Detection and Diagnosis of Breast Cancer Using Artificial Intelligence Based Assessment of Maximum Intensity Projection Dynamic Contrast-Enhanced Magnetic Resonance Images,10.3390/diagnostics10050330,crossref,"<jats:p>We aimed to evaluate an artificial intelligence (AI) system that can detect and diagnose lesions of maximum intensity projection (MIP) in dynamic contrast-enhanced (DCE) breast magnetic resonance imaging (MRI). We retrospectively gathered MIPs of DCE breast MRI for training and validation data from 30 and 7 normal individuals, 49 and 20 benign cases, and 135 and 45 malignant cases, respectively. Breast lesions were indicated with a bounding box and labeled as benign or malignant by a radiologist, while the AI system was trained to detect and calculate possibilities of malignancy using RetinaNet. The AI system was analyzed using test sets of 13 normal, 20 benign, and 52 malignant cases. Four human readers also scored these test data with and without the assistance of the AI system for the possibility of a malignancy in each breast. Sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) were 0.926, 0.828, and 0.925 for the AI system; 0.847, 0.841, and 0.884 for human readers without AI; and 0.889, 0.823, and 0.899 for human readers with AI using a cutoff value of 2%, respectively. The AI system showed better diagnostic performance compared to the human readers (p = 0.002), and because of the increased performance of human readers with the assistance of the AI system, the AUC of human readers was significantly higher with than without the AI system (p = 0.039). Our AI system showed a high performance ability in detecting and diagnosing lesions in MIPs of DCE breast MRI and increased the diagnostic performance of human readers.</jats:p>","we aimed to evaluate an artificial intelligence (ai) system that can detect and diagnose lesions of maximum intensity projection (mip) in dynamic contrast-enhanced (dce) breast magnetic resonance imaging (mri). we retrospectively gathered mips of dce breast mri for training and validation data from 30 and 7 normal individuals, 49 and 20 benign cases, and 135 and 45 malignant cases, respectively. breast lesions were indicated with a bounding box and labeled as benign or malignant by a radiologist, while the ai system was trained to detect and calculate possibilities of malignancy using retinanet. the ai system was analyzed using test sets of 13 normal, 20 benign, and 52 malignant cases. four human readers also scored these test data with and without the assistance of the ai system for the possibility of a malignancy in each breast. sensitivity, specificity, and area under the receiver operating characteristic curve (auc) were 0.926, 0.828, and 0.925 for the ai system; 0.847, 0.841, and 0.884 for human readers without ai; and 0.889, 0.823, and 0.899 for human readers with ai using a cutoff value of 2%, respectively. the ai system showed better diagnostic performance compared to the human readers (p = 0.002), and because of the increased performance of human readers with the assistance of the ai system, the auc of human readers was significantly higher with than without the ai system (p = 0.039). our ai system showed a high performance ability in detecting and diagnosing lesions in mips of dce breast mri and increased the diagnostic performance of human readers."
http://orkg.org/orkg/resource/R141880,Integration of AlN piezoelectric thin films on ultralow fatigue TiNiCu shape memory alloys,10.1557/jmr.2020.106,crossref,"<jats:title>Abstract</jats:title><jats:p><jats:fig position=""anchor""><jats:graphic xmlns:xlink=""http://www.w3.org/1999/xlink"" mime-subtype=""png"" mimetype=""image"" position=""float"" xlink:href=""S0884291420001065_figAb.png"" /></jats:fig></jats:p>",abstract
http://orkg.org/orkg/resource/R141896,"Addition of dithi(ol)anylium tetrafluoroborates to α,β-unsaturated ketones",10.3762/bjoc.14.37,crossref,"<jats:p>In the presented study, dithi(ol)anylium tetrafluoroborates are added to α,β-unsaturated ketones in a Michael-type reaction yielding diverse substituted ketene diothi(ol)anes. The reactions proceed at room temperature in 1 or 13 h without the need of further additives. The presented procedure is in particular useful for dithi(ol)anylium tetrafluoroborates without electron-withdrawing groups in α-position. This is advantageous with respect to previous approaches, which were limited to the use of ketene dithioacetals substituted with electron-withdrawing groups. Aiming for the systematic investigation of possible steric and electronic influences on the outcome of the reaction, various combinations of electrophiles and nucleophiles were used and the results of the reactions were compared based on the type of the used dithioacetal. The scope of the presented procedure is shown with four additional transformations including the use of additional electrophiles and nucleophiles, the use of a chiral auxiliary and subsequent reduction of selected products. Additionally, we extended the reaction to the synthesis of diene dithiolanes by addition of an ynone to α-alkyl or aryl-substitued dithiolanylium TFBs.</jats:p>","in the presented study, dithi(ol)anylium tetrafluoroborates are added to α,β-unsaturated ketones in a michael-type reaction yielding diverse substituted ketene diothi(ol)anes. the reactions proceed at room temperature in 1 or 13 h without the need of further additives. the presented procedure is in particular useful for dithi(ol)anylium tetrafluoroborates without electron-withdrawing groups in α-position. this is advantageous with respect to previous approaches, which were limited to the use of ketene dithioacetals substituted with electron-withdrawing groups. aiming for the systematic investigation of possible steric and electronic influences on the outcome of the reaction, various combinations of electrophiles and nucleophiles were used and the results of the reactions were compared based on the type of the used dithioacetal. the scope of the presented procedure is shown with four additional transformations including the use of additional electrophiles and nucleophiles, the use of a chiral auxiliary and subsequent reduction of selected products. additionally, we extended the reaction to the synthesis of diene dithiolanes by addition of an ynone to α-alkyl or aryl-substitued dithiolanylium tfbs."
http://orkg.org/orkg/resource/R141943,Smart city intellectual capital: an emerging view of territorial systems innovation management,10.1108/jic-02-2015-0018,crossref,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>– The purpose of this paper is to explore whether and how the intellectual capital (IC) approach and concepts could be fruitfully adapted to study the smart city phenomenon from a managerial point of view.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>– This study is based on a long-term, in-depth ethnographic exploration of the vast global community, which is created around the smart city movement.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>– The analysis suggests that, in order to effectively analyse a smart city context through the IC lens, the traditional IC framework needs to be extended for: expected outcomes, which should also include sustainability, resilience and quality of life; categories of key resources, which should also include institutional capital and environmental capital; units of analysis, which should also include territorial systems, such as transportation or waste; and key managerial challenges implied. As a final result, a smart city intellectual capital (SC-IC) framework is proposed.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>– Most of the cases analysed in this study are European; further studies are advisable to better investigate non-European smart city contexts.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>– The paper suggests that the knowledge management, project portfolio management and network management approaches are crucial to better support managerial practices in smart city organizations.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>– The SC-IC framework allows for a clear definition of the smart city organization, as a new knowledge-based, project-oriented, network-shaped type of organization. Therefore, the SC-IC framework provides smart city research with a consistent rooting in management studies. Further, this paper contributes to the fourth stage of IC research.</jats:p></jats:sec>","purpose – the purpose of this paper is to explore whether and how the intellectual capital (ic) approach and concepts could be fruitfully adapted to study the smart city phenomenon from a managerial point of view. design/methodology/approach – this study is based on a long-term, in-depth ethnographic exploration of the vast global community, which is created around the smart city movement. findings – the analysis suggests that, in order to effectively analyse a smart city context through the ic lens, the traditional ic framework needs to be extended for: expected outcomes, which should also include sustainability, resilience and quality of life; categories of key resources, which should also include institutional capital and environmental capital; units of analysis, which should also include territorial systems, such as transportation or waste; and key managerial challenges implied. as a final result, a smart city intellectual capital (sc-ic) framework is proposed. research limitations/implications – most of the cases analysed in this study are european; further studies are advisable to better investigate non-european smart city contexts. practical implications – the paper suggests that the knowledge management, project portfolio management and network management approaches are crucial to better support managerial practices in smart city organizations. originality/value – the sc-ic framework allows for a clear definition of the smart city organization, as a new knowledge-based, project-oriented, network-shaped type of organization. therefore, the sc-ic framework provides smart city research with a consistent rooting in management studies. further, this paper contributes to the fourth stage of ic research."
http://orkg.org/orkg/resource/R141961,Smart Cities at the Crossroads: New Tensions in City Transformation,10.1177/0008125616683949,crossref,"<jats:p> The Smart Cities movement has produced a large number of projects and experiments around the world. To understand the primary ones, as well as their underlying tensions and the insights emerging from them, the editors of this special issue of the California Management Review enlisted a panel of experts, academics, and practitioners from different nationalities, backgrounds, experiences, and perspectives. The panel focused its discussion on three main areas: new governance models for Smart Cities, how to spur growth and renewal, and the sharing economy—both commons and market based. </jats:p>","the smart cities movement has produced a large number of projects and experiments around the world. to understand the primary ones, as well as their underlying tensions and the insights emerging from them, the editors of this special issue of the california management review enlisted a panel of experts, academics, and practitioners from different nationalities, backgrounds, experiences, and perspectives. the panel focused its discussion on three main areas: new governance models for smart cities, how to spur growth and renewal, and the sharing economy—both commons and market based."
http://orkg.org/orkg/resource/R141970,ICT and sustainability in smart cities management,10.1108/ijpsm-07-2015-0132,crossref,"<jats:sec>\n               <jats:title content-type=""abstract-heading"">Purpose</jats:title>\n               <jats:p> – Contemporary debate is increasingly focused on ICT and sustainability, especially in relation to the modern configuration of urban and metropolitan areas in the so-called smartization process. The purpose of this paper is to observe the connections between smart city features as conceptualized in the framework proposed by Giffinger <jats:italic>et al.</jats:italic> (2007) and new technologies as tools, and sustainability as the goal. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title>\n               <jats:p> – The connections are identified through a content analysis performed using NVivo on official reports issued by organizations, known as industry players within smart city projects, listed in the Navigant Research Report 2013. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Findings</jats:title>\n               <jats:p> – The results frame ICT and sustainability as “across-the-board elements” because they connect with all of the services provided to communities in a smart city and play a key role in smart city planning. Specifically, sustainability and ICT can be seen as tools to enable the smartization process. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title>\n               <jats:p> – An all-in-one perspective emerges by embedding sustainability and ICT in smart interventions; further research could be conduct through direct interviews to city managers and industry players in order to understand their attitude towards the development of smart city projects. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Practical implications</jats:title>\n               <jats:p> – Potential approaches emerging from this research are useful to city managers or large corporations partnering with local agencies in order to increase the opportunities for the long-term success of smart projects. </jats:p>\n            </jats:sec>\n            <jats:sec>\n               <jats:title content-type=""abstract-heading"">Originality/value</jats:title>\n               <jats:p> – The results of this paper delineate a new research path looking at the development of new models that integrate drivers, ICT, and sustainability in an all-in-one perspective and new indicators for the evaluation of the interventions.</jats:p>\n            </jats:sec>","\n purpose \n – contemporary debate is increasingly focused on ict and sustainability, especially in relation to the modern configuration of urban and metropolitan areas in the so-called smartization process. the purpose of this paper is to observe the connections between smart city features as conceptualized in the framework proposed by giffinger et al. (2007) and new technologies as tools, and sustainability as the goal. \n \n \n design/methodology/approach \n – the connections are identified through a content analysis performed using nvivo on official reports issued by organizations, known as industry players within smart city projects, listed in the navigant research report 2013. \n \n \n findings \n – the results frame ict and sustainability as “across-the-board elements” because they connect with all of the services provided to communities in a smart city and play a key role in smart city planning. specifically, sustainability and ict can be seen as tools to enable the smartization process. \n \n \n research limitations/implications \n – an all-in-one perspective emerges by embedding sustainability and ict in smart interventions; further research could be conduct through direct interviews to city managers and industry players in order to understand their attitude towards the development of smart city projects. \n \n \n practical implications \n – potential approaches emerging from this research are useful to city managers or large corporations partnering with local agencies in order to increase the opportunities for the long-term success of smart projects. \n \n \n originality/value \n – the results of this paper delineate a new research path looking at the development of new models that integrate drivers, ict, and sustainability in an all-in-one perspective and new indicators for the evaluation of the interventions. \n"
http://orkg.org/orkg/resource/R141708,"N,S co-doped carbon dots as a stable bio-imaging probe for detection of intracellular temperature and tetracycline",10.1039/c7tb00810d,crossref,"<p>N,S-CDs display an unambiguous bioimaging ability in the detection of intracellular temperature and tetracycline with satisfactory results.</p>","n,s-cds display an unambiguous bioimaging ability in the detection of intracellular temperature and tetracycline with satisfactory results."
http://orkg.org/orkg/resource/R141748,"Dual functional highly luminescence B, N Co-doped carbon nanodots as nanothermometer and Fe3+/Fe2+ sensor",10.1038/s41598-020-59958-5,crossref,"<jats:title>Abstract</jats:title><jats:p>Dual functional fluorescence nanosensors have many potential applications in biology and medicine. Monitoring temperature with higher precision at localized small length scales or in a nanocavity is a necessity in various applications. As well as the detection of biologically interesting metal ions using low-cost and sensitive approach is of great importance in bioanalysis. In this paper, we describe the preparation of dual-function highly fluorescent B, N-co-doped carbon nanodots (CDs) that work as chemical and thermal sensors. The CDs emit blue fluorescence peaked at 450\u2009nm and exhibit up to 70% photoluminescence quantum yield with showing excitation-independent fluorescence. We also show that water-soluble CDs display temperature-dependent fluorescence and can serve as highly sensitive and reliable nanothermometers with a thermo-sensitivity 1.8% °C<jats:sup>−1</jats:sup>, and wide range thermo-sensing between 0–90\u2009°C with excellent recovery. Moreover, the fluorescence emission of CDs are selectively quenched after the addition of Fe<jats:sup>2+</jats:sup> and Fe<jats:sup>3+</jats:sup> ions while show no quenching with adding other common metal cations and anions. The fluorescence emission shows a good linear correlation with concentration of Fe<jats:sup>2+</jats:sup> and Fe<jats:sup>3+</jats:sup> (R<jats:sup>2</jats:sup>\u2009=\u20090.9908 for Fe<jats:sup>2+</jats:sup> and R<jats:sup>2</jats:sup>\u2009=\u20090.9892 for Fe<jats:sup>3+</jats:sup>) with a detection limit of of 80.0\u2009±\u20090.5\u2009nM for Fe<jats:sup>2+</jats:sup> and 110.0\u2009±\u20090.5\u2009nM for Fe<jats:sup>3+</jats:sup>. Considering the high quantum yield and selectivity, CDs are exploited to design a nanoprobe towards iron detection in a biological sample. The fluorimetric assay is used to detect Fe<jats:sup>2+</jats:sup> in iron capsules and total iron in serum samples successfully.</jats:p>","abstract dual functional fluorescence nanosensors have many potential applications in biology and medicine. monitoring temperature with higher precision at localized small length scales or in a nanocavity is a necessity in various applications. as well as the detection of biologically interesting metal ions using low-cost and sensitive approach is of great importance in bioanalysis. in this paper, we describe the preparation of dual-function highly fluorescent b, n-co-doped carbon nanodots (cds) that work as chemical and thermal sensors. the cds emit blue fluorescence peaked at 450\u2009nm and exhibit up to 70% photoluminescence quantum yield with showing excitation-independent fluorescence. we also show that water-soluble cds display temperature-dependent fluorescence and can serve as highly sensitive and reliable nanothermometers with a thermo-sensitivity 1.8% °c −1 , and wide range thermo-sensing between 0–90\u2009°c with excellent recovery. moreover, the fluorescence emission of cds are selectively quenched after the addition of fe 2+ and fe 3+ ions while show no quenching with adding other common metal cations and anions. the fluorescence emission shows a good linear correlation with concentration of fe 2+ and fe 3+ (r 2 \u2009=\u20090.9908 for fe 2+ and r 2 \u2009=\u20090.9892 for fe 3+ ) with a detection limit of of 80.0\u2009±\u20090.5\u2009nm for fe 2+ and 110.0\u2009±\u20090.5\u2009nm for fe 3+ . considering the high quantum yield and selectivity, cds are exploited to design a nanoprobe towards iron detection in a biological sample. the fluorimetric assay is used to detect fe 2+ in iron capsules and total iron in serum samples successfully."
http://orkg.org/orkg/resource/R141724,Intracellular ratiometric temperature sensing using fluorescent carbon dots,10.1039/c8na00255j,crossref,<p>A self-referencing dual fluorescing carbon dot-based nanothermometer can ratiometrically sense thermal events in HeLa cells with very high sensitivity.</p>,a self-referencing dual fluorescing carbon dot-based nanothermometer can ratiometrically sense thermal events in hela cells with very high sensitivity.
http://orkg.org/orkg/resource/R142029,Smart governance as key to multi-jurisdictional smart city initiatives: The case of the eCityGov Alliance,10.1177/0539018416629230,crossref,"<jats:p> Quite a number of smart-city initiatives from around the world have been analyzed and documented, and a growing body of academic knowledge is evolving around the phenomenon of the smart city. Smart-city government is seen as an important driver for developing a smart urban environment. The eCityGov Alliance in the Pacific Northwest of the USA represents a special case of a successful smart-city collaboration between nine neighboring municipalities, which combined forces to provide smart services to citizens and businesses that no single municipality could have provided alone. Developing and maintaining a collaborative governance model appears as the most important key success factor in such multi-jurisdictional smart-city undertakings. This study investigates the governance model of the eCityGov Alliance and its opportunities, and potential pitfalls. It concludes that the eCityGov Alliance can serve as a role model for such multi-jurisdictional smart-city initiatives. </jats:p>","quite a number of smart-city initiatives from around the world have been analyzed and documented, and a growing body of academic knowledge is evolving around the phenomenon of the smart city. smart-city government is seen as an important driver for developing a smart urban environment. the ecitygov alliance in the pacific northwest of the usa represents a special case of a successful smart-city collaboration between nine neighboring municipalities, which combined forces to provide smart services to citizens and businesses that no single municipality could have provided alone. developing and maintaining a collaborative governance model appears as the most important key success factor in such multi-jurisdictional smart-city undertakings. this study investigates the governance model of the ecitygov alliance and its opportunities, and potential pitfalls. it concludes that the ecitygov alliance can serve as a role model for such multi-jurisdictional smart-city initiatives."
http://orkg.org/orkg/resource/R141395,Enhanced Ability of Oligomeric Nanobodies Targeting MERS Coronavirus Receptor-Binding Domain,10.3390/v11020166,crossref,"<jats:p>Middle East respiratory syndrome (MERS) coronavirus (MERS-CoV), an infectious coronavirus first reported in 2012, has a mortality rate greater than 35%. Therapeutic antibodies are key tools for preventing and treating MERS-CoV infection, but to date no such agents have been approved for treatment of this virus. Nanobodies (Nbs) are camelid heavy chain variable domains with properties distinct from those of conventional antibodies and antibody fragments. We generated two oligomeric Nbs by linking two or three monomeric Nbs (Mono-Nbs) targeting the MERS-CoV receptor-binding domain (RBD), and compared their RBD-binding affinity, RBD–receptor binding inhibition, stability, and neutralizing and cross-neutralizing activity against MERS-CoV. Relative to Mono-Nb, dimeric Nb (Di-Nb) and trimeric Nb (Tri-Nb) had significantly greater ability to bind MERS-CoV RBD proteins with or without mutations in the RBD, thereby potently blocking RBD–MERS-CoV receptor binding. The engineered oligomeric Nbs were very stable under extreme conditions, including low or high pH, protease (pepsin), chaotropic denaturant (urea), and high temperature. Importantly, Di-Nb and Tri-Nb exerted significantly elevated broad-spectrum neutralizing activity against at least 19 human and camel MERS-CoV strains isolated in different countries and years. Overall, the engineered Nbs could be developed into effective therapeutic agents for prevention and treatment of MERS-CoV infection.</jats:p>","middle east respiratory syndrome (mers) coronavirus (mers-cov), an infectious coronavirus first reported in 2012, has a mortality rate greater than 35%. therapeutic antibodies are key tools for preventing and treating mers-cov infection, but to date no such agents have been approved for treatment of this virus. nanobodies (nbs) are camelid heavy chain variable domains with properties distinct from those of conventional antibodies and antibody fragments. we generated two oligomeric nbs by linking two or three monomeric nbs (mono-nbs) targeting the mers-cov receptor-binding domain (rbd), and compared their rbd-binding affinity, rbd–receptor binding inhibition, stability, and neutralizing and cross-neutralizing activity against mers-cov. relative to mono-nb, dimeric nb (di-nb) and trimeric nb (tri-nb) had significantly greater ability to bind mers-cov rbd proteins with or without mutations in the rbd, thereby potently blocking rbd–mers-cov receptor binding. the engineered oligomeric nbs were very stable under extreme conditions, including low or high ph, protease (pepsin), chaotropic denaturant (urea), and high temperature. importantly, di-nb and tri-nb exerted significantly elevated broad-spectrum neutralizing activity against at least 19 human and camel mers-cov strains isolated in different countries and years. overall, the engineered nbs could be developed into effective therapeutic agents for prevention and treatment of mers-cov infection."
http://orkg.org/orkg/resource/R141399,A novel nanobody targeting Middle East respiratory syndrome coronavirus (MERS-CoV) receptor-binding domain has potent cross-neutralizing activity and protective efficacy against MERS-CoV, 10.1128/JVI.00837-18,crossref,"""<jats:p>\n            Therapeutic development is critical for preventing and treating continual MERS-CoV infections in humans and camels. Because of their small size, nanobodies (Nbs) have advantages as antiviral therapeutics (e.g., high expression yield and robustness for storage and transportation) and also potential limitations (e.g., low antigen-binding affinity and fast renal clearance). Here, we have developed novel Nbs that specifically target the receptor-binding domain (RBD) of MERS-CoV spike protein. They bind to a conserved site on MERS-CoV RBD with high affinity, blocking RBD's binding to MERS-CoV receptor. Through engineering a C-terminal human Fc tag, the\n            <jats:italic>in vivo</jats:italic>\n            half-life of the Nbs is significantly extended. Moreover, the Nbs can potently cross-neutralize the infections of diverse MERS-CoV strains isolated from humans and camels. The Fc-tagged Nb also completely protects humanized mice from lethal MERS-CoV challenge. Taken together, our study has discovered novel Nbs that hold promise as potent, cost-effective, and broad-spectrum anti-MERS-CoV therapeutic agents.\n          </jats:p>""",""" \n therapeutic development is critical for preventing and treating continual mers-cov infections in humans and camels. because of their small size, nanobodies (nbs) have advantages as antiviral therapeutics (e.g., high expression yield and robustness for storage and transportation) and also potential limitations (e.g., low antigen-binding affinity and fast renal clearance). here, we have developed novel nbs that specifically target the receptor-binding domain (rbd) of mers-cov spike protein. they bind to a conserved site on mers-cov rbd with high affinity, blocking rbd's binding to mers-cov receptor. through engineering a c-terminal human fc tag, the\n in vivo \n half-life of the nbs is significantly extended. moreover, the nbs can potently cross-neutralize the infections of diverse mers-cov strains isolated from humans and camels. the fc-tagged nb also completely protects humanized mice from lethal mers-cov challenge. taken together, our study has discovered novel nbs that hold promise as potent, cost-effective, and broad-spectrum anti-mers-cov therapeutic agents.\n """
http://orkg.org/orkg/resource/R141397,Chimeric camel/human heavy-chain antibodies protect against MERS-CoV infection,10.1126/sciadv.aas9667,crossref,<jats:p>Dromedary camel heavy chain–only antibodies may provide novel intervention strategies against MERS coronavirus.</jats:p>,dromedary camel heavy chain–only antibodies may provide novel intervention strategies against mers coronavirus.
http://orkg.org/orkg/resource/R141419,Identification of sialic acid-binding function for the Middle East respiratory syndrome coronavirus spike glycoprotein,10.1073/pnas.1712592114,crossref,"<jats:title>Significance</jats:title>\n          <jats:p>Middle East respiratory syndrome coronavirus (MERS-CoV) recurrently infects humans from its dromedary camel reservoir, causing severe respiratory disease with an ∼35% fatality rate. The virus binds to the dipeptidyl peptidase 4 (DPP4) entry receptor on respiratory epithelial cells via its spike protein. We here report that the MERS-CoV spike protein selectively binds to sialic acid (Sia) and demonstrate that cell-surface sialoglycoconjugates can serve as an attachment factor. Our observations warrant further research into the role of Sia binding in the virus’s host and tissue tropism and transmission, which may be influenced by the observed Sia-binding fine specificity and by differences in sialoglycomes among host species.</jats:p>","significance \n middle east respiratory syndrome coronavirus (mers-cov) recurrently infects humans from its dromedary camel reservoir, causing severe respiratory disease with an ∼35% fatality rate. the virus binds to the dipeptidyl peptidase 4 (dpp4) entry receptor on respiratory epithelial cells via its spike protein. we here report that the mers-cov spike protein selectively binds to sialic acid (sia) and demonstrate that cell-surface sialoglycoconjugates can serve as an attachment factor. our observations warrant further research into the role of sia binding in the virus’s host and tissue tropism and transmission, which may be influenced by the observed sia-binding fine specificity and by differences in sialoglycomes among host species."
http://orkg.org/orkg/resource/R141421,Species-Specific Colocalization of Middle East Respiratory Syndrome Coronavirus Attachment and Entry Receptors,10.1128/JVI.00107-19,crossref,"<jats:p>\n            MERS-CoV uses the S1\n            <jats:sup>B</jats:sup>\n            domain of its spike protein to attach to its host receptor, dipeptidyl peptidase 4 (DPP4). The tissue localization of DPP4 has been mapped in different susceptible species. On the other hand, the S1\n            <jats:sup>A</jats:sup>\n            domain, the N-terminal domain of this spike protein, preferentially binds to several glycotopes of α2,3-sialic acids, the attachment factor of MERS-CoV. Here we show, using a novel method, that the S1\n            <jats:sup>A</jats:sup>\n            domain specifically binds to the nasal epithelium of dromedary camels, alveolar epithelium of humans, and intestinal epithelium of common pipistrelle bats. In contrast, it does not bind to the nasal epithelium of pigs or rabbits, nor does it bind to the intestinal epithelium of serotine bats and frugivorous bat species. This finding supports the importance of the S1\n            <jats:sup>A</jats:sup>\n            domain in MERS-CoV infection and tropism, suggests its role in transmission, and highlights its potential use as a component of novel vaccine candidates.\n          </jats:p>","\n mers-cov uses the s1\n b \n domain of its spike protein to attach to its host receptor, dipeptidyl peptidase 4 (dpp4). the tissue localization of dpp4 has been mapped in different susceptible species. on the other hand, the s1\n a \n domain, the n-terminal domain of this spike protein, preferentially binds to several glycotopes of α2,3-sialic acids, the attachment factor of mers-cov. here we show, using a novel method, that the s1\n a \n domain specifically binds to the nasal epithelium of dromedary camels, alveolar epithelium of humans, and intestinal epithelium of common pipistrelle bats. in contrast, it does not bind to the nasal epithelium of pigs or rabbits, nor does it bind to the intestinal epithelium of serotine bats and frugivorous bat species. this finding supports the importance of the s1\n a \n domain in mers-cov infection and tropism, suggests its role in transmission, and highlights its potential use as a component of novel vaccine candidates.\n"
http://orkg.org/orkg/resource/R142050,Competitive urbanism and the limits to smart city innovation: The UK Future Cities initiative,10.1177/0042098015597162,crossref,"<jats:p> The technological vision of smart urbanism has been promoted as a silver bullet for urban problems and a major market opportunity. The search is on for firms and governments to find effective and transferable demonstrations of advanced urban technology. This paper examines initiatives by the UK national government to facilitate urban technological innovation through a range of strategies, particularly the TSB Future Cities Demonstrator Competition. This case study is used to explore opportunities and tensions in the practical realisation of the smart city imaginary. Tensions are shown to be partly about the conjectural nature of the smart city debate. Attention is also drawn to weakened capacity of urban governments to control their infrastructural destiny and also constraints on the ability of the public and private sectors to innovate. The paper contributes to smart city debates by providing further evidence of the difficulties in substantiating the smart city imaginary. </jats:p>","the technological vision of smart urbanism has been promoted as a silver bullet for urban problems and a major market opportunity. the search is on for firms and governments to find effective and transferable demonstrations of advanced urban technology. this paper examines initiatives by the uk national government to facilitate urban technological innovation through a range of strategies, particularly the tsb future cities demonstrator competition. this case study is used to explore opportunities and tensions in the practical realisation of the smart city imaginary. tensions are shown to be partly about the conjectural nature of the smart city debate. attention is also drawn to weakened capacity of urban governments to control their infrastructural destiny and also constraints on the ability of the public and private sectors to innovate. the paper contributes to smart city debates by providing further evidence of the difficulties in substantiating the smart city imaginary."
http://orkg.org/orkg/resource/R141621,Probing the highly efficient room temperature ammonia gas sensing properties of a luminescent ZnO nanowire array prepared via an AAO-assisted template route,10.1039/c3dt53305k,crossref,<p>A highly ordered luminescent ZnO nanowire array was synthesized which has excellent sensitivity and fast response to NH<sub>3</sub> gas.</p>,a highly ordered luminescent zno nanowire array was synthesized which has excellent sensitivity and fast response to nh 3 gas.
http://orkg.org/orkg/resource/R141337,Nitrogen Uptake in the Northeastern Arabian Sea during Winter Cooling,10.1155/2010/819029,crossref,"<jats:p>The uptake of dissolved inorganic nitrogen by phytoplankton is an important aspect of the nitrogen cycle of oceans. Here, we present nitrate (<mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mtext>N</mml:mtext><mml:mtext>O</mml:mtext></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msup></mml:math>) and ammonium (<mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mtext>N</mml:mtext><mml:mtext>H</mml:mtext></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:math>) uptake rates in the northeastern Arabian Sea using <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mtext>N</mml:mtext></mml:math> tracer technique. In this relatively underexplored region, productivity is high during winter due to supply of nutrients by convective mixing caused by the cooling of the surface by the northeast monsoon winds. Studies done during different months (January and late February-early March) of the northeast monsoon 2003 revealed a fivefold increase in the average euphotic zone integrated <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mtext>N</mml:mtext><mml:mtext>O</mml:mtext></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msup></mml:math> uptake from January (2.3\u2009mmolN <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:msup><mml:mtext>d</mml:mtext><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math>) to late February-early March (12.7\u2009mmolN <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:msup><mml:mtext>d</mml:mtext><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math>). The <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi>f</mml:mi></mml:math>-ratio during January appeared to be affected by the winter cooling effect and increased by more than 50% from the southernmost station to the northern open ocean stations, indicating hydrographic and meteorological control. Estimates of <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mtext>N</mml:mtext><mml:mtext>O</mml:mtext></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msup></mml:math> residence time suggested that <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mtext>N</mml:mtext><mml:mtext>O</mml:mtext></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msup></mml:math> entrained in the water column during January contributed to the development of blooms during late February-early March.</jats:p>","the uptake of dissolved inorganic nitrogen by phytoplankton is an important aspect of the nitrogen cycle of oceans. here, we present nitrate ( n o 3 - ) and ammonium ( n h 4 + ) uptake rates in the northeastern arabian sea using 15 n tracer technique. in this relatively underexplored region, productivity is high during winter due to supply of nutrients by convective mixing caused by the cooling of the surface by the northeast monsoon winds. studies done during different months (january and late february-early march) of the northeast monsoon 2003 revealed a fivefold increase in the average euphotic zone integrated n o 3 - uptake from january (2.3\u2009mmoln m − 2 d − 1 ) to late february-early march (12.7\u2009mmoln m − 2 d − 1 ). the f -ratio during january appeared to be affected by the winter cooling effect and increased by more than 50% from the southernmost station to the northern open ocean stations, indicating hydrographic and meteorological control. estimates of n o 3 - residence time suggested that n o 3 - entrained in the water column during january contributed to the development of blooms during late february-early march."
http://orkg.org/orkg/resource/R141363,"N
            2
            Fixation by Unicellular Bacterioplankton from the Atlantic and Pacific Oceans: Phylogeny and In Situ Rates",10.1128/AEM.70.2.765-770.2004,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            N\n            <jats:sub>2</jats:sub>\n            -fixing proteobacteria (α and γ) and unicellular cyanobacteria are common in both the tropical North Atlantic and Pacific oceans. In near-surface waters proteobacterial\n            <jats:italic>nifH</jats:italic>\n            transcripts were present during both night and day while unicellular cyanobacterial\n            <jats:italic>nifH</jats:italic>\n            transcripts were present during the nighttime only, suggesting separation of N\n            <jats:sub>2</jats:sub>\n            fixation and photosynthesis by unicellular cyanobacteria. Phylogenetic relationships among unicellular cyanobacteria from both oceans were determined after sequencing of a conserved region of 16S ribosomal DNA (rDNA) of cyanobacteria, and results showed that they clustered together, regardless of the ocean of origin. However, sequencing of\n            <jats:italic>nifH</jats:italic>\n            transcripts of unicellular cyanobacteria from both oceans showed that they clustered separately. This suggests that unicellular cyanobacteria from the tropical North Atlantic and subtropical North Pacific share a common ancestry (16S rDNA) and that potential unicellular N\n            <jats:sub>2</jats:sub>\n            fixers have diverged (\n            <jats:italic>nifH</jats:italic>\n            ). N\n            <jats:sub>2</jats:sub>\n            fixation rates for unicellular bacterioplankton (including small cyanobacteria) from both oceans were determined in situ according to the acetylene reduction and\n            <jats:sup>15</jats:sup>\n            N\n            <jats:sub>2</jats:sub>\n            protocols. The results showed that rates of fixation by bacterioplankton can be almost as high as those of fixation by the colonial N\n            <jats:sub>2</jats:sub>\n            -fixing marine cyanobacteria\n            <jats:italic>Trichodesmium</jats:italic>\n            spp. in the tropical North Atlantic but that rates are much lower in the subtropical North Pacific.\n          </jats:p>","abstract \n \n n\n 2 \n -fixing proteobacteria (α and γ) and unicellular cyanobacteria are common in both the tropical north atlantic and pacific oceans. in near-surface waters proteobacterial\n nifh \n transcripts were present during both night and day while unicellular cyanobacterial\n nifh \n transcripts were present during the nighttime only, suggesting separation of n\n 2 \n fixation and photosynthesis by unicellular cyanobacteria. phylogenetic relationships among unicellular cyanobacteria from both oceans were determined after sequencing of a conserved region of 16s ribosomal dna (rdna) of cyanobacteria, and results showed that they clustered together, regardless of the ocean of origin. however, sequencing of\n nifh \n transcripts of unicellular cyanobacteria from both oceans showed that they clustered separately. this suggests that unicellular cyanobacteria from the tropical north atlantic and subtropical north pacific share a common ancestry (16s rdna) and that potential unicellular n\n 2 \n fixers have diverged (\n nifh \n ). n\n 2 \n fixation rates for unicellular bacterioplankton (including small cyanobacteria) from both oceans were determined in situ according to the acetylene reduction and\n 15 \n n\n 2 \n protocols. the results showed that rates of fixation by bacterioplankton can be almost as high as those of fixation by the colonial n\n 2 \n -fixing marine cyanobacteria\n trichodesmium \n spp. in the tropical north atlantic but that rates are much lower in the subtropical north pacific.\n"
http://orkg.org/orkg/resource/R141143,"Going the full distance: Strategic support for digital libraries in distance education at the University of Education, Winneba in Ghana",10.1177/0961000618772871,crossref,"<jats:p> Many African universities have employed distance education to expand access to education and digital libraries can ensure seamless access to information for distance learners. The purpose of this study was to investigate the availability of policies and strategic initiatives for developing digital library services in distance education at the University of Education, Winneba in Ghana. The study relies on the results of semi-structured interviews with selected librarians of the University, and institutional document analysis. The study found that digital library use in distance education at the University is seriously hampered due to the absence of strategic support, consistent policies and dedicated funding for digital library initiatives. </jats:p>","many african universities have employed distance education to expand access to education and digital libraries can ensure seamless access to information for distance learners. the purpose of this study was to investigate the availability of policies and strategic initiatives for developing digital library services in distance education at the university of education, winneba in ghana. the study relies on the results of semi-structured interviews with selected librarians of the university, and institutional document analysis. the study found that digital library use in distance education at the university is seriously hampered due to the absence of strategic support, consistent policies and dedicated funding for digital library initiatives."
http://orkg.org/orkg/resource/R141600,"CoxBase: an online platform for epidemiological surveillance, visualization, analysis and typing of Coxiella burnetii genomic sequences",10.1101/2020.11.29.402362,crossref,"<jats:title>ABSTRACT</jats:title><jats:p>Q (query) fever is an infectious zoonotic disease caused by the Gram-negative bacteria <jats:italic>Coxiella burnetii</jats:italic>. Although the disease has been studied since decades, it still represents a threat due to sporadic outbreaks across farms in Europe. The absence of a central platform for <jats:italic>Coxiella</jats:italic> typing data management in an important epidemiological gap which is relevant in the case of an outbreak. To fill this gap, we have designed and implemented an online, open-source, and, web-based platform called CoxBase (<jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://coxbase.q-gaps.de"">https://coxbase.q-gaps.de</jats:ext-link>). This platform includes a database that holds genotyping information of more than 400 <jats:italic>Coxiella</jats:italic> isolates alongside metadata that annotates them. We have also implemented features for <jats:italic>in silico</jats:italic> genotyping of completely or minimally assembled <jats:italic>Coxiella</jats:italic> sequences using five different typing methods, querying existing isolates, visualization of isolate’s geodata via aggregation on a world map and submission of new isolates. We tested our <jats:italic>in silico</jats:italic> typing method on 50 <jats:italic>Coxiella</jats:italic> genomes downloaded from the RefSeq database and we successfully genotyped all except for cases where the sequence quality was poor. We identified new spacer sequences using our implementation of the MST <jats:italic>in silico</jats:italic> typing method, and established adaA gene phenotypes for all 50 genomes as well as their plasmid types.</jats:p>","abstract q (query) fever is an infectious zoonotic disease caused by the gram-negative bacteria coxiella burnetii . although the disease has been studied since decades, it still represents a threat due to sporadic outbreaks across farms in europe. the absence of a central platform for coxiella typing data management in an important epidemiological gap which is relevant in the case of an outbreak. to fill this gap, we have designed and implemented an online, open-source, and, web-based platform called coxbase ( https://coxbase.q-gaps.de ). this platform includes a database that holds genotyping information of more than 400 coxiella isolates alongside metadata that annotates them. we have also implemented features for in silico genotyping of completely or minimally assembled coxiella sequences using five different typing methods, querying existing isolates, visualization of isolate’s geodata via aggregation on a world map and submission of new isolates. we tested our in silico typing method on 50 coxiella genomes downloaded from the refseq database and we successfully genotyped all except for cases where the sequence quality was poor. we identified new spacer sequences using our implementation of the mst in silico typing method, and established adaa gene phenotypes for all 50 genomes as well as their plasmid types."
http://orkg.org/orkg/resource/R141357,Latitudinal distribution of &lt;i&gt;Trichodesmium&lt;/i&gt; spp. and N&lt;sub&gt;2&lt;/sub&gt; fixation in the Atlantic Ocean,10.5194/bg-7-3167-2010,crossref,"<jats:p>Abstract. We have determined the latitudinal distribution of Trichodesmium spp. abundance and community N2 fixation in the Atlantic Ocean along a meridional transect from ca. 30° N to 30° S in November–December 2007 and April–May 2008. The observations from both cruises were highly consistent in terms of absolute magnitude and latitudinal distribution, showing a strong association between Trichodesmium abundance and community N2 fixation. The highest Trichodesmium abundances (mean = 220 trichomes L−1,) and community N2 fixation rates (mean = 60 μmol m−2 d−1) occurred in the Equatorial region between 5° S–15° N. In the South Atlantic gyre, Trichodesmium abundance was very low (ca. 1 trichome L−1) but N2 fixation was always measurable, averaging 3 and 10 μmol m2 d−1 in 2007 and 2008, respectively. We suggest that N2 fixation in the South Atlantic was sustained by other, presumably unicellular, diazotrophs. Comparing these distributions with the geographical pattern in atmospheric dust deposition points to iron supply as the main factor determining the large scale latitudinal variability of Trichodesmium spp. abundance and N2 fixation in the Atlantic Ocean. We observed a marked South to North decrease in surface phosphate concentration, which argues against a role for phosphorus availability in controlling the large scale distribution of N2 fixation. Scaling up from all our measurements (42 stations) results in conservative estimates for total N2 fixation of ∼6 TgN yr−1 in the North Atlantic (0–40° N) and ~1.2 TgN yr−1 in the South Atlantic (0–40° S).\n                    </jats:p>","abstract. we have determined the latitudinal distribution of trichodesmium spp. abundance and community n2 fixation in the atlantic ocean along a meridional transect from ca. 30° n to 30° s in november–december 2007 and april–may 2008. the observations from both cruises were highly consistent in terms of absolute magnitude and latitudinal distribution, showing a strong association between trichodesmium abundance and community n2 fixation. the highest trichodesmium abundances (mean = 220 trichomes l−1,) and community n2 fixation rates (mean = 60 μmol m−2 d−1) occurred in the equatorial region between 5° s–15° n. in the south atlantic gyre, trichodesmium abundance was very low (ca. 1 trichome l−1) but n2 fixation was always measurable, averaging 3 and 10 μmol m2 d−1 in 2007 and 2008, respectively. we suggest that n2 fixation in the south atlantic was sustained by other, presumably unicellular, diazotrophs. comparing these distributions with the geographical pattern in atmospheric dust deposition points to iron supply as the main factor determining the large scale latitudinal variability of trichodesmium spp. abundance and n2 fixation in the atlantic ocean. we observed a marked south to north decrease in surface phosphate concentration, which argues against a role for phosphorus availability in controlling the large scale distribution of n2 fixation. scaling up from all our measurements (42 stations) results in conservative estimates for total n2 fixation of ∼6 tgn yr−1 in the north atlantic (0–40° n) and ~1.2 tgn yr−1 in the south atlantic (0–40° s).\n"
http://orkg.org/orkg/resource/R140812,Characterization and mapping of hematite ore mineral classes using hyperspectral remote sensing technique: a case study from Bailadila iron ore mining region,10.1007/s42452-021-04213-3,crossref,"<jats:title>Abstract</jats:title><jats:p>The study demonstrates a methodology for mapping various hematite ore classes based on their reflectance and absorption spectra, using Hyperion satellite imagery. Substantial validation is carried out, using the spectral feature fitting technique, with the field spectra measured over the Bailadila hill range in Chhattisgarh State in India. The results of the study showed a good correlation between the concentration of iron oxide with the depth of the near-infrared absorption feature (R<jats:sup>2</jats:sup>\u2009=\u20090.843) and the width of the near-infrared absorption feature (R<jats:sup>2</jats:sup>\u2009=\u20090.812) through different empirical models, with a root-mean-square error (RMSE) between\u2009&lt;\u20090.317 and\u2009&lt;\u20090.409. The overall accuracy of the study is 88.2% with a Kappa coefficient value of 0.81. Geochemical analysis and X-ray fluorescence (XRF) of field ore samples are performed to ensure different classes of hematite ore minerals. Results showed a high content of Fe\u2009&gt;\u200960\xa0wt% in most of the hematite ore samples, except banded hematite quartzite (BHQ) (&lt;\u200947\xa0wt%).</jats:p>","abstract the study demonstrates a methodology for mapping various hematite ore classes based on their reflectance and absorption spectra, using hyperion satellite imagery. substantial validation is carried out, using the spectral feature fitting technique, with the field spectra measured over the bailadila hill range in chhattisgarh state in india. the results of the study showed a good correlation between the concentration of iron oxide with the depth of the near-infrared absorption feature (r 2 \u2009=\u20090.843) and the width of the near-infrared absorption feature (r 2 \u2009=\u20090.812) through different empirical models, with a root-mean-square error (rmse) between\u2009&lt;\u20090.317 and\u2009&lt;\u20090.409. the overall accuracy of the study is 88.2% with a kappa coefficient value of 0.81. geochemical analysis and x-ray fluorescence (xrf) of field ore samples are performed to ensure different classes of hematite ore minerals. results showed a high content of fe\u2009&gt;\u200960\xa0wt% in most of the hematite ore samples, except banded hematite quartzite (bhq) (&lt;\u200947\xa0wt%)."
http://orkg.org/orkg/resource/R140827,Sub-pixel mineral mapping using EO-1 Hyperion hyperspectral data,10.5194/isprsarchives-XL-8-455-2014,crossref,"<jats:p>Abstract. This study describes the utility of Earth Observation (EO)-1 Hyperion data for sub-pixel mineral investigation using Mixture Tuned Target Constrained Interference Minimized Filter (MTTCIMF) algorithm in hostile mountainous terrain of Rajsamand district of Rajasthan, which hosts economic mineralization such as lead, zinc, and copper etc. The study encompasses pre-processing, data reduction, Pixel Purity Index (PPI) and endmember extraction from reflectance image of surface minerals such as illite, montmorillonite, phlogopite, dolomite and chlorite. These endmembers were then assessed with USGS mineral spectral library and lab spectra of rock samples collected from field for spectral inspection. Subsequently, MTTCIMF algorithm was implemented on processed image to obtain mineral distribution map of each detected mineral. A virtual verification method has been adopted to evaluate the classified image, which uses directly image information to evaluate the result and confirm the overall accuracy and kappa coefficient of 68 % and 0.6 respectively. The sub-pixel level mineral information with reasonable accuracy could be a valuable guide to geological and exploration community for expensive ground and/or lab experiments to discover economic deposits. Thus, the study demonstrates the feasibility of Hyperion data for sub-pixel mineral mapping using MTTCIMF algorithm with cost and time effective approach.\n                    </jats:p>","abstract. this study describes the utility of earth observation (eo)-1 hyperion data for sub-pixel mineral investigation using mixture tuned target constrained interference minimized filter (mttcimf) algorithm in hostile mountainous terrain of rajsamand district of rajasthan, which hosts economic mineralization such as lead, zinc, and copper etc. the study encompasses pre-processing, data reduction, pixel purity index (ppi) and endmember extraction from reflectance image of surface minerals such as illite, montmorillonite, phlogopite, dolomite and chlorite. these endmembers were then assessed with usgs mineral spectral library and lab spectra of rock samples collected from field for spectral inspection. subsequently, mttcimf algorithm was implemented on processed image to obtain mineral distribution map of each detected mineral. a virtual verification method has been adopted to evaluate the classified image, which uses directly image information to evaluate the result and confirm the overall accuracy and kappa coefficient of 68 % and 0.6 respectively. the sub-pixel level mineral information with reasonable accuracy could be a valuable guide to geological and exploration community for expensive ground and/or lab experiments to discover economic deposits. thus, the study demonstrates the feasibility of hyperion data for sub-pixel mineral mapping using mttcimf algorithm with cost and time effective approach.\n"
http://orkg.org/orkg/resource/R141389,Self-assembled star-shaped chiroplasmonic gold nanoparticles for an ultrasensitive chiro-immunosensor for viruses,10.1039/C7RA07175B,crossref,<p>Nanoengineered chiral gold nanoparticles and quantum dots for ultrasensitive chiroptical sensing of viruses in blood samples.</p>,nanoengineered chiral gold nanoparticles and quantum dots for ultrasensitive chiroptical sensing of viruses in blood samples.
http://orkg.org/orkg/resource/R141302,Engineered biomimetic nanovesicles show intrinsic anti-inflammatory properties for the treatment of inflammatory bowel diseases,10.1039/c7nr04734g,crossref,<p>We debut for the first time specialized leukosomes (SLKs) for the treatment of inflammatory bowel disease.</p>,we debut for the first time specialized leukosomes (slks) for the treatment of inflammatory bowel disease.
http://orkg.org/orkg/resource/R142020,"Smart City Research: Contextual Conditions, Governance Models, and Public Value Assessment",10.1177/0894439315618890,crossref,"<jats:p> There are three issues that are crucial to advancing our academic understanding of smart cities: (1) contextual conditions, (2) governance models, and (3) the assessment of public value. A brief review of recent literature and the analysis of the included papers provide support for the assumption that cities cannot simply copy good practices but must develop approaches that fit their own situation ( contingency) and concord with their own organization in terms of broader strategies, human resource policies, information policies, and so on ( configuration). A variety of insights into the mechanisms and building blocks of smart city practices are presented, and issues for further research are identified. </jats:p>","there are three issues that are crucial to advancing our academic understanding of smart cities: (1) contextual conditions, (2) governance models, and (3) the assessment of public value. a brief review of recent literature and the analysis of the included papers provide support for the assumption that cities cannot simply copy good practices but must develop approaches that fit their own situation ( contingency) and concord with their own organization in terms of broader strategies, human resource policies, information policies, and so on ( configuration). a variety of insights into the mechanisms and building blocks of smart city practices are presented, and issues for further research are identified."
http://orkg.org/orkg/resource/R140573,Atmospheric regional climate projections for the Baltic Sea Region until 2100,10.5194/esd-2021-51,crossref,"<jats:p>Abstract. The Baltic Sea Region is very sensitive to climate change; it is a region with spatially varying climate and diverse ecosystems, but also under pressure due to high population in large parts of the area. Climate change impacts could easily exacerbate other anthropogenic stressors such as biodiversity stress from society and eutrophication of the Baltic Sea considerably. Therefore, there has been a focus on estimations of future climate change and its impacts in recent research. In this review paper, we will concentrate on a presentation of recent climate projections from both atmosphere-only and coupled atmosphere-ocean regional climate models. The recent regional climate model projections strengthen the picture from previous assessments. This includes a strong warming, in particular in the north in winter. Precipitation is projected to increase in the whole region apart from the southern half during summer. Consequently, the new results lend more credibility to estimates of uncertainties and robust features of future climate change. Furthermore, the larger number of scenarios gives opportunities to better address impacts of mitigation measures. The coupled atmosphere-ocean model locally modifies the climate change signal relative to that in the stand-alone atmosphere regional climate model. Differences are largest in areas where the coupled system arrives at different sea-surface temperatures and sea-ice conditions.\n                        </jats:p>","abstract. the baltic sea region is very sensitive to climate change; it is a region with spatially varying climate and diverse ecosystems, but also under pressure due to high population in large parts of the area. climate change impacts could easily exacerbate other anthropogenic stressors such as biodiversity stress from society and eutrophication of the baltic sea considerably. therefore, there has been a focus on estimations of future climate change and its impacts in recent research. in this review paper, we will concentrate on a presentation of recent climate projections from both atmosphere-only and coupled atmosphere-ocean regional climate models. the recent regional climate model projections strengthen the picture from previous assessments. this includes a strong warming, in particular in the north in winter. precipitation is projected to increase in the whole region apart from the southern half during summer. consequently, the new results lend more credibility to estimates of uncertainties and robust features of future climate change. furthermore, the larger number of scenarios gives opportunities to better address impacts of mitigation measures. the coupled atmosphere-ocean model locally modifies the climate change signal relative to that in the stand-alone atmosphere regional climate model. differences are largest in areas where the coupled system arrives at different sea-surface temperatures and sea-ice conditions.\n"
http://orkg.org/orkg/resource/R140743,Flower-like Palladium Nanoclusters Decorated Graphene Electrodes for Ultrasensitive and Flexible Hydrogen Gas Sensing,10.1038/srep12294,crossref,"<jats:title>Abstract</jats:title><jats:p>Flower-like palladium nanoclusters (FPNCs) are electrodeposited onto graphene electrode that are prepared by chemical vapor deposition (CVD). The CVD graphene layer is transferred onto a poly(ethylene naphthalate) (PEN) film to provide a mechanical stability and flexibility. The surface of the CVD graphene is functionalized with diaminonaphthalene (DAN) to form flower shapes. Palladium nanoparticles act as templates to mediate the formation of FPNCs, which increase in size with reaction time. The population of FPNCs can be controlled by adjusting the DAN concentration as functionalization solution. These FPNCs_CG electrodes are sensitive to hydrogen gas at room temperature. The sensitivity and response time as a function of the FPNCs population are investigated, resulted in improved performance with increasing population. Furthermore, the minimum detectable level (MDL) of hydrogen is 0.1\u2009ppm, which is at least 2 orders of magnitude lower than that of chemical sensors based on other Pd-based hybrid materials.</jats:p>","abstract flower-like palladium nanoclusters (fpncs) are electrodeposited onto graphene electrode that are prepared by chemical vapor deposition (cvd). the cvd graphene layer is transferred onto a poly(ethylene naphthalate) (pen) film to provide a mechanical stability and flexibility. the surface of the cvd graphene is functionalized with diaminonaphthalene (dan) to form flower shapes. palladium nanoparticles act as templates to mediate the formation of fpncs, which increase in size with reaction time. the population of fpncs can be controlled by adjusting the dan concentration as functionalization solution. these fpncs_cg electrodes are sensitive to hydrogen gas at room temperature. the sensitivity and response time as a function of the fpncs population are investigated, resulted in improved performance with increasing population. furthermore, the minimum detectable level (mdl) of hydrogen is 0.1\u2009ppm, which is at least 2 orders of magnitude lower than that of chemical sensors based on other pd-based hybrid materials."
http://orkg.org/orkg/resource/R140156,OWL2Vec*: Embedding of OWL Ontologies,,crossref,"<jats:title>Abstract</jats:title><jats:p>Semantic embedding of knowledge graphs has been widely studied and used for prediction and statistical analysis tasks across various domains such as Natural Language Processing and the Semantic Web. However, less attention has been paid to developing robust methods for embedding OWL (Web Ontology Language) ontologies, which contain richer semantic information than plain knowledge graphs, and have been widely adopted in domains such as bioinformatics. In this paper, we propose a random walk and word embedding based ontology embedding method named , which encodes the semantics of an OWL ontology by taking into account its graph structure, lexical information and logical constructors. Our empirical evaluation with three real world datasets suggests that   benefits from these three different aspects of an ontology in class membership prediction and class subsumption prediction tasks. Furthermore,   often significantly outperforms the state-of-the-art methods in our experiments.</jats:p>","abstract semantic embedding of knowledge graphs has been widely studied and used for prediction and statistical analysis tasks across various domains such as natural language processing and the semantic web. however, less attention has been paid to developing robust methods for embedding owl (web ontology language) ontologies, which contain richer semantic information than plain knowledge graphs, and have been widely adopted in domains such as bioinformatics. in this paper, we propose a random walk and word embedding based ontology embedding method named , which encodes the semantics of an owl ontology by taking into account its graph structure, lexical information and logical constructors. our empirical evaluation with three real world datasets suggests that benefits from these three different aspects of an ontology in class membership prediction and class subsumption prediction tasks. furthermore, often significantly outperforms the state-of-the-art methods in our experiments."
http://orkg.org/orkg/resource/R140080,Interdisciplinary Online Hackathons as an Approach to Combat the COVID-19 Pandemic: Case Study,10.2196/25283,crossref,"<jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>The COVID-19 outbreak has affected the lives of millions of people by causing a dramatic impact on many health care systems and the global economy. This devastating pandemic has brought together communities across the globe to work on this issue in an unprecedented manner.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Objective</jats:title>\n            <jats:p>This case study describes the steps and methods employed in the conduction of a remote online health hackathon centered on challenges posed by the COVID-19 pandemic. It aims to deliver a clear implementation road map for other organizations to follow.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Methods</jats:title>\n            <jats:p>This 4-day hackathon was conducted in April 2020, based on six COVID-19–related challenges defined by frontline clinicians and researchers from various disciplines. An online survey was structured to assess: (1) individual experience satisfaction, (2) level of interprofessional skills exchange, (3) maturity of the projects realized, and (4) overall quality of the event. At the end of the event, participants were invited to take part in an online survey with 17 (+5 optional) items, including multiple-choice and open-ended questions that assessed their experience regarding the remote nature of the event and their individual project, interprofessional skills exchange, and their confidence in working on a digital health project before and after the hackathon. Mentors, who guided the participants through the event, also provided feedback to the organizers through an online survey.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>A total of 48 participants and 52 mentors based in 8 different countries participated and developed 14 projects. A total of 75 mentorship video sessions were held. Participants reported increased confidence in starting a digital health venture or a research project after successfully participating in the hackathon, and stated that they were likely to continue working on their projects. Of the participants who provided feedback, 60% (n=18) would not have started their project without this particular hackathon and indicated that the hackathon encouraged and enabled them to progress faster, for example, by building interdisciplinary teams, gaining new insights and feedback provided by their mentors, and creating a functional prototype.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>This study provides insights into how online hackathons can contribute to solving the challenges and effects of a pandemic in several regions of the world. The online format fosters team diversity, increases cross-regional collaboration, and can be executed much faster and at lower costs compared to in-person events. Results on preparation, organization, and evaluation of this online hackathon are useful for other institutions and initiatives that are willing to introduce similar event formats in the fight against COVID-19.</jats:p>\n          </jats:sec>","\n background \n the covid-19 outbreak has affected the lives of millions of people by causing a dramatic impact on many health care systems and the global economy. this devastating pandemic has brought together communities across the globe to work on this issue in an unprecedented manner. \n \n \n objective \n this case study describes the steps and methods employed in the conduction of a remote online health hackathon centered on challenges posed by the covid-19 pandemic. it aims to deliver a clear implementation road map for other organizations to follow. \n \n \n methods \n this 4-day hackathon was conducted in april 2020, based on six covid-19–related challenges defined by frontline clinicians and researchers from various disciplines. an online survey was structured to assess: (1) individual experience satisfaction, (2) level of interprofessional skills exchange, (3) maturity of the projects realized, and (4) overall quality of the event. at the end of the event, participants were invited to take part in an online survey with 17 (+5 optional) items, including multiple-choice and open-ended questions that assessed their experience regarding the remote nature of the event and their individual project, interprofessional skills exchange, and their confidence in working on a digital health project before and after the hackathon. mentors, who guided the participants through the event, also provided feedback to the organizers through an online survey. \n \n \n results \n a total of 48 participants and 52 mentors based in 8 different countries participated and developed 14 projects. a total of 75 mentorship video sessions were held. participants reported increased confidence in starting a digital health venture or a research project after successfully participating in the hackathon, and stated that they were likely to continue working on their projects. of the participants who provided feedback, 60% (n=18) would not have started their project without this particular hackathon and indicated that the hackathon encouraged and enabled them to progress faster, for example, by building interdisciplinary teams, gaining new insights and feedback provided by their mentors, and creating a functional prototype. \n \n \n conclusions \n this study provides insights into how online hackathons can contribute to solving the challenges and effects of a pandemic in several regions of the world. the online format fosters team diversity, increases cross-regional collaboration, and can be executed much faster and at lower costs compared to in-person events. results on preparation, organization, and evaluation of this online hackathon are useful for other institutions and initiatives that are willing to introduce similar event formats in the fight against covid-19. \n"
http://orkg.org/orkg/resource/R140059,Open data hackathons: an innovative strategy to enhance entrepreneurial intention,10.1108/ijis-06-2017-0055,crossref,"<jats:sec>\n<jats:title content-type=""abstract-subheading"">Purpose</jats:title>\n<jats:p>In terms of entrepreneurship, open data benefits include economic growth, innovation, empowerment and new or improved products and services. Hackathons encourage the development of new applications using open data and the creation of startups based on these applications. Researchers focus on factors that affect nascent entrepreneurs’ decision to create a startup but researches in the field of open data hackathons have not been fully investigated yet. This paper aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>\n<jats:p>In total, 70 papers were examined and analyzed using a three-phased literature review methodology, which was suggested by Webster and Watson (2002). These surveys investigated several factors that affect a nascent entrepreneur to create a startup.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Findings</jats:title>\n<jats:p>Eventually, by identifying the motivations for developers to participate in a hackathon, and understanding the benefits of the use of open data, researchers will be able to elaborate the proposed model and evaluate if the contest has contributed to the decision of establish a startup and what factors affect the decision to establish a startup apply to open data developers, and if the participants of the contest agree with these factors.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>\n<jats:p>The paper expands the scope of open data research on entrepreneurship field, stating the need for more research to be conducted regarding the open data in entrepreneurship through hackathons.</jats:p>\n</jats:sec>","\n purpose \n in terms of entrepreneurship, open data benefits include economic growth, innovation, empowerment and new or improved products and services. hackathons encourage the development of new applications using open data and the creation of startups based on these applications. researchers focus on factors that affect nascent entrepreneurs’ decision to create a startup but researches in the field of open data hackathons have not been fully investigated yet. this paper aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons. \n \n \n design/methodology/approach \n in total, 70 papers were examined and analyzed using a three-phased literature review methodology, which was suggested by webster and watson (2002). these surveys investigated several factors that affect a nascent entrepreneur to create a startup. \n \n \n findings \n eventually, by identifying the motivations for developers to participate in a hackathon, and understanding the benefits of the use of open data, researchers will be able to elaborate the proposed model and evaluate if the contest has contributed to the decision of establish a startup and what factors affect the decision to establish a startup apply to open data developers, and if the participants of the contest agree with these factors. \n \n \n originality/value \n the paper expands the scope of open data research on entrepreneurship field, stating the need for more research to be conducted regarding the open data in entrepreneurship through hackathons. \n"
http://orkg.org/orkg/resource/R140161,Semantic similarity and machine learning with ontologies,,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Ontologies have long been employed in the life sciences to formally represent and reason over domain knowledge and they are employed in almost every major biological database. Recently, ontologies are increasingly being used to provide background knowledge in similarity-based analysis and machine learning models. The methods employed to combine ontologies and machine learning are still novel and actively being developed. We provide an overview over the methods that use ontologies to compute similarity and incorporate them in machine learning methods; in particular, we outline how semantic similarity measures and ontology embeddings can exploit the background knowledge in ontologies and how ontologies can provide constraints that improve machine learning models. The methods and experiments we describe are available as a set of executable notebooks, and we also provide a set of slides and additional resources at https://github.com/bio-ontology-research-group/machine-learning-with-ontologies.</jats:p>","abstract \n ontologies have long been employed in the life sciences to formally represent and reason over domain knowledge and they are employed in almost every major biological database. recently, ontologies are increasingly being used to provide background knowledge in similarity-based analysis and machine learning models. the methods employed to combine ontologies and machine learning are still novel and actively being developed. we provide an overview over the methods that use ontologies to compute similarity and incorporate them in machine learning methods; in particular, we outline how semantic similarity measures and ontology embeddings can exploit the background knowledge in ontologies and how ontologies can provide constraints that improve machine learning models. the methods and experiments we describe are available as a set of executable notebooks, and we also provide a set of slides and additional resources at https://github.com/bio-ontology-research-group/machine-learning-with-ontologies."
http://orkg.org/orkg/resource/R140183,Bio-joie: Joint representation learning of biological knowledge bases,,crossref,"<jats:title>Abstract</jats:title><jats:p>The widespread of Coronavirus has led to a worldwide pandemic with a high mortality rate. Currently, the knowledge accumulated from different studies about this virus is very limited. Leveraging a wide-range of biological knowledge, such as gene on-tology and protein-protein interaction (PPI) networks from other closely related species presents a vital approach to infer the molecular impact of a new species. In this paper, we propose the transferred multi-relational embedding model <jats:monospace>Bio-JOIE</jats:monospace> to capture the knowledge of gene ontology and PPI networks, which demonstrates superb capability in modeling the SARS-CoV-2-human protein interactions. <jats:monospace>Bio-JOIE</jats:monospace> jointly trains two model components. The <jats:italic>knowledge model</jats:italic> encodes the relational facts from the protein and GO domains into separated embedding spaces, using a hierarchy-aware encoding technique employed for the GO terms. On top of that, the <jats:italic>transfer model</jats:italic> learns a non-linear transformation to transfer the knowledge of PPIs and gene ontology annotations across their embedding spaces. By leveraging only structured knowledge, <jats:monospace>Bio-JOIE</jats:monospace> significantly outperforms existing state-of-the-art methods in PPI type prediction on multiple species. Furthermore, we also demonstrate the potential of leveraging the learned representations on clustering proteins with enzymatic function into enzyme commission families. Finally, we show that <jats:monospace>Bio-JOIE</jats:monospace> can accurately identify PPIs between the SARS-CoV-2 proteins and human proteins, providing valuable insights for advancing research on this new disease.</jats:p>","abstract the widespread of coronavirus has led to a worldwide pandemic with a high mortality rate. currently, the knowledge accumulated from different studies about this virus is very limited. leveraging a wide-range of biological knowledge, such as gene on-tology and protein-protein interaction (ppi) networks from other closely related species presents a vital approach to infer the molecular impact of a new species. in this paper, we propose the transferred multi-relational embedding model bio-joie to capture the knowledge of gene ontology and ppi networks, which demonstrates superb capability in modeling the sars-cov-2-human protein interactions. bio-joie jointly trains two model components. the knowledge model encodes the relational facts from the protein and go domains into separated embedding spaces, using a hierarchy-aware encoding technique employed for the go terms. on top of that, the transfer model learns a non-linear transformation to transfer the knowledge of ppis and gene ontology annotations across their embedding spaces. by leveraging only structured knowledge, bio-joie significantly outperforms existing state-of-the-art methods in ppi type prediction on multiple species. furthermore, we also demonstrate the potential of leveraging the learned representations on clustering proteins with enzymatic function into enzyme commission families. finally, we show that bio-joie can accurately identify ppis between the sars-cov-2 proteins and human proteins, providing valuable insights for advancing research on this new disease."
http://orkg.org/orkg/resource/R140238,Oral Drug Delivery Systems for Ulcerative Colitis Therapy: A Comparative Study with Microparticles and Nanoparticles,10.2174/1568009618666181016152042,crossref,"<jats:sec>\n<jats:title>Background:</jats:title>\n<jats:p> Oral administrations of microparticles (MPs) and nanoparticles (NPs) have\nbeen widely employed as therapeutic approaches for the treatment of ulcerative colitis (UC). However,\nno previous study has comparatively investigated the therapeutic efficacies of MPs and NPs.\n</jats:p></jats:sec>\n<jats:sec>\n<jats:title>Methods:</jats:title>\n<jats:p> In this study, curcumin (CUR)-loaded MPs (CUR-MPs) and CUR-loaded NPs (CUR-NPs)\nwere prepared using a single water-in-oil emulsion solvent evaporation technique. Their therapeutic\noutcomes against UC were further comparatively studied.\n</jats:p></jats:sec>\n<jats:sec>\n<jats:title>Results:</jats:title>\n<jats:p> The resultant spherical MPs and NPs exhibited slightly negative zeta-potential with average\nparticle diameters of approximately 1.7 &amp;#181;m and 270 nm, respectively. It was found that NPs exhibited\na much higher CUR release rate than MPs within the same period of investigation. In vivo experiments\ndemonstrated that oral administration of CUR-MPs and CUR-NPs reduced the symptoms\nof inflammation in a UC mouse model induced by dextran sulfate sodium. Importantly, CUR-NPs\nshowed much better therapeutic outcomes in alleviating UC compared with CUR-MPs.\n</jats:p></jats:sec>\n<jats:sec>\n<jats:title>Conclusion:</jats:title>\n<jats:p> NPs can improve the anti-inflammatory activity of CUR by enhancing the drug release\nand cellular uptake efficiency, in comparison with MPs. Thus, they could be exploited as a promising\noral drug delivery system for effective UC treatment.</jats:p>\n</jats:sec>","\n background: \n oral administrations of microparticles (mps) and nanoparticles (nps) have\nbeen widely employed as therapeutic approaches for the treatment of ulcerative colitis (uc). however,\nno previous study has comparatively investigated the therapeutic efficacies of mps and nps.\n \n \n methods: \n in this study, curcumin (cur)-loaded mps (cur-mps) and cur-loaded nps (cur-nps)\nwere prepared using a single water-in-oil emulsion solvent evaporation technique. their therapeutic\noutcomes against uc were further comparatively studied.\n \n \n results: \n the resultant spherical mps and nps exhibited slightly negative zeta-potential with average\nparticle diameters of approximately 1.7 &amp;#181;m and 270 nm, respectively. it was found that nps exhibited\na much higher cur release rate than mps within the same period of investigation. in vivo experiments\ndemonstrated that oral administration of cur-mps and cur-nps reduced the symptoms\nof inflammation in a uc mouse model induced by dextran sulfate sodium. importantly, cur-nps\nshowed much better therapeutic outcomes in alleviating uc compared with cur-mps.\n \n \n conclusion: \n nps can improve the anti-inflammatory activity of cur by enhancing the drug release\nand cellular uptake efficiency, in comparison with mps. thus, they could be exploited as a promising\noral drug delivery system for effective uc treatment. \n"
http://orkg.org/orkg/resource/R139653,"Phyllosilicate Diversity and Past Aqueous Activity Revealed at Mawrth Vallis, Mars",10.1126/science.1159699,crossref,"<jats:p>\n            Observations by the Mars Reconnaissance Orbiter/Compact Reconnaissance Imaging Spectrometer for Mars in the Mawrth Vallis region show several phyllosilicate species, indicating a wide range of past aqueous activity. Iron/magnesium (Fe/Mg)–smectite is observed in light-toned outcrops that probably formed via aqueous alteration of basalt of the ancient cratered terrain. This unit is overlain by rocks rich in hydrated silica, montmorillonite, and kaolinite that may have formed via subsequent leaching of Fe and Mg through extended aqueous events or a change in aqueous chemistry. A spectral feature attributed to an Fe\n            <jats:sup>2+</jats:sup>\n            phase is present in many locations in the Mawrth Vallis region at the transition from Fe/Mg-smectite to aluminum/silicon (Al/Si)–rich units. Fe\n            <jats:sup>2+</jats:sup>\n            -bearing materials in terrestrial sediments are typically associated with microorganisms or changes in pH or cations and could be explained here by hydrothermal activity. The stratigraphy of Fe/Mg-smectite overlain by a ferrous phase, hydrated silica, and then Al-phyllosilicates implies a complex aqueous history.\n          </jats:p>","\n observations by the mars reconnaissance orbiter/compact reconnaissance imaging spectrometer for mars in the mawrth vallis region show several phyllosilicate species, indicating a wide range of past aqueous activity. iron/magnesium (fe/mg)–smectite is observed in light-toned outcrops that probably formed via aqueous alteration of basalt of the ancient cratered terrain. this unit is overlain by rocks rich in hydrated silica, montmorillonite, and kaolinite that may have formed via subsequent leaching of fe and mg through extended aqueous events or a change in aqueous chemistry. a spectral feature attributed to an fe\n 2+ \n phase is present in many locations in the mawrth vallis region at the transition from fe/mg-smectite to aluminum/silicon (al/si)–rich units. fe\n 2+ \n -bearing materials in terrestrial sediments are typically associated with microorganisms or changes in ph or cations and could be explained here by hydrothermal activity. the stratigraphy of fe/mg-smectite overlain by a ferrous phase, hydrated silica, and then al-phyllosilicates implies a complex aqueous history.\n"
http://orkg.org/orkg/resource/R139657,Orbital Identification of Carbonate-Bearing Rocks on Mars,10.1126/science.1164759,crossref,"""<jats:p>Geochemical models for Mars predict carbonate formation during aqueous alteration. Carbonate-bearing rocks had not previously been detected on Mars' surface, but Mars Reconnaissance Orbiter mapping reveals a regional rock layer with near-infrared spectral characteristics that are consistent with the presence of magnesium carbonate in the Nili Fossae region. The carbonate is closely associated with both phyllosilicate-bearing and olivine-rich rock units and probably formed during the Noachian or early Hesperian era from the alteration of olivine by either hydrothermal fluids or near-surface water. The presence of carbonate as well as accompanying clays suggests that waters were neutral to alkaline at the time of its formation and that acidic weathering, proposed to be characteristic of Hesperian Mars, did not destroy these carbonates and thus did not dominate all aqueous environments.</jats:p>""",""" geochemical models for mars predict carbonate formation during aqueous alteration. carbonate-bearing rocks had not previously been detected on mars' surface, but mars reconnaissance orbiter mapping reveals a regional rock layer with near-infrared spectral characteristics that are consistent with the presence of magnesium carbonate in the nili fossae region. the carbonate is closely associated with both phyllosilicate-bearing and olivine-rich rock units and probably formed during the noachian or early hesperian era from the alteration of olivine by either hydrothermal fluids or near-surface water. the presence of carbonate as well as accompanying clays suggests that waters were neutral to alkaline at the time of its formation and that acidic weathering, proposed to be characteristic of hesperian mars, did not destroy these carbonates and thus did not dominate all aqueous environments. """
http://orkg.org/orkg/resource/R139698,"Accessibility, Natural User Interfaces and Interactions in Museums: The IntARSI Project",10.3390/heritage4020034,crossref,"<jats:p>In a museum context, people have specific needs in terms of physical, cognitive, and social accessibility that cannot be ignored. Therefore, we need to find a way to make art and culture accessible to them through the aid of Universal Design principles, advanced technologies, and suitable interfaces and contents. Integration of such factors is a priority of the Museums General Direction of the Italian Ministry of Cultural Heritage, within the wider strategy of museum exploitation. In accordance with this issue, the IntARSI project, publicly funded, consists of a pre-evaluation and a report of technical specifications for a new concept of museology applied to the new Museum of Civilization in Rome (MuCIV). It relates to planning of multimedia, virtual, and mixed reality applications based on the concept of “augmented” and multisensory experience, innovative tangible user interfaces, and storytelling techniques. An inclusive approach is applied, taking into account the needs and attitudes of a wide audience with different ages, cultural interests, skills, and expectations, as well as cognitive and physical abilities.</jats:p>","in a museum context, people have specific needs in terms of physical, cognitive, and social accessibility that cannot be ignored. therefore, we need to find a way to make art and culture accessible to them through the aid of universal design principles, advanced technologies, and suitable interfaces and contents. integration of such factors is a priority of the museums general direction of the italian ministry of cultural heritage, within the wider strategy of museum exploitation. in accordance with this issue, the intarsi project, publicly funded, consists of a pre-evaluation and a report of technical specifications for a new concept of museology applied to the new museum of civilization in rome (muciv). it relates to planning of multimedia, virtual, and mixed reality applications based on the concept of “augmented” and multisensory experience, innovative tangible user interfaces, and storytelling techniques. an inclusive approach is applied, taking into account the needs and attitudes of a wide audience with different ages, cultural interests, skills, and expectations, as well as cognitive and physical abilities."
http://orkg.org/orkg/resource/R139975,CULTURAL HERITAGE IN SMART CITY ENVIRONMENTS: THE UPDATE,10.5194/isprs-annals-v-2-2020-957-2020,crossref,"<jats:p>Abstract. In 2017 we published a seminal research study in the International Archives of the Photogrammetry, Remote Sensing &amp;amp; Spatial Information Sciences about how smart city tools, solutions and applications underpinned historical and cultural heritage of cities at that time (Angelidou et al. 2017). We now return to investigate the progress that has been made during the past three years, and specifically whether the weak substantiation of cultural heritage in smart city strategies that we observed in 2017 has been improved. The newest literature suggests that smart cities should capitalize on local strengths and give prominence to local culture and traditions and provides a handful of solutions to this end. However, a more thorough examination of what has been actually implemented reveals a (still) rather immature approach. The smart city cases that were selected for the purposes of this research include Tarragona (Spain), Budapest (Hungary) and Karlsruhe (Germany). For each one we collected information regarding the overarching structure of the initiative, the positioning of cultural heritage and the inclusion of heritage-related smart city applications. We then performed a comparative analysis based on a simplified version of the Digital Strategy Canvas. Our findings suggest that a rich cultural heritage and a broader strategic focus on touristic branding and promotion are key ingredients of smart city development in this domain; this is a commonality of all the investigated cities. Moreover, three different strategy architectures emerge, representing the different interplays among the smart city, cultural heritage and sustainable urban development. We conclude that a new generation of smart city initiatives is emerging, in which cultural heritage is of increasing importance. This generation tends to associate cultural heritage with social and cultural values, liveability and sustainable urban development.\n                    </jats:p>","abstract. in 2017 we published a seminal research study in the international archives of the photogrammetry, remote sensing &amp;amp; spatial information sciences about how smart city tools, solutions and applications underpinned historical and cultural heritage of cities at that time (angelidou et al. 2017). we now return to investigate the progress that has been made during the past three years, and specifically whether the weak substantiation of cultural heritage in smart city strategies that we observed in 2017 has been improved. the newest literature suggests that smart cities should capitalize on local strengths and give prominence to local culture and traditions and provides a handful of solutions to this end. however, a more thorough examination of what has been actually implemented reveals a (still) rather immature approach. the smart city cases that were selected for the purposes of this research include tarragona (spain), budapest (hungary) and karlsruhe (germany). for each one we collected information regarding the overarching structure of the initiative, the positioning of cultural heritage and the inclusion of heritage-related smart city applications. we then performed a comparative analysis based on a simplified version of the digital strategy canvas. our findings suggest that a rich cultural heritage and a broader strategic focus on touristic branding and promotion are key ingredients of smart city development in this domain; this is a commonality of all the investigated cities. moreover, three different strategy architectures emerge, representing the different interplays among the smart city, cultural heritage and sustainable urban development. we conclude that a new generation of smart city initiatives is emerging, in which cultural heritage is of increasing importance. this generation tends to associate cultural heritage with social and cultural values, liveability and sustainable urban development.\n"
http://orkg.org/orkg/resource/R139469,PENYELESAIAN MULTI-OBJECTIVE FLEXIBLE JOB SHOP SCHEDULING PROBLEM  MENGGUNAKAN  HYBRID ALGORITMA IMUN,10.22146/teknosains.22901,crossref,"<jats:p>Flexible Job shop scheduling problem (FJSSP) is one of scheduling problems with specification: there is a job to be done in a certain order, each job contains a number of operations and each operation is processed on a machine of some available machine. The purpose of this paper is to solve Multi-objective Flexible Job Shop scheduling problem with minimizing the makespan, the biggest workload and the total workload of all machines. Because of complexity these problem, a integrated approach Immune Algorithm (IA) and Simulated Annealing (SA) algorithm are combined to solve the multi-objective FJSSP. A clonal selection is a strategy for generating new antibody based on selecting the antibody for reproduction. SA is used as a local search search algorithm for enhancing the local ability with certain probability to avoid becoming trapped in a local optimum. The simulation result have proved that this hybrid immune algorithm is an efficient and effective approach to solve the multi-objective FJSSP</jats:p>","flexible job shop scheduling problem (fjssp) is one of scheduling problems with specification: there is a job to be done in a certain order, each job contains a number of operations and each operation is processed on a machine of some available machine. the purpose of this paper is to solve multi-objective flexible job shop scheduling problem with minimizing the makespan, the biggest workload and the total workload of all machines. because of complexity these problem, a integrated approach immune algorithm (ia) and simulated annealing (sa) algorithm are combined to solve the multi-objective fjssp. a clonal selection is a strategy for generating new antibody based on selecting the antibody for reproduction. sa is used as a local search search algorithm for enhancing the local ability with certain probability to avoid becoming trapped in a local optimum. the simulation result have proved that this hybrid immune algorithm is an efficient and effective approach to solve the multi-objective fjssp"
http://orkg.org/orkg/resource/R139497,Congruence between morphology-based species and Barcode Index Numbers (BINs) in Neotropical Eumaeini (Lycaenidae),10.7717/peerj.11843,crossref,"<jats:sec>\n<jats:title>Background</jats:title>\n<jats:p>With about 1,000 species in the Neotropics, the Eumaeini (Theclinae) are one of the most diverse butterfly tribes. Correct morphology-based identifications are challenging in many genera due to relatively little interspecific differences in wing patterns. Geographic infraspecific variation is sometimes more substantial than variation between species. In this paper we present a large DNA barcode dataset of South American Lycaenidae. We analyze how well DNA barcode BINs match morphologically delimited species.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title>Methods</jats:title>\n<jats:p>We compare morphology-based species identifications with the clustering of molecular operational taxonomic units (MOTUs) delimitated by the RESL algorithm in BOLD, which assigns Barcode Index Numbers (BINs). We examine intra- and interspecific divergences for genera represented by at least four morphospecies. We discuss the existence of local barcode gaps in a genus by genus analysis. We also note differences in the percentage of species with barcode gaps in groups of lowland and high mountain genera.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title>Results</jats:title>\n<jats:p>We identified 2,213 specimens and obtained 1,839 sequences of 512 species in 90 genera. Overall, the mean intraspecific divergence value of CO1 sequences was 1.20%, while the mean interspecific divergence between nearest congeneric neighbors was 4.89%, demonstrating the presence of a barcode gap. However, the gap seemed to disappear from the entire set when comparing the maximum intraspecific distance (8.40%) with the minimum interspecific distance (0.40%). Clear barcode gaps are present in many genera but absent in others. From the set of specimens that yielded COI fragment lengths of at least 650 bp, 75% of the <jats:italic>a priori</jats:italic> morphology-based identifications were unambiguously assigned to a single Barcode Index Number (BIN). However, after a taxonomic <jats:italic>a posteriori</jats:italic> review, the percentage of matched identifications rose to 85%. BIN splitting was observed for 17% of the species and BIN sharing for 9%. We found that genera that contain primarily lowland species show higher percentages of local barcode gaps and congruence between BINs and morphology than genera that contain exclusively high montane species. The divergence values to the nearest neighbors were significantly lower in high Andean species while the intra-specific divergence values were significantly lower in the lowland species. These results raise questions regarding the causes of observed low inter and high intraspecific genetic variation. We discuss incomplete lineage sorting and hybridization as most likely causes of this phenomenon, as the montane species concerned are relatively young and hybridization is probable. The release of our data set represents an essential baseline for a reference library for biological assessment studies of butterflies in mega diverse countries using modern high-throughput technologies an highlights the necessity of taxonomic revisions for various genera combining both molecular and morphological data.</jats:p>\n</jats:sec>","\n background \n with about 1,000 species in the neotropics, the eumaeini (theclinae) are one of the most diverse butterfly tribes. correct morphology-based identifications are challenging in many genera due to relatively little interspecific differences in wing patterns. geographic infraspecific variation is sometimes more substantial than variation between species. in this paper we present a large dna barcode dataset of south american lycaenidae. we analyze how well dna barcode bins match morphologically delimited species. \n \n \n methods \n we compare morphology-based species identifications with the clustering of molecular operational taxonomic units (motus) delimitated by the resl algorithm in bold, which assigns barcode index numbers (bins). we examine intra- and interspecific divergences for genera represented by at least four morphospecies. we discuss the existence of local barcode gaps in a genus by genus analysis. we also note differences in the percentage of species with barcode gaps in groups of lowland and high mountain genera. \n \n \n results \n we identified 2,213 specimens and obtained 1,839 sequences of 512 species in 90 genera. overall, the mean intraspecific divergence value of co1 sequences was 1.20%, while the mean interspecific divergence between nearest congeneric neighbors was 4.89%, demonstrating the presence of a barcode gap. however, the gap seemed to disappear from the entire set when comparing the maximum intraspecific distance (8.40%) with the minimum interspecific distance (0.40%). clear barcode gaps are present in many genera but absent in others. from the set of specimens that yielded coi fragment lengths of at least 650 bp, 75% of the a priori morphology-based identifications were unambiguously assigned to a single barcode index number (bin). however, after a taxonomic a posteriori review, the percentage of matched identifications rose to 85%. bin splitting was observed for 17% of the species and bin sharing for 9%. we found that genera that contain primarily lowland species show higher percentages of local barcode gaps and congruence between bins and morphology than genera that contain exclusively high montane species. the divergence values to the nearest neighbors were significantly lower in high andean species while the intra-specific divergence values were significantly lower in the lowland species. these results raise questions regarding the causes of observed low inter and high intraspecific genetic variation. we discuss incomplete lineage sorting and hybridization as most likely causes of this phenomenon, as the montane species concerned are relatively young and hybridization is probable. the release of our data set represents an essential baseline for a reference library for biological assessment studies of butterflies in mega diverse countries using modern high-throughput technologies an highlights the necessity of taxonomic revisions for various genera combining both molecular and morphological data. \n"
http://orkg.org/orkg/resource/R140317,Costume in the dance archive: Towards a records-centred ethics of care,10.1386/scp_00012_1,crossref,"<jats:p>Focusing on the archival records of the production and performance of <jats:italic>Dance in Trees and Church</jats:italic> by the Swedish independent dance group Rubicon, this article conceptualizes a records-oriented costume ethics. Theorizations of costume as a co-creative agent of performance are brought into the dance archive to highlight the productivity of paying attention to costume in the making of performance history. Addressing recent developments within archival studies, a <jats:italic>feminist ethics of care</jats:italic> and <jats:italic>radical empathy</jats:italic> is employed, which is the capability to empathically engage with others, even if it can be difficult, as a means of exploring how a records-centred costume ethics can be conceptualized for the dance archive. The exploration resulted in two ethical stances useful for better attending to costume-bodies in the dance archive: (1) <jats:italic>caring for costume-body relations</jats:italic> in the dance archive means that a conventional, so-called static understanding of records as neutral carriers of facts is replaced by a more inclusive, expanding and infinite process. By moving across time and space, and with a caring attitude finding and exploring fragments from various, sometimes contradictory production processes, one can help scattered and poorly represented dance and costume histories to emerge and contribute to the formation of identity and memory. (2) The use of <jats:italic>bodily empathy</jats:italic> with records can respectfully bring together the understanding of costume in performance as inseparable from the performer’s body with dance as an art form that explicitly uses the <jats:italic>dancing</jats:italic> costume-body as an expressive tool. It is argued that bodily empathy with records in the dance archive helps one access bodily holisms that create possibilities for exploring the potential of art to critically expose and render strange ideological systems and normativities.</jats:p>","focusing on the archival records of the production and performance of dance in trees and church by the swedish independent dance group rubicon, this article conceptualizes a records-oriented costume ethics. theorizations of costume as a co-creative agent of performance are brought into the dance archive to highlight the productivity of paying attention to costume in the making of performance history. addressing recent developments within archival studies, a feminist ethics of care and radical empathy is employed, which is the capability to empathically engage with others, even if it can be difficult, as a means of exploring how a records-centred costume ethics can be conceptualized for the dance archive. the exploration resulted in two ethical stances useful for better attending to costume-bodies in the dance archive: (1) caring for costume-body relations in the dance archive means that a conventional, so-called static understanding of records as neutral carriers of facts is replaced by a more inclusive, expanding and infinite process. by moving across time and space, and with a caring attitude finding and exploring fragments from various, sometimes contradictory production processes, one can help scattered and poorly represented dance and costume histories to emerge and contribute to the formation of identity and memory. (2) the use of bodily empathy with records can respectfully bring together the understanding of costume in performance as inseparable from the performer’s body with dance as an art form that explicitly uses the dancing costume-body as an expressive tool. it is argued that bodily empathy with records in the dance archive helps one access bodily holisms that create possibilities for exploring the potential of art to critically expose and render strange ideological systems and normativities."
http://orkg.org/orkg/resource/R139538,High resolution DNA barcode library for European butterflies reveals continental patterns of mitochondrial genetic diversity,10.1038/s42003-021-01834-7,crossref,"<jats:title>Abstract</jats:title><jats:p>The study of global biodiversity will greatly benefit from access to comprehensive DNA barcode libraries at continental scale, but such datasets are still very rare. Here, we assemble the first high-resolution reference library for European butterflies that provides 97% taxon coverage (459 species) and 22,306 <jats:italic>COI</jats:italic> sequences. We estimate that we captured 62% of the total haplotype diversity and show that most species possess a few very common haplotypes and many rare ones. Specimens in the dataset have an average 95.3% probability of being correctly identified. Mitochondrial diversity displayed elevated haplotype richness in southern European refugia, establishing the generality of this key biogeographic pattern for an entire taxonomic group. Fifteen percent of the species are involved in barcode sharing, but two thirds of these cases may reflect the need for further taxonomic research. This dataset provides a unique resource for conservation and for studying evolutionary processes, cryptic species, phylogeography, and ecology.</jats:p>","abstract the study of global biodiversity will greatly benefit from access to comprehensive dna barcode libraries at continental scale, but such datasets are still very rare. here, we assemble the first high-resolution reference library for european butterflies that provides 97% taxon coverage (459 species) and 22,306 coi sequences. we estimate that we captured 62% of the total haplotype diversity and show that most species possess a few very common haplotypes and many rare ones. specimens in the dataset have an average 95.3% probability of being correctly identified. mitochondrial diversity displayed elevated haplotype richness in southern european refugia, establishing the generality of this key biogeographic pattern for an entire taxonomic group. fifteen percent of the species are involved in barcode sharing, but two thirds of these cases may reflect the need for further taxonomic research. this dataset provides a unique resource for conservation and for studying evolutionary processes, cryptic species, phylogeography, and ecology."
http://orkg.org/orkg/resource/R139761,The Story of the Markham Car Collection: A Cross-Platform Panoramic Tour of Contested Heritage,10.1177/1550190619832381,crossref,"<jats:p> In this article, we share our experiences of using digital technologies and various media to present historical narratives of a museum object collection aiming to provide an engaging experience on multiple platforms. Based on P. Joseph’s article, Dawson presented multiple interpretations and historical views of the Markham car collection across various platforms using multimedia resources. Through her creative production, she explored how to use cylindrical panoramas and rich media to offer new ways of telling the controversial story of the contested heritage of a museum’s veteran and vintage car collection. The production’s usability was investigated involving five experts before it was published online and the general users’ experience was investigated. In this article, we present an important component of findings which indicates that virtual panorama tours featuring multimedia elements could be successful in attracting new audiences and that using this type of storytelling technique can be effective in the museum sector. The storyteller panorama tour presented here may stimulate GLAM (galleries, libraries, archives, and museums) professionals to think of new approaches, implement new strategies or services to engage their audiences more effectively. The research may ameliorate the education of future professionals as well. </jats:p>","in this article, we share our experiences of using digital technologies and various media to present historical narratives of a museum object collection aiming to provide an engaging experience on multiple platforms. based on p. joseph’s article, dawson presented multiple interpretations and historical views of the markham car collection across various platforms using multimedia resources. through her creative production, she explored how to use cylindrical panoramas and rich media to offer new ways of telling the controversial story of the contested heritage of a museum’s veteran and vintage car collection. the production’s usability was investigated involving five experts before it was published online and the general users’ experience was investigated. in this article, we present an important component of findings which indicates that virtual panorama tours featuring multimedia elements could be successful in attracting new audiences and that using this type of storytelling technique can be effective in the museum sector. the storyteller panorama tour presented here may stimulate glam (galleries, libraries, archives, and museums) professionals to think of new approaches, implement new strategies or services to engage their audiences more effectively. the research may ameliorate the education of future professionals as well."
http://orkg.org/orkg/resource/R139596,"An Integrated Approach for Improving Brand Consistency of Web Content: Modeling, Analysis, and Recommendation",10.1145/3450445,crossref,"<jats:p>\n            A consumer-dependent (business-to-consumer) organization tends to present itself as possessing a set of human qualities, which is termed the\n            <jats:italic>brand personality</jats:italic>\n            of the company. The perception is impressed upon the consumer through the content, be it in the form of advertisement, blogs, or magazines, produced by the organization. A consistent brand will generate trust and retain customers over time as they develop an affinity toward regularity and common patterns. However, maintaining a consistent messaging tone for a brand has become more challenging with the virtual explosion in the amount of content that needs to be authored and pushed to the Internet to maintain an edge in the era of digital marketing. To understand the depth of the problem, we collect around 300K web page content from around 650 companies. We develop trait-specific classification models by considering the linguistic features of the content. The classifier automatically identifies the web articles that are not consistent with the mission and vision of a company and further helps us to discover the conditions under which the consistency cannot be maintained. To address the brand inconsistency issue, we then develop a sentence ranking system that outputs the top three sentences that need to be changed for making a web article more consistent with the company’s brand personality.\n          </jats:p>","\n a consumer-dependent (business-to-consumer) organization tends to present itself as possessing a set of human qualities, which is termed the\n brand personality \n of the company. the perception is impressed upon the consumer through the content, be it in the form of advertisement, blogs, or magazines, produced by the organization. a consistent brand will generate trust and retain customers over time as they develop an affinity toward regularity and common patterns. however, maintaining a consistent messaging tone for a brand has become more challenging with the virtual explosion in the amount of content that needs to be authored and pushed to the internet to maintain an edge in the era of digital marketing. to understand the depth of the problem, we collect around 300k web page content from around 650 companies. we develop trait-specific classification models by considering the linguistic features of the content. the classifier automatically identifies the web articles that are not consistent with the mission and vision of a company and further helps us to discover the conditions under which the consistency cannot be maintained. to address the brand inconsistency issue, we then develop a sentence ranking system that outputs the top three sentences that need to be changed for making a web article more consistent with the company’s brand personality.\n"
http://orkg.org/orkg/resource/R139522,Multisystem Optimization for an Integrated Production Scheduling with Resource Saving Problem in Textile Printing and Dyeing,10.1155/2020/8853735,crossref,"<jats:p>Resource saving has become an integral aspect of manufacturing in industry 4.0. This paper proposes a multisystem optimization (MSO) algorithm, inspired by implicit parallelism of heuristic methods, to solve an integrated production scheduling with resource saving problem in textile printing and dyeing. First, a real-world integrated production scheduling with resource saving is formulated as a multisystem optimization problem. Then, the MSO algorithm is proposed to solve multisystem optimization problems that consist of several coupled subsystems, and each of the subsystems may contain multiple objectives and multiple constraints. The proposed MSO algorithm is composed of within-subsystem evolution and cross-subsystem migration operators, and the former is to optimize each subsystem by excellent evolution operators and the later is to complete information sharing between multiple subsystems, to accelerate the global optimization of the whole system. Performance is tested on a set of multisystem benchmark functions and compared with improved NSGA-II and multiobjective multifactorial evolutionary algorithm (MO-MFEA). Simulation results show that the MSO algorithm is better than compared algorithms for the benchmark functions studied in this paper. Finally, the MSO algorithm is successfully applied to the proposed integrated production scheduling with resource saving problem, and the results show that MSO is a promising algorithm for the studied problem.</jats:p>","resource saving has become an integral aspect of manufacturing in industry 4.0. this paper proposes a multisystem optimization (mso) algorithm, inspired by implicit parallelism of heuristic methods, to solve an integrated production scheduling with resource saving problem in textile printing and dyeing. first, a real-world integrated production scheduling with resource saving is formulated as a multisystem optimization problem. then, the mso algorithm is proposed to solve multisystem optimization problems that consist of several coupled subsystems, and each of the subsystems may contain multiple objectives and multiple constraints. the proposed mso algorithm is composed of within-subsystem evolution and cross-subsystem migration operators, and the former is to optimize each subsystem by excellent evolution operators and the later is to complete information sharing between multiple subsystems, to accelerate the global optimization of the whole system. performance is tested on a set of multisystem benchmark functions and compared with improved nsga-ii and multiobjective multifactorial evolutionary algorithm (mo-mfea). simulation results show that the mso algorithm is better than compared algorithms for the benchmark functions studied in this paper. finally, the mso algorithm is successfully applied to the proposed integrated production scheduling with resource saving problem, and the results show that mso is a promising algorithm for the studied problem."
http://orkg.org/orkg/resource/R139290,Engineered Hierarchical CuO Nanoleaves Based Electrochemical Nonenzymatic Biosensor for Glucose Detection,10.1149/1945-7111/abd515,crossref,"<jats:p>In this study, we synthesized hierarchical CuO nanoleaves in large-quantity via the hydrothermal method. We employed different techniques to characterize the morphological, structural, optical properties of the as-prepared hierarchical CuO nanoleaves sample. An electrochemical based nonenzymatic glucose biosensor was fabricated using engineered hierarchical CuO nanoleaves. The electrochemical behavior of fabricated biosensor towards glucose was analyzed with cyclic voltammetry (CV) and amperometry (i–t) techniques. Owing to the high electroactive surface area, hierarchical CuO nanoleaves based nonenzymatic biosensor electrode shows enhanced electrochemical catalytic behavior for glucose electro-oxidation in 100 mM sodium hydroxide (NaOH) electrolyte. The nonenzymatic biosensor displays a high sensitivity (1467.32 <jats:italic>μ</jats:italic>A/(mM cm<jats:sup>2</jats:sup>)), linear range (0.005–5.89 mM), and detection limit of 12 nM (S/N = 3). Moreover, biosensor displayed good selectivity, reproducibility, repeatability, and stability at room temperature over three-week storage period. Further, as-fabricated nonenzymatic glucose biosensors were employed for practical applications in human serum sample measurements. The obtained data were compared to the commercial biosensor, which demonstrates the practical usability of nonenzymatic glucose biosensors in real sample analysis.</jats:p>","in this study, we synthesized hierarchical cuo nanoleaves in large-quantity via the hydrothermal method. we employed different techniques to characterize the morphological, structural, optical properties of the as-prepared hierarchical cuo nanoleaves sample. an electrochemical based nonenzymatic glucose biosensor was fabricated using engineered hierarchical cuo nanoleaves. the electrochemical behavior of fabricated biosensor towards glucose was analyzed with cyclic voltammetry (cv) and amperometry (i–t) techniques. owing to the high electroactive surface area, hierarchical cuo nanoleaves based nonenzymatic biosensor electrode shows enhanced electrochemical catalytic behavior for glucose electro-oxidation in 100 mm sodium hydroxide (naoh) electrolyte. the nonenzymatic biosensor displays a high sensitivity (1467.32 μ a/(mm cm 2 )), linear range (0.005–5.89 mm), and detection limit of 12 nm (s/n = 3). moreover, biosensor displayed good selectivity, reproducibility, repeatability, and stability at room temperature over three-week storage period. further, as-fabricated nonenzymatic glucose biosensors were employed for practical applications in human serum sample measurements. the obtained data were compared to the commercial biosensor, which demonstrates the practical usability of nonenzymatic glucose biosensors in real sample analysis."
http://orkg.org/orkg/resource/R139283,Glucose Biosensor Based on Disposable Activated Carbon Electrodes Modified with Platinum Nanoparticles Electrodeposited on Poly(Azure A),10.3390/s20164489,crossref,"<jats:p>Herein, a novel electrochemical glucose biosensor based on glucose oxidase (GOx) immobilized on a surface containing platinum nanoparticles (PtNPs) electrodeposited on poly(Azure A) (PAA) previously electropolymerized on activated screen-printed carbon electrodes (GOx-PtNPs-PAA-aSPCEs) is reported. The resulting electrochemical biosensor was validated towards glucose oxidation in real samples and further electrochemical measurement associated with the generated H2O2. The electrochemical biosensor showed an excellent sensitivity (42.7 μA mM−1 cm−2), limit of detection (7.6 μM), linear range (20 μM–2.3 mM), and good selectivity towards glucose determination. Furthermore, and most importantly, the detection of glucose was performed at a low potential (0.2 V vs. Ag). The high performance of the electrochemical biosensor was explained through surface exploration using field emission SEM, XPS, and impedance measurements. The electrochemical biosensor was successfully applied to glucose quantification in several real samples (commercial juices and a plant cell culture medium), exhibiting a high accuracy when compared with a classical spectrophotometric method. This electrochemical biosensor can be easily prepared and opens up a good alternative in the development of new sensitive glucose sensors.</jats:p>","herein, a novel electrochemical glucose biosensor based on glucose oxidase (gox) immobilized on a surface containing platinum nanoparticles (ptnps) electrodeposited on poly(azure a) (paa) previously electropolymerized on activated screen-printed carbon electrodes (gox-ptnps-paa-aspces) is reported. the resulting electrochemical biosensor was validated towards glucose oxidation in real samples and further electrochemical measurement associated with the generated h2o2. the electrochemical biosensor showed an excellent sensitivity (42.7 μa mm−1 cm−2), limit of detection (7.6 μm), linear range (20 μm–2.3 mm), and good selectivity towards glucose determination. furthermore, and most importantly, the detection of glucose was performed at a low potential (0.2 v vs. ag). the high performance of the electrochemical biosensor was explained through surface exploration using field emission sem, xps, and impedance measurements. the electrochemical biosensor was successfully applied to glucose quantification in several real samples (commercial juices and a plant cell culture medium), exhibiting a high accuracy when compared with a classical spectrophotometric method. this electrochemical biosensor can be easily prepared and opens up a good alternative in the development of new sensitive glucose sensors."
http://orkg.org/orkg/resource/R139340,Cu2O nanorods modified by reduced graphene oxide for NH3 sensing at room temperature,10.1039/c4ta06024e,crossref,"<p>In this work, Cu<sub>2</sub>O nanorods modified by reduced graphene oxide (rGO) were produced <italic>via</italic> a two-step synthesis method.</p>","in this work, cu 2 o nanorods modified by reduced graphene oxide (rgo) were produced via a two-step synthesis method."
http://orkg.org/orkg/resource/R139396,Synthesis and gas sensing properties of molybdenum oxide modified tungsten oxide microstructures for ppb-level hydrogen sulphide detection,10.1039/c7ra03864j,crossref,<p>Flower-like MoO<sub>3</sub>@WO<sub>3</sub> composite microstructures with abundant nanosheets were obtained by the hydrothermal and impregnation method and then used for high performance sensing of H<sub>2</sub>S.</p>,flower-like moo 3 @wo 3 composite microstructures with abundant nanosheets were obtained by the hydrothermal and impregnation method and then used for high performance sensing of h 2 s.
http://orkg.org/orkg/resource/R139386,Molybdenum trioxide nanopaper as a dual gas sensor for detecting trimethylamine and hydrogen sulfide,10.1039/c6ra26280e,crossref,"<p>The dual function of a free-standing, flexible, and semi-transparent MoO<sub>3</sub> nanopaper sensor to detect TMA and H<sub>2</sub>S in a highly selective manner.</p>","the dual function of a free-standing, flexible, and semi-transparent moo 3 nanopaper sensor to detect tma and h 2 s in a highly selective manner."
http://orkg.org/orkg/resource/R139493,Technological Innovation in Biomass Energy for the Sustainable Growth of Textile Industry,10.3390/su11020528,crossref,"<jats:p>The growing increase in world energy consumption favors the search for renewable energy sources. One of the existing options for the growth and sustainable development of such types of sources is through the use of biomass as an input. The employment of biomass as solid fuel is widely studied and is no longer a novelty nor presents any difficulty from the technical point of view. It presents, however, logistic obstacles, thus not allowing their direct dissemination in every organization that is willing to replace it as an energy source. Use of biomass can be rewarding due to the fact that it can bring significant economic gains attained due to the steadiness of the biomass price in Portugal. However, the price may rise as predicted in the coming years, although it will be a gradual rising. The main goal of this study was to analyze whether biomass in the case of the Portuguese textile industry can be a viable alternative that separates the possibility of sustainable growth from the lack of competitiveness due to high energy costs. The study showed that biomass can be a reliable, sustainable and permanent energy alternative to more traditional energy sources such as propane gas, naphtha and natural gas for the textile industry. At the same time, it can bring savings of 35% in energy costs related to steam generation. Also, with new technology systems related to the Internet of Things, a better on-time aware of needs, energy production and logistic chain information will be possible.</jats:p>","the growing increase in world energy consumption favors the search for renewable energy sources. one of the existing options for the growth and sustainable development of such types of sources is through the use of biomass as an input. the employment of biomass as solid fuel is widely studied and is no longer a novelty nor presents any difficulty from the technical point of view. it presents, however, logistic obstacles, thus not allowing their direct dissemination in every organization that is willing to replace it as an energy source. use of biomass can be rewarding due to the fact that it can bring significant economic gains attained due to the steadiness of the biomass price in portugal. however, the price may rise as predicted in the coming years, although it will be a gradual rising. the main goal of this study was to analyze whether biomass in the case of the portuguese textile industry can be a viable alternative that separates the possibility of sustainable growth from the lack of competitiveness due to high energy costs. the study showed that biomass can be a reliable, sustainable and permanent energy alternative to more traditional energy sources such as propane gas, naphtha and natural gas for the textile industry. at the same time, it can bring savings of 35% in energy costs related to steam generation. also, with new technology systems related to the internet of things, a better on-time aware of needs, energy production and logistic chain information will be possible."
http://orkg.org/orkg/resource/R139028,Natural Language Processing of Social Media as Screening for Suicide Risk,10.1177/1178222618792860,crossref,"<jats:p> Suicide is among the 10 most common causes of death, as assessed by the World Health Organization. For every death by suicide, an estimated 138 people’s lives are meaningfully affected, and almost any other statistic around suicide deaths is equally alarming. The pervasiveness of social media—and the near-ubiquity of mobile devices used to access social media networks—offers new types of data for understanding the behavior of those who (attempt to) take their own lives and suggests new possibilities for preventive intervention. We demonstrate the feasibility of using social media data to detect those at risk for suicide. Specifically, we use natural language processing and machine learning (specifically deep learning) techniques to detect quantifiable signals around suicide attempts, and describe designs for an automated system for estimating suicide risk, usable by those without specialized mental health training (eg, a primary care doctor). We also discuss the ethical use of such technology and examine privacy implications. Currently, this technology is only used for intervention for individuals who have “opted in” for the analysis and intervention, but the technology enables scalable screening for suicide risk, potentially identifying many people who are at risk preventively and prior to any engagement with a health care system. This raises a significant cultural question about the trade-off between privacy and prevention—we have potentially life-saving technology that is currently reaching only a fraction of the possible people at risk because of respect for their privacy. Is the current trade-off between privacy and prevention the right one? </jats:p>","suicide is among the 10 most common causes of death, as assessed by the world health organization. for every death by suicide, an estimated 138 people’s lives are meaningfully affected, and almost any other statistic around suicide deaths is equally alarming. the pervasiveness of social media—and the near-ubiquity of mobile devices used to access social media networks—offers new types of data for understanding the behavior of those who (attempt to) take their own lives and suggests new possibilities for preventive intervention. we demonstrate the feasibility of using social media data to detect those at risk for suicide. specifically, we use natural language processing and machine learning (specifically deep learning) techniques to detect quantifiable signals around suicide attempts, and describe designs for an automated system for estimating suicide risk, usable by those without specialized mental health training (eg, a primary care doctor). we also discuss the ethical use of such technology and examine privacy implications. currently, this technology is only used for intervention for individuals who have “opted in” for the analysis and intervention, but the technology enables scalable screening for suicide risk, potentially identifying many people who are at risk preventively and prior to any engagement with a health care system. this raises a significant cultural question about the trade-off between privacy and prevention—we have potentially life-saving technology that is currently reaching only a fraction of the possible people at risk because of respect for their privacy. is the current trade-off between privacy and prevention the right one?"
http://orkg.org/orkg/resource/R139463,"Energy management and optimization: case study of a textile plant in Istanbul, Turkey",10.1108/wje-08-2016-046,crossref,"<jats:sec>\n<jats:title content-type=""abstract-subheading"">Purpose</jats:title>\n<jats:p>This paper aims to present the results of energy management and optimization studies in one Turkish textile factory. In a case study of a print and dye factory in Istanbul, the authors identified energy-sensitive processes and proposed energy management applications.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>\n<jats:p>Appropriate energy management methods have been implemented in the factory, and the results were examined in terms of energy efficiency and cost reduction.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Findings</jats:title>\n<jats:p>By applying the methods for fuel distribution optimization, the authors demonstrated that energy costs could be decreased by approximately.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>\n<jats:p>Energy management is a vital issue for industries particularly in developing countries such as Turkey. Turkey is an energy poor country and imports more than half of its energy to satisfy its increasing domestic demands. An important share of these demands stems from the presence of a strong textile industry that operates throughout the country.</jats:p>\n</jats:sec>","\n purpose \n this paper aims to present the results of energy management and optimization studies in one turkish textile factory. in a case study of a print and dye factory in istanbul, the authors identified energy-sensitive processes and proposed energy management applications. \n \n \n design/methodology/approach \n appropriate energy management methods have been implemented in the factory, and the results were examined in terms of energy efficiency and cost reduction. \n \n \n findings \n by applying the methods for fuel distribution optimization, the authors demonstrated that energy costs could be decreased by approximately. \n \n \n originality/value \n energy management is a vital issue for industries particularly in developing countries such as turkey. turkey is an energy poor country and imports more than half of its energy to satisfy its increasing domestic demands. an important share of these demands stems from the presence of a strong textile industry that operates throughout the country. \n"
http://orkg.org/orkg/resource/R139115,Chemical kinetics in an atmospheric pressure helium plasma containing humidity,10.1039/c8cp02473a,crossref,<p>Investigating the formation and kinetics of O and OH in a He–H<sub>2</sub>O plasma jet using absorption spectroscopy and 0D modelling.</p>,investigating the formation and kinetics of o and oh in a he–h 2 o plasma jet using absorption spectroscopy and 0d modelling.
http://orkg.org/orkg/resource/R139135,2D spatially resolved O atom density profiles in an atmospheric pressure plasma jet: from the active plasma volume to the effluent,10.1088/1361-6463/ac09b9,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Two-dimensional spatially resolved absolute atomic oxygen densities are measured within an atmospheric pressure micro plasma jet and in its effluent. The plasma is operated in helium with an admixture of 0.5% of oxygen at 13.56\u2009MHz and with a power of 1\u2009W. Absolute atomic oxygen densities are obtained using two photon absorption laser induced fluorescence spectroscopy. The results are interpreted based on measurements of the electron dynamics by phase resolved optical emission spectroscopy in combination with a simple model that balances the production of atomic oxygen with its losses due to chemical reactions and diffusion. Within the discharge, the atomic oxygen density builds up with a rise time of 600 <jats:italic>µ</jats:italic>s along the gas flow and reaches a plateau of 8\u2009× 10<jats:sup>15</jats:sup>\u2009cm<jats:sup>−3</jats:sup>. In the effluent, the density decays exponentially with a decay time of 180 <jats:italic>µ</jats:italic>s (corresponding to a decay length of 3\u2009mm at a gas flow of 1.0\u2009slm). It is found that both, the species formation behavior and the maximum distance between the jet nozzle and substrates for possible oxygen treatments of surfaces can be controlled by adjusting the gas flow.</jats:p>","abstract \n two-dimensional spatially resolved absolute atomic oxygen densities are measured within an atmospheric pressure micro plasma jet and in its effluent. the plasma is operated in helium with an admixture of 0.5% of oxygen at 13.56\u2009mhz and with a power of 1\u2009w. absolute atomic oxygen densities are obtained using two photon absorption laser induced fluorescence spectroscopy. the results are interpreted based on measurements of the electron dynamics by phase resolved optical emission spectroscopy in combination with a simple model that balances the production of atomic oxygen with its losses due to chemical reactions and diffusion. within the discharge, the atomic oxygen density builds up with a rise time of 600 µ s along the gas flow and reaches a plateau of 8\u2009× 10 15 \u2009cm −3 . in the effluent, the density decays exponentially with a decay time of 180 µ s (corresponding to a decay length of 3\u2009mm at a gas flow of 1.0\u2009slm). it is found that both, the species formation behavior and the maximum distance between the jet nozzle and substrates for possible oxygen treatments of surfaces can be controlled by adjusting the gas flow."
http://orkg.org/orkg/resource/R138729,fMRIPrep: a robust preprocessing pipeline for functional MRI,10.1038/s41592-018-0235-4,crossref,"<jats:p>Preprocessing of functional MRI (fMRI) involves numerous steps to clean and standardize data before statistical analysis. Generally, researchers create <jats:italic>ad hoc</jats:italic> preprocessing workflows for each new dataset, building upon a large inventory of tools available for each step. The complexity of these workflows has snowballed with rapid advances in MR data acquisition and image processing techniques. We introduce <jats:italic>fMRIPrep</jats:italic>, an analysis-agnostic tool that addresses the challenge of robust and reproducible preprocessing for task-based and resting fMRI data. <jats:italic>FMRIPrep</jats:italic> automatically adapts a best-in-breed workflow to the idiosyncrasies of virtually any dataset, ensuring high-quality preprocessing with no manual intervention. By introducing visual assessment checkpoints into an iterative integration framework for software-testing, we show that <jats:italic>fMRIPrep</jats:italic> robustly produces high-quality results on a diverse fMRI data collection comprising participants from 54 different studies in the OpenfMRI repository. We review the distinctive features of <jats:italic>fMRIPrep</jats:italic> in a qualitative comparison to other preprocessing workflows. We demonstrate that <jats:italic>fMRIPrep</jats:italic> achieves higher spatial accuracy as it introduces less uncontrolled spatial smoothness than commonly used preprocessing tools. <jats:italic>FMRIPrep</jats:italic> has the potential to transform fMRI research by equipping neuroscientists with a high-quality, robust, easy-to-use and transparent preprocessing workflow which can help ensure the validity of inference and the interpretability of their results.</jats:p>","preprocessing of functional mri (fmri) involves numerous steps to clean and standardize data before statistical analysis. generally, researchers create ad hoc preprocessing workflows for each new dataset, building upon a large inventory of tools available for each step. the complexity of these workflows has snowballed with rapid advances in mr data acquisition and image processing techniques. we introduce fmriprep , an analysis-agnostic tool that addresses the challenge of robust and reproducible preprocessing for task-based and resting fmri data. fmriprep automatically adapts a best-in-breed workflow to the idiosyncrasies of virtually any dataset, ensuring high-quality preprocessing with no manual intervention. by introducing visual assessment checkpoints into an iterative integration framework for software-testing, we show that fmriprep robustly produces high-quality results on a diverse fmri data collection comprising participants from 54 different studies in the openfmri repository. we review the distinctive features of fmriprep in a qualitative comparison to other preprocessing workflows. we demonstrate that fmriprep achieves higher spatial accuracy as it introduces less uncontrolled spatial smoothness than commonly used preprocessing tools. fmriprep has the potential to transform fmri research by equipping neuroscientists with a high-quality, robust, easy-to-use and transparent preprocessing workflow which can help ensure the validity of inference and the interpretability of their results."
http://orkg.org/orkg/resource/R138920,Functionalization of Silver Nanoparticles Loaded with Paclitaxel-induced A549 Cells Apoptosis Through ROS-Mediated Signaling Pathways,10.2174/1568026619666191019102219,crossref,"<jats:sec>\n<jats:title>Background:</jats:title>\n<jats:p>Paclitaxel (PTX) is one of the most important and effective anticancer drugs for\nthe treatment of human cancer. However, its low solubility and severe adverse effects limited clinical\nuse. To overcome this limitation, nanotechnology has been used to overcome tumors due to its excellent\nantimicrobial activity.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title>Objective:</jats:title>\n<jats:p>This study was to demonstrate the anticancer properties of functionalization silver nanoparticles\nloaded with paclitaxel (Ag@PTX) induced A549 cells apoptosis through ROS-mediated signaling\npathways.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title>Methods:</jats:title>\n<jats:p> The Ag@PTX nanoparticles were charged with a zeta potential of about -17 mv and characterized\naround 2 nm with a narrow size distribution.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title>Results:</jats:title>\n<jats:p>Ag@PTX significantly decreased the viability of A549 cells and possessed selectivity between\ncancer and normal cells. Ag@PTX induced A549 cells apoptosis was confirmed by nuclear condensation,\nDNA fragmentation, and activation of caspase-3. Furthermore, Ag@PTX enhanced the anti-cancer\nactivity of A549 cells through ROS-mediated p53 and AKT signalling pathways. Finally, in a xenograft\nnude mice model, Ag@PTX suppressed the growth of tumors.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title>Conclusion:</jats:title>\n<jats:p>Our findings suggest that Ag@PTX may be a candidate as a chemopreventive agent and\ncould be a highly efficient way to achieve anticancer synergism for human cancers.</jats:p>\n</jats:sec>","\n background: \n paclitaxel (ptx) is one of the most important and effective anticancer drugs for\nthe treatment of human cancer. however, its low solubility and severe adverse effects limited clinical\nuse. to overcome this limitation, nanotechnology has been used to overcome tumors due to its excellent\nantimicrobial activity. \n \n \n objective: \n this study was to demonstrate the anticancer properties of functionalization silver nanoparticles\nloaded with paclitaxel (ag@ptx) induced a549 cells apoptosis through ros-mediated signaling\npathways. \n \n \n methods: \n the ag@ptx nanoparticles were charged with a zeta potential of about -17 mv and characterized\naround 2 nm with a narrow size distribution. \n \n \n results: \n ag@ptx significantly decreased the viability of a549 cells and possessed selectivity between\ncancer and normal cells. ag@ptx induced a549 cells apoptosis was confirmed by nuclear condensation,\ndna fragmentation, and activation of caspase-3. furthermore, ag@ptx enhanced the anti-cancer\nactivity of a549 cells through ros-mediated p53 and akt signalling pathways. finally, in a xenograft\nnude mice model, ag@ptx suppressed the growth of tumors. \n \n \n conclusion: \n our findings suggest that ag@ptx may be a candidate as a chemopreventive agent and\ncould be a highly efficient way to achieve anticancer synergism for human cancers. \n"
http://orkg.org/orkg/resource/R139004,Characterisation of mental health conditions in social media using Informed Deep Learning,10.1038/srep45141,crossref,"<jats:title>Abstract</jats:title>\n          <jats:p>The number of people affected by mental illness is on the increase and with it the burden on health and social care use, as well as the loss of both productivity and quality-adjusted life-years. Natural language processing of electronic health records is increasingly used to study mental health conditions and risk behaviours on a large scale. However, narrative notes written by clinicians do not capture first-hand the patients’ own experiences, and only record cross-sectional, professional impressions at the point of care. Social media platforms have become a source of ‘in the moment’ daily exchange, with topics including well-being and mental health. In this study, we analysed posts from the social media platform Reddit and developed classifiers to recognise and classify posts related to mental illness according to 11 disorder themes. Using a neural network and deep learning approach, we could automatically recognise mental illness-related posts in our balenced dataset with an accuracy of 91.08% and select the correct theme with a weighted average accuracy of 71.37%. We believe that these results are a first step in developing methods to characterise large amounts of user-generated content that could support content curation and targeted interventions.</jats:p>","abstract \n the number of people affected by mental illness is on the increase and with it the burden on health and social care use, as well as the loss of both productivity and quality-adjusted life-years. natural language processing of electronic health records is increasingly used to study mental health conditions and risk behaviours on a large scale. however, narrative notes written by clinicians do not capture first-hand the patients’ own experiences, and only record cross-sectional, professional impressions at the point of care. social media platforms have become a source of ‘in the moment’ daily exchange, with topics including well-being and mental health. in this study, we analysed posts from the social media platform reddit and developed classifiers to recognise and classify posts related to mental illness according to 11 disorder themes. using a neural network and deep learning approach, we could automatically recognise mental illness-related posts in our balenced dataset with an accuracy of 91.08% and select the correct theme with a weighted average accuracy of 71.37%. we believe that these results are a first step in developing methods to characterise large amounts of user-generated content that could support content curation and targeted interventions."
http://orkg.org/orkg/resource/R139853,SMART CITIES AND HERITAGE CONSERVATION: DEVELOPING A SMARTHERITAGE AGENDA FOR SUSTAINABLE INCLUSIVE COMMUNITIES,10.26687/archnet-ijar.v11i3.1398,crossref,"<jats:p>This paper discusses the potential of current advancements in Information Communication Technologies (ICT) for cultural heritage preservation, valorization and management within contemporary cities. The paper highlights the potential of virtual environments to assess the impacts of heritage policies on urban development. It does so by discussing the implications of virtual globes and crowdsourcing to support the participatory valuation and management of cultural heritage assets. To this purpose, a review of available valuation techniques is here presented together with a discussion on how these techniques might be coupled with ICT tools to promote inclusive governance.\xa0</jats:p>","this paper discusses the potential of current advancements in information communication technologies (ict) for cultural heritage preservation, valorization and management within contemporary cities. the paper highlights the potential of virtual environments to assess the impacts of heritage policies on urban development. it does so by discussing the implications of virtual globes and crowdsourcing to support the participatory valuation and management of cultural heritage assets. to this purpose, a review of available valuation techniques is here presented together with a discussion on how these techniques might be coupled with ict tools to promote inclusive governance.\xa0"
http://orkg.org/orkg/resource/R139736,Public History and Contested Heritage: Archival Memories of the Bombing of Italy,10.5130/phrj.v27i0.7088,crossref,"<jats:p>This article presents a case study of a collaborative public history project between participants in two countries, the United Kingdom and Italy. Its subject matter is the bombing war in Europe, 1939-1945, which is remembered and commemorated in very different ways in these two countries: the sensitivities involved thus constitute not only a case of public history conducted at the national level but also one involving contested heritage. An account of the ways in which public history has developed in the UK and Italy is presented. This is followed by an explanation of how the bombing war has been remembered in each country. In the UK, veterans of RAF Bomber Command have long felt a sense of neglect, largely because the deliberate targeting of civilians has not fitted comfortably into the dominant victor narrative. In Italy, recollections of being bombed have remained profoundly dissonant within the received liberation discourse. The International Bomber Command Centre Digital Archive (or Archive) is then described as a case study that employs a public history approach, focusing on various aspects of its inclusive ethos, intended to preserve multiple perspectives. The Italian component of the project is highlighted, problematising the digitisation of contested heritage within the broader context of twentieth-century history. Reflections on the use of digital archiving practices and working in partnership are offered, as well as a brief account of user analytics of the Archive through its first eighteen months online.</jats:p>","this article presents a case study of a collaborative public history project between participants in two countries, the united kingdom and italy. its subject matter is the bombing war in europe, 1939-1945, which is remembered and commemorated in very different ways in these two countries: the sensitivities involved thus constitute not only a case of public history conducted at the national level but also one involving contested heritage. an account of the ways in which public history has developed in the uk and italy is presented. this is followed by an explanation of how the bombing war has been remembered in each country. in the uk, veterans of raf bomber command have long felt a sense of neglect, largely because the deliberate targeting of civilians has not fitted comfortably into the dominant victor narrative. in italy, recollections of being bombed have remained profoundly dissonant within the received liberation discourse. the international bomber command centre digital archive (or archive) is then described as a case study that employs a public history approach, focusing on various aspects of its inclusive ethos, intended to preserve multiple perspectives. the italian component of the project is highlighted, problematising the digitisation of contested heritage within the broader context of twentieth-century history. reflections on the use of digital archiving practices and working in partnership are offered, as well as a brief account of user analytics of the archive through its first eighteen months online."
http://orkg.org/orkg/resource/R138635,Smart City Ontologies: Improving the effectiveness of smart city applications,10.18063/jsc.2015.01.001,crossref,"<jats:p>This paper addresses the problem of low impact of smart city applications observed in the fields of energy and transport, which constitute high-priority domains for the development of smart cities. However, these are not the only fields where the impact of smart cities has been limited. The paper provides an explanation for the low impact of various individual applications of smart cities and discusses ways of improving their effectiveness. We argue that the impact of applications depends primarily on their ontology, and secondarily on smart technology and programming features. Consequently, we start by creating an overall ontology for the smart city, defining the building blocks of this ontology with respect to the most cited definitions of smart cities, and structuring this ontology with the Protégé 5.0 editor, defining entities, class hierarchy, object properties, and data type properties. We then analyze how the ontologies of a sample of smart city applications fit into the overall Smart City Ontology, the consistency between digital spaces, knowledge processes, city domains targeted by the applications, and the types of innovation that determine their impact. In conclusion, we underline the relationships between innovation and ontology, and discuss how we can improve the effectiveness of smart city applications, combining expert and user-driven ontology design with the integration and or-chestration of applications over platforms and larger city entities such as neighborhoods, districts, clusters, and sectors of city activities.</jats:p>","this paper addresses the problem of low impact of smart city applications observed in the fields of energy and transport, which constitute high-priority domains for the development of smart cities. however, these are not the only fields where the impact of smart cities has been limited. the paper provides an explanation for the low impact of various individual applications of smart cities and discusses ways of improving their effectiveness. we argue that the impact of applications depends primarily on their ontology, and secondarily on smart technology and programming features. consequently, we start by creating an overall ontology for the smart city, defining the building blocks of this ontology with respect to the most cited definitions of smart cities, and structuring this ontology with the protégé 5.0 editor, defining entities, class hierarchy, object properties, and data type properties. we then analyze how the ontologies of a sample of smart city applications fit into the overall smart city ontology, the consistency between digital spaces, knowledge processes, city domains targeted by the applications, and the types of innovation that determine their impact. in conclusion, we underline the relationships between innovation and ontology, and discuss how we can improve the effectiveness of smart city applications, combining expert and user-driven ontology design with the integration and or-chestration of applications over platforms and larger city entities such as neighborhoods, districts, clusters, and sectors of city activities."
http://orkg.org/orkg/resource/R138421,Grand challenges in model-driven engineering: an analysis of the state of the research,10.1007/s10270-019-00773-6,crossref,"<jats:title>Abstract</jats:title><jats:p>In 2017 and 2018, two events were held—in Marburg, Germany, and San Vigilio di Marebbe, Italy, respectively—focusing on an analysis of the state of research, state of practice, and state of the art in model-driven engineering (MDE). The events brought together experts from industry, academia, and the open-source community to assess what has changed in research in MDE over the last 10\xa0years, what challenges remain, and what new challenges have arisen. This article reports on the results of those meetings, and presents a set of <jats:italic>grand challenges</jats:italic> that emerged from discussions and synthesis. These challenges could lead to research initiatives for the community going forward.\n</jats:p>","abstract in 2017 and 2018, two events were held—in marburg, germany, and san vigilio di marebbe, italy, respectively—focusing on an analysis of the state of research, state of practice, and state of the art in model-driven engineering (mde). the events brought together experts from industry, academia, and the open-source community to assess what has changed in research in mde over the last 10\xa0years, what challenges remain, and what new challenges have arisen. this article reports on the results of those meetings, and presents a set of grand challenges that emerged from discussions and synthesis. these challenges could lead to research initiatives for the community going forward.\n"
http://orkg.org/orkg/resource/R138422,Grand challenges in model-driven engineering: an analysis of the state of the research,10.1007/s10270-019-00773-6,crossref,"<jats:title>Abstract</jats:title><jats:p>In 2017 and 2018, two events were held—in Marburg, Germany, and San Vigilio di Marebbe, Italy, respectively—focusing on an analysis of the state of research, state of practice, and state of the art in model-driven engineering (MDE). The events brought together experts from industry, academia, and the open-source community to assess what has changed in research in MDE over the last 10\xa0years, what challenges remain, and what new challenges have arisen. This article reports on the results of those meetings, and presents a set of <jats:italic>grand challenges</jats:italic> that emerged from discussions and synthesis. These challenges could lead to research initiatives for the community going forward.\n</jats:p>","abstract in 2017 and 2018, two events were held—in marburg, germany, and san vigilio di marebbe, italy, respectively—focusing on an analysis of the state of research, state of practice, and state of the art in model-driven engineering (mde). the events brought together experts from industry, academia, and the open-source community to assess what has changed in research in mde over the last 10\xa0years, what challenges remain, and what new challenges have arisen. this article reports on the results of those meetings, and presents a set of grand challenges that emerged from discussions and synthesis. these challenges could lead to research initiatives for the community going forward.\n"
http://orkg.org/orkg/resource/R138459,Transforming XML documents to OWL ontologies: A survey,10.1177/0165551514565972,crossref,"<jats:p> The aims of XML data conversion to ontologies are the indexing, integration and enrichment of existing ontologies with knowledge acquired from these sources. The contribution of this paper consists in providing a classification of the approaches used for the conversion of XML documents into OWL ontologies. This classification underlines the usage profile of each conversion method, providing a clear description of the advantages and drawbacks belonging to each method. Hence, this paper focuses on two main processes, which are ontology enrichment and ontology population using XML data. Ontology enrichment is related to the schema of the ontology (TBox), and ontology population is related to an individual (Abox). In addition, the ontologies described in these methods are based on formal languages of the Semantic Web such as OWL (Ontology Web Language) or RDF (Resource Description Framework). These languages are formal because the semantics are formally defined and take advantage of the Description Logics. In contrast, XML data sources are without formal semantics. The XML language is used to store, export and share data between processes able to process the specific data structure. However, even if the semantics is not explicitly expressed, data structure contains the universe of discourse by using a qualified vocabulary regarding a consensual agreement. In order to formalize this semantics, the OWL language provides rich logical constraints. Therefore, these logical constraints are evolved in the transformation of XML documents into OWL documents, allowing the enrichment and the population of the target ontology. To design such a transformation, the current research field establishes connections between OWL constructs (classes, predicates, simple or complex data types, etc.) and XML constructs (elements, attributes, element lists, etc.). Two different approaches for the transformation process are exposed. The instance approaches are based on XML documents without any schema associated. The validation approaches are based on the XML schema and document validated by the associated schema. The second approaches benefit from the schema definition to provide automated transformations with logic constraints. Both approaches are discussed in the text. </jats:p>","the aims of xml data conversion to ontologies are the indexing, integration and enrichment of existing ontologies with knowledge acquired from these sources. the contribution of this paper consists in providing a classification of the approaches used for the conversion of xml documents into owl ontologies. this classification underlines the usage profile of each conversion method, providing a clear description of the advantages and drawbacks belonging to each method. hence, this paper focuses on two main processes, which are ontology enrichment and ontology population using xml data. ontology enrichment is related to the schema of the ontology (tbox), and ontology population is related to an individual (abox). in addition, the ontologies described in these methods are based on formal languages of the semantic web such as owl (ontology web language) or rdf (resource description framework). these languages are formal because the semantics are formally defined and take advantage of the description logics. in contrast, xml data sources are without formal semantics. the xml language is used to store, export and share data between processes able to process the specific data structure. however, even if the semantics is not explicitly expressed, data structure contains the universe of discourse by using a qualified vocabulary regarding a consensual agreement. in order to formalize this semantics, the owl language provides rich logical constraints. therefore, these logical constraints are evolved in the transformation of xml documents into owl documents, allowing the enrichment and the population of the target ontology. to design such a transformation, the current research field establishes connections between owl constructs (classes, predicates, simple or complex data types, etc.) and xml constructs (elements, attributes, element lists, etc.). two different approaches for the transformation process are exposed. the instance approaches are based on xml documents without any schema associated. the validation approaches are based on the xml schema and document validated by the associated schema. the second approaches benefit from the schema definition to provide automated transformations with logic constraints. both approaches are discussed in the text."
http://orkg.org/orkg/resource/R138070,Grand challenges in model-driven engineering: an analysis of the state of the research,10.1007/s10270-019-00773-6,crossref,"<jats:title>Abstract</jats:title><jats:p>In 2017 and 2018, two events were held—in Marburg, Germany, and San Vigilio di Marebbe, Italy, respectively—focusing on an analysis of the state of research, state of practice, and state of the art in model-driven engineering (MDE). The events brought together experts from industry, academia, and the open-source community to assess what has changed in research in MDE over the last 10\xa0years, what challenges remain, and what new challenges have arisen. This article reports on the results of those meetings, and presents a set of <jats:italic>grand challenges</jats:italic> that emerged from discussions and synthesis. These challenges could lead to research initiatives for the community going forward.\n</jats:p>","abstract in 2017 and 2018, two events were held—in marburg, germany, and san vigilio di marebbe, italy, respectively—focusing on an analysis of the state of research, state of practice, and state of the art in model-driven engineering (mde). the events brought together experts from industry, academia, and the open-source community to assess what has changed in research in mde over the last 10\xa0years, what challenges remain, and what new challenges have arisen. this article reports on the results of those meetings, and presents a set of grand challenges that emerged from discussions and synthesis. these challenges could lead to research initiatives for the community going forward.\n"
http://orkg.org/orkg/resource/R138825,Comprehensive functional genomic resource and integrative model for the human brain,10.1126/science.aat8464,crossref,"<jats:sec>\n            <jats:title>INTRODUCTION</jats:title>\n            <jats:p>Strong genetic associations have been found for a number of psychiatric disorders. However, understanding the underlying molecular mechanisms remains challenging.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>RATIONALE</jats:title>\n            <jats:p>To address this challenge, the PsychENCODE Consortium has developed a comprehensive online resource and integrative models for the functional genomics of the human brain.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>RESULTS</jats:title>\n            <jats:p>The base of the pyramidal resource is the datasets generated by PsychENCODE, including bulk transcriptome, chromatin, genotype, and Hi-C datasets and single-cell transcriptomic data from ~32,000 cells for major brain regions. We have merged these with data from Genotype-Tissue Expression (GTEx), ENCODE, Roadmap Epigenomics, and single-cell analyses. Via uniform processing, we created a harmonized resource, allowing us to survey functional genomics data on the brain over a sample size of 1866 individuals.</jats:p>\n            <jats:p>From this uniformly processed dataset, we created derived data products. These include lists of brain-expressed genes, coexpression modules, and single-cell expression profiles for many brain cell types; ~79,000 brain-active enhancers with associated Hi-C loops and topologically associating domains; and ~2.5 million expression quantitative-trait loci (QTLs) comprising ~238,000 linkage-disequilibrium–independent single-nucleotide polymorphisms and of other types of QTLs associated with splice isoforms, cell fractions, and chromatin activity. By using these, we found that &gt;88% of the cross-population variation in brain gene expression can be accounted for by cell fraction changes. Furthermore, a number of disorders and aging are associated with changes in cell-type proportions. The derived data also enable comparison between the brain and other tissues. In particular, by using spectral analyses, we found that the brain has distinct expression and epigenetic patterns, including a greater extent of noncoding transcription than other tissues.</jats:p>\n            <jats:p>The top level of the resource consists of integrative networks for regulation and machine-learning models for disease prediction. The networks include a full gene regulatory network (GRN) for the brain, linking transcription factors, enhancers, and target genes from merging of the QTLs, generalized element-activity correlations, and Hi-C data. By using this network, we link disease genes to genome-wide association study (GWAS) variants for psychiatric disorders. For schizophrenia, we linked 321 genes to the 142 reported GWAS loci. We then embedded the regulatory network into a deep-learning model to predict psychiatric phenotypes from genotype and expression. Our model gives a ~6-fold improvement in prediction over additive polygenic risk scores. Moreover, it achieves a ~3-fold improvement over additive models, even when the gene expression data are imputed, highlighting the value of having just a small amount of transcriptome data for disease prediction. Lastly, it highlights key genes and pathways associated with disorder prediction, including immunological, synaptic, and metabolic pathways, recapitulating de novo results from more targeted analyses.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>CONCLUSION</jats:title>\n            <jats:p>Our resource and integrative analyses have uncovered genomic elements and networks in the brain, which in turn have provided insight into the molecular mechanisms underlying psychiatric disorders. Our deep-learning model improves disease risk prediction over traditional approaches and can be extended with additional data types (e.g., microRNA and neuroimaging).</jats:p>\n            <jats:fig fig-type=""figure"" orientation=""portrait"" position=""float"">\n              <jats:caption>\n                <jats:title>A comprehensive functional genomic resource for the adult human brain.</jats:title>\n                <jats:p>The resource forms a three-layer pyramid. The bottom layer includes sequencing datasets for traits, such as schizophrenia. The middle layer represents derived datasets, including functional genomic elements and QTLs. The top layer contains integrated models, which link genotypes to phenotypes. DSPN, Deep Structured Phenotype Network; PC1 and PC2, principal components 1 and 2; ref, reference; alt, alternate; H3K27ac, histone H3 acetylation at lysine 27.</jats:p>\n              </jats:caption>\n              <jats:graphic xmlns:xlink=""http://www.w3.org/1999/xlink"" orientation=""portrait"" position=""float"" xlink:href=""362_aat8464_fa.jpeg"" />\n            </jats:fig>\n          </jats:sec>","\n introduction \n strong genetic associations have been found for a number of psychiatric disorders. however, understanding the underlying molecular mechanisms remains challenging. \n \n \n rationale \n to address this challenge, the psychencode consortium has developed a comprehensive online resource and integrative models for the functional genomics of the human brain. \n \n \n results \n the base of the pyramidal resource is the datasets generated by psychencode, including bulk transcriptome, chromatin, genotype, and hi-c datasets and single-cell transcriptomic data from ~32,000 cells for major brain regions. we have merged these with data from genotype-tissue expression (gtex), encode, roadmap epigenomics, and single-cell analyses. via uniform processing, we created a harmonized resource, allowing us to survey functional genomics data on the brain over a sample size of 1866 individuals. \n from this uniformly processed dataset, we created derived data products. these include lists of brain-expressed genes, coexpression modules, and single-cell expression profiles for many brain cell types; ~79,000 brain-active enhancers with associated hi-c loops and topologically associating domains; and ~2.5 million expression quantitative-trait loci (qtls) comprising ~238,000 linkage-disequilibrium–independent single-nucleotide polymorphisms and of other types of qtls associated with splice isoforms, cell fractions, and chromatin activity. by using these, we found that &gt;88% of the cross-population variation in brain gene expression can be accounted for by cell fraction changes. furthermore, a number of disorders and aging are associated with changes in cell-type proportions. the derived data also enable comparison between the brain and other tissues. in particular, by using spectral analyses, we found that the brain has distinct expression and epigenetic patterns, including a greater extent of noncoding transcription than other tissues. \n the top level of the resource consists of integrative networks for regulation and machine-learning models for disease prediction. the networks include a full gene regulatory network (grn) for the brain, linking transcription factors, enhancers, and target genes from merging of the qtls, generalized element-activity correlations, and hi-c data. by using this network, we link disease genes to genome-wide association study (gwas) variants for psychiatric disorders. for schizophrenia, we linked 321 genes to the 142 reported gwas loci. we then embedded the regulatory network into a deep-learning model to predict psychiatric phenotypes from genotype and expression. our model gives a ~6-fold improvement in prediction over additive polygenic risk scores. moreover, it achieves a ~3-fold improvement over additive models, even when the gene expression data are imputed, highlighting the value of having just a small amount of transcriptome data for disease prediction. lastly, it highlights key genes and pathways associated with disorder prediction, including immunological, synaptic, and metabolic pathways, recapitulating de novo results from more targeted analyses. \n \n \n conclusion \n our resource and integrative analyses have uncovered genomic elements and networks in the brain, which in turn have provided insight into the molecular mechanisms underlying psychiatric disorders. our deep-learning model improves disease risk prediction over traditional approaches and can be extended with additional data types (e.g., microrna and neuroimaging). \n \n \n a comprehensive functional genomic resource for the adult human brain. \n the resource forms a three-layer pyramid. the bottom layer includes sequencing datasets for traits, such as schizophrenia. the middle layer represents derived datasets, including functional genomic elements and qtls. the top layer contains integrated models, which link genotypes to phenotypes. dspn, deep structured phenotype network; pc1 and pc2, principal components 1 and 2; ref, reference; alt, alternate; h3k27ac, histone h3 acetylation at lysine 27. \n \n \n \n"
http://orkg.org/orkg/resource/R138430,Selective photocatalytic C–C bond cleavage under ambient conditions with earth abundant vanadium complexes,10.1039/c5sc02923f,crossref,<p>Chemoselective aliphatic carbon–carbon bond activation photocatalyzed by vanadium oxo complexes under ambient conditions and visible light.</p>,chemoselective aliphatic carbon–carbon bond activation photocatalyzed by vanadium oxo complexes under ambient conditions and visible light.
http://orkg.org/orkg/resource/R137698,Low dietary fiber promotes enteric expansion of a Crohn's disease-associated pathobiont independent of obesity.,10.1152/ajpendo.00134.2021,crossref,"<jats:p> It is commonly thought that obesity or a high-fat diet alters pathogenic bacteria and promotes inflammatory gut diseases. We found that lower dietary fiber is a key factor that expands a gut pathobiont linked to Crohn’s disease, independent of obesity status in mice. </jats:p>","it is commonly thought that obesity or a high-fat diet alters pathogenic bacteria and promotes inflammatory gut diseases. we found that lower dietary fiber is a key factor that expands a gut pathobiont linked to crohn’s disease, independent of obesity status in mice."
http://orkg.org/orkg/resource/R137441,Photons and particles emitted from cold atmospheric-pressure plasma inactivate bacteria and biomolecules independently and synergistically,10.1098/rsif.2013.0591,crossref,"<jats:p>Cold atmospheric-pressure plasmas are currently in use in medicine as surgical tools and are being evaluated for new applications, including wound treatment and cosmetic care. The disinfecting properties of plasmas are of particular interest, given the threat of antibiotic resistance to modern medicine. Plasma effluents comprise (V)UV photons and various reactive particles, such as accelerated ions and radicals, that modify biomolecules; however, a full understanding of the molecular mechanisms that underlie plasma-based disinfection has been lacking. Here, we investigate the antibacterial mechanisms of plasma, including the separate, additive and synergistic effects of plasma-generated (V)UV photons and particles at the cellular and molecular levels. Using scanning electron microscopy, we show that plasma-emitted particles cause physical damage to the cell envelope, whereas UV radiation does not. The lethal effects of the plasma effluent exceed the zone of physical damage. We demonstrate that both plasma-generated particles and (V)UV photons modify DNA nucleobases. The particles also induce breaks in the DNA backbone. The plasma effluent, and particularly the plasma-generated particles, also rapidly inactivate proteins in the cellular milieu. Thus, in addition to physical damage to the cellular envelope, modifications to DNA and proteins contribute to the bactericidal properties of cold atmospheric-pressure plasma.</jats:p>","cold atmospheric-pressure plasmas are currently in use in medicine as surgical tools and are being evaluated for new applications, including wound treatment and cosmetic care. the disinfecting properties of plasmas are of particular interest, given the threat of antibiotic resistance to modern medicine. plasma effluents comprise (v)uv photons and various reactive particles, such as accelerated ions and radicals, that modify biomolecules; however, a full understanding of the molecular mechanisms that underlie plasma-based disinfection has been lacking. here, we investigate the antibacterial mechanisms of plasma, including the separate, additive and synergistic effects of plasma-generated (v)uv photons and particles at the cellular and molecular levels. using scanning electron microscopy, we show that plasma-emitted particles cause physical damage to the cell envelope, whereas uv radiation does not. the lethal effects of the plasma effluent exceed the zone of physical damage. we demonstrate that both plasma-generated particles and (v)uv photons modify dna nucleobases. the particles also induce breaks in the dna backbone. the plasma effluent, and particularly the plasma-generated particles, also rapidly inactivate proteins in the cellular milieu. thus, in addition to physical damage to the cellular envelope, modifications to dna and proteins contribute to the bactericidal properties of cold atmospheric-pressure plasma."
http://orkg.org/orkg/resource/R137143,Optimal Decomposition and Reconstruction of Discrete Wavelet Transformation for Short-Term Load Forecasting,10.3390/en12244654,crossref,"<jats:p>To achieve high accuracy in prediction, a load forecasting algorithm must model various consumer behaviors in response to weather conditions or special events. Different triggers will have various effects on different customers and lead to difficulties in constructing an adequate prediction model due to non-stationary and uncertain characteristics in load variations. This paper proposes an open-ended model of short-term load forecasting (STLF) which has general prediction ability to capture the non-linear relationship between the load demand and the exogenous inputs. The prediction method uses the whale optimization algorithm, discrete wavelet transform, and multiple linear regression model (WOA-DWT-MLR model) to predict both system load and aggregated load of power consumers. WOA is used to optimize the best combination of detail and approximation signals from DWT to construct an optimal MLR model. The proposed model is validated with both the system-side data set and the end-user data set for Independent System Operator-New England (ISO-NE) and smart meter load data, respectively, based on Mean Absolute Percentage Error (MAPE) criterion. The results demonstrate that the proposed method achieves lower prediction error than existing methods and can have consistent prediction of non-stationary load conditions that exist in both test systems. The proposed method is, thus, beneficial to use in the energy management system.</jats:p>","to achieve high accuracy in prediction, a load forecasting algorithm must model various consumer behaviors in response to weather conditions or special events. different triggers will have various effects on different customers and lead to difficulties in constructing an adequate prediction model due to non-stationary and uncertain characteristics in load variations. this paper proposes an open-ended model of short-term load forecasting (stlf) which has general prediction ability to capture the non-linear relationship between the load demand and the exogenous inputs. the prediction method uses the whale optimization algorithm, discrete wavelet transform, and multiple linear regression model (woa-dwt-mlr model) to predict both system load and aggregated load of power consumers. woa is used to optimize the best combination of detail and approximation signals from dwt to construct an optimal mlr model. the proposed model is validated with both the system-side data set and the end-user data set for independent system operator-new england (iso-ne) and smart meter load data, respectively, based on mean absolute percentage error (mape) criterion. the results demonstrate that the proposed method achieves lower prediction error than existing methods and can have consistent prediction of non-stationary load conditions that exist in both test systems. the proposed method is, thus, beneficial to use in the energy management system."
http://orkg.org/orkg/resource/R137068,Visible-light photoredox-catalyzed C–O bond cleavage of diaryl ethers by acridinium photocatalysts at room temperature,10.1038/s41467-020-19944-x,crossref,"<jats:title>Abstract</jats:title><jats:p>Cleavage of C–O bonds in lignin can afford the renewable aryl sources for fine chemicals. However, the high bond energies of these C–O bonds, especially the 4-O-5-type diaryl ether C–O bonds (~314\u2009kJ/mol) make the cleavage very challenging. Here, we report visible-light photoredox-catalyzed C–O bond cleavage of diaryl ethers by an acidolysis with an aryl carboxylic acid and a following one-pot hydrolysis. Two molecules of phenols are obtained from one molecule of diaryl ether at room temperature. The aryl carboxylic acid used for the acidolysis can be recovered. The key to success of the acidolysis is merging visible-light photoredox catalysis using an acridinium photocatalyst and Lewis acid catalysis using Cu(TMHD)<jats:sub>2</jats:sub>. Preliminary mechanistic studies indicate that the catalytic cycle occurs via a rare selective electrophilic attack of the generated aryl carboxylic radical on the electron-rich aryl ring of the diphenyl ether. This transformation is applied to a gram-scale reaction and the model of 4-O-5 lignin linkages.</jats:p>","abstract cleavage of c–o bonds in lignin can afford the renewable aryl sources for fine chemicals. however, the high bond energies of these c–o bonds, especially the 4-o-5-type diaryl ether c–o bonds (~314\u2009kj/mol) make the cleavage very challenging. here, we report visible-light photoredox-catalyzed c–o bond cleavage of diaryl ethers by an acidolysis with an aryl carboxylic acid and a following one-pot hydrolysis. two molecules of phenols are obtained from one molecule of diaryl ether at room temperature. the aryl carboxylic acid used for the acidolysis can be recovered. the key to success of the acidolysis is merging visible-light photoredox catalysis using an acridinium photocatalyst and lewis acid catalysis using cu(tmhd) 2 . preliminary mechanistic studies indicate that the catalytic cycle occurs via a rare selective electrophilic attack of the generated aryl carboxylic radical on the electron-rich aryl ring of the diphenyl ether. this transformation is applied to a gram-scale reaction and the model of 4-o-5 lignin linkages."
http://orkg.org/orkg/resource/R136019,Ontology-based E-learning Content Recommender System for Addressing the Pure Cold-start Problem,10.1145/3429251,crossref,"<jats:p>E-learning recommender systems are gaining significance nowadays due to its ability to enhance the learning experience by providing tailor-made services based on learner preferences. A Personalized Learning Environment (PLE) that automatically adapts to learner characteristics such as learning styles and knowledge level can recommend appropriate learning resources that would favor the learning process and improve learning outcomes. The pure cold-start problem is a relevant issue in PLEs, which arises due to the lack of prior information about the new learner in the PLE to create appropriate recommendations. This article introduces a semantic framework based on ontology to address the pure cold-start problem in content recommenders. The ontology encapsulates the domain knowledge about the learners as well as Learning Objects (LOs). The semantic model that we built has been experimented with different combinations of the key learner parameters such as learning style, knowledge level, and background knowledge. The proposed framework utilizes these parameters to build natural learner groups from the learner ontology using SPARQL queries. The ontology holds 480 learners’ data, 468 annotated learning objects with 5,600 learner ratings. A multivariate k-means clustering algorithm, an unsupervised machine learning technique for grouping similar data, is used to evaluate the learner similarity computation accuracy. The learner satisfaction achieved with the proposed model is measured based on the ratings given by the 40 participants of the experiments. From the evaluation perspective, it is evident that 79% of the learners are satisfied with the recommendations generated by the proposed model in pure cold-start condition.</jats:p>","e-learning recommender systems are gaining significance nowadays due to its ability to enhance the learning experience by providing tailor-made services based on learner preferences. a personalized learning environment (ple) that automatically adapts to learner characteristics such as learning styles and knowledge level can recommend appropriate learning resources that would favor the learning process and improve learning outcomes. the pure cold-start problem is a relevant issue in ples, which arises due to the lack of prior information about the new learner in the ple to create appropriate recommendations. this article introduces a semantic framework based on ontology to address the pure cold-start problem in content recommenders. the ontology encapsulates the domain knowledge about the learners as well as learning objects (los). the semantic model that we built has been experimented with different combinations of the key learner parameters such as learning style, knowledge level, and background knowledge. the proposed framework utilizes these parameters to build natural learner groups from the learner ontology using sparql queries. the ontology holds 480 learners’ data, 468 annotated learning objects with 5,600 learner ratings. a multivariate k-means clustering algorithm, an unsupervised machine learning technique for grouping similar data, is used to evaluate the learner similarity computation accuracy. the learner satisfaction achieved with the proposed model is measured based on the ratings given by the 40 participants of the experiments. from the evaluation perspective, it is evident that 79% of the learners are satisfied with the recommendations generated by the proposed model in pure cold-start condition."
http://orkg.org/orkg/resource/R136067,EduCOR: An Educational and Career-Oriented Recommendation Ontology,,crossref,"<jats:title>Abstract</jats:title><jats:p>With the increased dependence on online learning platforms and educational resource repositories, a unified representation of digital learning resources becomes essential to support a dynamic and multi-source learning experience. We introduce the EduCOR ontology, an educational, career-oriented ontology that provides a foundation for representing online learning resources for personalised learning systems. The ontology is designed to enable learning material repositories to offer learning path recommendations, which correspond to the user’s learning goals and preferences, academic and psychological parameters, and labour-market skills. We present the multiple patterns that compose the EduCOR ontology, highlighting its cross-domain applicability and integrability with other ontologies. A demonstration of the proposed ontology on the real-life learning platform eDoer is discussed as a use case. We evaluate the EduCOR ontology using both gold standard and task-based approaches. The comparison of EduCOR to three gold schemata, and its application in two use-cases, shows its coverage and adaptability to multiple OER repositories, which allows generating user-centric and labour-market oriented recommendations.</jats:p><jats:p><jats:bold>Resource</jats:bold>: <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://tibonto.github.io/educor/"">https://tibonto.github.io/educor/</jats:ext-link>.</jats:p>","abstract with the increased dependence on online learning platforms and educational resource repositories, a unified representation of digital learning resources becomes essential to support a dynamic and multi-source learning experience. we introduce the educor ontology, an educational, career-oriented ontology that provides a foundation for representing online learning resources for personalised learning systems. the ontology is designed to enable learning material repositories to offer learning path recommendations, which correspond to the user’s learning goals and preferences, academic and psychological parameters, and labour-market skills. we present the multiple patterns that compose the educor ontology, highlighting its cross-domain applicability and integrability with other ontologies. a demonstration of the proposed ontology on the real-life learning platform edoer is discussed as a use case. we evaluate the educor ontology using both gold standard and task-based approaches. the comparison of educor to three gold schemata, and its application in two use-cases, shows its coverage and adaptability to multiple oer repositories, which allows generating user-centric and labour-market oriented recommendations. resource : https://tibonto.github.io/educor/ ."
http://orkg.org/orkg/resource/R135948,Application of ALD-Al2O3 in CdS/CdTe Thin-Film Solar Cells,10.3390/en12061123,crossref,"<jats:p>The application of thinner cadmium sulfide (CdS) window layer is a feasible approach to improve the performance of cadmium telluride (CdTe) thin film solar cells. However, the reduction of compactness and continuity of thinner CdS always deteriorates the device performance. In this work, transparent Al2O3 films with different thicknesses, deposited by using atomic layer deposition (ALD), were utilized as buffer layers between the front electrode transparent conductive oxide (TCO) and CdS layers to solve this problem, and then, thin-film solar cells with a structure of TCO/Al2O3/CdS/CdTe/BC/Ni were fabricated. The characteristics of the ALD-Al2O3 films were studied by UV–visible transmittance spectrum, Raman spectroscopy, and atomic force microscopy (AFM). The light and dark J–V performances of solar cells were also measured by specific instrumentations. The transmittance measurement conducted on the TCO/Al2O3 films verified that the transmittance of TCO/Al2O3 were comparable to that of single TCO layer, meaning that no extra absorption loss occurred when Al2O3 buffer layers were introduced into cells. Furthermore, due to the advantages of the ALD method, the ALD-Al2O3 buffer layers formed an extremely continuous and uniform coverage on the substrates to effectively fill and block the tiny leakage channels in CdS/CdTe polycrystalline films and improve the characteristics of the interface between TCO and CdS. However, as the thickness of alumina increased, the negative effects of cells were gradually exposed, especially the increase of the series resistance (Rs) and the more serious “roll-over” phenomenon. Finally, the cell conversion efficiency (η) of more than 13.0% accompanied by optimized uniformity performances was successfully achieved corresponding to the 10 nm thick ALD-Al2O3 thin film.</jats:p>","the application of thinner cadmium sulfide (cds) window layer is a feasible approach to improve the performance of cadmium telluride (cdte) thin film solar cells. however, the reduction of compactness and continuity of thinner cds always deteriorates the device performance. in this work, transparent al2o3 films with different thicknesses, deposited by using atomic layer deposition (ald), were utilized as buffer layers between the front electrode transparent conductive oxide (tco) and cds layers to solve this problem, and then, thin-film solar cells with a structure of tco/al2o3/cds/cdte/bc/ni were fabricated. the characteristics of the ald-al2o3 films were studied by uv–visible transmittance spectrum, raman spectroscopy, and atomic force microscopy (afm). the light and dark j–v performances of solar cells were also measured by specific instrumentations. the transmittance measurement conducted on the tco/al2o3 films verified that the transmittance of tco/al2o3 were comparable to that of single tco layer, meaning that no extra absorption loss occurred when al2o3 buffer layers were introduced into cells. furthermore, due to the advantages of the ald method, the ald-al2o3 buffer layers formed an extremely continuous and uniform coverage on the substrates to effectively fill and block the tiny leakage channels in cds/cdte polycrystalline films and improve the characteristics of the interface between tco and cds. however, as the thickness of alumina increased, the negative effects of cells were gradually exposed, especially the increase of the series resistance (rs) and the more serious “roll-over” phenomenon. finally, the cell conversion efficiency (η) of more than 13.0% accompanied by optimized uniformity performances was successfully achieved corresponding to the 10 nm thick ald-al2o3 thin film."
http://orkg.org/orkg/resource/R137073,"Selective, Nickel-Catalyzed Hydrogenolysis of Aryl Ethers",10.1126/science.1200437,crossref,<jats:p>A catalyst that cleaves aryl-oxygen bonds but not carbon-carbon bonds may help improve lignin processing.</jats:p>,a catalyst that cleaves aryl-oxygen bonds but not carbon-carbon bonds may help improve lignin processing.
http://orkg.org/orkg/resource/R135750,Characterization and comparison of poorly known moth communities through DNA barcoding in two Afrotropical environments in Gabon,10.1139/gen-2018-0063,crossref,"<jats:p> Biodiversity research in tropical ecosystems—popularized as the most biodiverse habitats on Earth—often neglects invertebrates, yet invertebrates represent the bulk of local species richness. Insect communities in particular remain strongly impeded by both Linnaean and Wallacean shortfalls, and identifying species often remains a formidable challenge inhibiting the use of these organisms as indicators for ecological and conservation studies. Here we use DNA barcoding as an alternative to the traditional taxonomic approach for characterizing and comparing the diversity of moth communities in two different ecosystems in Gabon. Though sampling remains very incomplete, as evidenced by the high proportion (59%) of species represented by singletons, our results reveal an outstanding diversity. With about 3500 specimens sequenced and representing 1385 BINs (Barcode Index Numbers, used as a proxy to species) in 23 families, the diversity of moths in the two sites sampled is higher than the current number of species listed for the entire country, highlighting the huge gap in biodiversity knowledge for this country. Both seasonal and spatial turnovers are strikingly high (18.3% of BINs shared between seasons, and 13.3% between sites) and draw attention to the need to account for these when running regional surveys. Our results also highlight the richness and singularity of savannah environments and emphasize the status of Central African ecosystems as hotspots of biodiversity. </jats:p>","biodiversity research in tropical ecosystems—popularized as the most biodiverse habitats on earth—often neglects invertebrates, yet invertebrates represent the bulk of local species richness. insect communities in particular remain strongly impeded by both linnaean and wallacean shortfalls, and identifying species often remains a formidable challenge inhibiting the use of these organisms as indicators for ecological and conservation studies. here we use dna barcoding as an alternative to the traditional taxonomic approach for characterizing and comparing the diversity of moth communities in two different ecosystems in gabon. though sampling remains very incomplete, as evidenced by the high proportion (59%) of species represented by singletons, our results reveal an outstanding diversity. with about 3500 specimens sequenced and representing 1385 bins (barcode index numbers, used as a proxy to species) in 23 families, the diversity of moths in the two sites sampled is higher than the current number of species listed for the entire country, highlighting the huge gap in biodiversity knowledge for this country. both seasonal and spatial turnovers are strikingly high (18.3% of bins shared between seasons, and 13.3% between sites) and draw attention to the need to account for these when running regional surveys. our results also highlight the richness and singularity of savannah environments and emphasize the status of central african ecosystems as hotspots of biodiversity."
http://orkg.org/orkg/resource/R136193,Complete DNA barcode reference library for a country's butterfly fauna reveals high performance for temperate Europe,10.1098/rspb.2010.1089,crossref,"<jats:p>DNA barcoding aims to accelerate species identification and discovery, but performance tests have shown marked differences in identification success. As a consequence, there remains a great need for comprehensive studies which objectively test the method in groups with a solid taxonomic framework. This study focuses on the 180 species of butterflies in Romania, accounting for about one third of the European butterfly fauna. This country includes five eco-regions, the highest of any in the European Union, and is a good representative for temperate areas. Morphology and DNA barcodes of more than 1300 specimens were carefully studied and compared. Our results indicate that 90 per cent of the species form barcode clusters allowing their reliable identification. The remaining cases involve nine closely related species pairs, some whose taxonomic status is controversial or that hybridize regularly. Interestingly, DNA barcoding was found to be the most effective identification tool, outperforming external morphology, and being slightly better than male genitalia. Romania is now the first country to have a comprehensive DNA barcode reference database for butterflies. Similar barcoding efforts based on comprehensive sampling of specific geographical regions can act as functional modules that will foster the early application of DNA barcoding while a global system is under development.</jats:p>","dna barcoding aims to accelerate species identification and discovery, but performance tests have shown marked differences in identification success. as a consequence, there remains a great need for comprehensive studies which objectively test the method in groups with a solid taxonomic framework. this study focuses on the 180 species of butterflies in romania, accounting for about one third of the european butterfly fauna. this country includes five eco-regions, the highest of any in the european union, and is a good representative for temperate areas. morphology and dna barcodes of more than 1300 specimens were carefully studied and compared. our results indicate that 90 per cent of the species form barcode clusters allowing their reliable identification. the remaining cases involve nine closely related species pairs, some whose taxonomic status is controversial or that hybridize regularly. interestingly, dna barcoding was found to be the most effective identification tool, outperforming external morphology, and being slightly better than male genitalia. romania is now the first country to have a comprehensive dna barcode reference database for butterflies. similar barcoding efforts based on comprehensive sampling of specific geographical regions can act as functional modules that will foster the early application of dna barcoding while a global system is under development."
http://orkg.org/orkg/resource/R135869,"Evaluating FAIR-Compliance Through an Objective, Automated, Community-Governed Framework",10.1101/418376,crossref,"<jats:title>Abstract</jats:title><jats:p>With the increased adoption of the FAIR Principles, a wide range of stakeholders, from scientists to publishers, funding agencies and policy makers, are seeking ways to transparently evaluate resource FAIRness. We describe the FAIR Evaluator, a software infrastructure to register and execute tests of compliance with the recently published FAIR Metrics. The Evaluator enables digital resources to be assessed objectively and transparently. We illustrate its application to three widely used generalist repositories - Dataverse, Dryad, and Zenodo - and report their feedback. Evaluations allow communities to select relevant Metric subsets to deliver FAIRness measurements in diverse and specialized applications. Evaluations are executed in a semi-automated manner through Web Forms filled-in by a user, or through a JSON-based API. A comparison of manual vs automated evaluation reveals that automated evaluations are generally stricter, resulting in lower, though more accurate, FAIRness scores. Finally, we highlight the need for enhanced infrastructure such as standards registries, like FAIRsharing, as well as additional community involvement in domain-specific data infrastructure creation.</jats:p>","abstract with the increased adoption of the fair principles, a wide range of stakeholders, from scientists to publishers, funding agencies and policy makers, are seeking ways to transparently evaluate resource fairness. we describe the fair evaluator, a software infrastructure to register and execute tests of compliance with the recently published fair metrics. the evaluator enables digital resources to be assessed objectively and transparently. we illustrate its application to three widely used generalist repositories - dataverse, dryad, and zenodo - and report their feedback. evaluations allow communities to select relevant metric subsets to deliver fairness measurements in diverse and specialized applications. evaluations are executed in a semi-automated manner through web forms filled-in by a user, or through a json-based api. a comparison of manual vs automated evaluation reveals that automated evaluations are generally stricter, resulting in lower, though more accurate, fairness scores. finally, we highlight the need for enhanced infrastructure such as standards registries, like fairsharing, as well as additional community involvement in domain-specific data infrastructure creation."
http://orkg.org/orkg/resource/R135569,A Highly Sensitive and Flexible Capacitive Pressure Sensor Based on a Porous Three-Dimensional PDMS/Microsphere Composite,10.3390/polym12061412,crossref,"<jats:p>In recent times, polymer-based flexible pressure sensors have been attracting a lot of attention because of their various applications. A highly sensitive and flexible sensor is suggested, capable of being attached to the human body, based on a three-dimensional dielectric elastomeric structure of polydimethylsiloxane (PDMS) and microsphere composite. This sensor has maximal porosity due to macropores created by sacrificial layer grains and micropores generated by microspheres pre-mixed with PDMS, allowing it to operate at a wider pressure range (~150 kPa) while maintaining a sensitivity (of 0.124 kPa−1 in a range of 0~15 kPa) better than in previous studies. The maximized pores can cause deformation in the structure, allowing for the detection of small changes in pressure. In addition to exhibiting a fast rise time (~167 ms) and fall time (~117 ms), as well as excellent reproducibility, the fabricated pressure sensor exhibits reliability in its response to repeated mechanical stimuli (2.5 kPa, 1000 cycles). As an application, we develop a wearable device for monitoring repeated tiny motions, such as the pulse on the human neck and swallowing at the Adam’s apple. This sensory device is also used to detect movements in the index finger and to monitor an insole system in real-time.</jats:p>","in recent times, polymer-based flexible pressure sensors have been attracting a lot of attention because of their various applications. a highly sensitive and flexible sensor is suggested, capable of being attached to the human body, based on a three-dimensional dielectric elastomeric structure of polydimethylsiloxane (pdms) and microsphere composite. this sensor has maximal porosity due to macropores created by sacrificial layer grains and micropores generated by microspheres pre-mixed with pdms, allowing it to operate at a wider pressure range (~150 kpa) while maintaining a sensitivity (of 0.124 kpa−1 in a range of 0~15 kpa) better than in previous studies. the maximized pores can cause deformation in the structure, allowing for the detection of small changes in pressure. in addition to exhibiting a fast rise time (~167 ms) and fall time (~117 ms), as well as excellent reproducibility, the fabricated pressure sensor exhibits reliability in its response to repeated mechanical stimuli (2.5 kpa, 1000 cycles). as an application, we develop a wearable device for monitoring repeated tiny motions, such as the pulse on the human neck and swallowing at the adam’s apple. this sensory device is also used to detect movements in the index finger and to monitor an insole system in real-time."
http://orkg.org/orkg/resource/R135477,A learning object ontology repository to support annotation and discovery of educational resources using semantic thesauri,10.1177/0340035217737559,crossref,"<jats:p> Open educational resources are currently becoming increasingly available from a multitude of sources and are consequently annotated in many diverse ways. Interoperability concerns that naturally arise can often be resolved through the semantification of metadata descriptions, while at the same time strengthening the knowledge value of resources. SKOS can be a solid linking point offering a standard vocabulary for thematic descriptions, by referencing semantic thesauri. We propose the enhancement and maintenance of educational resources’ metadata in the form of learning object ontologies and introduce the notion of a learning object ontology repository that can help towards their publication, discovery and reuse. At the same time, linking to thesauri datasets and contextualized sources interrelates learning objects with linked data and exposes them to the Web of Data. We build a set of extensions and workflows on top of contemporary ontology management tools, such as WebProtégé, that can make it suitable as a learning object ontology repository. The proposed approach and implementation can help libraries and universities in discovering, managing and incorporating open educational resources and enhancing current curricula. </jats:p>","open educational resources are currently becoming increasingly available from a multitude of sources and are consequently annotated in many diverse ways. interoperability concerns that naturally arise can often be resolved through the semantification of metadata descriptions, while at the same time strengthening the knowledge value of resources. skos can be a solid linking point offering a standard vocabulary for thematic descriptions, by referencing semantic thesauri. we propose the enhancement and maintenance of educational resources’ metadata in the form of learning object ontologies and introduce the notion of a learning object ontology repository that can help towards their publication, discovery and reuse. at the same time, linking to thesauri datasets and contextualized sources interrelates learning objects with linked data and exposes them to the web of data. we build a set of extensions and workflows on top of contemporary ontology management tools, such as webprotégé, that can make it suitable as a learning object ontology repository. the proposed approach and implementation can help libraries and universities in discovering, managing and incorporating open educational resources and enhancing current curricula."
http://orkg.org/orkg/resource/R135489,Identification of Leukemia Subtypes from Microscopic Images Using Convolutional Neural Network,10.3390/diagnostics9030104,crossref,"<jats:p>Leukemia is a fatal cancer and has two main types: Acute and chronic. Each type has two more subtypes: Lymphoid and myeloid. Hence, in total, there are four subtypes of leukemia. This study proposes a new approach for diagnosis of all subtypes of leukemia from microscopic blood cell images using convolutional neural networks (CNN), which requires a large training data set. Therefore, we also investigated the effects of data augmentation for an increasing number of training samples synthetically. We used two publicly available leukemia data sources: ALL-IDB and ASH Image Bank. Next, we applied seven different image transformation techniques as data augmentation. We designed a CNN architecture capable of recognizing all subtypes of leukemia. Besides, we also explored other well-known machine learning algorithms such as naive Bayes, support vector machine, k-nearest neighbor, and decision tree. To evaluate our approach, we set up a set of experiments and used 5-fold cross-validation. The results we obtained from experiments showed that our CNN model performance has 88.25% and 81.74% accuracy, in leukemia versus healthy and multiclass classification of all subtypes, respectively. Finally, we also showed that the CNN model has a better performance than other wellknown machine learning algorithms.</jats:p>","leukemia is a fatal cancer and has two main types: acute and chronic. each type has two more subtypes: lymphoid and myeloid. hence, in total, there are four subtypes of leukemia. this study proposes a new approach for diagnosis of all subtypes of leukemia from microscopic blood cell images using convolutional neural networks (cnn), which requires a large training data set. therefore, we also investigated the effects of data augmentation for an increasing number of training samples synthetically. we used two publicly available leukemia data sources: all-idb and ash image bank. next, we applied seven different image transformation techniques as data augmentation. we designed a cnn architecture capable of recognizing all subtypes of leukemia. besides, we also explored other well-known machine learning algorithms such as naive bayes, support vector machine, k-nearest neighbor, and decision tree. to evaluate our approach, we set up a set of experiments and used 5-fold cross-validation. the results we obtained from experiments showed that our cnn model performance has 88.25% and 81.74% accuracy, in leukemia versus healthy and multiclass classification of all subtypes, respectively. finally, we also showed that the cnn model has a better performance than other wellknown machine learning algorithms."
http://orkg.org/orkg/resource/R135842,Considerations for the Conduction and Interpretation of FAIRness Evaluations,10.1162/dint_a_00051,crossref,"<jats:p> The FAIR principles were received with broad acceptance in several scientific communities. However, there is still some degree of uncertainty on how they should be implemented. Several self-report questionnaires have been proposed to assess the implementation of the FAIR principles. Moreover, the FAIRmetrics group released 14, general-purpose maturity for representing FAIRness. Initially, these metrics were conducted as open-answer questionnaires. Recently, these metrics have been implemented into a software that can automatically harvest metadata from metadata providers and generate a principle-specific FAIRness evaluation. With so many different approaches for FAIRness evaluations, we believe that further clarification on their limitations and advantages, as well as on their interpretation and interplay should be considered. </jats:p>","the fair principles were received with broad acceptance in several scientific communities. however, there is still some degree of uncertainty on how they should be implemented. several self-report questionnaires have been proposed to assess the implementation of the fair principles. moreover, the fairmetrics group released 14, general-purpose maturity for representing fairness. initially, these metrics were conducted as open-answer questionnaires. recently, these metrics have been implemented into a software that can automatically harvest metadata from metadata providers and generate a principle-specific fairness evaluation. with so many different approaches for fairness evaluations, we believe that further clarification on their limitations and advantages, as well as on their interpretation and interplay should be considered."
http://orkg.org/orkg/resource/R135546,Acute Lymphoblastic Leukemia Detection from Microscopic Images Using Weighted Ensemble of Convolutional Neural Networks,10.20944/preprints202105.0429.v1,crossref,"""<jats:p>Although automated Acute Lymphoblastic Leukemia (ALL) detection is essential, it is challenging due to the morphological correlation between malignant and normal cells. The traditional ALL classification strategy is arduous, time-consuming, often suffers inter-observer variations, and necessitates experienced pathologists. This article has automated the ALL detection task, employing deep Convolutional Neural Networks (CNNs). We explore the weighted ensemble of deep CNNs to recommend a better ALL cell classifier. The weights are estimated from ensemble candidates' corresponding metrics, such as accuracy, F1-score, AUC, and kappa values. Various data augmentations and pre-processing are incorporated for achieving a better generalization of the network. We train and evaluate the proposed model utilizing the publicly available C-NMC-2019 ALL dataset. Our proposed weighted ensemble model has outputted a weighted F1-score of 88.6%, a balanced accuracy of 86.2%, and an AUC of 0.941 in the preliminary test set. The qualitative results displaying the gradient class activation maps confirm that the introduced model has a concentrated learned region. In contrast, the ensemble candidate models, such as Xception, VGG-16, DenseNet-121, MobileNet, and InceptionResNet-V2, separately produce coarse and scatter learned areas for most example cases. Since the proposed ensemble yields a better result for the aimed task, it can experiment in other domains of medical diagnostic applications.</jats:p>""",""" although automated acute lymphoblastic leukemia (all) detection is essential, it is challenging due to the morphological correlation between malignant and normal cells. the traditional all classification strategy is arduous, time-consuming, often suffers inter-observer variations, and necessitates experienced pathologists. this article has automated the all detection task, employing deep convolutional neural networks (cnns). we explore the weighted ensemble of deep cnns to recommend a better all cell classifier. the weights are estimated from ensemble candidates' corresponding metrics, such as accuracy, f1-score, auc, and kappa values. various data augmentations and pre-processing are incorporated for achieving a better generalization of the network. we train and evaluate the proposed model utilizing the publicly available c-nmc-2019 all dataset. our proposed weighted ensemble model has outputted a weighted f1-score of 88.6%, a balanced accuracy of 86.2%, and an auc of 0.941 in the preliminary test set. the qualitative results displaying the gradient class activation maps confirm that the introduced model has a concentrated learned region. in contrast, the ensemble candidate models, such as xception, vgg-16, densenet-121, mobilenet, and inceptionresnet-v2, separately produce coarse and scatter learned areas for most example cases. since the proposed ensemble yields a better result for the aimed task, it can experiment in other domains of medical diagnostic applications. """
http://orkg.org/orkg/resource/R135895,A Semi-Automated Workflow for FAIR Maturity Indicators in the Life Sciences,10.3390/nano10102068,crossref,"<jats:p>Data sharing and reuse are crucial to enhance scientific progress and maximize return of investments in science. Although attitudes are increasingly favorable, data reuse remains difficult due to lack of infrastructures, standards, and policies. The FAIR (findable, accessible, interoperable, reusable) principles aim to provide recommendations to increase data reuse. Because of the broad interpretation of the FAIR principles, maturity indicators are necessary to determine the FAIRness of a dataset. In this work, we propose a reproducible computational workflow to assess data FAIRness in the life sciences. Our implementation follows principles and guidelines recommended by the maturity indicator authoring group and integrates concepts from the literature. In addition, we propose a FAIR balloon plot to summarize and compare dataset FAIRness. We evaluated the feasibility of our method on three real use cases where researchers looked for six datasets to answer their scientific questions. We retrieved information from repositories (ArrayExpress, Gene Expression Omnibus, eNanoMapper, caNanoLab, NanoCommons and ChEMBL), a registry of repositories, and a searchable resource (Google Dataset Search) via application program interfaces (API) wherever possible. With our analysis, we found that the six datasets met the majority of the criteria defined by the maturity indicators, and we showed areas where improvements can easily be reached. We suggest that use of standard schema for metadata and the presence of specific attributes in registries of repositories could increase FAIRness of datasets.</jats:p>","data sharing and reuse are crucial to enhance scientific progress and maximize return of investments in science. although attitudes are increasingly favorable, data reuse remains difficult due to lack of infrastructures, standards, and policies. the fair (findable, accessible, interoperable, reusable) principles aim to provide recommendations to increase data reuse. because of the broad interpretation of the fair principles, maturity indicators are necessary to determine the fairness of a dataset. in this work, we propose a reproducible computational workflow to assess data fairness in the life sciences. our implementation follows principles and guidelines recommended by the maturity indicator authoring group and integrates concepts from the literature. in addition, we propose a fair balloon plot to summarize and compare dataset fairness. we evaluated the feasibility of our method on three real use cases where researchers looked for six datasets to answer their scientific questions. we retrieved information from repositories (arrayexpress, gene expression omnibus, enanomapper, cananolab, nanocommons and chembl), a registry of repositories, and a searchable resource (google dataset search) via application program interfaces (api) wherever possible. with our analysis, we found that the six datasets met the majority of the criteria defined by the maturity indicators, and we showed areas where improvements can easily be reached. we suggest that use of standard schema for metadata and the presence of specific attributes in registries of repositories could increase fairness of datasets."
http://orkg.org/orkg/resource/R135836,Making FAIR Easy with FAIR Tools: From Creolization to Convergence,https://doi.org/10.1162/dint_a_00031,crossref,"<jats:p> Since their publication in 2016 we have seen a rapid adoption of the FAIR principles in many scientific disciplines where the inherent value of research data and, therefore, the importance of good data management and data stewardship, is recognized. This has led to many communities asking “What is FAIR?” and “How FAIR are we currently?”, questions which were addressed respectively by a publication revisiting the principles and the emergence of FAIR metrics. However, early adopters of the FAIR principles have already run into the next question: “How can we become (more) FAIR?” This question is more difficult to answer, as the principles do not prescribe any specific standard or implementation. Moreover, there does not yet exist a mature ecosystem of tools, platforms and standards to support human and machine agents to manage, produce, publish and consume FAIR data in a user-friendly and efficient (i.e., “easy”) way. In this paper we will show, however, that there are already many emerging examples of FAIR tools under development. This paper puts forward the position that we are likely already in a creolization phase where FAIR tools and technologies are merging and combining, before converging in a subsequent phase to solutions that make FAIR feasible in daily practice. </jats:p>","since their publication in 2016 we have seen a rapid adoption of the fair principles in many scientific disciplines where the inherent value of research data and, therefore, the importance of good data management and data stewardship, is recognized. this has led to many communities asking “what is fair?” and “how fair are we currently?”, questions which were addressed respectively by a publication revisiting the principles and the emergence of fair metrics. however, early adopters of the fair principles have already run into the next question: “how can we become (more) fair?” this question is more difficult to answer, as the principles do not prescribe any specific standard or implementation. moreover, there does not yet exist a mature ecosystem of tools, platforms and standards to support human and machine agents to manage, produce, publish and consume fair data in a user-friendly and efficient (i.e., “easy”) way. in this paper we will show, however, that there are already many emerging examples of fair tools under development. this paper puts forward the position that we are likely already in a creolization phase where fair tools and technologies are merging and combining, before converging in a subsequent phase to solutions that make fair feasible in daily practice."
http://orkg.org/orkg/resource/R134227,Rainbow: Combining Improvements in Deep Reinforcement Learning,,crossref,"<jats:p>\n      \n        The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.\n      \n    </jats:p>","\n \n the deep reinforcement learning community has made several independent improvements to the dqn algorithm. however, it is unclear which of these extensions are complementary and can be fruitfully combined. this paper examines six extensions to the dqn algorithm and empirically studies their combination. our experiments show that the combination provides state-of-the-art performance on the atari 2600 benchmark, both in terms of data efficiency and final performance. we also provide results from a detailed ablation study that shows the contribution of each component to overall performance.\n \n"
http://orkg.org/orkg/resource/R134360,Count-Based Exploration in Feature Space for Reinforcement Learning,,crossref,"<jats:p>We introduce a new count-based optimistic exploration algorithm for Reinforcement Learning (RL) that is feasible in environments with high-dimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our \\phi-pseudocount achieves generalisation by exploiting same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The \\phi-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional RL benchmarks.</jats:p>","we introduce a new count-based optimistic exploration algorithm for reinforcement learning (rl) that is feasible in environments with high-dimensional state-action spaces. the success of rl algorithms in these domains depends crucially on generalisation from limited training experience. function approximation techniques enable rl agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. this has prevented the combination of scalable rl algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. we present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. our \\phi-pseudocount achieves generalisation by exploiting same feature representation of the state space that is used for value function approximation. states that have less frequently observed features are deemed more uncertain. the \\phi-exploration-bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. the method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional rl benchmarks."
http://orkg.org/orkg/resource/R134383,Count-Based Exploration with the Successor Representation,,crossref,"<jats:p>In this paper we introduce a simple approach for exploration in reinforcement learning (RL) that allows us to develop theoretically justified algorithms in the tabular case but that is also extendable to settings where function approximation is required. Our approach is based on the successor representation (SR), which was originally introduced as a representation defining state generalization by the similarity of successor states. Here we show that the norm of the SR, while it is being learned, can be used as a reward bonus to incentivize exploration. In order to better understand this transient behavior of the norm of the SR we introduce the substochastic successor representation (SSR) and we show that it implicitly counts the number of times each state (or feature) has been observed. We use this result to introduce an algorithm that performs as well as some theoretically sample-efficient approaches. Finally, we extend these ideas to a deep RL algorithm and show that it achieves state-of-the-art performance in Atari 2600 games when in a low sample-complexity regime.</jats:p>","in this paper we introduce a simple approach for exploration in reinforcement learning (rl) that allows us to develop theoretically justified algorithms in the tabular case but that is also extendable to settings where function approximation is required. our approach is based on the successor representation (sr), which was originally introduced as a representation defining state generalization by the similarity of successor states. here we show that the norm of the sr, while it is being learned, can be used as a reward bonus to incentivize exploration. in order to better understand this transient behavior of the norm of the sr we introduce the substochastic successor representation (ssr) and we show that it implicitly counts the number of times each state (or feature) has been observed. we use this result to introduce an algorithm that performs as well as some theoretically sample-efficient approaches. finally, we extend these ideas to a deep rl algorithm and show that it achieves state-of-the-art performance in atari 2600 games when in a low sample-complexity regime."
http://orkg.org/orkg/resource/R131694,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,,crossref,"<jats:p> We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI data set), cross-lingual document classification (MLDoc data set), and parallel corpus mining (BUCC data set) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low- resource languages. Our implementation, the pre-trained encoder, and the multilingual test set are available at https://github.com/facebookresearch/LASER . </jats:p>","we introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. our system uses a single bilstm encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. this enables us to learn a classifier on top of the resulting embeddings using english annotated data only, and transfer it to any of the 93 languages without any modification. our experiments in cross-lingual natural language inference (xnli data set), cross-lingual document classification (mldoc data set), and parallel corpus mining (bucc data set) show the effectiveness of our approach. we also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low- resource languages. our implementation, the pre-trained encoder, and the multilingual test set are available at https://github.com/facebookresearch/laser ."
http://orkg.org/orkg/resource/R131839,Joint Parsing and Generation for Abstractive Summarization,,crossref,"<jats:p>Sentences produced by abstractive summarization systems can be ungrammatical and fail to preserve the original meanings, despite being locally fluent. In this paper we propose to remedy this problem by jointly generating a sentence and its syntactic dependency parse while performing abstraction. If generating a word can introduce an erroneous relation to the summary, the behavior must be discouraged. The proposed method thus holds promise for producing grammatical sentences and encouraging the summary to stay true-to-original. Our contributions of this work are twofold. First, we present a novel neural architecture for abstractive summarization that combines a sequential decoder with a tree-based decoder in a synchronized manner to generate a summary sentence and its syntactic parse. Secondly, we describe a novel human evaluation protocol to assess if, and to what extent, a summary remains true to its original meanings. We evaluate our method on a number of summarization datasets and demonstrate competitive results against strong baselines.</jats:p>","sentences produced by abstractive summarization systems can be ungrammatical and fail to preserve the original meanings, despite being locally fluent. in this paper we propose to remedy this problem by jointly generating a sentence and its syntactic dependency parse while performing abstraction. if generating a word can introduce an erroneous relation to the summary, the behavior must be discouraged. the proposed method thus holds promise for producing grammatical sentences and encouraging the summary to stay true-to-original. our contributions of this work are twofold. first, we present a novel neural architecture for abstractive summarization that combines a sequential decoder with a tree-based decoder in a synchronized manner to generate a summary sentence and its syntactic parse. secondly, we describe a novel human evaluation protocol to assess if, and to what extent, a summary remains true to its original meanings. we evaluate our method on a number of summarization datasets and demonstrate competitive results against strong baselines."
http://orkg.org/orkg/resource/R131873,ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks,,crossref,"<jats:p>Scientific article summarization is challenging: large, annotated corpora are not available, and the summary should ideally include the article’s impacts on research community. This paper provides novel solutions to these two challenges. We 1) develop and release the first large-scale manually-annotated corpus for scientific papers (on computational linguistics) by enabling faster annotation, and 2) propose summarization methods that integrate the authors’ original highlights (abstract) and the article’s actual impacts on the community (citations), to create comprehensive, hybrid summaries. We conduct experiments to demonstrate the efficacy of our corpus in training data-driven models for scientific paper summarization and the advantage of our hybrid summaries over abstracts and traditional citation-based summaries. Our large annotated corpus and hybrid methods provide a new framework for scientific paper summarization research.</jats:p>","scientific article summarization is challenging: large, annotated corpora are not available, and the summary should ideally include the article’s impacts on research community. this paper provides novel solutions to these two challenges. we 1) develop and release the first large-scale manually-annotated corpus for scientific papers (on computational linguistics) by enabling faster annotation, and 2) propose summarization methods that integrate the authors’ original highlights (abstract) and the article’s actual impacts on the community (citations), to create comprehensive, hybrid summaries. we conduct experiments to demonstrate the efficacy of our corpus in training data-driven models for scientific paper summarization and the advantage of our hybrid summaries over abstracts and traditional citation-based summaries. our large annotated corpus and hybrid methods provide a new framework for scientific paper summarization research."
http://orkg.org/orkg/resource/R131153,Message Passing Attention Networks for Document Understanding,,crossref,"<jats:p>Graph neural networks have recently emerged as a very effective framework for processing graph-structured data. These models have achieved state-of-the-art performance in many tasks. Most graph neural networks can be described in terms of message passing, vertex update, and readout functions. In this paper, we represent documents as word co-occurrence networks and propose an application of the message passing framework to NLP, the Message Passing Attention network for Document understanding (MPAD). We also propose several hierarchical variants of MPAD. Experiments conducted on 10 standard text classification datasets show that our architectures are competitive with the state-of-the-art. Ablation studies reveal further insights about the impact of the different components on performance. Code is publicly available at: https://github.com/giannisnik/mpad.</jats:p>","graph neural networks have recently emerged as a very effective framework for processing graph-structured data. these models have achieved state-of-the-art performance in many tasks. most graph neural networks can be described in terms of message passing, vertex update, and readout functions. in this paper, we represent documents as word co-occurrence networks and propose an application of the message passing framework to nlp, the message passing attention network for document understanding (mpad). we also propose several hierarchical variants of mpad. experiments conducted on 10 standard text classification datasets show that our architectures are competitive with the state-of-the-art. ablation studies reveal further insights about the impact of the different components on performance. code is publicly available at: https://github.com/giannisnik/mpad."
http://orkg.org/orkg/resource/R131297,Multi-task Deep Reinforcement Learning with PopArt,,crossref,"<jats:p>The reinforcement learning (RL) community has made great strides in designing algorithms capable of exceeding human performance on specific tasks. These algorithms are mostly trained one task at the time, each new task requiring to train a brand new agent instance. This means the learning algorithm is general, but each solution is not; each agent can only solve the one task it was trained on. In this work, we study the problem of learning to master not one but multiple sequentialdecision tasks at once. A general issue in multi-task learning is that a balance must be found between the needs of multiple tasks competing for the limited resources of a single learning system. Many learning algorithms can get distracted by certain tasks in the set of tasks to solve. Such tasks appear more salient to the learning process, for instance because of the density or magnitude of the in-task rewards. This causes the algorithm to focus on those salient tasks at the expense of generality. We propose to automatically adapt the contribution of each task to the agent’s updates, so that all tasks have a similar impact on the learning dynamics. This resulted in state of the art performance on learning to play all games in a set of 57 diverse Atari games. Excitingly, our method learned a single trained policy - with a single set of weights - that exceeds median human performance. To our knowledge, this was the first time a single agent surpassed human-level performance on this multi-task domain. The same approach also demonstrated state of the art performance on a set of 30 tasks in the 3D reinforcement learning platform DeepMind Lab.</jats:p>","the reinforcement learning (rl) community has made great strides in designing algorithms capable of exceeding human performance on specific tasks. these algorithms are mostly trained one task at the time, each new task requiring to train a brand new agent instance. this means the learning algorithm is general, but each solution is not; each agent can only solve the one task it was trained on. in this work, we study the problem of learning to master not one but multiple sequentialdecision tasks at once. a general issue in multi-task learning is that a balance must be found between the needs of multiple tasks competing for the limited resources of a single learning system. many learning algorithms can get distracted by certain tasks in the set of tasks to solve. such tasks appear more salient to the learning process, for instance because of the density or magnitude of the in-task rewards. this causes the algorithm to focus on those salient tasks at the expense of generality. we propose to automatically adapt the contribution of each task to the agent’s updates, so that all tasks have a similar impact on the learning dynamics. this resulted in state of the art performance on learning to play all games in a set of 57 diverse atari games. excitingly, our method learned a single trained policy - with a single set of weights - that exceeds median human performance. to our knowledge, this was the first time a single agent surpassed human-level performance on this multi-task domain. the same approach also demonstrated state of the art performance on a set of 30 tasks in the 3d reinforcement learning platform deepmind lab."
http://orkg.org/orkg/resource/R130689,Character-Level Language Modeling with Deeper Self-Attention,,crossref,"<jats:p>LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model (Vaswani et al. 2017) with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.</jats:p>","lstms and other rnn variants have shown strong performance on character-level language modeling. these models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. in this paper, we show that a deep (64-layer) transformer model (vaswani et al. 2017) with fixed context outperforms rnn variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. to get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions."
http://orkg.org/orkg/resource/R130466,Data-to-Text Generation with Content Selection and Planning,,crossref,"<jats:p>Recent advances in data-to-text generation have led to the use of large-scale datasets and neural network models which are trained end-to-end, without explicitly modeling what to say and in what order. In this work, we present a neural network architecture which incorporates content selection and planning without sacrificing end-to-end training. We decompose the generation task into two stages. Given a corpus of data records (paired with descriptive documents), we first generate a content plan highlighting which information should be mentioned and in which order and then generate the document while taking the content plan into account. Automatic and human-based evaluation experiments show that our model1 outperforms strong baselines improving the state-of-the-art on the recently released RotoWIRE dataset.</jats:p>","recent advances in data-to-text generation have led to the use of large-scale datasets and neural network models which are trained end-to-end, without explicitly modeling what to say and in what order. in this work, we present a neural network architecture which incorporates content selection and planning without sacrificing end-to-end training. we decompose the generation task into two stages. given a corpus of data records (paired with descriptive documents), we first generate a content plan highlighting which information should be mentioned and in which order and then generate the document while taking the content plan into account. automatic and human-based evaluation experiments show that our model1 outperforms strong baselines improving the state-of-the-art on the recently released rotowire dataset."
http://orkg.org/orkg/resource/R129839,Unsupervised Neural Machine Translation with SMT as Posterior Regularization,,crossref,"<jats:p>Without real bilingual corpus available, unsupervised Neural Machine Translation (NMT) typically requires pseudo parallel data generated with the back-translation method for the model training. However, due to weak supervision, the pseudo data inevitably contain noises and errors that will be accumulated and reinforced in the subsequent training process, leading to bad translation performance. To address this issue, we introduce phrase based Statistic Machine Translation (SMT) models which are robust to noisy data, as posterior regularizations to guide the training of unsupervised NMT models in the iterative back-translation process. Our method starts from SMT models built with pre-trained language models and word-level translation tables inferred from cross-lingual embeddings. Then SMT and NMT models are optimized jointly and boost each other incrementally in a unified EM framework. In this way, (1) the negative effect caused by errors in the iterative back-translation process can be alleviated timely by SMT filtering noises from its phrase tables; meanwhile, (2) NMT can compensate for the deficiency of fluency inherent in SMT. Experiments conducted on en-fr and en-de translation tasks show that our method outperforms the strong baseline and achieves new state-of-the-art unsupervised machine translation performance.</jats:p>","without real bilingual corpus available, unsupervised neural machine translation (nmt) typically requires pseudo parallel data generated with the back-translation method for the model training. however, due to weak supervision, the pseudo data inevitably contain noises and errors that will be accumulated and reinforced in the subsequent training process, leading to bad translation performance. to address this issue, we introduce phrase based statistic machine translation (smt) models which are robust to noisy data, as posterior regularizations to guide the training of unsupervised nmt models in the iterative back-translation process. our method starts from smt models built with pre-trained language models and word-level translation tables inferred from cross-lingual embeddings. then smt and nmt models are optimized jointly and boost each other incrementally in a unified em framework. in this way, (1) the negative effect caused by errors in the iterative back-translation process can be alleviated timely by smt filtering noises from its phrase tables; meanwhile, (2) nmt can compensate for the deficiency of fluency inherent in smt. experiments conducted on en-fr and en-de translation tasks show that our method outperforms the strong baseline and achieves new state-of-the-art unsupervised machine translation performance."
http://orkg.org/orkg/resource/R129390,Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction,,crossref,"<jats:p>A relation tuple consists of two entities and the relation between them, and often such tuples are found in unstructured text. There may be multiple relation tuples present in a text and they may share one or both entities among them. Extracting such relation tuples from a sentence is a difficult task and sharing of entities or overlapping entities among the tuples makes it more challenging. Most prior work adopted a pipeline approach where entities were identified first followed by finding the relations among them, thus missing the interaction among the relation tuples in a sentence. In this paper, we propose two approaches to use encoder-decoder architecture for jointly extracting entities and relations. In the first approach, we propose a representation scheme for relation tuples which enables the decoder to generate one word at a time like machine translation models and still finds all the tuples present in a sentence with full entity names of different length and with overlapping entities. Next, we propose a pointer network-based decoding approach where an entire tuple is generated at every time step. Experiments on the publicly available New York Times corpus show that our proposed approaches outperform previous work and achieve significantly higher F1 scores.</jats:p>","a relation tuple consists of two entities and the relation between them, and often such tuples are found in unstructured text. there may be multiple relation tuples present in a text and they may share one or both entities among them. extracting such relation tuples from a sentence is a difficult task and sharing of entities or overlapping entities among the tuples makes it more challenging. most prior work adopted a pipeline approach where entities were identified first followed by finding the relations among them, thus missing the interaction among the relation tuples in a sentence. in this paper, we propose two approaches to use encoder-decoder architecture for jointly extracting entities and relations. in the first approach, we propose a representation scheme for relation tuples which enables the decoder to generate one word at a time like machine translation models and still finds all the tuples present in a sentence with full entity names of different length and with overlapping entities. next, we propose a pointer network-based decoding approach where an entire tuple is generated at every time step. experiments on the publicly available new york times corpus show that our proposed approaches outperform previous work and achieve significantly higher f1 scores."
http://orkg.org/orkg/resource/R129608,Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing,,crossref,"<jats:p>\n            Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning Benchmark) at\n            <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""url"" xlink:href=""https://aka.ms/BLURB"">https://aka.ms/BLURB</jats:ext-link>\n            .\n          </jats:p>","\n pretraining large neural language models, such as bert, has led to impressive gains on many natural language processing (nlp) tasks. however, most pretraining efforts focus on general domain corpora, such as newswire and web. a prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. in this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. to facilitate this investigation, we compile a comprehensive biomedical nlp benchmark from publicly available datasets. our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical nlp tasks, leading to new state-of-the-art results across the board. further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with bert models, such as using complex tagging schemes in named entity recognition. to help accelerate research in biomedical nlp, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our blurb benchmark (short for biomedical language understanding &amp; reasoning benchmark) at\n https://aka.ms/blurb \n .\n"
http://orkg.org/orkg/resource/R107613,Static analysis and optimization of semantic web queries,10.1145/2500130,crossref,"<jats:p>\n            Static analysis is a fundamental task in query optimization. In this article we study static analysis and optimization techniques for SPARQL, which is the standard language for querying Semantic Web data. Of particular interest for us is the\n            <jats:italic>optionality</jats:italic>\n            feature in SPARQL. It is crucial in Semantic Web data management, where data sources are inherently incomplete and the user is usually interested in partial answers to queries. This feature is one of the most complicated constructors in SPARQL and also the one that makes this language depart from classical query languages such as relational conjunctive queries. We focus on the class of well-designed SPARQL queries, which has been proposed in the literature as a fragment of the language with good properties regarding query evaluation. We first propose a tree representation for SPARQL queries, called pattern trees, which captures the class of well-designed SPARQL graph patterns. Among other results, we propose several rules that can be used to transform pattern trees into a simple normal form, and study equivalence and containment. We also study the evaluation and enumeration problems for this class of queries.\n          </jats:p>","\n static analysis is a fundamental task in query optimization. in this article we study static analysis and optimization techniques for sparql, which is the standard language for querying semantic web data. of particular interest for us is the\n optionality \n feature in sparql. it is crucial in semantic web data management, where data sources are inherently incomplete and the user is usually interested in partial answers to queries. this feature is one of the most complicated constructors in sparql and also the one that makes this language depart from classical query languages such as relational conjunctive queries. we focus on the class of well-designed sparql queries, which has been proposed in the literature as a fragment of the language with good properties regarding query evaluation. we first propose a tree representation for sparql queries, called pattern trees, which captures the class of well-designed sparql graph patterns. among other results, we propose several rules that can be used to transform pattern trees into a simple normal form, and study equivalence and containment. we also study the evaluation and enumeration problems for this class of queries.\n"
http://orkg.org/orkg/resource/R107637,Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets,10.1145/3165713,crossref,"<jats:p>\n            Although the ultimate objective of Linked Data is linking and integration, it is not currently evident how\n            <jats:italic>connected</jats:italic>\n            the current Linked Open Data (LOD) cloud is. In this article, we focus on methods, supported by special indexes and algorithms, for performing measurements related to the connectivity of more than two datasets that are useful in various tasks including (a)\n            <jats:italic>Dataset Discovery</jats:italic>\n            and\n            <jats:italic>Selection</jats:italic>\n            ; (b)\n            <jats:italic>Object Coreference</jats:italic>\n            , i.e., for obtaining\n            <jats:italic>complete information</jats:italic>\n            about a set of entities, including provenance information; (c)\n            <jats:italic>Data Quality Assessment and Improvement</jats:italic>\n            , i.e., for assessing the connectivity between any set of datasets and monitoring their evolution over time, as well as for estimating data veracity; (d)\n            <jats:italic>Dataset Visualizations</jats:italic>\n            ; and various other tasks. Since it would be prohibitively expensive to perform all these measurements in a naïve way, in this article, we introduce indexes (and their construction algorithms) that can speed up such tasks. In brief, we introduce (i) a namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and transitive closure of the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based incremental algorithms for speeding up the computation of the intersection of URIs of any set of datasets. For enhancing scalability, we propose parallel index construction algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved speedup using either a single machine or a cluster of machines, and we provide insights regarding the factors that affect efficiency. Finally, we report measurements about the connectivity of the (billion triples-sized) LOD cloud that have never been carried out so far.\n          </jats:p>","\n although the ultimate objective of linked data is linking and integration, it is not currently evident how\n connected \n the current linked open data (lod) cloud is. in this article, we focus on methods, supported by special indexes and algorithms, for performing measurements related to the connectivity of more than two datasets that are useful in various tasks including (a)\n dataset discovery \n and\n selection \n ; (b)\n object coreference \n , i.e., for obtaining\n complete information \n about a set of entities, including provenance information; (c)\n data quality assessment and improvement \n , i.e., for assessing the connectivity between any set of datasets and monitoring their evolution over time, as well as for estimating data veracity; (d)\n dataset visualizations \n ; and various other tasks. since it would be prohibitively expensive to perform all these measurements in a naïve way, in this article, we introduce indexes (and their construction algorithms) that can speed up such tasks. in brief, we introduce (i) a namespace-based prefix index, (ii) a sameas catalog for computing the symmetric and transitive closure of the owl:sameas relationships encountered in the datasets, (iii) a semantics-aware element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based incremental algorithms for speeding up the computation of the intersection of uris of any set of datasets. for enhancing scalability, we propose parallel index construction algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved speedup using either a single machine or a cluster of machines, and we provide insights regarding the factors that affect efficiency. finally, we report measurements about the connectivity of the (billion triples-sized) lod cloud that have never been carried out so far.\n"
http://orkg.org/orkg/resource/R107744,The trade-off behaviours between virtual and physical activities during the first wave of the COVID-19 pandemic period,10.1186/s12544-021-00473-7,crossref,"""<jats:title>Abstract</jats:title><jats:sec>\n                <jats:title>Introduction</jats:title>\n                <jats:p>The first wave of COVID-19 pandemic period has drastically changed people’s lives all over the world. To cope with the disruption, digital solutions have become more popular. However, the ability to adopt digitalised alternatives is different across socio-economic and socio-demographic groups.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Objective</jats:title>\n                <jats:p>This study investigates how individuals have changed their activity-travel patterns and internet usage during the first wave of the COVID-19 pandemic period, and which of these changes may be kept.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Methods</jats:title>\n                <jats:p>An empirical data collection was deployed through online forms. 781 responses from different countries (Italy, Sweden, India and others) have been collected, and a series of multivariate analyses was carried out. Two linear regression models are presented, related to the change of travel activities and internet usage, before and during the pandemic period. Furthermore, a binary regression model is used to examine the likelihood of the respondents to adopt and keep their behaviours beyond the pandemic period.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Results</jats:title>\n                <jats:p>The results show that the possibility to change the behaviour matter. External restrictions and personal characteristics are the driving factors of the reduction in ones' daily trips. However, the estimation results do not show a strong correlation between the countries' restriction policy and the respondents' likelihood to adopt the new and online-based behaviours for any of the activities after the restriction period.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Conclusion</jats:title>\n                <jats:p>The acceptance and long-term adoption of the online alternatives for activities are correlated with the respondents' personality and socio-demographic group, highlighting the importance of promoting alternatives as a part of longer-term behavioural and lifestyle changes.</jats:p>\n              </jats:sec>""",""" abstract \n introduction \n the first wave of covid-19 pandemic period has drastically changed people’s lives all over the world. to cope with the disruption, digital solutions have become more popular. however, the ability to adopt digitalised alternatives is different across socio-economic and socio-demographic groups. \n \n objective \n this study investigates how individuals have changed their activity-travel patterns and internet usage during the first wave of the covid-19 pandemic period, and which of these changes may be kept. \n \n methods \n an empirical data collection was deployed through online forms. 781 responses from different countries (italy, sweden, india and others) have been collected, and a series of multivariate analyses was carried out. two linear regression models are presented, related to the change of travel activities and internet usage, before and during the pandemic period. furthermore, a binary regression model is used to examine the likelihood of the respondents to adopt and keep their behaviours beyond the pandemic period. \n \n results \n the results show that the possibility to change the behaviour matter. external restrictions and personal characteristics are the driving factors of the reduction in ones' daily trips. however, the estimation results do not show a strong correlation between the countries' restriction policy and the respondents' likelihood to adopt the new and online-based behaviours for any of the activities after the restriction period. \n \n conclusion \n the acceptance and long-term adoption of the online alternatives for activities are correlated with the respondents' personality and socio-demographic group, highlighting the importance of promoting alternatives as a part of longer-term behavioural and lifestyle changes. \n """
http://orkg.org/orkg/resource/R108704,Anastomosis Groups and Pathogenicity of Rhizoctonia solani and Binucleate Rhizoctonia from Potato in South Africa,10.1094/pdis-02-15-0236-re,crossref,"<jats:p> A survey of anastomosis groups (AG) of Rhizoctonia spp. associated with potato diseases was conducted in South Africa. In total, 112 Rhizoctonia solani and 19 binucleate Rhizoctonia (BNR) isolates were recovered from diseased potato plants, characterized for AG and pathogenicity. The AG identity of the isolates was confirmed using phylogenetic analysis of the internal transcribed spacer region of ribosomal DNA. R. solani isolates recovered belonged to AG 3-PT, AG 2-2IIIB, AG 4HG-I, AG 4HG-III, and AG 5, while BNR isolates belonged to AG A and AG R, with frequencies of 74, 6.1, 2.3, 2.3, 0.8, 12.2, and 2.3%, respectively. R. solani AG 3-PT was the most predominant AG and occurred in all the potato-growing regions sampled, whereas the other AG occurred in distinct locations. Different AG grouped into distinct clades, with high maximum parsimony and maximum-likelihood bootstrap support for both R. solani and BNR. An experiment under greenhouse conditions with representative isolates from different AG showed differences in aggressiveness between and within AG. Isolates of AG 2-2IIIB, AG 4HG-III, and AG R were the most aggressive in causing stem canker while AG 3-PT, AG 5, and AG R caused black scurf. This is the first comprehensive survey of R. solani and BNR on potato in South Africa using a molecular-based approach. This is the first report of R. solani AG 2-2IIIB and AG 4 HG-I causing stem and stolon canker and BNR AG A and AG R causing stem canker and black scurf on potato in South Africa. </jats:p>","a survey of anastomosis groups (ag) of rhizoctonia spp. associated with potato diseases was conducted in south africa. in total, 112 rhizoctonia solani and 19 binucleate rhizoctonia (bnr) isolates were recovered from diseased potato plants, characterized for ag and pathogenicity. the ag identity of the isolates was confirmed using phylogenetic analysis of the internal transcribed spacer region of ribosomal dna. r. solani isolates recovered belonged to ag 3-pt, ag 2-2iiib, ag 4hg-i, ag 4hg-iii, and ag 5, while bnr isolates belonged to ag a and ag r, with frequencies of 74, 6.1, 2.3, 2.3, 0.8, 12.2, and 2.3%, respectively. r. solani ag 3-pt was the most predominant ag and occurred in all the potato-growing regions sampled, whereas the other ag occurred in distinct locations. different ag grouped into distinct clades, with high maximum parsimony and maximum-likelihood bootstrap support for both r. solani and bnr. an experiment under greenhouse conditions with representative isolates from different ag showed differences in aggressiveness between and within ag. isolates of ag 2-2iiib, ag 4hg-iii, and ag r were the most aggressive in causing stem canker while ag 3-pt, ag 5, and ag r caused black scurf. this is the first comprehensive survey of r. solani and bnr on potato in south africa using a molecular-based approach. this is the first report of r. solani ag 2-2iiib and ag 4 hg-i causing stem and stolon canker and bnr ag a and ag r causing stem canker and black scurf on potato in south africa."
http://orkg.org/orkg/resource/R108328,Modeling Techniques for Knowledge Management: ,10.4018/978-1-59904-603-7.ch003,crossref,<jats:p>Knowledge management is an umbrella concept for different management tasks and activities. Various modeling abstractions and techniques have been developed providing specialized support for different knowledge management tasks. This article gives an overview of modeling abstractions that are frequently discussed in the knowledge management literature as well as some promising techniques in a mature research state. Six groups of modeling techniques are presented and additionally evaluated with respect to their suitability for different fields of applications within the knowledge management domain.</jats:p>,knowledge management is an umbrella concept for different management tasks and activities. various modeling abstractions and techniques have been developed providing specialized support for different knowledge management tasks. this article gives an overview of modeling abstractions that are frequently discussed in the knowledge management literature as well as some promising techniques in a mature research state. six groups of modeling techniques are presented and additionally evaluated with respect to their suitability for different fields of applications within the knowledge management domain.
http://orkg.org/orkg/resource/R108803,Dinitrogen fixation rates in the Bay of Bengal during summer monsoon,10.1088/2515-7620/ab89fa,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Biological dinitrogen (N<jats:sub>2</jats:sub>) fixation exerts an important control on oceanic primary production by providing bioavailable form of nitrogen (such as ammonium) to photosynthetic microorganisms. N<jats:sub>2</jats:sub> fixation is dominant in nutrient poor and warm surface waters. The Bay of Bengal is one such region where no measurements of phototrophic N<jats:sub>2</jats:sub> fixation rates exist. The surface water of the Bay of Bengal is generally nitrate-poor and warm due to prevailing stratification and thus, could favour N<jats:sub>2</jats:sub> fixation. We commenced the first N<jats:sub>2</jats:sub> fixation study in the photic zone of the Bay of Bengal using <jats:sup>15</jats:sup>N<jats:sub>2</jats:sub> gas tracer incubation experiment during summer monsoon 2018. We collected seawater samples from four depths (covering the mixed layer depth of up to 75 m) at eight stations. N<jats:sub>2</jats:sub> fixation rates varied from 4 to 75 <jats:italic>μ</jats:italic>mol N m<jats:sup>−2</jats:sup> d<jats:sup>−1</jats:sup>. The contribution of N<jats:sub>2</jats:sub> fixation to primary production was negligible (&lt;1%). However, the upper bound of observed N<jats:sub>2</jats:sub> fixation rates is higher than the rates measured in other oceanic regimes, such as the Eastern Tropical South Pacific, the Tropical Northwest Atlantic, and the Equatorial and Southern Indian Ocean.</jats:p>","abstract \n biological dinitrogen (n 2 ) fixation exerts an important control on oceanic primary production by providing bioavailable form of nitrogen (such as ammonium) to photosynthetic microorganisms. n 2 fixation is dominant in nutrient poor and warm surface waters. the bay of bengal is one such region where no measurements of phototrophic n 2 fixation rates exist. the surface water of the bay of bengal is generally nitrate-poor and warm due to prevailing stratification and thus, could favour n 2 fixation. we commenced the first n 2 fixation study in the photic zone of the bay of bengal using 15 n 2 gas tracer incubation experiment during summer monsoon 2018. we collected seawater samples from four depths (covering the mixed layer depth of up to 75 m) at eight stations. n 2 fixation rates varied from 4 to 75 μ mol n m −2 d −1 . the contribution of n 2 fixation to primary production was negligible (&lt;1%). however, the upper bound of observed n 2 fixation rates is higher than the rates measured in other oceanic regimes, such as the eastern tropical south pacific, the tropical northwest atlantic, and the equatorial and southern indian ocean."
http://orkg.org/orkg/resource/R107843,Getting the Mix Right Again: An updated and theoretical rationale for interaction,10.19173/irrodl.v4i2.149 ,crossref,"<jats:p>No topic raises more contentious debate among educators than the role of interaction as a crucial component of the education process. This debate is fueled by surface problems of definition and vested interests of professional educators, but is more deeply marked by epistemological assumptions relative to the role of humans and human interaction in education and learning. The seminal article by Daniel and Marquis (1979) challenged distance educators to get the mixture right between independent study and interactive learning strategies and activities. They quite rightly pointed out that these two primary forms of education have differing economic, pedagogical, and social characteristics, and that we are unlikely to find a “perfect” mix that meets all learner and institutional needs across all curricula and content. Nonetheless, hard decisions have to be made.&#x0D;\nEven more than in 1979, the development of newer, cost effective technologies and the nearly ubiquitous (in developed countries) Net-based telecommunications system is transforming, at least, the cost and access implications of getting the mix right. Further, developments in social cognitive based learning theories are providing increased evidence of the importance of collaborative activity as a component of all forms of education – including those delivered at a distance. Finally, the context in which distance education is developed and delivered is changing in response to the capacity of the semantic Web (Berners-Lee, 1999) to support interaction, not only amongst humans, but also between and among autonomous agents and human beings.&#x0D;\nThus, the landscape and challenges of “getting the mix right” have not lessened in the past 25 years, and, in fact, have become even more complicated. This paper attempts to provide a theoretical rationale and guide for instructional designers and teachers interested in developing distance education systems that are both effective and efficient in meeting diverse student learning needs.</jats:p>","no topic raises more contentious debate among educators than the role of interaction as a crucial component of the education process. this debate is fueled by surface problems of definition and vested interests of professional educators, but is more deeply marked by epistemological assumptions relative to the role of humans and human interaction in education and learning. the seminal article by daniel and marquis (1979) challenged distance educators to get the mixture right between independent study and interactive learning strategies and activities. they quite rightly pointed out that these two primary forms of education have differing economic, pedagogical, and social characteristics, and that we are unlikely to find a “perfect” mix that meets all learner and institutional needs across all curricula and content. nonetheless, hard decisions have to be made.&#x0d;\neven more than in 1979, the development of newer, cost effective technologies and the nearly ubiquitous (in developed countries) net-based telecommunications system is transforming, at least, the cost and access implications of getting the mix right. further, developments in social cognitive based learning theories are providing increased evidence of the importance of collaborative activity as a component of all forms of education – including those delivered at a distance. finally, the context in which distance education is developed and delivered is changing in response to the capacity of the semantic web (berners-lee, 1999) to support interaction, not only amongst humans, but also between and among autonomous agents and human beings.&#x0d;\nthus, the landscape and challenges of “getting the mix right” have not lessened in the past 25 years, and, in fact, have become even more complicated. this paper attempts to provide a theoretical rationale and guide for instructional designers and teachers interested in developing distance education systems that are both effective and efficient in meeting diverse student learning needs."
http://orkg.org/orkg/resource/R107906,"Phytochemical and Physicochemical Properties of Leaf, Stem and Flowers of Luffa aegyptiaca (Johann Veslingius)",,crossref,"<jats:p>The leaf, stem and flowers of Luffa aegyptiaca were screened for their phytochemical and physicochemical properties. The phytochemical evaluation of the leaf, stem and flowers revealed the presence of saponins, tannins and cardiac glycosides. Alkaloids were only present in the flowers. Cyanogenetic glycosides and phlobatannin were absent in the leaf and stem respectively. The nutrient value shows that the leaves contain 10.01% of moisture, 0.78% of crude protein, 2.40% of lipids, 14.61% of crude fibre, 3.65%of ash and 48.02% of carbohydrate. The stem contain7.02% of moisture, 3.01% of crude protein, 4.50% of lipids, 8.10% of crude fibre, 2.50% of ash and 61% of carbohydrate. While the flowers contain4.01% of moisture, 0.5% of crude protein, 5.1% of lipids, 7.01% of crude fibre, 1.5% of ash and 45.1% of carbohydrate. These findings prove that Luffa aegyptiaca contains bioactive compounds that may be useful in nutrition and explains its popular use in traditional medicine in Nigeria.</jats:p>","the leaf, stem and flowers of luffa aegyptiaca were screened for their phytochemical and physicochemical properties. the phytochemical evaluation of the leaf, stem and flowers revealed the presence of saponins, tannins and cardiac glycosides. alkaloids were only present in the flowers. cyanogenetic glycosides and phlobatannin were absent in the leaf and stem respectively. the nutrient value shows that the leaves contain 10.01% of moisture, 0.78% of crude protein, 2.40% of lipids, 14.61% of crude fibre, 3.65%of ash and 48.02% of carbohydrate. the stem contain7.02% of moisture, 3.01% of crude protein, 4.50% of lipids, 8.10% of crude fibre, 2.50% of ash and 61% of carbohydrate. while the flowers contain4.01% of moisture, 0.5% of crude protein, 5.1% of lipids, 7.01% of crude fibre, 1.5% of ash and 45.1% of carbohydrate. these findings prove that luffa aegyptiaca contains bioactive compounds that may be useful in nutrition and explains its popular use in traditional medicine in nigeria."
http://orkg.org/orkg/resource/R109777,Staﬀ Shortage in German Intensive Care Units During the COVID-19 Pandemic - Not only a Sensed Dilemma: Results from a Nationwide Survey,10.21203/rs.3.rs-323586/v1,crossref,"<jats:title>Abstract</jats:title>\n        <jats:p><jats:bold>Background</jats:bold>: The surge in patients during the COVID-19 pandemic has exacerbated the looming problem of staﬀ shortage in German ICUs possibly leading to worse outcomes for patients. <jats:bold>Methods</jats:bold>: Within the German Evidence Ecosystem CEOsys network, we conducted an online national mixed-methods survey assessing the standard of care in German ICUs treating patients with COVID-19. <jats:bold>Results</jats:bold>: A total of 171 German ICUs reported a median ideal number of patients per intensivist of 8 (interquartile range, IQR = 3rd quartile - 1st quartile = 4.0) and per nurse of 2.0 (IQR = 1.0). For COVID-19 patients, the median target was a maximum of 6.0 (IQR = 2.0) patients per intensivist or 2.0 (IQR = 0.0) patients per nurse. Targets for intensivists were rarely met by 15.2% and never met by 3.5% of responding institutions. Targets for nursing staﬃng could rarely be met in 32.2% and never in 5.3% of responding institutions.<jats:bold>Conclusions</jats:bold>: Shortages of staﬃng in the critical care setting are eminent during the COVID-19 pandemic and might not only negatively aﬀect patient outcomes, but also staﬀ wellbeing and healthcare costs. A joint eﬀort that scrutinizes the demands and structures of our health care system seems fundamental to be prepared for the future.</jats:p>","abstract \n background : the surge in patients during the covid-19 pandemic has exacerbated the looming problem of staﬀ shortage in german icus possibly leading to worse outcomes for patients. methods : within the german evidence ecosystem ceosys network, we conducted an online national mixed-methods survey assessing the standard of care in german icus treating patients with covid-19. results : a total of 171 german icus reported a median ideal number of patients per intensivist of 8 (interquartile range, iqr = 3rd quartile - 1st quartile = 4.0) and per nurse of 2.0 (iqr = 1.0). for covid-19 patients, the median target was a maximum of 6.0 (iqr = 2.0) patients per intensivist or 2.0 (iqr = 0.0) patients per nurse. targets for intensivists were rarely met by 15.2% and never met by 3.5% of responding institutions. targets for nursing staﬃng could rarely be met in 32.2% and never in 5.3% of responding institutions. conclusions : shortages of staﬃng in the critical care setting are eminent during the covid-19 pandemic and might not only negatively aﬀect patient outcomes, but also staﬀ wellbeing and healthcare costs. a joint eﬀort that scrutinizes the demands and structures of our health care system seems fundamental to be prepared for the future."
http://orkg.org/orkg/resource/R109690,Reconfiguration of dominant coupling modes in mild traumatic brain injury mediated by δ-band activity: A resting state MEG study,10.1016/j.neuroscience.2017.05.032,crossref,"<jats:title>Abstract</jats:title><jats:p>During the last few years, rich-club (RC) organization has been studied as a possible brain-connectivity organization model for large-scale brain networks. At the same time, empirical and simulated data of neurophysiological models have demonstrated the significant role of intra-frequency and inter-frequency coupling among distinct brain areas. The current study investigates further the importance of these couplings using recordings of resting-state magnetoencephalographic activity obtained from 30 mild traumatic brain injury (mTBI) subjects and 50 healthy controls. Intra-frequency and inter-frequency coupling modes are incorporated in a single graph to detect group differences within individual rich-club subnetworks (type I networks) and networks connecting RC nodes with the rest of the nodes (type II networks). Our results show a higher probability of inter-frequency coupling for (δ−γ<jats:sub>1</jats:sub>), (δ−γ<jats:sub>2</jats:sub>), (θ−β), (θ−γ<jats:sub>2</jats:sub>), (α−γ<jats:sub>2</jats:sub>), (γ<jats:sub>1</jats:sub>−γ<jats:sub>2</jats:sub>) and intra-frequency coupling for (γ<jats:sub>1</jats:sub>−γ<jats:sub>1</jats:sub>) and (δ−δ) for both type I and type II networks in the mTBI group. Additionally, mTBI and control subjects can be correctly classified with high accuracy (98.6%), whereas a general linear regression model can effectively predict the subject group using the ratio of type I and type II coupling in the (δ, θ), (δ, β), (δ, γ<jats:sub>1</jats:sub>), and (δ, γ<jats:sub>2</jats:sub>) frequency pairs. These findings support the presence of an RC organization simultaneously with dominant frequency interactions within a single functional graph. Our results demonstrate a hyperactivation of intrinsic RC networks in mTBI subjects compared to controls, which can be seen as a plausible compensatory mechanism for alternative frequency-dependent routes of information flow in mTBI subjects.</jats:p>","abstract during the last few years, rich-club (rc) organization has been studied as a possible brain-connectivity organization model for large-scale brain networks. at the same time, empirical and simulated data of neurophysiological models have demonstrated the significant role of intra-frequency and inter-frequency coupling among distinct brain areas. the current study investigates further the importance of these couplings using recordings of resting-state magnetoencephalographic activity obtained from 30 mild traumatic brain injury (mtbi) subjects and 50 healthy controls. intra-frequency and inter-frequency coupling modes are incorporated in a single graph to detect group differences within individual rich-club subnetworks (type i networks) and networks connecting rc nodes with the rest of the nodes (type ii networks). our results show a higher probability of inter-frequency coupling for (δ−γ 1 ), (δ−γ 2 ), (θ−β), (θ−γ 2 ), (α−γ 2 ), (γ 1 −γ 2 ) and intra-frequency coupling for (γ 1 −γ 1 ) and (δ−δ) for both type i and type ii networks in the mtbi group. additionally, mtbi and control subjects can be correctly classified with high accuracy (98.6%), whereas a general linear regression model can effectively predict the subject group using the ratio of type i and type ii coupling in the (δ, θ), (δ, β), (δ, γ 1 ), and (δ, γ 2 ) frequency pairs. these findings support the presence of an rc organization simultaneously with dominant frequency interactions within a single functional graph. our results demonstrate a hyperactivation of intrinsic rc networks in mtbi subjects compared to controls, which can be seen as a plausible compensatory mechanism for alternative frequency-dependent routes of information flow in mtbi subjects."
http://orkg.org/orkg/resource/R111023,Access to divalent lanthanide NHC complexes by redox-transmetallation from silver and CO2 insertion reactions,10.1039/c8cc08120d,crossref,<p>Divalent NHC–lanthanide complexes were obtained by redox-transmetallation. Treatment with CO<sub>2</sub> led to insertion reactions without oxidation of the metal centre.</p>,divalent nhc–lanthanide complexes were obtained by redox-transmetallation. treatment with co 2 led to insertion reactions without oxidation of the metal centre.
http://orkg.org/orkg/resource/R108690,Open Science meets Food Modelling: Introducing the Food Modelling Journal (FMJ),10.3897/fmj.1.46561,crossref,"<jats:p>This Editorial describes the rationale, focus, scope and technology behind the newly launched, open access, innovative Food Modelling Journal (FMJ). The Journal is designed to publish those outputs of the research cycle that usually precede the publication of the research article, but have their own value and re-usability potential. Such outputs are methods, models, software and data. The Food Modelling Journal is launched by the AGINFRA+ community and is integrated with the AGINFRA+ Virtual Research Environment (VRE) to facilitate and streamline the authoring, peer review and publication of the manuscripts via the ARPHA Publishing Platform.</jats:p>","this editorial describes the rationale, focus, scope and technology behind the newly launched, open access, innovative food modelling journal (fmj). the journal is designed to publish those outputs of the research cycle that usually precede the publication of the research article, but have their own value and re-usability potential. such outputs are methods, models, software and data. the food modelling journal is launched by the aginfra+ community and is integrated with the aginfra+ virtual research environment (vre) to facilitate and streamline the authoring, peer review and publication of the manuscripts via the arpha publishing platform."
http://orkg.org/orkg/resource/R108321,Modelling knowledge transfer: A knowledge dynamics perspective,10.1177/1063293x15592185,crossref,"<jats:p> The increasing complexity in design activities leads designers to collaborate and share knowledge within distributed teams. This makes designers use systems such as knowledge management systems to reach their goal. In this article, our aim is to investigate on improving the use of knowledge management systems by defining a framework for modelling knowledge transfer in such context. The proposed framework is partly based on reuse of existing models found in the literature and on a participant observation methodology. Then, we tested this framework through several case studies presented in this article. These investigations enable us to observe, define and model more finely the knowledge dynamics that occur between knowledge workers and knowledge management systems. </jats:p>","the increasing complexity in design activities leads designers to collaborate and share knowledge within distributed teams. this makes designers use systems such as knowledge management systems to reach their goal. in this article, our aim is to investigate on improving the use of knowledge management systems by defining a framework for modelling knowledge transfer in such context. the proposed framework is partly based on reuse of existing models found in the literature and on a participant observation methodology. then, we tested this framework through several case studies presented in this article. these investigations enable us to observe, define and model more finely the knowledge dynamics that occur between knowledge workers and knowledge management systems."
http://orkg.org/orkg/resource/R109846,"Effects of grazing and climate warming on plant diversity, productivity and living state in the alpine rangelands and cultivated grasslands of the Qinghai-Tibetan Plateau",10.1071/rj14080,crossref,"<jats:p>\n\nOvergrazing and climate warming may be important drivers of alpine rangeland degradation in the Qinghai-Tibetan Plateau (QTP). In this study, the effects of grazing and experimental warming on the vegetation of cultivated grasslands, alpine steppe and alpine meadows on the QTP were investigated. The three treatments were a control, a warming treatment and a grazing treatment and were replicated three times on each vegetation type. The warming treatment was applied using fibreglass open-top chambers and the grazing treatment was continuous grazing by yaks at a moderately high stocking rate. Both grazing and warming negatively affected vegetation cover. Grazing reduced vegetation height while warming increased vegetation height. Grazing increased but warming reduced plant diversity. Grazing decreased and warming increased the aboveground plant biomass. Grazing increased the preferred forage species in native rangelands (alpine steppe and alpine meadow), while warming increased the preferred forage species in the cultivated grassland. Grazing reduced the vegetation living state (VLS) of all three alpine grasslands by nearly 70%, while warming reduced the VLS of the cultivated grassland and the alpine steppe by 32% and 56%, respectively, and promoted the VLS of the alpine meadow by 20.5%. It was concluded that overgrazing was the main driver of change to the alpine grassland vegetation on the QTP. The findings suggest that grazing regimes should be adapted in order for them to be sustainable in a warmer future.\n</jats:p>","\n\novergrazing and climate warming may be important drivers of alpine rangeland degradation in the qinghai-tibetan plateau (qtp). in this study, the effects of grazing and experimental warming on the vegetation of cultivated grasslands, alpine steppe and alpine meadows on the qtp were investigated. the three treatments were a control, a warming treatment and a grazing treatment and were replicated three times on each vegetation type. the warming treatment was applied using fibreglass open-top chambers and the grazing treatment was continuous grazing by yaks at a moderately high stocking rate. both grazing and warming negatively affected vegetation cover. grazing reduced vegetation height while warming increased vegetation height. grazing increased but warming reduced plant diversity. grazing decreased and warming increased the aboveground plant biomass. grazing increased the preferred forage species in native rangelands (alpine steppe and alpine meadow), while warming increased the preferred forage species in the cultivated grassland. grazing reduced the vegetation living state (vls) of all three alpine grasslands by nearly 70%, while warming reduced the vls of the cultivated grassland and the alpine steppe by 32% and 56%, respectively, and promoted the vls of the alpine meadow by 20.5%. it was concluded that overgrazing was the main driver of change to the alpine grassland vegetation on the qtp. the findings suggest that grazing regimes should be adapted in order for them to be sustainable in a warmer future.\n"
http://orkg.org/orkg/resource/R110083,Optimal Sizing and Scheduling of Hybrid Energy Systems: The Cases of Morona Santiago and the Galapagos Islands,10.3390/en13153933,crossref,"<jats:p>Hybrid energy systems (HESs) generate electricity from multiple energy sources that complement each other. Recently, due to the reduction in costs of photovoltaic (PV) modules and wind turbines, these types of systems have become economically competitive. In this study, a mathematical programming model is applied to evaluate the techno-economic feasibility of autonomous units located in two isolated areas of Ecuador: first, the province of Galapagos (subtropical island) and second, the province of Morona Santiago (Amazonian tropical forest). The two case studies suggest that HESs are potential solutions to reduce the dependence of rural villages on fossil fuels and viable mechanisms to bring electrical power to isolated communities in Ecuador. Our results reveal that not only from the economic but also from the environmental point of view, for the case of the Galapagos province, a hybrid energy system with a PV–wind–battery configuration and a levelized cost of energy (LCOE) equal to 0.36 $/kWh is the optimal energy supply system. For the case of Morona Santiago, a hybrid energy system with a PV–diesel–battery configuration and an LCOE equal to 0.37 $/kWh is the most suitable configuration to meet the load of a typical isolated community in Ecuador. The proposed optimization model can be used as a decision-support tool for evaluating the viability of autonomous HES projects at any other location.</jats:p>","hybrid energy systems (hess) generate electricity from multiple energy sources that complement each other. recently, due to the reduction in costs of photovoltaic (pv) modules and wind turbines, these types of systems have become economically competitive. in this study, a mathematical programming model is applied to evaluate the techno-economic feasibility of autonomous units located in two isolated areas of ecuador: first, the province of galapagos (subtropical island) and second, the province of morona santiago (amazonian tropical forest). the two case studies suggest that hess are potential solutions to reduce the dependence of rural villages on fossil fuels and viable mechanisms to bring electrical power to isolated communities in ecuador. our results reveal that not only from the economic but also from the environmental point of view, for the case of the galapagos province, a hybrid energy system with a pv–wind–battery configuration and a levelized cost of energy (lcoe) equal to 0.36 $/kwh is the optimal energy supply system. for the case of morona santiago, a hybrid energy system with a pv–diesel–battery configuration and an lcoe equal to 0.37 $/kwh is the most suitable configuration to meet the load of a typical isolated community in ecuador. the proposed optimization model can be used as a decision-support tool for evaluating the viability of autonomous hes projects at any other location."
http://orkg.org/orkg/resource/R108392,Parametrizing the Conditionally Gaussian Prior Model for Source Localization with Reference to the P20/N20 Component of Median Nerve SEP/SEF,,crossref,"<jats:p>In this article, we focused on developing the conditionally Gaussian hierarchical Bayesian model (CG-HBM), which forms a superclass of several inversion methods for source localization of brain activity using somatosensory evoked potential (SEP) and field (SEF) measurements. The goal of this proof-of-concept study was to improve the applicability of the CG-HBM as a superclass by proposing a robust approach for the parametrization of focal source scenarios. We aimed at a parametrization that is invariant with respect to altering the noise level and the source space size. The posterior difference between the gamma and inverse gamma hyperprior was minimized by optimizing the shape parameter, while a suitable range for the scale parameter can be obtained via the prior-over-measurement signal-to-noise ratio, which we introduce as a new concept in this study. In the source localization experiments, the primary generator of the P20/N20 component was detected in the Brodmann area 3b using the CG-HBM approach and a parameter range derived from the existing knowledge of the Tikhonov-regularized minimum norm estimate, i.e., the classical Gaussian prior model. Moreover, it seems that the detection of deep thalamic activity simultaneously with the P20/N20 component with the gamma hyperprior can be enhanced while using a close-to-optimal shape parameter value.</jats:p>","in this article, we focused on developing the conditionally gaussian hierarchical bayesian model (cg-hbm), which forms a superclass of several inversion methods for source localization of brain activity using somatosensory evoked potential (sep) and field (sef) measurements. the goal of this proof-of-concept study was to improve the applicability of the cg-hbm as a superclass by proposing a robust approach for the parametrization of focal source scenarios. we aimed at a parametrization that is invariant with respect to altering the noise level and the source space size. the posterior difference between the gamma and inverse gamma hyperprior was minimized by optimizing the shape parameter, while a suitable range for the scale parameter can be obtained via the prior-over-measurement signal-to-noise ratio, which we introduce as a new concept in this study. in the source localization experiments, the primary generator of the p20/n20 component was detected in the brodmann area 3b using the cg-hbm approach and a parameter range derived from the existing knowledge of the tikhonov-regularized minimum norm estimate, i.e., the classical gaussian prior model. moreover, it seems that the detection of deep thalamic activity simultaneously with the p20/n20 component with the gamma hyperprior can be enhanced while using a close-to-optimal shape parameter value."
http://orkg.org/orkg/resource/R108382,Data-driven Topological Filtering based on Orthogonal Minimal Spanning Trees: Application to Multi-Group MEG Resting-State Connectivity,,crossref,"<jats:title>Abstract</jats:title><jats:p>In the present study a novel data-driven topological filtering technique is introduced to derive the backbone of functional brain networks relying on orthogonal minimal spanning trees (OMST). The method aims to identify the essential functional connections to ensure optimal information flow via the objective criterion of global efficiency minus the cost of surviving connections. The OMST technique was applied to multichannel, resting-state neuromagnetic recordings from four groups of participants: healthy adults (n=50), adults who have suffered mild traumatic brain injury (n=30), typically developing children (n=27), and reading-disabled children (n=25). Weighted interactions between network nodes (sensors) were computed using an integrated approach of dominant intrinsic coupling modes based on two alternative metrics (symbolic mutual information and phase lag index), resulting in excellent discrimination of individual cases according to their group membership. Classification results using OMST-derived functional networks were clearly superior to results using either relative power spectrum features or functional networks derived through the conventional minimal spanning trees algorithm.</jats:p>","abstract in the present study a novel data-driven topological filtering technique is introduced to derive the backbone of functional brain networks relying on orthogonal minimal spanning trees (omst). the method aims to identify the essential functional connections to ensure optimal information flow via the objective criterion of global efficiency minus the cost of surviving connections. the omst technique was applied to multichannel, resting-state neuromagnetic recordings from four groups of participants: healthy adults (n=50), adults who have suffered mild traumatic brain injury (n=30), typically developing children (n=27), and reading-disabled children (n=25). weighted interactions between network nodes (sensors) were computed using an integrated approach of dominant intrinsic coupling modes based on two alternative metrics (symbolic mutual information and phase lag index), resulting in excellent discrimination of individual cases according to their group membership. classification results using omst-derived functional networks were clearly superior to results using either relative power spectrum features or functional networks derived through the conventional minimal spanning trees algorithm."
http://orkg.org/orkg/resource/R108494,Dynamic Outsourced Proofs of Retrievability Enabling Auditing Migration for Remote Storage Security,,crossref,"<jats:p>Remote data auditing service is important for mobile clients to guarantee the intactness of their outsourced data stored at cloud side. To relieve mobile client from the nonnegligible burden incurred by performing the frequent data auditing, more and more literatures propose that the execution of such data auditing should be migrated from mobile client to third-party auditor (TPA). However, existing public auditing schemes always assume that TPA is reliable, which is the potential risk for outsourced data security. Although Outsourced Proofs of Retrievability (OPOR) have been proposed to further protect against the malicious TPA and collusion among any two entities, the original OPOR scheme applies only to the static data, which is the limitation that should be solved for enabling data dynamics. In this paper, we design a novel authenticated data structure called bv23Tree, which enables client to batch-verify the indices and values of any number of appointed leaves all at once for efficiency. By utilizing bv23Tree and a hierarchical storage structure, we present the first solution for Dynamic OPOR (DOPOR), which extends the OPOR model to support dynamic updates of the outsourced data. Extensive security and performance analyses show the reliability and effectiveness of our proposed scheme.</jats:p>","remote data auditing service is important for mobile clients to guarantee the intactness of their outsourced data stored at cloud side. to relieve mobile client from the nonnegligible burden incurred by performing the frequent data auditing, more and more literatures propose that the execution of such data auditing should be migrated from mobile client to third-party auditor (tpa). however, existing public auditing schemes always assume that tpa is reliable, which is the potential risk for outsourced data security. although outsourced proofs of retrievability (opor) have been proposed to further protect against the malicious tpa and collusion among any two entities, the original opor scheme applies only to the static data, which is the limitation that should be solved for enabling data dynamics. in this paper, we design a novel authenticated data structure called bv23tree, which enables client to batch-verify the indices and values of any number of appointed leaves all at once for efficiency. by utilizing bv23tree and a hierarchical storage structure, we present the first solution for dynamic opor (dopor), which extends the opor model to support dynamic updates of the outsourced data. extensive security and performance analyses show the reliability and effectiveness of our proposed scheme."
http://orkg.org/orkg/resource/R109043,A DNA barcode library for the butterflies of North America,10.7717/peerj.11157,crossref,"<jats:p>Although the butterflies of North America have received considerable taxonomic attention, overlooked species and instances of hybridization continue to be revealed. The present study assembles a DNA barcode reference library for this fauna to identify groups whose patterns of sequence variation suggest the need for further taxonomic study. Based on 14,626 records from 814 species, DNA barcodes were obtained for 96% of the fauna. The maximum intraspecific distance averaged 1/4 the minimum distance to the nearest neighbor, producing a barcode gap in 76% of the species. Most species (80%) were monophyletic, the others were para- or polyphyletic. Although 15% of currently recognized species shared barcodes, the incidence of such taxa was far higher in regions exposed to Pleistocene glaciations than in those that were ice-free. Nearly 10% of species displayed high intraspecific variation (&gt;2.5%), suggesting the need for further investigation to assess potential cryptic diversity. Aside from aiding the identification of all life stages of North American butterflies, the reference library has provided new perspectives on the incidence of both cryptic and potentially over-split species, setting the stage for future studies that can further explore the evolutionary dynamics of this group.</jats:p>","although the butterflies of north america have received considerable taxonomic attention, overlooked species and instances of hybridization continue to be revealed. the present study assembles a dna barcode reference library for this fauna to identify groups whose patterns of sequence variation suggest the need for further taxonomic study. based on 14,626 records from 814 species, dna barcodes were obtained for 96% of the fauna. the maximum intraspecific distance averaged 1/4 the minimum distance to the nearest neighbor, producing a barcode gap in 76% of the species. most species (80%) were monophyletic, the others were para- or polyphyletic. although 15% of currently recognized species shared barcodes, the incidence of such taxa was far higher in regions exposed to pleistocene glaciations than in those that were ice-free. nearly 10% of species displayed high intraspecific variation (&gt;2.5%), suggesting the need for further investigation to assess potential cryptic diversity. aside from aiding the identification of all life stages of north american butterflies, the reference library has provided new perspectives on the incidence of both cryptic and potentially over-split species, setting the stage for future studies that can further explore the evolutionary dynamics of this group."
http://orkg.org/orkg/resource/R109205,Directing the development of constraint languages by checking constraints on rdf data,,crossref,"<jats:p> For research institutes, data libraries, and data archives, validating RDF data according to predefined constraints is a much sought-after feature, particularly as this is taken for granted in the XML world. Based on our work in two international working groups on RDF validation and jointly identified requirements to formulate constraints and validate RDF data, we have published 81 types of constraints that are required by various stakeholders for data applications. </jats:p><jats:p> In this paper, we evaluate the usability of identified constraint types for assessing RDF data quality by (1) collecting and classifying 115 constraints on vocabularies commonly used in the social, behavioral, and economic sciences, either from the vocabularies themselves or from domain experts, and (2) validating 15,694 data sets (4.26 billion triples) of research data against these constraints. We classify each constraint according to (1) the severity of occurring violations and (2) based on which types of constraint languages are able to express its constraint type. Based on the large-scale evaluation, we formulate several findings to direct the further development of constraint languages. </jats:p>","for research institutes, data libraries, and data archives, validating rdf data according to predefined constraints is a much sought-after feature, particularly as this is taken for granted in the xml world. based on our work in two international working groups on rdf validation and jointly identified requirements to formulate constraints and validate rdf data, we have published 81 types of constraints that are required by various stakeholders for data applications. in this paper, we evaluate the usability of identified constraint types for assessing rdf data quality by (1) collecting and classifying 115 constraints on vocabularies commonly used in the social, behavioral, and economic sciences, either from the vocabularies themselves or from domain experts, and (2) validating 15,694 data sets (4.26 billion triples) of research data against these constraints. we classify each constraint according to (1) the severity of occurring violations and (2) based on which types of constraint languages are able to express its constraint type. based on the large-scale evaluation, we formulate several findings to direct the further development of constraint languages."
http://orkg.org/orkg/resource/R108835,A Parallelization Scheme for New DPD-B Thermostats,10.1155/2013/579696,crossref,"<jats:p>This paper presents the MPI parallelization of a new algorithm—DPD-B thermostat—for molecular dynamics simulations. The presented results are using Martini Coarse Grained Water System. It should be taken into account that molecular dynamics simulations are time consuming. In some cases the running time varies from days to weeks and even months. Therefore, parallelization is one solution for reducing the execution time. The paper describes the new algorithm, the main characteristics of the MPI parallelization of the new algorithm, and the simulation performances.</jats:p>","this paper presents the mpi parallelization of a new algorithm—dpd-b thermostat—for molecular dynamics simulations. the presented results are using martini coarse grained water system. it should be taken into account that molecular dynamics simulations are time consuming. in some cases the running time varies from days to weeks and even months. therefore, parallelization is one solution for reducing the execution time. the paper describes the new algorithm, the main characteristics of the mpi parallelization of the new algorithm, and the simulation performances."
http://orkg.org/orkg/resource/R108865,Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction in realistic settings,10.1186/s12859-019-3284-5,crossref,"<jats:title>Abstract</jats:title><jats:sec>\n                <jats:title>Background</jats:title>\n                <jats:p>Current approaches to identifying drug-drug interactions (DDIs), include safety studies during drug development and post-marketing surveillance after approval, offer important opportunities to identify potential safety issues, but are unable to provide complete set of all possible DDIs. Thus, the drug discovery researchers and healthcare professionals might not be fully aware of potentially dangerous DDIs. Predicting potential drug-drug interaction helps reduce unanticipated drug interactions and drug development costs and optimizes the drug design process. Methods for prediction of DDIs have the tendency to report high accuracy but still have little impact on translational research due to systematic biases induced by networked/paired data. In this work, we aimed to present realistic evaluation settings to predict DDIs using knowledge graph embeddings. We propose a simple disjoint cross-validation scheme to evaluate drug-drug interaction predictions for the scenarios where the drugs have no known DDIs.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Results</jats:title>\n                <jats:p>We designed different evaluation settings to accurately assess the performance for predicting DDIs. The settings for disjoint cross-validation produced lower performance scores, as expected, but still were good at predicting the drug interactions. We have applied Logistic Regression, Naive Bayes and Random Forest on DrugBank knowledge graph with the 10-fold traditional cross validation using RDF2Vec, TransE and TransD. RDF2Vec with Skip-Gram generally surpasses other embedding methods. We also tested RDF2Vec on various drug knowledge graphs such as DrugBank, PharmGKB and KEGG to predict unknown drug-drug interactions. The performance was not enhanced significantly when an integrated knowledge graph including these three datasets was used.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Conclusion</jats:title>\n                <jats:p>We showed that the knowledge embeddings are powerful predictors and comparable to current state-of-the-art methods for inferring new DDIs. We addressed the evaluation biases by introducing drug-wise and pairwise disjoint test classes. Although the performance scores for drug-wise and pairwise disjoint seem to be low, the results can be considered to be realistic in predicting the interactions for drugs with limited interaction information.</jats:p>\n              </jats:sec>","abstract \n background \n current approaches to identifying drug-drug interactions (ddis), include safety studies during drug development and post-marketing surveillance after approval, offer important opportunities to identify potential safety issues, but are unable to provide complete set of all possible ddis. thus, the drug discovery researchers and healthcare professionals might not be fully aware of potentially dangerous ddis. predicting potential drug-drug interaction helps reduce unanticipated drug interactions and drug development costs and optimizes the drug design process. methods for prediction of ddis have the tendency to report high accuracy but still have little impact on translational research due to systematic biases induced by networked/paired data. in this work, we aimed to present realistic evaluation settings to predict ddis using knowledge graph embeddings. we propose a simple disjoint cross-validation scheme to evaluate drug-drug interaction predictions for the scenarios where the drugs have no known ddis. \n \n results \n we designed different evaluation settings to accurately assess the performance for predicting ddis. the settings for disjoint cross-validation produced lower performance scores, as expected, but still were good at predicting the drug interactions. we have applied logistic regression, naive bayes and random forest on drugbank knowledge graph with the 10-fold traditional cross validation using rdf2vec, transe and transd. rdf2vec with skip-gram generally surpasses other embedding methods. we also tested rdf2vec on various drug knowledge graphs such as drugbank, pharmgkb and kegg to predict unknown drug-drug interactions. the performance was not enhanced significantly when an integrated knowledge graph including these three datasets was used. \n \n conclusion \n we showed that the knowledge embeddings are powerful predictors and comparable to current state-of-the-art methods for inferring new ddis. we addressed the evaluation biases by introducing drug-wise and pairwise disjoint test classes. although the performance scores for drug-wise and pairwise disjoint seem to be low, the results can be considered to be realistic in predicting the interactions for drugs with limited interaction information. \n"
http://orkg.org/orkg/resource/R108885,Geographical index of concentration as an indicator of the spatial distribution of tourist attractions in Belgrade,10.5937/turizam25-27553,crossref,"<jats:p>The spatial structure of tourist attractions can be presented both qualitatively and quantitatively. One of the indicators of the spatial structure of tourism is the index of geographical concentration of tourist attractions. The geographical concentration of tourist attractions represents the ratio of the number of tourist attractions in the observed area and its structural parts and the total number of structural units of the analyzed area. This paper aims to determine the spatial distribution of attractions in the administrative territories of Belgrade municipalities and to establish correlations with tourist attendance. The number and spatial distribution of accommodation capacities are the largest in the central city municipalities so that the number of visitors is the largest in them. At the same time, the central city municipalities have the highest concentration of tourist attractions. For data collection, the authors used field research, OSM (Open Street Maps), Google maps, with software processing ArcGIS 10.2. The research results enabled the definition of the model of distribution of tourist attractions and indicated its application. This model of distribution of tourist attractions shows that they are mostly concentrated in the city center. This also means a small spatial connection of tourist attractions in the city center and peripheral parts.</jats:p>","the spatial structure of tourist attractions can be presented both qualitatively and quantitatively. one of the indicators of the spatial structure of tourism is the index of geographical concentration of tourist attractions. the geographical concentration of tourist attractions represents the ratio of the number of tourist attractions in the observed area and its structural parts and the total number of structural units of the analyzed area. this paper aims to determine the spatial distribution of attractions in the administrative territories of belgrade municipalities and to establish correlations with tourist attendance. the number and spatial distribution of accommodation capacities are the largest in the central city municipalities so that the number of visitors is the largest in them. at the same time, the central city municipalities have the highest concentration of tourist attractions. for data collection, the authors used field research, osm (open street maps), google maps, with software processing arcgis 10.2. the research results enabled the definition of the model of distribution of tourist attractions and indicated its application. this model of distribution of tourist attractions shows that they are mostly concentrated in the city center. this also means a small spatial connection of tourist attractions in the city center and peripheral parts."
http://orkg.org/orkg/resource/R108909,Conundrum or paradox: deconstructing the spurious case of water scarcity in the Himalayan Region through an institutional economics narrative,10.2166/wp.2018.115,crossref,"""<jats:title>Abstract</jats:title>\n               <jats:p>Water scarcity in mountain regions such as the Himalaya has been studied with a pre-existing notion of scarcity justified by decades of communities' suffering from physical water shortages combined by difficulties of access. The Eastern Himalayan Region (EHR) of India receives significantly high amounts of annual precipitation. Studies have nonetheless shown that this region faces a strange dissonance: an acute water scarcity in a supposedly ‘water-rich’ region. The main objective of this paper is to decipher various drivers of water scarcity by locating the contemporary history of water institutions within the development trajectory of the Darjeeling region, particularly Darjeeling Municipal Town in West Bengal, India. A key feature of the region's urban water governance that defines the water scarcity narrative is the multiplicity of water institutions and the intertwining of formal and informal institutions at various scales. These factors affect the availability of and basic access to domestic water by communities in various ways resulting in the creation of a preferred water bundle consisting of informal water markets over and above traditional sourcing from springs and the formal water supply from the town municipality.</jats:p>""",""" abstract \n water scarcity in mountain regions such as the himalaya has been studied with a pre-existing notion of scarcity justified by decades of communities' suffering from physical water shortages combined by difficulties of access. the eastern himalayan region (ehr) of india receives significantly high amounts of annual precipitation. studies have nonetheless shown that this region faces a strange dissonance: an acute water scarcity in a supposedly ‘water-rich’ region. the main objective of this paper is to decipher various drivers of water scarcity by locating the contemporary history of water institutions within the development trajectory of the darjeeling region, particularly darjeeling municipal town in west bengal, india. a key feature of the region's urban water governance that defines the water scarcity narrative is the multiplicity of water institutions and the intertwining of formal and informal institutions at various scales. these factors affect the availability of and basic access to domestic water by communities in various ways resulting in the creation of a preferred water bundle consisting of informal water markets over and above traditional sourcing from springs and the formal water supply from the town municipality. """
http://orkg.org/orkg/resource/R108960,Use of species delimitation approaches to tackle the cryptic diversity of an assemblage of high Andean butterflies (Lepidoptera: Papilionoidea),10.1139/gen-2020-0100,crossref,"<jats:p> Cryptic biological diversity has generated ambiguity in taxonomic and evolutionary studies. Single-locus methods and other approaches for species delimitation are useful for addressing this challenge, enabling the practical processing of large numbers of samples for identification and inventory purposes. This study analyzed an assemblage of high Andean butterflies using DNA barcoding and compared the identifications based on the current morphological taxonomy with three methods of species delimitation (automatic barcode gap discovery, generalized mixed Yule coalescent model, and Poisson tree processes). Sixteen potential cryptic species were recognized using these three methods, representing a net richness increase of 11.3% in the assemblage. A well-studied taxon of the genus Vanessa, which has a wide geographical distribution, appeared with the potential cryptic species that had a higher genetic differentiation at the local level than at the continental level. The analyses were useful for identifying the potential cryptic species in Pedaliodes and Forsterinaria complexes, which also show differentiation along altitudinal and latitudinal gradients. This genetic assessment of an entire assemblage of high Andean butterflies (Papilionoidea) provides baseline information for future research in a region characterized by high rates of endemism and population isolation. </jats:p>","cryptic biological diversity has generated ambiguity in taxonomic and evolutionary studies. single-locus methods and other approaches for species delimitation are useful for addressing this challenge, enabling the practical processing of large numbers of samples for identification and inventory purposes. this study analyzed an assemblage of high andean butterflies using dna barcoding and compared the identifications based on the current morphological taxonomy with three methods of species delimitation (automatic barcode gap discovery, generalized mixed yule coalescent model, and poisson tree processes). sixteen potential cryptic species were recognized using these three methods, representing a net richness increase of 11.3% in the assemblage. a well-studied taxon of the genus vanessa, which has a wide geographical distribution, appeared with the potential cryptic species that had a higher genetic differentiation at the local level than at the continental level. the analyses were useful for identifying the potential cryptic species in pedaliodes and forsterinaria complexes, which also show differentiation along altitudinal and latitudinal gradients. this genetic assessment of an entire assemblage of high andean butterflies (papilionoidea) provides baseline information for future research in a region characterized by high rates of endemism and population isolation."
http://orkg.org/orkg/resource/R109037,Drug–drug interaction prediction with Wasserstein Adversarial Autoencoder-based knowledge graph embeddings,10.1093/bib/bbaa256,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>An interaction between pharmacological agents can trigger unexpected adverse events. Capturing richer and more comprehensive information about drug–drug interactions (DDIs) is one of the key tasks in public health and drug development. Recently, several knowledge graph (KG) embedding approaches have received increasing attention in the DDI domain due to their capability of projecting drugs and interactions into a low-dimensional feature space for predicting links and classifying triplets. However, existing methods only apply a uniformly random mode to construct negative samples. As a consequence, these samples are often too simplistic to train an effective model. In this paper, we propose a new KG embedding framework by introducing adversarial autoencoders (AAEs) based on Wasserstein distances and Gumbel-Softmax relaxation for DDI tasks. In our framework, the autoencoder is employed to generate high-quality negative samples and the hidden vector of the autoencoder is regarded as a plausible drug candidate. Afterwards, the discriminator learns the embeddings of drugs and interactions based on both positive and negative triplets. Meanwhile, in order to solve vanishing gradient problems on the discrete representation—an inherent flaw in traditional generative models—we utilize the Gumbel-Softmax relaxation and the Wasserstein distance to train the embedding model steadily. We empirically evaluate our method on two tasks: link prediction and DDI classification. The experimental results show that our framework can attain significant improvements and noticeably outperform competitive baselines. Supplementary information: Supplementary data and code are available at https://github.com/dyf0631/AAE_FOR_KG.</jats:p>","abstract \n an interaction between pharmacological agents can trigger unexpected adverse events. capturing richer and more comprehensive information about drug–drug interactions (ddis) is one of the key tasks in public health and drug development. recently, several knowledge graph (kg) embedding approaches have received increasing attention in the ddi domain due to their capability of projecting drugs and interactions into a low-dimensional feature space for predicting links and classifying triplets. however, existing methods only apply a uniformly random mode to construct negative samples. as a consequence, these samples are often too simplistic to train an effective model. in this paper, we propose a new kg embedding framework by introducing adversarial autoencoders (aaes) based on wasserstein distances and gumbel-softmax relaxation for ddi tasks. in our framework, the autoencoder is employed to generate high-quality negative samples and the hidden vector of the autoencoder is regarded as a plausible drug candidate. afterwards, the discriminator learns the embeddings of drugs and interactions based on both positive and negative triplets. meanwhile, in order to solve vanishing gradient problems on the discrete representation—an inherent flaw in traditional generative models—we utilize the gumbel-softmax relaxation and the wasserstein distance to train the embedding model steadily. we empirically evaluate our method on two tasks: link prediction and ddi classification. the experimental results show that our framework can attain significant improvements and noticeably outperform competitive baselines. supplementary information: supplementary data and code are available at https://github.com/dyf0631/aae_for_kg."
http://orkg.org/orkg/resource/R109029,Deep learning improves prediction of drug–drug and drug–food interactions,10.1073/pnas.1803294115,crossref,"<jats:title>Significance</jats:title>\n          <jats:p>Drug interactions, including drug–drug interactions (DDIs) and drug–food constituent interactions, can trigger unexpected pharmacological effects such as adverse drug events (ADEs). Several existing methods predict drug interactions, but require detailed, but often unavailable drug information as inputs, such as drug targets. To this end, we present a computational framework DeepDDI that accurately predicts DDI types for given drug pairs and drug–food constituent pairs using only name and structural information as inputs. We show four applications of DeepDDI to better understand drug interactions, including prediction of DDI mechanisms causing ADEs, suggestion of alternative drug members for the intended pharmacological effects without negative health effects, prediction of the effects of food constituents on interacting drugs, and prediction of bioactivities of food constituents.</jats:p>","significance \n drug interactions, including drug–drug interactions (ddis) and drug–food constituent interactions, can trigger unexpected pharmacological effects such as adverse drug events (ades). several existing methods predict drug interactions, but require detailed, but often unavailable drug information as inputs, such as drug targets. to this end, we present a computational framework deepddi that accurately predicts ddi types for given drug pairs and drug–food constituent pairs using only name and structural information as inputs. we show four applications of deepddi to better understand drug interactions, including prediction of ddi mechanisms causing ades, suggestion of alternative drug members for the intended pharmacological effects without negative health effects, prediction of the effects of food constituents on interacting drugs, and prediction of bioactivities of food constituents."
http://orkg.org/orkg/resource/R109385,Alkaloids and flavonoids from African phytochemicals as potential inhibitors of SARS-Cov-2 RNA-dependent RNA polymerase: an in silico perspective,10.1177/2040206620984076,crossref,"<jats:p> Corona Virus Disease 2019 (COVID-19) is a pandemic caused by Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV-2). Exploiting the potentials of phytocompounds is an integral component of the international response to this pandemic. In this study, a virtual screening through molecular docking analysis was used to screen a total of 226 bioactive compounds from African herbs and medicinal plants for direct interactions with SARS-CoV-2 RNA-dependent RNA polymerase (RdRp). From these, 36 phytocompounds with binding affinities higher than the approved reference drugs (remdesivir and sobosivir), were further docked targeting the active sites of SARS-CoV-2, as well as SARS-CoV and HCV RdRp. A hit list of 7 compounds alongside two positive controls (remdesivir and sofosbuvir) and two negative controls (cinnamaldehyde and Thymoquinone) were further docked into the active site of 8 different conformations of SARS-CoV-2 RdRp gotten from molecular dynamics simulation (MDS) system equilibration. The top docked compounds were further subjected to predictive druglikeness and ADME/tox filtering analyses. Drugable alkaloids (10’–hydroxyusambarensine, cryptospirolepine, strychnopentamine) and flavonoids (usararotenoid A, and 12α-epi-millettosin), were reported to exhibit strong affinity binding and interactions with key amino acid residues in the catalytic site, the divalent-cation–binding site, and the NTP entry channel in the active region of the RdRp enzyme as the positive controls. These phytochemicals, in addition to other promising antivirals such as remdesivir and sofosbuvir, may be exploited towards the development of a cocktail of anti-coronavirus treatments in COVID-19. Experimental studies are recommended to validate these study. </jats:p>","corona virus disease 2019 (covid-19) is a pandemic caused by severe acute respiratory syndrome coronavirus-2 (sars-cov-2). exploiting the potentials of phytocompounds is an integral component of the international response to this pandemic. in this study, a virtual screening through molecular docking analysis was used to screen a total of 226 bioactive compounds from african herbs and medicinal plants for direct interactions with sars-cov-2 rna-dependent rna polymerase (rdrp). from these, 36 phytocompounds with binding affinities higher than the approved reference drugs (remdesivir and sobosivir), were further docked targeting the active sites of sars-cov-2, as well as sars-cov and hcv rdrp. a hit list of 7 compounds alongside two positive controls (remdesivir and sofosbuvir) and two negative controls (cinnamaldehyde and thymoquinone) were further docked into the active site of 8 different conformations of sars-cov-2 rdrp gotten from molecular dynamics simulation (mds) system equilibration. the top docked compounds were further subjected to predictive druglikeness and adme/tox filtering analyses. drugable alkaloids (10’–hydroxyusambarensine, cryptospirolepine, strychnopentamine) and flavonoids (usararotenoid a, and 12α-epi-millettosin), were reported to exhibit strong affinity binding and interactions with key amino acid residues in the catalytic site, the divalent-cation–binding site, and the ntp entry channel in the active region of the rdrp enzyme as the positive controls. these phytochemicals, in addition to other promising antivirals such as remdesivir and sofosbuvir, may be exploited towards the development of a cocktail of anti-coronavirus treatments in covid-19. experimental studies are recommended to validate these study."
http://orkg.org/orkg/resource/R109386,Alkaloids and flavonoids from African phytochemicals as potential inhibitors of SARS-Cov-2 RNA-dependent RNA polymerase: an in silico perspective,10.1177/2040206620984076,crossref,"<jats:p> Corona Virus Disease 2019 (COVID-19) is a pandemic caused by Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV-2). Exploiting the potentials of phytocompounds is an integral component of the international response to this pandemic. In this study, a virtual screening through molecular docking analysis was used to screen a total of 226 bioactive compounds from African herbs and medicinal plants for direct interactions with SARS-CoV-2 RNA-dependent RNA polymerase (RdRp). From these, 36 phytocompounds with binding affinities higher than the approved reference drugs (remdesivir and sobosivir), were further docked targeting the active sites of SARS-CoV-2, as well as SARS-CoV and HCV RdRp. A hit list of 7 compounds alongside two positive controls (remdesivir and sofosbuvir) and two negative controls (cinnamaldehyde and Thymoquinone) were further docked into the active site of 8 different conformations of SARS-CoV-2 RdRp gotten from molecular dynamics simulation (MDS) system equilibration. The top docked compounds were further subjected to predictive druglikeness and ADME/tox filtering analyses. Drugable alkaloids (10’–hydroxyusambarensine, cryptospirolepine, strychnopentamine) and flavonoids (usararotenoid A, and 12α-epi-millettosin), were reported to exhibit strong affinity binding and interactions with key amino acid residues in the catalytic site, the divalent-cation–binding site, and the NTP entry channel in the active region of the RdRp enzyme as the positive controls. These phytochemicals, in addition to other promising antivirals such as remdesivir and sofosbuvir, may be exploited towards the development of a cocktail of anti-coronavirus treatments in COVID-19. Experimental studies are recommended to validate these study. </jats:p>","corona virus disease 2019 (covid-19) is a pandemic caused by severe acute respiratory syndrome coronavirus-2 (sars-cov-2). exploiting the potentials of phytocompounds is an integral component of the international response to this pandemic. in this study, a virtual screening through molecular docking analysis was used to screen a total of 226 bioactive compounds from african herbs and medicinal plants for direct interactions with sars-cov-2 rna-dependent rna polymerase (rdrp). from these, 36 phytocompounds with binding affinities higher than the approved reference drugs (remdesivir and sobosivir), were further docked targeting the active sites of sars-cov-2, as well as sars-cov and hcv rdrp. a hit list of 7 compounds alongside two positive controls (remdesivir and sofosbuvir) and two negative controls (cinnamaldehyde and thymoquinone) were further docked into the active site of 8 different conformations of sars-cov-2 rdrp gotten from molecular dynamics simulation (mds) system equilibration. the top docked compounds were further subjected to predictive druglikeness and adme/tox filtering analyses. drugable alkaloids (10’–hydroxyusambarensine, cryptospirolepine, strychnopentamine) and flavonoids (usararotenoid a, and 12α-epi-millettosin), were reported to exhibit strong affinity binding and interactions with key amino acid residues in the catalytic site, the divalent-cation–binding site, and the ntp entry channel in the active region of the rdrp enzyme as the positive controls. these phytochemicals, in addition to other promising antivirals such as remdesivir and sofosbuvir, may be exploited towards the development of a cocktail of anti-coronavirus treatments in covid-19. experimental studies are recommended to validate these study."
http://orkg.org/orkg/resource/R109387,Alkaloids and flavonoids from African phytochemicals as potential inhibitors of SARS-Cov-2 RNA-dependent RNA polymerase: an in silico perspective,10.1177/2040206620984076,crossref,"<jats:p> Corona Virus Disease 2019 (COVID-19) is a pandemic caused by Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV-2). Exploiting the potentials of phytocompounds is an integral component of the international response to this pandemic. In this study, a virtual screening through molecular docking analysis was used to screen a total of 226 bioactive compounds from African herbs and medicinal plants for direct interactions with SARS-CoV-2 RNA-dependent RNA polymerase (RdRp). From these, 36 phytocompounds with binding affinities higher than the approved reference drugs (remdesivir and sobosivir), were further docked targeting the active sites of SARS-CoV-2, as well as SARS-CoV and HCV RdRp. A hit list of 7 compounds alongside two positive controls (remdesivir and sofosbuvir) and two negative controls (cinnamaldehyde and Thymoquinone) were further docked into the active site of 8 different conformations of SARS-CoV-2 RdRp gotten from molecular dynamics simulation (MDS) system equilibration. The top docked compounds were further subjected to predictive druglikeness and ADME/tox filtering analyses. Drugable alkaloids (10’–hydroxyusambarensine, cryptospirolepine, strychnopentamine) and flavonoids (usararotenoid A, and 12α-epi-millettosin), were reported to exhibit strong affinity binding and interactions with key amino acid residues in the catalytic site, the divalent-cation–binding site, and the NTP entry channel in the active region of the RdRp enzyme as the positive controls. These phytochemicals, in addition to other promising antivirals such as remdesivir and sofosbuvir, may be exploited towards the development of a cocktail of anti-coronavirus treatments in COVID-19. Experimental studies are recommended to validate these study. </jats:p>","corona virus disease 2019 (covid-19) is a pandemic caused by severe acute respiratory syndrome coronavirus-2 (sars-cov-2). exploiting the potentials of phytocompounds is an integral component of the international response to this pandemic. in this study, a virtual screening through molecular docking analysis was used to screen a total of 226 bioactive compounds from african herbs and medicinal plants for direct interactions with sars-cov-2 rna-dependent rna polymerase (rdrp). from these, 36 phytocompounds with binding affinities higher than the approved reference drugs (remdesivir and sobosivir), were further docked targeting the active sites of sars-cov-2, as well as sars-cov and hcv rdrp. a hit list of 7 compounds alongside two positive controls (remdesivir and sofosbuvir) and two negative controls (cinnamaldehyde and thymoquinone) were further docked into the active site of 8 different conformations of sars-cov-2 rdrp gotten from molecular dynamics simulation (mds) system equilibration. the top docked compounds were further subjected to predictive druglikeness and adme/tox filtering analyses. drugable alkaloids (10’–hydroxyusambarensine, cryptospirolepine, strychnopentamine) and flavonoids (usararotenoid a, and 12α-epi-millettosin), were reported to exhibit strong affinity binding and interactions with key amino acid residues in the catalytic site, the divalent-cation–binding site, and the ntp entry channel in the active region of the rdrp enzyme as the positive controls. these phytochemicals, in addition to other promising antivirals such as remdesivir and sofosbuvir, may be exploited towards the development of a cocktail of anti-coronavirus treatments in covid-19. experimental studies are recommended to validate these study."
http://orkg.org/orkg/resource/R109313,KGen: a knowledge graph generator from biomedical scientific literature,10.1186/s12911-020-01341-5,crossref,"<jats:title>Abstract</jats:title><jats:sec>\n                <jats:title>Background</jats:title>\n                <jats:p>Knowledge is often produced from data generated in scientific investigations. An ever-growing number of scientific studies in several domains result into a massive amount of data, from which obtaining new knowledge requires computational help. For example, Alzheimer’s Disease, a life-threatening degenerative disease that is not yet curable. As the scientific community strives to better understand it and find a cure, great amounts of data have been generated, and new knowledge can be produced. A proper representation of such knowledge brings great benefits to researchers, to the scientific community, and consequently, to society.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Methods</jats:title>\n                <jats:p>In this article, we study and evaluate a semi-automatic method that generates knowledge graphs (KGs) from biomedical texts in the scientific literature. Our solution explores natural language processing techniques with the aim of extracting and representing scientific literature knowledge encoded in KGs. Our method links entities and relations represented in KGs to concepts from existing biomedical ontologies available on the Web. We demonstrate the effectiveness of our method by generating KGs from unstructured texts obtained from a set of abstracts taken from scientific papers on the Alzheimer’s Disease. We involve physicians to compare our extracted triples from their manual extraction via their analysis of the abstracts. The evaluation further concerned a qualitative analysis by the physicians of the generated KGs with our software tool.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Results</jats:title>\n                <jats:p>The experimental results indicate the quality of the generated KGs. The proposed method extracts a great amount of triples, showing the effectiveness of our rule-based method employed in the identification of relations in texts. In addition, ontology links are successfully obtained, which demonstrates the effectiveness of the ontology linking method proposed in this investigation.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Conclusions</jats:title>\n                <jats:p>We demonstrate that our proposal is effective on building ontology-linked KGs representing the knowledge obtained from biomedical scientific texts. Such representation can add value to the research in various domains, enabling researchers to compare the occurrence of concepts from different studies. The KGs generated may pave the way to potential proposal of new theories based on data analysis to advance the state of the art in their research domains.</jats:p>\n              </jats:sec>","abstract \n background \n knowledge is often produced from data generated in scientific investigations. an ever-growing number of scientific studies in several domains result into a massive amount of data, from which obtaining new knowledge requires computational help. for example, alzheimer’s disease, a life-threatening degenerative disease that is not yet curable. as the scientific community strives to better understand it and find a cure, great amounts of data have been generated, and new knowledge can be produced. a proper representation of such knowledge brings great benefits to researchers, to the scientific community, and consequently, to society. \n \n methods \n in this article, we study and evaluate a semi-automatic method that generates knowledge graphs (kgs) from biomedical texts in the scientific literature. our solution explores natural language processing techniques with the aim of extracting and representing scientific literature knowledge encoded in kgs. our method links entities and relations represented in kgs to concepts from existing biomedical ontologies available on the web. we demonstrate the effectiveness of our method by generating kgs from unstructured texts obtained from a set of abstracts taken from scientific papers on the alzheimer’s disease. we involve physicians to compare our extracted triples from their manual extraction via their analysis of the abstracts. the evaluation further concerned a qualitative analysis by the physicians of the generated kgs with our software tool. \n \n results \n the experimental results indicate the quality of the generated kgs. the proposed method extracts a great amount of triples, showing the effectiveness of our rule-based method employed in the identification of relations in texts. in addition, ontology links are successfully obtained, which demonstrates the effectiveness of the ontology linking method proposed in this investigation. \n \n conclusions \n we demonstrate that our proposal is effective on building ontology-linked kgs representing the knowledge obtained from biomedical scientific texts. such representation can add value to the research in various domains, enabling researchers to compare the occurrence of concepts from different studies. the kgs generated may pave the way to potential proposal of new theories based on data analysis to advance the state of the art in their research domains. \n"
http://orkg.org/orkg/resource/R111098,The Future of e-Learning in Medical Education: Current Trend and Future Opportunity,https://dx.doi.org/10.3352%2Fjeehp.2006.3.3,crossref,"<jats:p>A wide range of e-learning modalities are widely integrated in medical education. However, some of the key questions related to the role of e-learning remain unanswered, such as (1) what is an effective approach to integrating technology into pre-clinical vs. clinical training?; (2) what evidence exists regarding the type and format of e-learning technology suitable for medical specialties and clinical settings?; (3) which design features are known to be effective in designing on-line patient simulation cases, tutorials, or clinical exams?; and (4) what guidelines exist for determining an appropriate blend of instructional strategies, including online learning, face-to-face instruction, and performance-based skill practices? Based on the existing literature and a variety of e-learning examples of synchronous learning tools and simulation technology, this paper addresses the following three questions: (1) what is the current trend of e-learning in medical education?; (2) what do we know about the effective use of e-learning?; and (3) what is the role of e-learning in facilitating newly emerging competency-based training? As e-learning continues to be widely integrated in training future physicians, it is critical that our efforts in conducting evaluative studies should target specific e-learning features that can best mediate intended learning goals and objectives. Without an evolving knowledge base on how best to design e-learning applications, the gap between what we know about technology use and how we deploy e-learning in training settings will continue to widen.</jats:p>","a wide range of e-learning modalities are widely integrated in medical education. however, some of the key questions related to the role of e-learning remain unanswered, such as (1) what is an effective approach to integrating technology into pre-clinical vs. clinical training?; (2) what evidence exists regarding the type and format of e-learning technology suitable for medical specialties and clinical settings?; (3) which design features are known to be effective in designing on-line patient simulation cases, tutorials, or clinical exams?; and (4) what guidelines exist for determining an appropriate blend of instructional strategies, including online learning, face-to-face instruction, and performance-based skill practices? based on the existing literature and a variety of e-learning examples of synchronous learning tools and simulation technology, this paper addresses the following three questions: (1) what is the current trend of e-learning in medical education?; (2) what do we know about the effective use of e-learning?; and (3) what is the role of e-learning in facilitating newly emerging competency-based training? as e-learning continues to be widely integrated in training future physicians, it is critical that our efforts in conducting evaluative studies should target specific e-learning features that can best mediate intended learning goals and objectives. without an evolving knowledge base on how best to design e-learning applications, the gap between what we know about technology use and how we deploy e-learning in training settings will continue to widen."
http://orkg.org/orkg/resource/R109340,Factors Influencing the Selection of Precision Farming Information Sources by Cotton Producers,10.22004/AG.ECON.117779,crossref,"<jats:p>Precision farming information demanded by cotton producers is provided by various suppliers, including consultants, farm input dealerships, University Extension systems, and media sources. Factors associated with the decisions to select among information sources to search for precision farming information are analyzed using a multivariate probit regression accounting for correlation among the different selection decisions. Factors influencing these decisions are age, education, and income. These findings should be valuable to precision farming information providers who may be able to better meet their target clientele needs.</jats:p>","precision farming information demanded by cotton producers is provided by various suppliers, including consultants, farm input dealerships, university extension systems, and media sources. factors associated with the decisions to select among information sources to search for precision farming information are analyzed using a multivariate probit regression accounting for correlation among the different selection decisions. factors influencing these decisions are age, education, and income. these findings should be valuable to precision farming information providers who may be able to better meet their target clientele needs."
http://orkg.org/orkg/resource/R109370,In silico Screening of Natural Compounds as Potential Inhibitors of SARS-CoV-2 Main Protease and Spike RBD: Targets for COVID-19,10.3389/fmolb.2020.599079,crossref,"<jats:p>Historically, plants have been sought after as bio-factories for the production of diverse chemical compounds that offer a multitude of possibilities to cure diseases. To combat the current pandemic coronavirus disease 2019 (COVID-19), plant-based natural compounds are explored for their potential to inhibit the SARS-CoV-2 (severe acute respiratory syndrome coronavirus 2), the cause of COVID-19. The present study is aimed at the investigation of antiviral action of several groups of phytoconstituents against SARS-CoV-2 using a molecular docking approach to inhibit Main Protease (Mpro) (PDB code: 6LU7) and spike (S) glycoprotein receptor binding domain (RBD) to ACE2 (PDB code: 6M0J) of SARS-CoV-2. For binding affinity evaluation, the docking scores were calculated using the Extra Precision (XP) protocol of the Glide docking module of Maestro. CovDock was also used to investigate covalent docking. The OPLS3e force field was used in simulations. The docking score was calculated by preferring the conformation of the ligand that has the lowest binding free energy (best pose). The results are indicative of better potential of solanine, acetoside, and rutin, as Mpro and spike glycoprotein RBD dual inhibitors. Acetoside and curcumin were found to inhibit Mpro covalently. Curcumin also possessed all the physicochemical and pharmacokinetic parameters in the range. Thus, phytochemicals like solanine, acetoside, rutin, and curcumin hold potential to be developed as treatment options against COVID-19.</jats:p>","historically, plants have been sought after as bio-factories for the production of diverse chemical compounds that offer a multitude of possibilities to cure diseases. to combat the current pandemic coronavirus disease 2019 (covid-19), plant-based natural compounds are explored for their potential to inhibit the sars-cov-2 (severe acute respiratory syndrome coronavirus 2), the cause of covid-19. the present study is aimed at the investigation of antiviral action of several groups of phytoconstituents against sars-cov-2 using a molecular docking approach to inhibit main protease (mpro) (pdb code: 6lu7) and spike (s) glycoprotein receptor binding domain (rbd) to ace2 (pdb code: 6m0j) of sars-cov-2. for binding affinity evaluation, the docking scores were calculated using the extra precision (xp) protocol of the glide docking module of maestro. covdock was also used to investigate covalent docking. the opls3e force field was used in simulations. the docking score was calculated by preferring the conformation of the ligand that has the lowest binding free energy (best pose). the results are indicative of better potential of solanine, acetoside, and rutin, as mpro and spike glycoprotein rbd dual inhibitors. acetoside and curcumin were found to inhibit mpro covalently. curcumin also possessed all the physicochemical and pharmacokinetic parameters in the range. thus, phytochemicals like solanine, acetoside, rutin, and curcumin hold potential to be developed as treatment options against covid-19."
http://orkg.org/orkg/resource/R109940,Prevalence of Shiga Toxin–Producing Escherichia coli in Beef Cattle,10.4315/0362-028x-68.10.2224,crossref,"<jats:p>A large number of Shiga toxin–producing Escherichia coli (STEC) strains have caused major outbreaks and sporadic cases of human illnesses, including mild diarrhea, bloody diarrhea, hemorrhagic colitis, and the life-threatening hemolytic uremic syndrome. These illnesses have been traced to both O157 and non-O157 STEC. In a large number of STEC-associated outbreaks, the infections were attributed to consumption of ground beef or other beef products contaminated with cattle feces. Thus, beef cattle are considered reservoirs of STEC and can pose significant health risks to humans. The global nature of the human food supply suggests that safety concerns with beef will continue and the challenges facing the beef industry will increase at the production and processing levels. To be prepared to address these concerns and challenges, it is critical to assess the role of beef cattle in human STEC infections. In this review, published reports on STEC in beef cattle were evaluated to achieve the following specific objectives: (i) assess the prevalence of STEC in beef cattle, and (ii) determine the potential health risks of STEC strains from beef cattle. The latter objective is critically important because many beef STEC isolates are highly virulent. Global testing of beef cattle feces revealed wide ranges of prevalence rates for O157 STEC (i.e., 0.2 to 27.8%) and non-O157 STEC (i.e., 2.1 to 70.1%). Of the 261 STEC serotypes found in beef cattle, 44 cause hemolytic uremic syndrome and 37 cause other illnesses.</jats:p>","a large number of shiga toxin–producing escherichia coli (stec) strains have caused major outbreaks and sporadic cases of human illnesses, including mild diarrhea, bloody diarrhea, hemorrhagic colitis, and the life-threatening hemolytic uremic syndrome. these illnesses have been traced to both o157 and non-o157 stec. in a large number of stec-associated outbreaks, the infections were attributed to consumption of ground beef or other beef products contaminated with cattle feces. thus, beef cattle are considered reservoirs of stec and can pose significant health risks to humans. the global nature of the human food supply suggests that safety concerns with beef will continue and the challenges facing the beef industry will increase at the production and processing levels. to be prepared to address these concerns and challenges, it is critical to assess the role of beef cattle in human stec infections. in this review, published reports on stec in beef cattle were evaluated to achieve the following specific objectives: (i) assess the prevalence of stec in beef cattle, and (ii) determine the potential health risks of stec strains from beef cattle. the latter objective is critically important because many beef stec isolates are highly virulent. global testing of beef cattle feces revealed wide ranges of prevalence rates for o157 stec (i.e., 0.2 to 27.8%) and non-o157 stec (i.e., 2.1 to 70.1%). of the 261 stec serotypes found in beef cattle, 44 cause hemolytic uremic syndrome and 37 cause other illnesses."
http://orkg.org/orkg/resource/R109945,Prevalence of Shiga Toxin–Producing Escherichia coli in Beef Cattle,10.4315/0362-028x-68.10.2224,crossref,"<jats:p>A large number of Shiga toxin–producing Escherichia coli (STEC) strains have caused major outbreaks and sporadic cases of human illnesses, including mild diarrhea, bloody diarrhea, hemorrhagic colitis, and the life-threatening hemolytic uremic syndrome. These illnesses have been traced to both O157 and non-O157 STEC. In a large number of STEC-associated outbreaks, the infections were attributed to consumption of ground beef or other beef products contaminated with cattle feces. Thus, beef cattle are considered reservoirs of STEC and can pose significant health risks to humans. The global nature of the human food supply suggests that safety concerns with beef will continue and the challenges facing the beef industry will increase at the production and processing levels. To be prepared to address these concerns and challenges, it is critical to assess the role of beef cattle in human STEC infections. In this review, published reports on STEC in beef cattle were evaluated to achieve the following specific objectives: (i) assess the prevalence of STEC in beef cattle, and (ii) determine the potential health risks of STEC strains from beef cattle. The latter objective is critically important because many beef STEC isolates are highly virulent. Global testing of beef cattle feces revealed wide ranges of prevalence rates for O157 STEC (i.e., 0.2 to 27.8%) and non-O157 STEC (i.e., 2.1 to 70.1%). Of the 261 STEC serotypes found in beef cattle, 44 cause hemolytic uremic syndrome and 37 cause other illnesses.</jats:p>","a large number of shiga toxin–producing escherichia coli (stec) strains have caused major outbreaks and sporadic cases of human illnesses, including mild diarrhea, bloody diarrhea, hemorrhagic colitis, and the life-threatening hemolytic uremic syndrome. these illnesses have been traced to both o157 and non-o157 stec. in a large number of stec-associated outbreaks, the infections were attributed to consumption of ground beef or other beef products contaminated with cattle feces. thus, beef cattle are considered reservoirs of stec and can pose significant health risks to humans. the global nature of the human food supply suggests that safety concerns with beef will continue and the challenges facing the beef industry will increase at the production and processing levels. to be prepared to address these concerns and challenges, it is critical to assess the role of beef cattle in human stec infections. in this review, published reports on stec in beef cattle were evaluated to achieve the following specific objectives: (i) assess the prevalence of stec in beef cattle, and (ii) determine the potential health risks of stec strains from beef cattle. the latter objective is critically important because many beef stec isolates are highly virulent. global testing of beef cattle feces revealed wide ranges of prevalence rates for o157 stec (i.e., 0.2 to 27.8%) and non-o157 stec (i.e., 2.1 to 70.1%). of the 261 stec serotypes found in beef cattle, 44 cause hemolytic uremic syndrome and 37 cause other illnesses."
http://orkg.org/orkg/resource/R109373,Progress in Developing Inhibitors of SARS-CoV-2 3C-Like Protease,10.3390/microorganisms8081250,crossref,"<jats:p>Coronavirus disease 2019 (COVID-19) is caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The viral outbreak started in late 2019 and rapidly became a serious health threat to the global population. COVID-19 was declared a pandemic by the World Health Organization in March 2020. Several therapeutic options have been adopted to prevent the spread of the virus. Although vaccines have been developed, antivirals are still needed to combat the infection of this virus. SARS-CoV-2 is an enveloped virus, and its genome encodes polyproteins that can be processed into structural and nonstructural proteins. Maturation of viral proteins requires cleavages by proteases. Therefore, the main protease (3 chymotrypsin-like protease (3CLpro) or Mpro) encoded by the viral genome is an attractive drug target because it plays an important role in cleaving viral polyproteins into functional proteins. Inhibiting this enzyme is an efficient strategy to block viral replication. Structural studies provide valuable insight into the function of this protease and structural basis for rational inhibitor design. In this review, we describe structural studies on the main protease of SARS-CoV-2. The strategies applied in developing inhibitors of the main protease of SARS-CoV-2 and currently available protein inhibitors are summarized. Due to the availability of high-resolution structures, structure-guided drug design will play an important role in developing antivirals. The availability of high-resolution structures, potent peptidic inhibitors, and diverse compound scaffolds indicate the feasibility of developing potent protease inhibitors as antivirals for COVID-19.</jats:p>","coronavirus disease 2019 (covid-19) is caused by severe acute respiratory syndrome coronavirus 2 (sars-cov-2). the viral outbreak started in late 2019 and rapidly became a serious health threat to the global population. covid-19 was declared a pandemic by the world health organization in march 2020. several therapeutic options have been adopted to prevent the spread of the virus. although vaccines have been developed, antivirals are still needed to combat the infection of this virus. sars-cov-2 is an enveloped virus, and its genome encodes polyproteins that can be processed into structural and nonstructural proteins. maturation of viral proteins requires cleavages by proteases. therefore, the main protease (3 chymotrypsin-like protease (3clpro) or mpro) encoded by the viral genome is an attractive drug target because it plays an important role in cleaving viral polyproteins into functional proteins. inhibiting this enzyme is an efficient strategy to block viral replication. structural studies provide valuable insight into the function of this protease and structural basis for rational inhibitor design. in this review, we describe structural studies on the main protease of sars-cov-2. the strategies applied in developing inhibitors of the main protease of sars-cov-2 and currently available protein inhibitors are summarized. due to the availability of high-resolution structures, structure-guided drug design will play an important role in developing antivirals. the availability of high-resolution structures, potent peptidic inhibitors, and diverse compound scaffolds indicate the feasibility of developing potent protease inhibitors as antivirals for covid-19."
http://orkg.org/orkg/resource/R109396,No nitrogen fixation in the Bay of Bengal?,10.5194/bg-17-851-2020,crossref,"""<jats:p>Abstract. The Bay of Bengal (BoB) has long stood as a biogeochemical enigma, with\nsubsurface waters containing extremely low, but persistent, concentrations\nof oxygen in the nanomolar range which – for some, yet unconstrained, reason –\nare prevented from becoming anoxic. One reason for this may be the low\nproductivity of the BoB waters due to nutrient limitation and the resulting\nlack of respiration of organic material at intermediate waters. Thus, the\nparameters determining primary production are key in understanding what\nprevents the BoB from developing anoxia. Primary productivity in the sunlit\nsurface layers of tropical oceans is mostly limited by the supply of\nreactive nitrogen through upwelling, riverine flux, atmospheric deposition,\nand biological dinitrogen (N2) fixation. In the BoB, a stable\nstratification limits nutrient supply via upwelling in the open waters, and\nriverine or atmospheric fluxes have been shown to support only less than one-quarter of the nitrogen for primary production. This leaves a large\nuncertainty for most of the BoB's nitrogen input, suggesting a potential\nrole of N2 fixation in those waters. Here, we present a survey of N2 fixation and carbon fixation in the BoB\nduring the winter monsoon season. We detected a community of N2 fixers\ncomparable to other oxygen minimum zone (OMZ) regions, with only a few\ncyanobacterial clades and a broad diversity of non-phototrophic N2\nfixers present throughout the water column (samples collected between 10\nand 560\u2009m water depth). While similar communities of N2 fixers were\nshown to actively fix N2 in other OMZs, N2 fixation rates were\nbelow the detection limit in our samples covering the water column between\nthe deep chlorophyll maximum and the OMZ. Consistent with this, no N2\nfixation signal was visible in δ15N signatures. We suggest that\nthe absence of N2 fixation may be a consequence of a micronutrient\nlimitation or of an O2 sensitivity of the OMZ diazotrophs in the BoB.\nExploring how the onset of N2 fixation by cyanobacteria compared to\nnon-phototrophic N2 fixers would impact on OMZ O2 concentrations,\na simple model exercise was carried out. We observed that both photic-zone-based and OMZ-based N2 fixation are very sensitive to even\nminimal changes in water column stratification, with stronger mixing\nincreasing organic matter production and export, which can exhaust\nremaining O2 traces in the BoB.\n                    </jats:p>""",""" abstract. the bay of bengal (bob) has long stood as a biogeochemical enigma, with\nsubsurface waters containing extremely low, but persistent, concentrations\nof oxygen in the nanomolar range which – for some, yet unconstrained, reason –\nare prevented from becoming anoxic. one reason for this may be the low\nproductivity of the bob waters due to nutrient limitation and the resulting\nlack of respiration of organic material at intermediate waters. thus, the\nparameters determining primary production are key in understanding what\nprevents the bob from developing anoxia. primary productivity in the sunlit\nsurface layers of tropical oceans is mostly limited by the supply of\nreactive nitrogen through upwelling, riverine flux, atmospheric deposition,\nand biological dinitrogen (n2) fixation. in the bob, a stable\nstratification limits nutrient supply via upwelling in the open waters, and\nriverine or atmospheric fluxes have been shown to support only less than one-quarter of the nitrogen for primary production. this leaves a large\nuncertainty for most of the bob's nitrogen input, suggesting a potential\nrole of n2 fixation in those waters. here, we present a survey of n2 fixation and carbon fixation in the bob\nduring the winter monsoon season. we detected a community of n2 fixers\ncomparable to other oxygen minimum zone (omz) regions, with only a few\ncyanobacterial clades and a broad diversity of non-phototrophic n2\nfixers present throughout the water column (samples collected between 10\nand 560\u2009m water depth). while similar communities of n2 fixers were\nshown to actively fix n2 in other omzs, n2 fixation rates were\nbelow the detection limit in our samples covering the water column between\nthe deep chlorophyll maximum and the omz. consistent with this, no n2\nfixation signal was visible in δ15n signatures. we suggest that\nthe absence of n2 fixation may be a consequence of a micronutrient\nlimitation or of an o2 sensitivity of the omz diazotrophs in the bob.\nexploring how the onset of n2 fixation by cyanobacteria compared to\nnon-phototrophic n2 fixers would impact on omz o2 concentrations,\na simple model exercise was carried out. we observed that both photic-zone-based and omz-based n2 fixation are very sensitive to even\nminimal changes in water column stratification, with stronger mixing\nincreasing organic matter production and export, which can exhaust\nremaining o2 traces in the bob.\n """
http://orkg.org/orkg/resource/R109475,"“Hamro Jhora, Hamro Pani” (Our Spring, Our Water): Water and the Politics of Appropriation of ‘Commons’ in Darjeeling Town, India",10.3126/hn.v22i0.18992,crossref,"<jats:p>Based on the study of Darjeeling Municipality, the paper engages with issues pertaining to understanding the matrixes of power relations involved in the supply of water in Darjeeling town in India. The discussions in the paper focuses on urbanization, the shrinking water resources, and increased demand for water on the one hand; and the role of local administration, the emergence of the water mafia, and the ‘Samaj’ (society) all contributing to a skewed and inequitable distribution of water and the assumption of proprietorship or the appropriation of water commons, culminating in the accentuation of water-rights deprivation in Darjeeling Municipal Area.\xa0HYDRO Nepal JournalJournal of Water Energy and EnvironmentIssue No: 22Page: 16-24Uploaded date: January 14, 2018</jats:p>","based on the study of darjeeling municipality, the paper engages with issues pertaining to understanding the matrixes of power relations involved in the supply of water in darjeeling town in india. the discussions in the paper focuses on urbanization, the shrinking water resources, and increased demand for water on the one hand; and the role of local administration, the emergence of the water mafia, and the ‘samaj’ (society) all contributing to a skewed and inequitable distribution of water and the assumption of proprietorship or the appropriation of water commons, culminating in the accentuation of water-rights deprivation in darjeeling municipal area.\xa0hydro nepal journaljournal of water energy and environmentissue no: 22page: 16-24uploaded date: january 14, 2018"
http://orkg.org/orkg/resource/R109854,A systematic metadata harvesting workflow for analysing scientific networks,10.7717/peerj-cs.421,crossref,"<jats:p>One of the disciplines behind the science of science is the study of scientific networks. This work focuses on scientific networks as a social network having different nodes and connections. Nodes can be represented by authors, articles or journals while connections by citation, co-citation or co-authorship. One of the challenges in creating scientific networks is the lack of publicly available comprehensive data set. It limits the variety of analyses on the same set of nodes of different scientific networks. To supplement such analyses we have worked on publicly available citation metadata from Crossref and OpenCitatons. Using this data a workflow is developed to create scientific networks. Analysis of these networks gives insights into academic research and scholarship. Different techniques of social network analysis have been applied in the literature to study these networks. It includes centrality analysis, community detection, and clustering coefficient. We have used metadata of Scientometrics journal, as a case study, to present our workflow. We did a sample run of the proposed workflow to identify prominent authors using centrality analysis. This work is not a bibliometric study of any field rather it presents replicable Python scripts to perform network analysis. With an increase in the popularity of open access and open metadata, we hypothesise that this workflow shall provide an avenue for understanding scientific scholarship in multiple dimensions.</jats:p>","one of the disciplines behind the science of science is the study of scientific networks. this work focuses on scientific networks as a social network having different nodes and connections. nodes can be represented by authors, articles or journals while connections by citation, co-citation or co-authorship. one of the challenges in creating scientific networks is the lack of publicly available comprehensive data set. it limits the variety of analyses on the same set of nodes of different scientific networks. to supplement such analyses we have worked on publicly available citation metadata from crossref and opencitatons. using this data a workflow is developed to create scientific networks. analysis of these networks gives insights into academic research and scholarship. different techniques of social network analysis have been applied in the literature to study these networks. it includes centrality analysis, community detection, and clustering coefficient. we have used metadata of scientometrics journal, as a case study, to present our workflow. we did a sample run of the proposed workflow to identify prominent authors using centrality analysis. this work is not a bibliometric study of any field rather it presents replicable python scripts to perform network analysis. with an increase in the popularity of open access and open metadata, we hypothesise that this workflow shall provide an avenue for understanding scientific scholarship in multiple dimensions."
http://orkg.org/orkg/resource/R109703,Simulation of Severe Accident Progression Using ROSHNI: A New Integrated Simulation Code for PHWR Severe Accidents,10.1115/icone2020-16633,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>As analysts still grapple with understanding core damage accident progression at Three Mile Island and Fukushima that caught the nuclear industry off-guard once too many times, one notices the very limited detail with which the large reactor cores of these subject reactors have been modelled in their severe accident simulation code packages. At the same time, modelling of CANDU severe accidents have largely borrowed from and suffered from the limitations of the same LWR codes (see IAEA TECDOC 1727) whose applications to PHWRs have poorly caught critical PHWR design specifics and vulnerabilities. As a result, accident management measures that have been instituted at CANDU PHWRs, while meeting the important industry objective of publically seeming to be doing something about lessons learnt from say Fukushima and showing that the reactor designs are oh so close to perfect and the off-site consequences of severe accidents happily benign.</jats:p>\n               <jats:p>Integrated PHWR severe accident progression and consequence assessment code ROSHNI can make a significant contribution to actual, practical understanding of severe accident progression in CANDU PHWRs, improving significantly on the other PHWR specific computer codes developed three decades ago when modeling decisions were constrained by limited computing power and poor understanding of and interest in severe core damage accidents. These codes force gross simplifications in reactor core modelling and do not adequately represent all the right CANDU core details, materials, fluids, vessels or phenomena. But they produce results that are familiar and palatable. They do, however to their credit, also excel in their computational speed, largely because they model and compute so little and with such un-necessary simplifications.</jats:p>\n               <jats:p>ROSHNI sheds most previous modelling simplifications and represents each of the 380 channels, 4560 bundle, 37 elements in four concentric ring, Zircaloy clad fuel geometry, materials and fluids more faithfully in a 2000 MW(Th) CANDU6 reactor. It can be used easily for other PHWRs with different number of fuel channels and bundles per each channel. Each of horizontal PHWR reactor channels with all their bundles, fuel rings, sheaths, appendages, end fittings and feeders are modelled and in detail that reflects large across core differences. While other codes model at best a few hundred core fuel entities, thermo-chemical transient behaviour of about 73,000 different fuel channel entities within the core is considered by ROSHNI simultaneously along with other 15,000 or so other flow path segments. At each location all known thermo-chemical and hydraulic phenomena are computed. With such detail, ROSHNI is able to provide information on their progressive and parallel thermo-chemical contribution to accident progression and a more realistic fission product release source term that would belie the miniscule one (100 TBq of Cs-137 or 0.15% of core inventory) used by EMOs now in Canada on recommendation of our national regulator CNSC. ROSHNI has an advanced, more CANDU specific consideration of each bundle transitioning to a solid debris behaviour in the Calandria vessel without reverting to a simplified molten corium formulation that happily ignores interaction of debris with vessel welds, further vessel failures and energetic interactions. The code is able to follow behaviour of each fuel bundle following its disassembly from the fuel channel and thus demonstrate that the gross assumption of a core collapse made in some analyses is wrong and misleading. It is able to thus demonstrate that PHWR core disassembly is not only gradual, it will be also be incomplete with a large number of low power, peripheral fuel channels never disassembling under most credible scenarios.</jats:p>\n               <jats:p>The code is designed to grow into and use its voluminous results in a severe accident simulator for operator training. It’s phenomenological models are able to examine design inadequacies / issues that affect accident progression and several simple to implement design improvements that have a profound effect on results. For example, an early pressure boundary failure due to inadequacy of heat sinks in a station blackout scenario can be examined along with the effect of improved and adequate over pressure protection.</jats:p>\n               <jats:p>A best effort code such as ROSHNI can be instrumental in identifying the risk reduction benefits of undertaking certain design, operational and accidental management improvements for PHWRs, with some of the multi-unit ones handicapped by poor pressurizer placement and leaky containments with vulnerable materials, poor overpressure protection, ad-hoc mitigation measures and limited instrumentation common to all CANDUs. Case in point is the PSA supported design and installed number of Hydrogen recombiners that are neither for the right gas (designed mysteriously for H2 instead of D2) or its potential release quantity (they are sparse and will cause explosions).</jats:p>\n               <jats:p>The paper presents ROSHNI results of simulations of a postulated station blackout scenario and sheds a light on the challenges ahead in minimizing risk from operation of these otherwise unique power reactors.</jats:p>","abstract \n as analysts still grapple with understanding core damage accident progression at three mile island and fukushima that caught the nuclear industry off-guard once too many times, one notices the very limited detail with which the large reactor cores of these subject reactors have been modelled in their severe accident simulation code packages. at the same time, modelling of candu severe accidents have largely borrowed from and suffered from the limitations of the same lwr codes (see iaea tecdoc 1727) whose applications to phwrs have poorly caught critical phwr design specifics and vulnerabilities. as a result, accident management measures that have been instituted at candu phwrs, while meeting the important industry objective of publically seeming to be doing something about lessons learnt from say fukushima and showing that the reactor designs are oh so close to perfect and the off-site consequences of severe accidents happily benign. \n integrated phwr severe accident progression and consequence assessment code roshni can make a significant contribution to actual, practical understanding of severe accident progression in candu phwrs, improving significantly on the other phwr specific computer codes developed three decades ago when modeling decisions were constrained by limited computing power and poor understanding of and interest in severe core damage accidents. these codes force gross simplifications in reactor core modelling and do not adequately represent all the right candu core details, materials, fluids, vessels or phenomena. but they produce results that are familiar and palatable. they do, however to their credit, also excel in their computational speed, largely because they model and compute so little and with such un-necessary simplifications. \n roshni sheds most previous modelling simplifications and represents each of the 380 channels, 4560 bundle, 37 elements in four concentric ring, zircaloy clad fuel geometry, materials and fluids more faithfully in a 2000 mw(th) candu6 reactor. it can be used easily for other phwrs with different number of fuel channels and bundles per each channel. each of horizontal phwr reactor channels with all their bundles, fuel rings, sheaths, appendages, end fittings and feeders are modelled and in detail that reflects large across core differences. while other codes model at best a few hundred core fuel entities, thermo-chemical transient behaviour of about 73,000 different fuel channel entities within the core is considered by roshni simultaneously along with other 15,000 or so other flow path segments. at each location all known thermo-chemical and hydraulic phenomena are computed. with such detail, roshni is able to provide information on their progressive and parallel thermo-chemical contribution to accident progression and a more realistic fission product release source term that would belie the miniscule one (100 tbq of cs-137 or 0.15% of core inventory) used by emos now in canada on recommendation of our national regulator cnsc. roshni has an advanced, more candu specific consideration of each bundle transitioning to a solid debris behaviour in the calandria vessel without reverting to a simplified molten corium formulation that happily ignores interaction of debris with vessel welds, further vessel failures and energetic interactions. the code is able to follow behaviour of each fuel bundle following its disassembly from the fuel channel and thus demonstrate that the gross assumption of a core collapse made in some analyses is wrong and misleading. it is able to thus demonstrate that phwr core disassembly is not only gradual, it will be also be incomplete with a large number of low power, peripheral fuel channels never disassembling under most credible scenarios. \n the code is designed to grow into and use its voluminous results in a severe accident simulator for operator training. it’s phenomenological models are able to examine design inadequacies / issues that affect accident progression and several simple to implement design improvements that have a profound effect on results. for example, an early pressure boundary failure due to inadequacy of heat sinks in a station blackout scenario can be examined along with the effect of improved and adequate over pressure protection. \n a best effort code such as roshni can be instrumental in identifying the risk reduction benefits of undertaking certain design, operational and accidental management improvements for phwrs, with some of the multi-unit ones handicapped by poor pressurizer placement and leaky containments with vulnerable materials, poor overpressure protection, ad-hoc mitigation measures and limited instrumentation common to all candus. case in point is the psa supported design and installed number of hydrogen recombiners that are neither for the right gas (designed mysteriously for h2 instead of d2) or its potential release quantity (they are sparse and will cause explosions). \n the paper presents roshni results of simulations of a postulated station blackout scenario and sheds a light on the challenges ahead in minimizing risk from operation of these otherwise unique power reactors."
http://orkg.org/orkg/resource/R109768,Case Report: Interferon-γ Restores Monocytic Human Leukocyte Antigen Receptor (mHLA-DR) in Severe COVID-19 With Acquired Immunosuppression Syndrome,10.3389/fimmu.2021.645124,crossref,"<jats:sec><jats:title>Background</jats:title><jats:p>The major histocompatibility complex (MHC) class II characterized by monocytes CD14+ expression of human leukocyte antigen receptors (HLA-DR), is essential for the synapse between innate and adaptive immune response in infectious disease. Its reduced expression is associated with a high risk of secondary infections in septic patients and can be safely corrected by Interferon-y (IFNy) injection. Coronavirus disease (COVID-19) induces an alteration of Interferon (IFN) genes expression potentially responsible for the observed low HLA-DR expression in circulating monocytes (mHLA-DR).</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>We report a case of one-time INFy injection (100 mcg s.c.) in a superinfected 61-year-old man with COVID-19–associated acute respiratory distress syndrome (ARDS), with monitoring of mHLA-DR expression and clinical tolerance.</jats:p></jats:sec><jats:sec><jats:title>Observations</jats:title><jats:p>Low mHLA-DR pretreatment expression (26.7%) was observed. IFNy therapy leading to a rapid increase in mHLA-DR expression (83.1%).</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>Severe ARDS in a COVID-19 patient has a deep reduction in mHLA-DR expression concomitantly with secondary infections. The unique IFNy injection was safe and led to a sharp increase in the expression of mHLA-DR. Based on immune and infection monitoring, more cases of severe COVID-19 patients with low mHLA-DR should be treated by IFNy to test the clinical effectiveness.</jats:p></jats:sec>","background the major histocompatibility complex (mhc) class ii characterized by monocytes cd14+ expression of human leukocyte antigen receptors (hla-dr), is essential for the synapse between innate and adaptive immune response in infectious disease. its reduced expression is associated with a high risk of secondary infections in septic patients and can be safely corrected by interferon-y (ifny) injection. coronavirus disease (covid-19) induces an alteration of interferon (ifn) genes expression potentially responsible for the observed low hla-dr expression in circulating monocytes (mhla-dr). methods we report a case of one-time infy injection (100 mcg s.c.) in a superinfected 61-year-old man with covid-19–associated acute respiratory distress syndrome (ards), with monitoring of mhla-dr expression and clinical tolerance. observations low mhla-dr pretreatment expression (26.7%) was observed. ifny therapy leading to a rapid increase in mhla-dr expression (83.1%). conclusions severe ards in a covid-19 patient has a deep reduction in mhla-dr expression concomitantly with secondary infections. the unique ifny injection was safe and led to a sharp increase in the expression of mhla-dr. based on immune and infection monitoring, more cases of severe covid-19 patients with low mhla-dr should be treated by ifny to test the clinical effectiveness."
http://orkg.org/orkg/resource/R110029,Trasjanka: A code of rural migrants in Minsk,10.1177/1367006909348678,crossref,"<jats:p> The article deals with an oral speech phenomenon widespread in the Republic of Belarus, where it is known as trasjanka. This code originated through constant contact between Russian and Belarusian, two closely related East Slavonic languages. Discussed are the main features of this code (as used in the city of Minsk), the sources of its origin, different linguistic definitions and the attitude towards this code from those who dwell in the city of Minsk. Special attention is paid to the problem of distinction between trasjanka and different forms of codeswitching, also widely used in the Minsk language community. </jats:p>","the article deals with an oral speech phenomenon widespread in the republic of belarus, where it is known as trasjanka. this code originated through constant contact between russian and belarusian, two closely related east slavonic languages. discussed are the main features of this code (as used in the city of minsk), the sources of its origin, different linguistic definitions and the attitude towards this code from those who dwell in the city of minsk. special attention is paid to the problem of distinction between trasjanka and different forms of codeswitching, also widely used in the minsk language community."
http://orkg.org/orkg/resource/R110357,Simultaneous laser doping and annealing to form lateral p–n junction diode structure on silicon carbide films,10.1177/25165984211016281,crossref,"<jats:p> Laser-assisted doping of intrinsic silicon carbide (SiC) films deposited on Si (100) substrates by pulsed laser deposition (PLD) method and its influence on simultaneous annealing of the thin film is studied. PLD grown intrinsic SiC films are transformed to p-type SiC and n-type SiC, using laser-assisted doping in aqueous aluminum chloride and phosphoric solutions, respectively. Simultaneous doping and annealing of the SiC film are observed during laser-assisted doping. By precisely positioning the selectively doped region, lateral p–n diodes are formed on the SiC films without using any mask. Electric characteristics confirmed the formation of a lateral p–n diode structure. Numerical analysis of temperature distribution along the depth of the SiC films explains the mechanism of simultaneous doping and annealing during the laser treatment. </jats:p>","laser-assisted doping of intrinsic silicon carbide (sic) films deposited on si (100) substrates by pulsed laser deposition (pld) method and its influence on simultaneous annealing of the thin film is studied. pld grown intrinsic sic films are transformed to p-type sic and n-type sic, using laser-assisted doping in aqueous aluminum chloride and phosphoric solutions, respectively. simultaneous doping and annealing of the sic film are observed during laser-assisted doping. by precisely positioning the selectively doped region, lateral p–n diodes are formed on the sic films without using any mask. electric characteristics confirmed the formation of a lateral p–n diode structure. numerical analysis of temperature distribution along the depth of the sic films explains the mechanism of simultaneous doping and annealing during the laser treatment."
http://orkg.org/orkg/resource/R110005,New Technique to Determine the Total Organic Carbon Based on Well Logs Using Artificial Neural Network (White Box),,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Total organic carbon (TOC) is the amount of carbon present in an organic compound and is often used as an essential factor for unconventional shale resources evaluation. The previous models for TOC determination were either based on density log data only and considered the presence of organic matter is proportional to the bulk density, or based on resistivity log, sonic or density logs as well as the formation level of maturity (LOM), where these models assumed a linear relation between resistivity and porosity logs. The average absolute deviation (ADD) of the previous model was not less than 1.20wt% of TOC with a coefficient of determination (R2) of less than 0.85.</jats:p>\n               <jats:p>The objective of this research is to develop new empirical correlation to determine the TOC based on well logs using artificial neural network for Barnett shale formation. Core TOC data (442 data point) and well logs (resistivity, gamma ray, sonic transit time, and bulk density) were used to develop the ANN model. For the first time, the ANN model will change to a white box by extracting the weights and biases of the model to form the empirical equation.</jats:p>\n               <jats:p>The results obtained showed that TOC is strong function of bulk density, and moderate function of gamma ray, compressional sonic time, and week function of deep resistivity. The developed ANN model is able to predict the TOC based on conventional log data with high accuracy (the ADD is 0.91wt% of TOC and R2 between estimated and actual TOC is 0.93). The developed empirical equation for TOC determination from the ANN model outperformed the previous available models, which had an ADD of 1.20 wt% or more and R2 of less than 0.85. The developed TOC model and equation can be applied using simple computer without the need for a specific software.</jats:p>\n               <jats:p>The novelty of this new research is the simplicity and high accuracy of the developed model for estimating the total organic carbon based on conventional log data. The developed empirical equation will help the geologists and reservoir engineers to predict the TOC without the need for hard lab work or complicated softwares.</jats:p>","abstract \n total organic carbon (toc) is the amount of carbon present in an organic compound and is often used as an essential factor for unconventional shale resources evaluation. the previous models for toc determination were either based on density log data only and considered the presence of organic matter is proportional to the bulk density, or based on resistivity log, sonic or density logs as well as the formation level of maturity (lom), where these models assumed a linear relation between resistivity and porosity logs. the average absolute deviation (add) of the previous model was not less than 1.20wt% of toc with a coefficient of determination (r2) of less than 0.85. \n the objective of this research is to develop new empirical correlation to determine the toc based on well logs using artificial neural network for barnett shale formation. core toc data (442 data point) and well logs (resistivity, gamma ray, sonic transit time, and bulk density) were used to develop the ann model. for the first time, the ann model will change to a white box by extracting the weights and biases of the model to form the empirical equation. \n the results obtained showed that toc is strong function of bulk density, and moderate function of gamma ray, compressional sonic time, and week function of deep resistivity. the developed ann model is able to predict the toc based on conventional log data with high accuracy (the add is 0.91wt% of toc and r2 between estimated and actual toc is 0.93). the developed empirical equation for toc determination from the ann model outperformed the previous available models, which had an add of 1.20 wt% or more and r2 of less than 0.85. the developed toc model and equation can be applied using simple computer without the need for a specific software. \n the novelty of this new research is the simplicity and high accuracy of the developed model for estimating the total organic carbon based on conventional log data. the developed empirical equation will help the geologists and reservoir engineers to predict the toc without the need for hard lab work or complicated softwares."
http://orkg.org/orkg/resource/R110617,Early Onset Intrauterine Growth Restriction in a Mouse Model of Gestational Hypercholesterolemia and Atherosclerosis,10.1155/2014/280497,crossref,"<jats:p>The susceptibility to develop atherosclerosis is increased by intrauterine growth restriction and prenatal exposure to maternal hypercholesterolemia. Here, we studied whether mouse gestational hypercholesterolemia and atherosclerosis affected fetal development and growth at different stages of gestation. Female LDLR KO mice fed a proatherogenic, high cholesterol (HC) diet for 3 weeks before conception and during pregnancy exhibited a significant increase in non-HDL cholesterol and developed atherosclerosis. At embryonic days 12.5 (E12.5), E15.5, and E18.5, maternal gestational hypercholesterolemia and atherosclerosis were associated to a 22–24% reduction in male and female fetal weight without alterations in fetal number/litter or morphology nor placental weight or structure. Feeding the HC diet exclusively at the periconceptional period did not alter fetal growth, suggesting that maternal hypercholesterolemia affected fetal weight only after implantation. Vitamin E supplementation (1,000\u2009UI of α-tocopherol/kg) of HC-fed females did not change the mean weight of E18.5 fetuses but reduced the percentage of fetuses exhibiting body weights below the 10th percentile of weight (HC: 90% vs. HC/VitE: 68%). In conclusion, our results showed that maternal gestational hypercholesterolemia and atherosclerosis in mice were associated to early onset fetal growth restriction and that dietary vitamin E supplementation had a beneficial impact on this condition.</jats:p>","the susceptibility to develop atherosclerosis is increased by intrauterine growth restriction and prenatal exposure to maternal hypercholesterolemia. here, we studied whether mouse gestational hypercholesterolemia and atherosclerosis affected fetal development and growth at different stages of gestation. female ldlr ko mice fed a proatherogenic, high cholesterol (hc) diet for 3 weeks before conception and during pregnancy exhibited a significant increase in non-hdl cholesterol and developed atherosclerosis. at embryonic days 12.5 (e12.5), e15.5, and e18.5, maternal gestational hypercholesterolemia and atherosclerosis were associated to a 22–24% reduction in male and female fetal weight without alterations in fetal number/litter or morphology nor placental weight or structure. feeding the hc diet exclusively at the periconceptional period did not alter fetal growth, suggesting that maternal hypercholesterolemia affected fetal weight only after implantation. vitamin e supplementation (1,000\u2009ui of α-tocopherol/kg) of hc-fed females did not change the mean weight of e18.5 fetuses but reduced the percentage of fetuses exhibiting body weights below the 10th percentile of weight (hc: 90% vs. hc/vite: 68%). in conclusion, our results showed that maternal gestational hypercholesterolemia and atherosclerosis in mice were associated to early onset fetal growth restriction and that dietary vitamin e supplementation had a beneficial impact on this condition."
http://orkg.org/orkg/resource/R110550,An embodied approach to music semantics,10.1177/10298649100140s104,crossref,"<jats:p> This paper looks at a framework for music semantics, based on the concept of embodied music cognition (Leman, 2007). The first part presents the representational viewpoint on semantics and why one should aim at transcending this. The second part deals with different types of semantics, ranging from more mental to more corporeal ones. The third part discusses issues of semantics related to embodiment. It is argued that the embodiment approach, by focusing on the role of the human body as mediator in a musical meaning formation process, may be a useful extension, and may perhaps be an alternative to the representational approach. An embodied approach to music semantics is fully embedded within an empirical and evidence-based research approach. Within this approach, methodologies from both the natural sciences and the human sciences are combined. It focuses on the human body as a mediator between mind and physical environment, thus allowing a fully multimodal approach to music semantics. </jats:p>","this paper looks at a framework for music semantics, based on the concept of embodied music cognition (leman, 2007). the first part presents the representational viewpoint on semantics and why one should aim at transcending this. the second part deals with different types of semantics, ranging from more mental to more corporeal ones. the third part discusses issues of semantics related to embodiment. it is argued that the embodiment approach, by focusing on the role of the human body as mediator in a musical meaning formation process, may be a useful extension, and may perhaps be an alternative to the representational approach. an embodied approach to music semantics is fully embedded within an empirical and evidence-based research approach. within this approach, methodologies from both the natural sciences and the human sciences are combined. it focuses on the human body as a mediator between mind and physical environment, thus allowing a fully multimodal approach to music semantics."
http://orkg.org/orkg/resource/R110342,Structure and optical properties of TiO2 thin films deposited by ALD method,10.1515/phys-2017-0137,crossref,<jats:title>Abstract</jats:title>\n               <jats:p>This paper presents the results of study on titanium dioxide thin films prepared by atomic layer deposition method on a silicon substrate. The changes of surface morphology have been observed in topographic images performed with the atomic force microscope (AFM) and scanning electron microscope (SEM). Obtained roughness parameters have been calculated with XEI Park Systems software. Qualitative studies of chemical composition were also performed using the energy dispersive spectrometer (EDS). The structure of titanium dioxide was investigated by X-ray crystallography. A variety of crystalline TiO<jats:sub>2</jats:sub> was also confirmed by using the Raman spectrometer. The optical reflection spectra have been measured with UV-Vis spectrophotometry.</jats:p>,abstract \n this paper presents the results of study on titanium dioxide thin films prepared by atomic layer deposition method on a silicon substrate. the changes of surface morphology have been observed in topographic images performed with the atomic force microscope (afm) and scanning electron microscope (sem). obtained roughness parameters have been calculated with xei park systems software. qualitative studies of chemical composition were also performed using the energy dispersive spectrometer (eds). the structure of titanium dioxide was investigated by x-ray crystallography. a variety of crystalline tio 2 was also confirmed by using the raman spectrometer. the optical reflection spectra have been measured with uv-vis spectrophotometry.
http://orkg.org/orkg/resource/R110409,Predicting drug resistance evolution: insights from antimicrobial peptides and antibiotics,10.1098/rspb.2017.2687,crossref,"<jats:p>Antibiotic resistance constitutes one of the most pressing public health concerns. Antimicrobial peptides (AMPs) of multicellular organisms are considered part of a solution to this problem, and AMPs produced by bacteria such as colistin are last-resort drugs. Importantly, AMPs differ from many antibiotics in their pharmacodynamic characteristics. Here we implement these differences within a theoretical framework to predict the evolution of resistance against AMPs and compare it to antibiotic resistance. Our analysis of resistance evolution finds that pharmacodynamic differences all combine to produce a much lower probability that resistance will evolve against AMPs. The finding can be generalized to all drugs with pharmacodynamics similar to AMPs. Pharmacodynamic concepts are familiar to most practitioners of medical microbiology, and data can be easily obtained for any drug or drug combination. Our theoretical and conceptual framework is, therefore, widely applicable and can help avoid resistance evolution if implemented in antibiotic stewardship schemes or the rational choice of new drug candidates.</jats:p>","antibiotic resistance constitutes one of the most pressing public health concerns. antimicrobial peptides (amps) of multicellular organisms are considered part of a solution to this problem, and amps produced by bacteria such as colistin are last-resort drugs. importantly, amps differ from many antibiotics in their pharmacodynamic characteristics. here we implement these differences within a theoretical framework to predict the evolution of resistance against amps and compare it to antibiotic resistance. our analysis of resistance evolution finds that pharmacodynamic differences all combine to produce a much lower probability that resistance will evolve against amps. the finding can be generalized to all drugs with pharmacodynamics similar to amps. pharmacodynamic concepts are familiar to most practitioners of medical microbiology, and data can be easily obtained for any drug or drug combination. our theoretical and conceptual framework is, therefore, widely applicable and can help avoid resistance evolution if implemented in antibiotic stewardship schemes or the rational choice of new drug candidates."
http://orkg.org/orkg/resource/R110364,Silicon Carbide and MRI: Towards Developing a MRI Safe Neural Interface,10.3390/mi12020126,crossref,"<jats:p>An essential method to investigate neuromodulation effects of an invasive neural interface (INI) is magnetic resonance imaging (MRI). Presently, MRI imaging of patients with neural implants is highly restricted in high field MRI (e.g., 3 T and higher) due to patient safety concerns. This results in lower resolution MRI images and, consequently, degrades the efficacy of MRI imaging for diagnostic purposes in these patients. Cubic silicon carbide (3C-SiC) is a biocompatible wide-band-gap semiconductor with a high thermal conductivity and magnetic susceptibility compatible with brain tissue. It also has modifiable electrical conductivity through doping level control. These properties can improve the MRI compliance of 3C-SiC INIs, specifically in high field MRI scanning. In this work, the MRI compliance of epitaxial SiC films grown on various Si wafers, used to implement a monolithic neural implant (all-SiC), was studied. Via finite element method (FEM) and Fourier-based simulations, the specific absorption rate (SAR), induced heating, and image artifacts caused by the portion of the implant within a brain tissue phantom located in a 7 T small animal MRI machine were estimated and measured. The specific goal was to compare implant materials; thus, the effect of leads outside the tissue was not considered. The results of the simulations were validated via phantom experiments in the same 7 T MRI system. The simulation and experimental results revealed that free-standing 3C-SiC films had little to no image artifacts compared to silicon and platinum reference materials inside the MRI at 7 T. In addition, FEM simulations predicted an ~30% SAR reduction for 3C-SiC compared to Pt. These initial simulations and experiments indicate an all-SiC INI may effectively reduce MRI induced heating and image artifacts in high field MRI. In order to evaluate the MRI safety of a closed-loop, fully functional all-SiC INI as per ISO/TS 10974:2018 standard, additional research and development is being conducted and will be reported at a later date.</jats:p>","an essential method to investigate neuromodulation effects of an invasive neural interface (ini) is magnetic resonance imaging (mri). presently, mri imaging of patients with neural implants is highly restricted in high field mri (e.g., 3 t and higher) due to patient safety concerns. this results in lower resolution mri images and, consequently, degrades the efficacy of mri imaging for diagnostic purposes in these patients. cubic silicon carbide (3c-sic) is a biocompatible wide-band-gap semiconductor with a high thermal conductivity and magnetic susceptibility compatible with brain tissue. it also has modifiable electrical conductivity through doping level control. these properties can improve the mri compliance of 3c-sic inis, specifically in high field mri scanning. in this work, the mri compliance of epitaxial sic films grown on various si wafers, used to implement a monolithic neural implant (all-sic), was studied. via finite element method (fem) and fourier-based simulations, the specific absorption rate (sar), induced heating, and image artifacts caused by the portion of the implant within a brain tissue phantom located in a 7 t small animal mri machine were estimated and measured. the specific goal was to compare implant materials; thus, the effect of leads outside the tissue was not considered. the results of the simulations were validated via phantom experiments in the same 7 t mri system. the simulation and experimental results revealed that free-standing 3c-sic films had little to no image artifacts compared to silicon and platinum reference materials inside the mri at 7 t. in addition, fem simulations predicted an ~30% sar reduction for 3c-sic compared to pt. these initial simulations and experiments indicate an all-sic ini may effectively reduce mri induced heating and image artifacts in high field mri. in order to evaluate the mri safety of a closed-loop, fully functional all-sic ini as per iso/ts 10974:2018 standard, additional research and development is being conducted and will be reported at a later date."
http://orkg.org/orkg/resource/R110400,Pharmacodynamic Evaluation of Factors Associated with the Development of Bacterial Resistance in Acutely Ill Patients during Therapy,10.1128/aac.42.3.521,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            The selection of bacterial resistance was examined in relationship to antibiotic pharmacokinetics (PK) and organism MICs in the patients from four nosocomial lower respiratory tract infection clinical trials. The evaluable database included 107 acutely ill patients, 128 pathogens, and five antimicrobial regimens. Antimicrobial pharmacokinetics were characterized by using serum concentrations, and culture and sensitivity tests were performed daily on tracheal aspirates to examine resistance. Pharmacodynamic (PD) models were developed to identify factors associated with the probability of developing bacterial resistance. Overall, in 32 of 128 (25%) initially susceptible cases resistance developed during therapy. An initial univariate screen and a classification and regression tree analysis identified the ratio of the area under the concentration-time curve from 0 to 24 h to the MIC (AUC\n            <jats:sub>0–24</jats:sub>\n            /MIC) as a significant predictor of the development of resistance (\n            <jats:italic>P</jats:italic>\n            &lt; 0.001). The final PK/PD model, a variant of the Hill equation, demonstrated that the probability of developing resistance during therapy increased significantly when antimicrobial exposure was at an AUC\n            <jats:sub>0–24</jats:sub>\n            /MIC ratio of less than 100. This relationship was observed across all treatments and within all organism groupings, with the exception of β-lactamase-producing gram-negative organisms (consistent with type I β-lactamase producers) treated with β-lactam monotherapy. Combination therapy resulted in much lower rates of resistance than monotherapy, probably because all of the combination regimens examined had an AUC\n            <jats:sub>0–24</jats:sub>\n            /MIC ratio in excess of 100. In summary, the selection of antimicrobial resistance appears to be strongly associated with suboptimal antimicrobial exposure, defined as an AUC\n            <jats:sub>0–24</jats:sub>\n            /MIC ratio of less than 100.\n          </jats:p>","abstract \n \n the selection of bacterial resistance was examined in relationship to antibiotic pharmacokinetics (pk) and organism mics in the patients from four nosocomial lower respiratory tract infection clinical trials. the evaluable database included 107 acutely ill patients, 128 pathogens, and five antimicrobial regimens. antimicrobial pharmacokinetics were characterized by using serum concentrations, and culture and sensitivity tests were performed daily on tracheal aspirates to examine resistance. pharmacodynamic (pd) models were developed to identify factors associated with the probability of developing bacterial resistance. overall, in 32 of 128 (25%) initially susceptible cases resistance developed during therapy. an initial univariate screen and a classification and regression tree analysis identified the ratio of the area under the concentration-time curve from 0 to 24 h to the mic (auc\n 0–24 \n /mic) as a significant predictor of the development of resistance (\n p \n &lt; 0.001). the final pk/pd model, a variant of the hill equation, demonstrated that the probability of developing resistance during therapy increased significantly when antimicrobial exposure was at an auc\n 0–24 \n /mic ratio of less than 100. this relationship was observed across all treatments and within all organism groupings, with the exception of β-lactamase-producing gram-negative organisms (consistent with type i β-lactamase producers) treated with β-lactam monotherapy. combination therapy resulted in much lower rates of resistance than monotherapy, probably because all of the combination regimens examined had an auc\n 0–24 \n /mic ratio in excess of 100. in summary, the selection of antimicrobial resistance appears to be strongly associated with suboptimal antimicrobial exposure, defined as an auc\n 0–24 \n /mic ratio of less than 100.\n"
http://orkg.org/orkg/resource/R110372,Fabrication of a Monolithic Implantable Neural Interface from Cubic Silicon Carbide,10.3390/mi10070430,crossref,"<jats:p>One of the main issues with micron-sized intracortical neural interfaces (INIs) is their long-term reliability, with one major factor stemming from the material failure caused by the heterogeneous integration of multiple materials used to realize the implant. Single crystalline cubic silicon carbide (3C-SiC) is a semiconductor material that has been long recognized for its mechanical robustness and chemical inertness. It has the benefit of demonstrated biocompatibility, which makes it a promising candidate for chronically-stable, implantable INIs. Here, we report on the fabrication and initial electrochemical characterization of a nearly monolithic, Michigan-style 3C-SiC microelectrode array (MEA) probe. The probe consists of a single 5 mm-long shank with 16 electrode sites. An ~8 µm-thick p-type 3C-SiC epilayer was grown on a silicon-on-insulator (SOI) wafer, which was followed by a ~2 µm-thick epilayer of heavily n-type (n+) 3C-SiC in order to form conductive traces and the electrode sites. Diodes formed between the p and n+ layers provided substrate isolation between the channels. A thin layer of amorphous silicon carbide (a-SiC) was deposited via plasma-enhanced chemical vapor deposition (PECVD) to insulate the surface of the probe from the external environment. Forming the probes on a SOI wafer supported the ease of probe removal from the handle wafer by simple immersion in HF, thus aiding in the manufacturability of the probes. Free-standing probes and planar single-ended test microelectrodes were fabricated from the same 3C-SiC epiwafers. Cyclic voltammetry (CV) and electrochemical impedance spectroscopy (EIS) were performed on test microelectrodes with an area of 491 µm2 in phosphate buffered saline (PBS) solution. The measurements showed an impedance magnitude of 165 kΩ ± 14.7 kΩ (mean ± standard deviation) at 1 kHz, anodic charge storage capacity (CSC) of 15.4 ± 1.46 mC/cm2, and a cathodic CSC of 15.2 ± 1.03 mC/cm2. Current-voltage tests were conducted to characterize the p-n diode, n-p-n junction isolation, and leakage currents. The turn-on voltage was determined to be on the order of ~1.4 V and the leakage current was less than 8 μArms. This all-SiC neural probe realizes nearly monolithic integration of device components to provide a likely neurocompatible INI that should mitigate long-term reliability issues associated with chronic implantation.</jats:p>","one of the main issues with micron-sized intracortical neural interfaces (inis) is their long-term reliability, with one major factor stemming from the material failure caused by the heterogeneous integration of multiple materials used to realize the implant. single crystalline cubic silicon carbide (3c-sic) is a semiconductor material that has been long recognized for its mechanical robustness and chemical inertness. it has the benefit of demonstrated biocompatibility, which makes it a promising candidate for chronically-stable, implantable inis. here, we report on the fabrication and initial electrochemical characterization of a nearly monolithic, michigan-style 3c-sic microelectrode array (mea) probe. the probe consists of a single 5 mm-long shank with 16 electrode sites. an ~8 µm-thick p-type 3c-sic epilayer was grown on a silicon-on-insulator (soi) wafer, which was followed by a ~2 µm-thick epilayer of heavily n-type (n+) 3c-sic in order to form conductive traces and the electrode sites. diodes formed between the p and n+ layers provided substrate isolation between the channels. a thin layer of amorphous silicon carbide (a-sic) was deposited via plasma-enhanced chemical vapor deposition (pecvd) to insulate the surface of the probe from the external environment. forming the probes on a soi wafer supported the ease of probe removal from the handle wafer by simple immersion in hf, thus aiding in the manufacturability of the probes. free-standing probes and planar single-ended test microelectrodes were fabricated from the same 3c-sic epiwafers. cyclic voltammetry (cv) and electrochemical impedance spectroscopy (eis) were performed on test microelectrodes with an area of 491 µm2 in phosphate buffered saline (pbs) solution. the measurements showed an impedance magnitude of 165 kω ± 14.7 kω (mean ± standard deviation) at 1 khz, anodic charge storage capacity (csc) of 15.4 ± 1.46 mc/cm2, and a cathodic csc of 15.2 ± 1.03 mc/cm2. current-voltage tests were conducted to characterize the p-n diode, n-p-n junction isolation, and leakage currents. the turn-on voltage was determined to be on the order of ~1.4 v and the leakage current was less than 8 μarms. this all-sic neural probe realizes nearly monolithic integration of device components to provide a likely neurocompatible ini that should mitigate long-term reliability issues associated with chronic implantation."
http://orkg.org/orkg/resource/R110387,Aldehyde dehydrogenase 2 activity and aldehydic load contribute to neuroinflammation and Alzheimer’s disease related pathology,10.1186/s40478-019-0839-7,crossref,"<jats:title>Abstract</jats:title><jats:p>Aldehyde dehydrogenase 2 deficiency (ALDH2*2) causes facial flushing in response to alcohol consumption in approximately 560 million East Asians. Recent meta-analysis demonstrated the potential link between ALDH2*2 mutation and Alzheimer’s Disease (AD). Other studies have linked chronic alcohol consumption as a risk factor for AD. In the present study, we show that fibroblasts of an AD patient that also has an ALDH2*2 mutation or overexpression of ALDH2*2 in fibroblasts derived from AD patients harboring ApoE ε4 allele exhibited increased aldehydic load, oxidative stress, and increased mitochondrial dysfunction relative to healthy subjects and exposure to ethanol exacerbated these dysfunctions. In an in vivo model, daily exposure of WT mice to ethanol for 11\u2009weeks resulted in mitochondrial dysfunction, oxidative stress and increased aldehyde levels in their brains and these pathologies were greater in ALDH2*2/*2 (homozygous) mice. Following chronic ethanol exposure, the levels of the AD-associated protein, amyloid-β, and neuroinflammation were higher in the brains of the ALDH2*2/*2 mice relative to WT. Cultured primary cortical neurons of ALDH2*2/*2 mice showed increased sensitivity to ethanol and there was a greater activation of their primary astrocytes relative to the responses of neurons or astrocytes from the WT mice. Importantly, an activator of ALDH2 and ALDH2*2, Alda-1, blunted the ethanol-induced increases in Aβ, and the neuroinflammation in vitro and in vivo. These data indicate that impairment in the metabolism of aldehydes, and specifically ethanol-derived acetaldehyde, is a contributor to AD associated pathology and highlights the likely risk of alcohol consumption in the general population and especially in East Asians that carry ALDH2*2 mutation.</jats:p>","abstract aldehyde dehydrogenase 2 deficiency (aldh2*2) causes facial flushing in response to alcohol consumption in approximately 560 million east asians. recent meta-analysis demonstrated the potential link between aldh2*2 mutation and alzheimer’s disease (ad). other studies have linked chronic alcohol consumption as a risk factor for ad. in the present study, we show that fibroblasts of an ad patient that also has an aldh2*2 mutation or overexpression of aldh2*2 in fibroblasts derived from ad patients harboring apoe ε4 allele exhibited increased aldehydic load, oxidative stress, and increased mitochondrial dysfunction relative to healthy subjects and exposure to ethanol exacerbated these dysfunctions. in an in vivo model, daily exposure of wt mice to ethanol for 11\u2009weeks resulted in mitochondrial dysfunction, oxidative stress and increased aldehyde levels in their brains and these pathologies were greater in aldh2*2/*2 (homozygous) mice. following chronic ethanol exposure, the levels of the ad-associated protein, amyloid-β, and neuroinflammation were higher in the brains of the aldh2*2/*2 mice relative to wt. cultured primary cortical neurons of aldh2*2/*2 mice showed increased sensitivity to ethanol and there was a greater activation of their primary astrocytes relative to the responses of neurons or astrocytes from the wt mice. importantly, an activator of aldh2 and aldh2*2, alda-1, blunted the ethanol-induced increases in aβ, and the neuroinflammation in vitro and in vivo. these data indicate that impairment in the metabolism of aldehydes, and specifically ethanol-derived acetaldehyde, is a contributor to ad associated pathology and highlights the likely risk of alcohol consumption in the general population and especially in east asians that carry aldh2*2 mutation."
http://orkg.org/orkg/resource/R110403,Evolution of Staphylococcus aureus under Vancomycin Selective Pressure: the Role of the Small-Colony Variant Phenotype,10.1128/aac.04508-14,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            <jats:named-content xmlns:xlink=""http://www.w3.org/1999/xlink"" content-type=""genus-species"" xlink:type=""simple"">Staphylococcus aureus</jats:named-content>\n            small-colony variants (SCVs) often persist despite antibiotic therapy. Against a 10\n            <jats:sup>8</jats:sup>\n            -CFU/ml methicillin-resistant\n            <jats:named-content xmlns:xlink=""http://www.w3.org/1999/xlink"" content-type=""genus-species"" xlink:type=""simple"">S. aureus</jats:named-content>\n            (MRSA) (strain COL) population of which 0%, 1%, 10%, 50%, or 100% was an isogenic\n            <jats:italic>hemB</jats:italic>\n            knockout (Ia48) subpopulation displaying the SCV phenotype, vancomycin achieved maximal reductions of 4.99, 5.39, 4.50, 3.28, and 1.66 log\n            <jats:sub>10</jats:sub>\n            CFU/ml over 48 h. Vancomycin at ≥16 mg/liter shifted a population from 50% SCV cells at 0 h to 100% SCV cells at 48 h, which was well characterized by a Hill-type model (\n            <jats:italic>R</jats:italic>\n            <jats:sup>2</jats:sup>\n            &gt; 0.90).\n          </jats:p>","abstract \n \n staphylococcus aureus \n small-colony variants (scvs) often persist despite antibiotic therapy. against a 10\n 8 \n -cfu/ml methicillin-resistant\n s. aureus \n (mrsa) (strain col) population of which 0%, 1%, 10%, 50%, or 100% was an isogenic\n hemb \n knockout (ia48) subpopulation displaying the scv phenotype, vancomycin achieved maximal reductions of 4.99, 5.39, 4.50, 3.28, and 1.66 log\n 10 \n cfu/ml over 48 h. vancomycin at ≥16 mg/liter shifted a population from 50% scv cells at 0 h to 100% scv cells at 48 h, which was well characterized by a hill-type model (\n r \n 2 \n &gt; 0.90).\n"
http://orkg.org/orkg/resource/R110414,Predicting drug resistance evolution: insights from antimicrobial peptides and antibiotics,10.1098/rspb.2017.2687,crossref,"<jats:p>Antibiotic resistance constitutes one of the most pressing public health concerns. Antimicrobial peptides (AMPs) of multicellular organisms are considered part of a solution to this problem, and AMPs produced by bacteria such as colistin are last-resort drugs. Importantly, AMPs differ from many antibiotics in their pharmacodynamic characteristics. Here we implement these differences within a theoretical framework to predict the evolution of resistance against AMPs and compare it to antibiotic resistance. Our analysis of resistance evolution finds that pharmacodynamic differences all combine to produce a much lower probability that resistance will evolve against AMPs. The finding can be generalized to all drugs with pharmacodynamics similar to AMPs. Pharmacodynamic concepts are familiar to most practitioners of medical microbiology, and data can be easily obtained for any drug or drug combination. Our theoretical and conceptual framework is, therefore, widely applicable and can help avoid resistance evolution if implemented in antibiotic stewardship schemes or the rational choice of new drug candidates.</jats:p>","antibiotic resistance constitutes one of the most pressing public health concerns. antimicrobial peptides (amps) of multicellular organisms are considered part of a solution to this problem, and amps produced by bacteria such as colistin are last-resort drugs. importantly, amps differ from many antibiotics in their pharmacodynamic characteristics. here we implement these differences within a theoretical framework to predict the evolution of resistance against amps and compare it to antibiotic resistance. our analysis of resistance evolution finds that pharmacodynamic differences all combine to produce a much lower probability that resistance will evolve against amps. the finding can be generalized to all drugs with pharmacodynamics similar to amps. pharmacodynamic concepts are familiar to most practitioners of medical microbiology, and data can be easily obtained for any drug or drug combination. our theoretical and conceptual framework is, therefore, widely applicable and can help avoid resistance evolution if implemented in antibiotic stewardship schemes or the rational choice of new drug candidates."
http://orkg.org/orkg/resource/R110448,The effect of social distance measures on COVID-19 epidemics in Europe: an interrupted time series analysis,10.1007/s11357-020-00205-0,crossref,"<jats:title>Abstract</jats:title><jats:p>Following the introduction of unprecedented “stay-at-home” national policies, the COVID-19 pandemic recently started declining in Europe. Our research aims were to characterize the changepoint in the flow of the COVID-19 epidemic in each European country and to evaluate the association of the level of social distancing with the observed decline in the national epidemics. Interrupted time series analyses were conducted in 28 European countries. Social distance index was calculated based on Google Community Mobility Reports. Changepoints were estimated by threshold regression, national findings were analyzed by Poisson regression, and the effect of social distancing in mixed effects Poisson regression model. Our findings identified the most probable changepoints in 28 European countries. Before changepoint, incidence of new COVID-19 cases grew by 24% per day on average. From the changepoint, this growth rate was reduced to 0.9%, 0.3% increase, and to 0.7% and 1.7% decrease by increasing social distancing quartiles. The beneficial effect of higher social distance quartiles (i.e., turning the increase into decline) was statistically significant for the fourth quartile. Notably, many countries in lower quartiles also achieved a flat epidemic curve. In these countries, other plausible COVID-19 containment measures could contribute to controlling the first wave of the disease. The association of social distance quartiles with viral spread could also be hindered by local bottlenecks in infection control. Our results allow for moderate optimism related to the gradual lifting of social distance measures in the general population, and call for specific attention to the protection of focal micro-societies enriching high-risk elderly subjects, including nursing homes and chronic care facilities.</jats:p>","abstract following the introduction of unprecedented “stay-at-home” national policies, the covid-19 pandemic recently started declining in europe. our research aims were to characterize the changepoint in the flow of the covid-19 epidemic in each european country and to evaluate the association of the level of social distancing with the observed decline in the national epidemics. interrupted time series analyses were conducted in 28 european countries. social distance index was calculated based on google community mobility reports. changepoints were estimated by threshold regression, national findings were analyzed by poisson regression, and the effect of social distancing in mixed effects poisson regression model. our findings identified the most probable changepoints in 28 european countries. before changepoint, incidence of new covid-19 cases grew by 24% per day on average. from the changepoint, this growth rate was reduced to 0.9%, 0.3% increase, and to 0.7% and 1.7% decrease by increasing social distancing quartiles. the beneficial effect of higher social distance quartiles (i.e., turning the increase into decline) was statistically significant for the fourth quartile. notably, many countries in lower quartiles also achieved a flat epidemic curve. in these countries, other plausible covid-19 containment measures could contribute to controlling the first wave of the disease. the association of social distance quartiles with viral spread could also be hindered by local bottlenecks in infection control. our results allow for moderate optimism related to the gradual lifting of social distance measures in the general population, and call for specific attention to the protection of focal micro-societies enriching high-risk elderly subjects, including nursing homes and chronic care facilities."
http://orkg.org/orkg/resource/R110453,Social isolation and the speed of covid-19 cases: measures to prevent transmission,10.1590/1983-1447.2021.20200238,crossref,"""<jats:p>ABSTRACT  Objective: To evaluate the social isolation index and the speed of new cases of Covid-19 in Brazil.  Methods: Quantitative ecological, documentary, descriptive study using secondary data, comparing the period from March 14 to May 1, 2020, carried out with the 27 Brazilian federative units, characterizing the study population. The data were analyzed through descriptive statistics using the Statistical Package for the Social Sciences-SPSS® software, evaluating the correlation between the social isolation index and the number of new cases of Covid-19, using Pearson’s correlation coefficient.  Results: The increase in Covid-19 cases is exponential. There was a significant, negative correlation regarding the social isolation index and the speed of the number of new cases by Pearson's coefficient, which means that as the first one increases, the second one decreases.  Conclusion: Social isolation measures have significant effects on the rate of coronavirus infection in the population.</jats:p>""",""" abstract objective: to evaluate the social isolation index and the speed of new cases of covid-19 in brazil. methods: quantitative ecological, documentary, descriptive study using secondary data, comparing the period from march 14 to may 1, 2020, carried out with the 27 brazilian federative units, characterizing the study population. the data were analyzed through descriptive statistics using the statistical package for the social sciences-spss® software, evaluating the correlation between the social isolation index and the number of new cases of covid-19, using pearson’s correlation coefficient. results: the increase in covid-19 cases is exponential. there was a significant, negative correlation regarding the social isolation index and the speed of the number of new cases by pearson's coefficient, which means that as the first one increases, the second one decreases. conclusion: social isolation measures have significant effects on the rate of coronavirus infection in the population. """
http://orkg.org/orkg/resource/R110432,Driving Simulators for Human Vehicle Interaction Design,10.1515/cplbu-2015-0034,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>The interaction human-vehicle, as well as driver’s behavior are subject long debated in the automotive engineering domain. Driving simulators have an extraordinary important role allowing research that would not be possible to study in real world scenarios.</jats:p>\n               <jats:p>A driver uses his sensory inputs to obtain the required input to base his decision on. The bandwidth of the required input signal should be in accordance to the driver’s task. For simple tasks, like turning on the screen wipers or direction indicator, low frequency information is sufficient. High frequency information is required when cornering on a busy road or when driving in relatively limit situations.</jats:p>\n               <jats:p>The optimal configuration of each sub-system remains a significant cause for debate and still poses a major challenge when considering the ability of simulators to extract realistic driver behavior. If a difference is observed between real and virtual conditions, the factors specifically cause these differences are very difficult to be explained.</jats:p>","abstract \n the interaction human-vehicle, as well as driver’s behavior are subject long debated in the automotive engineering domain. driving simulators have an extraordinary important role allowing research that would not be possible to study in real world scenarios. \n a driver uses his sensory inputs to obtain the required input to base his decision on. the bandwidth of the required input signal should be in accordance to the driver’s task. for simple tasks, like turning on the screen wipers or direction indicator, low frequency information is sufficient. high frequency information is required when cornering on a busy road or when driving in relatively limit situations. \n the optimal configuration of each sub-system remains a significant cause for debate and still poses a major challenge when considering the ability of simulators to extract realistic driver behavior. if a difference is observed between real and virtual conditions, the factors specifically cause these differences are very difficult to be explained."
http://orkg.org/orkg/resource/R110440,How many could have been saved? Effects of social distancing on COVID-19,10.1590/0034-761220200530,crossref,"<jats:p>Abstract What is the effect of social distancing policies on the spread of the new coronavirus? Social distancing policies rose to prominence as most capable of containing contagion and saving lives. Our purpose in this paper is to identify the causal effect of social distancing policies on the number of confirmed cases of COVID-19 and on contagion velocity. We align our main argument with the existing scientific consensus: social distancing policies negatively affect the number of cases. To test this hypothesis, we construct a dataset with daily information on 78 affected countries in the world. We compute several relevant measures from publicly available information on the number of cases and deaths to estimate causal effects for short-term and cumulative effects of social distancing policies. We use a time-series cross-sectional matching approach to match countries’ observable histories. Causal effects (ATTs and ATEs) can be extracted via a dif-in-dif estimator. Results show that social distancing policies reduce the aggregated number of cases by 4,832 on average (or 17.5/100 thousand), but only when strict measures are adopted. This effect seems to manifest from the third week onwards.</jats:p>","abstract what is the effect of social distancing policies on the spread of the new coronavirus? social distancing policies rose to prominence as most capable of containing contagion and saving lives. our purpose in this paper is to identify the causal effect of social distancing policies on the number of confirmed cases of covid-19 and on contagion velocity. we align our main argument with the existing scientific consensus: social distancing policies negatively affect the number of cases. to test this hypothesis, we construct a dataset with daily information on 78 affected countries in the world. we compute several relevant measures from publicly available information on the number of cases and deaths to estimate causal effects for short-term and cumulative effects of social distancing policies. we use a time-series cross-sectional matching approach to match countries’ observable histories. causal effects (atts and ates) can be extracted via a dif-in-dif estimator. results show that social distancing policies reduce the aggregated number of cases by 4,832 on average (or 17.5/100 thousand), but only when strict measures are adopted. this effect seems to manifest from the third week onwards."
http://orkg.org/orkg/resource/R110509,Dominant factors that govern pressure natriuresis in diuresis and antidiuresis: a mathematical model,10.1152/ajprenal.00500.2013,crossref,"<jats:p>We have developed a whole kidney model of the urine concentrating mechanism and renal autoregulation. The model represents the tubuloglomerular feedback (TGF) and myogenic mechanisms, which together affect the resistance of the afferent arteriole and thus glomerular filtration rate. TGF is activated by fluctuations in macula densa [Cl<jats:sup>−</jats:sup>] and the myogefnic mechanism by changes in hydrostatic pressure. The model was used to investigate the relative contributions of medullary blood flow autoregulation and inhibition of transport in the proximal convoluted tubule to pressure natriuresis in both diuresis and antidiuresis. The model predicts that medullary blood flow autoregulation, which only affects the interstitial solute composition in the model, has negligible influence on the rate of NaCl excretion. However, it exerts a significant effect on urine flow, particularly in the antidiuretic kidney. This suggests that interstitial washout has significant implications for the maintenance of hydration status but little direct bearing on salt excretion, and that medullary blood flow may only play a signaling role for stimulating a pressure-natriuresis response. Inhibited reabsorption in the model proximal convoluted tubule is capable of driving pressure natriuresis when the known actions of vasopressin on the collecting duct epithelium are taken into account.</jats:p>","we have developed a whole kidney model of the urine concentrating mechanism and renal autoregulation. the model represents the tubuloglomerular feedback (tgf) and myogenic mechanisms, which together affect the resistance of the afferent arteriole and thus glomerular filtration rate. tgf is activated by fluctuations in macula densa [cl − ] and the myogefnic mechanism by changes in hydrostatic pressure. the model was used to investigate the relative contributions of medullary blood flow autoregulation and inhibition of transport in the proximal convoluted tubule to pressure natriuresis in both diuresis and antidiuresis. the model predicts that medullary blood flow autoregulation, which only affects the interstitial solute composition in the model, has negligible influence on the rate of nacl excretion. however, it exerts a significant effect on urine flow, particularly in the antidiuretic kidney. this suggests that interstitial washout has significant implications for the maintenance of hydration status but little direct bearing on salt excretion, and that medullary blood flow may only play a signaling role for stimulating a pressure-natriuresis response. inhibited reabsorption in the model proximal convoluted tubule is capable of driving pressure natriuresis when the known actions of vasopressin on the collecting duct epithelium are taken into account."
http://orkg.org/orkg/resource/R110641,Multicentric experience with interferon gamma therapy in sepsis induced immunosuppression. A case series,,crossref,"<jats:title>Abstract</jats:title>\n              <jats:sec>\n                <jats:title>Background</jats:title>\n                <jats:p>The sepsis-induced immunodepression contributes to impaired clinical outcomes of various stress conditions. This syndrome is well documented and characterized by attenuated function of innate and adaptive immune cells. Several pharmacological interventions aimed to restore the immune response are emerging of which interferon-gamma (IFNγ) is one. It is of paramount relevance to obtain clinical information on optimal timing of the IFNγ-treatment, −tolerance, −effectiveness and outcome before performing a RCT. We describe the effects of IFNγ in a cohort of 18 adult and 2 pediatric sepsis patients.</jats:p>\n              </jats:sec>\n              <jats:sec>\n                <jats:title>Methods</jats:title>\n                <jats:p>In this open-label prospective multi-center case-series, IFNγ treatment was initiated in patients selected on clinical and immunological criteria early (&lt;\u20094\u2009days) or late (&gt;\u20097\u2009days) following the onset of sepsis. The data collected in 18 adults and 2 liver transplanted pediatric patients were: clinical scores, monocyte expression of HLA-DR (flow cytometry), lymphocyte immune-phenotyping (flow cytometry), IL-6 and IL-10 plasma levels (ELISA), bacterial cultures, disease severity, and mortality.</jats:p>\n              </jats:sec>\n              <jats:sec>\n                <jats:title>Results</jats:title>\n                <jats:p>In 15 out of 18 patients IFNγ treatment was associated with an increase of median HLA-DR expression from 2666 [IQ 1547; 4991] to 12,451 [IQ 4166; 19,707], while the absolute number of lymphocyte subpopulations were not affected, except for the decrease number of NK cells 94.5 [23; 136] to 32.5 [13; 90.8] (0.0625)]. Plasma levels of IL-6 464 [201–770] to 108 (89–140) ng/mL (<jats:italic>p</jats:italic>\u2009=\u20090.04) and IL-10 from IL-10 from 29 [12–59] to 9 [1–15] pg/mL decreased significantly. Three patients who received IFNγ early after ICU admission (&lt;4\u2009days) died. The other patients had a rapid clinical improvement assessed by the SOFA score and bacterial cultures that were repeatedly positive became negative. The 2 pediatric cases improved rapidly, but 1 died for hemorrhagic complication.</jats:p>\n              </jats:sec>\n              <jats:sec>\n                <jats:title>Conclusion</jats:title>\n                <jats:p>Guided by clinical and immunological monitoring, adjunctive immunotherapy with IFNγ appears well-tolerated in our cases and improves immune host defense in sepsis induced immuno suppression. Randomized clinical studies to assess its potential clinical benefit are warranted.</jats:p>\n              </jats:sec>","abstract \n \n background \n the sepsis-induced immunodepression contributes to impaired clinical outcomes of various stress conditions. this syndrome is well documented and characterized by attenuated function of innate and adaptive immune cells. several pharmacological interventions aimed to restore the immune response are emerging of which interferon-gamma (ifnγ) is one. it is of paramount relevance to obtain clinical information on optimal timing of the ifnγ-treatment, −tolerance, −effectiveness and outcome before performing a rct. we describe the effects of ifnγ in a cohort of 18 adult and 2 pediatric sepsis patients. \n \n \n methods \n in this open-label prospective multi-center case-series, ifnγ treatment was initiated in patients selected on clinical and immunological criteria early (&lt;\u20094\u2009days) or late (&gt;\u20097\u2009days) following the onset of sepsis. the data collected in 18 adults and 2 liver transplanted pediatric patients were: clinical scores, monocyte expression of hla-dr (flow cytometry), lymphocyte immune-phenotyping (flow cytometry), il-6 and il-10 plasma levels (elisa), bacterial cultures, disease severity, and mortality. \n \n \n results \n in 15 out of 18 patients ifnγ treatment was associated with an increase of median hla-dr expression from 2666 [iq 1547; 4991] to 12,451 [iq 4166; 19,707], while the absolute number of lymphocyte subpopulations were not affected, except for the decrease number of nk cells 94.5 [23; 136] to 32.5 [13; 90.8] (0.0625)]. plasma levels of il-6 464 [201–770] to 108 (89–140) ng/ml ( p \u2009=\u20090.04) and il-10 from il-10 from 29 [12–59] to 9 [1–15] pg/ml decreased significantly. three patients who received ifnγ early after icu admission (&lt;4\u2009days) died. the other patients had a rapid clinical improvement assessed by the sofa score and bacterial cultures that were repeatedly positive became negative. the 2 pediatric cases improved rapidly, but 1 died for hemorrhagic complication. \n \n \n conclusion \n guided by clinical and immunological monitoring, adjunctive immunotherapy with ifnγ appears well-tolerated in our cases and improves immune host defense in sepsis induced immuno suppression. randomized clinical studies to assess its potential clinical benefit are warranted. \n"
http://orkg.org/orkg/resource/R110568,Longitudinal assessment of IFN-I activity and immune profile in critically ill COVID-19 patients with acute respiratory distress syndrome,,crossref,"<jats:title>Abstract</jats:title><jats:sec>\n                <jats:title>Background</jats:title>\n                <jats:p>Since the onset of the pandemic, only few studies focused on longitudinal immune monitoring in critically ill COVID-19 patients with acute respiratory distress syndrome (ARDS) whereas their hospital stay may last for several weeks. Consequently, the question of whether immune parameters may drive or associate with delayed unfavorable outcome in these critically ill patients remains unsolved.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Methods</jats:title>\n                <jats:p>We present a dynamic description of immuno-inflammatory derangements in 64 critically ill COVID-19 patients including plasma IFNα2 levels and IFN-stimulated genes (ISG) score measurements.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Results</jats:title>\n                <jats:p>ARDS patients presented with persistently decreased lymphocyte count and mHLA-DR expression and increased cytokine levels. Type-I IFN response was initially induced with elevation of IFNα2 levels and ISG score followed by a rapid decrease over time. Survivors and non-survivors presented with apparent common immune responses over the first 3\xa0weeks after ICU admission mixing gradual return to normal values of cellular markers and progressive decrease of cytokines levels including IFNα2. Only plasma TNF-α presented with a slow increase over time and higher values in non-survivors compared with survivors. This paralleled with an extremely high occurrence of secondary infections in COVID-19 patients with ARDS.</jats:p>\n              </jats:sec><jats:sec>\n                <jats:title>Conclusions</jats:title>\n                <jats:p>Occurrence of ARDS in response to SARS-CoV2 infection appears to be strongly associated with the intensity of immune alterations upon ICU admission of COVID-19 patients. In these critically ill patients, immune profile presents with similarities with the delayed step of immunosuppression described in bacterial sepsis.\n</jats:p>\n              </jats:sec>","abstract \n background \n since the onset of the pandemic, only few studies focused on longitudinal immune monitoring in critically ill covid-19 patients with acute respiratory distress syndrome (ards) whereas their hospital stay may last for several weeks. consequently, the question of whether immune parameters may drive or associate with delayed unfavorable outcome in these critically ill patients remains unsolved. \n \n methods \n we present a dynamic description of immuno-inflammatory derangements in 64 critically ill covid-19 patients including plasma ifnα2 levels and ifn-stimulated genes (isg) score measurements. \n \n results \n ards patients presented with persistently decreased lymphocyte count and mhla-dr expression and increased cytokine levels. type-i ifn response was initially induced with elevation of ifnα2 levels and isg score followed by a rapid decrease over time. survivors and non-survivors presented with apparent common immune responses over the first 3\xa0weeks after icu admission mixing gradual return to normal values of cellular markers and progressive decrease of cytokines levels including ifnα2. only plasma tnf-α presented with a slow increase over time and higher values in non-survivors compared with survivors. this paralleled with an extremely high occurrence of secondary infections in covid-19 patients with ards. \n \n conclusions \n occurrence of ards in response to sars-cov2 infection appears to be strongly associated with the intensity of immune alterations upon icu admission of covid-19 patients. in these critically ill patients, immune profile presents with similarities with the delayed step of immunosuppression described in bacterial sepsis.\n \n"
http://orkg.org/orkg/resource/R110557,The evolutionary nature of musical meaning,10.1177/1029864909013002091,crossref,"<jats:p> The paper will draw on ethnomusicological, cognitive and neuroscientific evidence in suggesting that music and language constitute complementary components of the human communicative toolkit. It will start by outlining an operational definition of music as a mode of social interaction in terms of its generic, cross-cultural properties that facilitates comparison with language as a universal human faculty. It will argue that, despite the fact that music appears much more heterogeneous and differentiated in function from culture to culture than does language, music possesses common attributes across cultures: it exploits the human capacity to entrain to external (particularly social) stimuli, and presents a rich set of semantic fields while under-determining meaning. </jats:p><jats:p> While language is held to possess both combinatoriality and semanticity, music is often claimed to be combinatorial but to lack semanticity. This paper will argue that music has semanticity, but that this semanticity is adapted for a different function from that of language. Music exploits the human capacity for entrainment, increasing the likelihood that participants will experience a sense of ‘shared intentionality’. It presents the characteristics of an ‘honest signal’ while under-specifying goals in ways that permit individuals to interact even while holding to personal interpretations of goals and meanings that may actually be in conflict. Music allows participants to explore the prospective consequences of their actions and attitudes towards others within a temporal framework that promotes the alignment of participants’ sense of goals. As a generic human faculty music thus provides a medium that is adapted to situations of social uncertainty, a medium by means of which a capacity for flexible social interaction can be explored and reinforced. It will be argued that a faculty for music is likely to have been exaptive in the evolution of the human capacity for complex social interaction. </jats:p>","the paper will draw on ethnomusicological, cognitive and neuroscientific evidence in suggesting that music and language constitute complementary components of the human communicative toolkit. it will start by outlining an operational definition of music as a mode of social interaction in terms of its generic, cross-cultural properties that facilitates comparison with language as a universal human faculty. it will argue that, despite the fact that music appears much more heterogeneous and differentiated in function from culture to culture than does language, music possesses common attributes across cultures: it exploits the human capacity to entrain to external (particularly social) stimuli, and presents a rich set of semantic fields while under-determining meaning. while language is held to possess both combinatoriality and semanticity, music is often claimed to be combinatorial but to lack semanticity. this paper will argue that music has semanticity, but that this semanticity is adapted for a different function from that of language. music exploits the human capacity for entrainment, increasing the likelihood that participants will experience a sense of ‘shared intentionality’. it presents the characteristics of an ‘honest signal’ while under-specifying goals in ways that permit individuals to interact even while holding to personal interpretations of goals and meanings that may actually be in conflict. music allows participants to explore the prospective consequences of their actions and attitudes towards others within a temporal framework that promotes the alignment of participants’ sense of goals. as a generic human faculty music thus provides a medium that is adapted to situations of social uncertainty, a medium by means of which a capacity for flexible social interaction can be explored and reinforced. it will be argued that a faculty for music is likely to have been exaptive in the evolution of the human capacity for complex social interaction."
http://orkg.org/orkg/resource/R110665,Combination Effects of Antimicrobial Peptides,10.1128/aac.02434-15,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            Antimicrobial peptides (AMPs) are ancient and conserved across the tree of life. Their efficacy over evolutionary time has been largely attributed to their mechanisms of killing. Yet, the understanding of their pharmacodynamics both\n            <jats:italic>in vivo</jats:italic>\n            and\n            <jats:italic>in vitro</jats:italic>\n            is very limited. This is, however, crucial for applications of AMPs as drugs and also informs the understanding of the action of AMPs in natural immune systems. Here, we selected six different AMPs from different organisms to test their individual and combined effects\n            <jats:italic>in vitro</jats:italic>\n            . We analyzed their pharmacodynamics based on the Hill function and evaluated the interaction of combinations of two and three AMPs. Interactions of AMPs in our study were mostly synergistic, and three-AMP combinations displayed stronger synergism than two-AMP combinations. This suggests synergism to be a common phenomenon in AMP interaction. Additionally, AMPs displayed a sharp increase in killing within a narrow dose range, contrasting with those of antibiotics. We suggest that our results could lead a way toward better evaluation of AMP application in practice and shed some light on the evolutionary consequences of antimicrobial peptide interactions within the immune system of organisms.\n          </jats:p>","abstract \n \n antimicrobial peptides (amps) are ancient and conserved across the tree of life. their efficacy over evolutionary time has been largely attributed to their mechanisms of killing. yet, the understanding of their pharmacodynamics both\n in vivo \n and\n in vitro \n is very limited. this is, however, crucial for applications of amps as drugs and also informs the understanding of the action of amps in natural immune systems. here, we selected six different amps from different organisms to test their individual and combined effects\n in vitro \n . we analyzed their pharmacodynamics based on the hill function and evaluated the interaction of combinations of two and three amps. interactions of amps in our study were mostly synergistic, and three-amp combinations displayed stronger synergism than two-amp combinations. this suggests synergism to be a common phenomenon in amp interaction. additionally, amps displayed a sharp increase in killing within a narrow dose range, contrasting with those of antibiotics. we suggest that our results could lead a way toward better evaluation of amp application in practice and shed some light on the evolutionary consequences of antimicrobial peptide interactions within the immune system of organisms.\n"
http://orkg.org/orkg/resource/R110614,Hypercholesterolemia in pregnant mice increases the susceptibility to atherosclerosis in adult life,10.1177/1708538113492516,crossref,"<jats:sec><jats:title>Purpose</jats:title><jats:p> To determine the effects of hypercholesterolemia in pregnant mice on the susceptibility to atherosclerosis in adult life through a new animal modeling approach. </jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p> Male offspring from apoE−/− mice fed with regular (R) or high (H) cholesterol chow during pregnancy were randomly subjected to regular (Groups R–R and H–R, n\u2009=\u200910) or high cholesterol diet (Groups R–H and H–H, n\u2009=\u200910) for 14 weeks. Plasma lipid profiles were determined in all rats. The abdominal aorta was examined for the severity of atherosclerotic lesions in offspring. </jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p> Lipids significantly increased while high-density lipoprotein-cholesterol/low-density lipoprotein-cholesterol decreased in mothers fed high cholesterol chow after delivery compared with before pregnancy ( p\u2009&lt;\u20090.01). Groups R–H and H–R indicated dyslipidemia and significant atherosclerotic lesions. Group H–H demonstrated the highest lipids, lowest high-density lipoprotein-cholesterol/low-density lipoprotein-cholesterol, highest incidence (90%), plaque area to luminal area ratio (0.78\u2009±\u20090.02) and intima to media ratio (1.57\u2009±\u20090.05). </jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p> Hypercholesterolemia in pregnant mice may increase susceptibility to atherosclerosis in their adult offspring. </jats:p></jats:sec>","purpose to determine the effects of hypercholesterolemia in pregnant mice on the susceptibility to atherosclerosis in adult life through a new animal modeling approach. methods male offspring from apoe−/− mice fed with regular (r) or high (h) cholesterol chow during pregnancy were randomly subjected to regular (groups r–r and h–r, n\u2009=\u200910) or high cholesterol diet (groups r–h and h–h, n\u2009=\u200910) for 14 weeks. plasma lipid profiles were determined in all rats. the abdominal aorta was examined for the severity of atherosclerotic lesions in offspring. results lipids significantly increased while high-density lipoprotein-cholesterol/low-density lipoprotein-cholesterol decreased in mothers fed high cholesterol chow after delivery compared with before pregnancy ( p\u2009&lt;\u20090.01). groups r–h and h–r indicated dyslipidemia and significant atherosclerotic lesions. group h–h demonstrated the highest lipids, lowest high-density lipoprotein-cholesterol/low-density lipoprotein-cholesterol, highest incidence (90%), plaque area to luminal area ratio (0.78\u2009±\u20090.02) and intima to media ratio (1.57\u2009±\u20090.05). conclusion hypercholesterolemia in pregnant mice may increase susceptibility to atherosclerosis in their adult offspring."
http://orkg.org/orkg/resource/R110780,Uncovering the Relationship between Human Connectivity Dynamics and Land Use,10.3390/ijgi9030140,crossref,"<jats:p>CDR (Call Detail Record) data are one type of mobile phone data collected by operators each time a user initiates/receives a phone call or sends/receives an sms. CDR data are a rich geo-referenced source of user behaviour information. In this work, we perform an analysis of CDR data for the city of Milan that originate from Telecom Italia Big Data Challenge. A set of graphs is generated from aggregated CDR data, where each node represents a centroid of an RBS (Radio Base Station) polygon, and each edge represents aggregated telecom traffic between two RBSs. To explore the community structure, we apply a modularity-based algorithm. Community structure between days is highly dynamic, with variations in number, size and spatial distribution. One general rule observed is that communities formed over the urban core of the city are small in size and prone to dynamic change in spatial distribution, while communities formed in the suburban areas are larger in size and more consistent with respect to their spatial distribution. To evaluate the dynamics of change in community structure between days, we introduced different graph based and spatial community properties which contain latent footprint of human dynamics. We created land use profiles for each RBS polygon based on the Copernicus Land Monitoring Service Urban Atlas data set to quantify the correlation and predictivennes of human dynamics properties based on land use. The results reveal a strong correlation between some properties and land use which motivated us to further explore this topic. The proposed methodology has been implemented in the programming language Scala inside the Apache Spark engine to support the most computationally intensive tasks and in Python using the rich portfolio of data analytics and machine learning libraries for the less demanding tasks.</jats:p>","cdr (call detail record) data are one type of mobile phone data collected by operators each time a user initiates/receives a phone call or sends/receives an sms. cdr data are a rich geo-referenced source of user behaviour information. in this work, we perform an analysis of cdr data for the city of milan that originate from telecom italia big data challenge. a set of graphs is generated from aggregated cdr data, where each node represents a centroid of an rbs (radio base station) polygon, and each edge represents aggregated telecom traffic between two rbss. to explore the community structure, we apply a modularity-based algorithm. community structure between days is highly dynamic, with variations in number, size and spatial distribution. one general rule observed is that communities formed over the urban core of the city are small in size and prone to dynamic change in spatial distribution, while communities formed in the suburban areas are larger in size and more consistent with respect to their spatial distribution. to evaluate the dynamics of change in community structure between days, we introduced different graph based and spatial community properties which contain latent footprint of human dynamics. we created land use profiles for each rbs polygon based on the copernicus land monitoring service urban atlas data set to quantify the correlation and predictivennes of human dynamics properties based on land use. the results reveal a strong correlation between some properties and land use which motivated us to further explore this topic. the proposed methodology has been implemented in the programming language scala inside the apache spark engine to support the most computationally intensive tasks and in python using the rich portfolio of data analytics and machine learning libraries for the less demanding tasks."
http://orkg.org/orkg/resource/R111420,Empirical Validation of Component-based Software Systems Generation and Evaluation Approaches,10.19153/cleiej.13.1.6,crossref,"<jats:p>Component-based software development needs to formalize a process of generation, evaluation andselection of Composite COTS-based Software Systems (CCSS), enabling software architects to make earlydecisions; the Azimut approach and its associated software tool were proposed to tackle this problem.This article presents an experimental study conduced to compare Azimut approach with a SystematizedAd-Hoc approach, regarding generated solutions quality, cost and e®ort. Results suggest that: (1) Azimutgenerate better quality solutions at lower cost, but not statistically signi¯cant, and (2) there is strongevidence showing that the e®ort required is higher than for Systematized Ad-Hoc approach; re-samplingmethods (Bootstrap and Jackknife) were applied to reinforce these conclusions. Also this study serves asa framework for validating approaches, process and tools for generating and evaluating component-basedsoftware systems.</jats:p>","component-based software development needs to formalize a process of generation, evaluation andselection of composite cots-based software systems (ccss), enabling software architects to make earlydecisions; the azimut approach and its associated software tool were proposed to tackle this problem.this article presents an experimental study conduced to compare azimut approach with a systematizedad-hoc approach, regarding generated solutions quality, cost and e®ort. results suggest that: (1) azimutgenerate better quality solutions at lower cost, but not statistically signi¯cant, and (2) there is strongevidence showing that the e®ort required is higher than for systematized ad-hoc approach; re-samplingmethods (bootstrap and jackknife) were applied to reinforce these conclusions. also this study serves asa framework for validating approaches, process and tools for generating and evaluating component-basedsoftware systems."
http://orkg.org/orkg/resource/R111005,White-light emission from discrete heterometallic lanthanide-directed self-assembled complexes in solution,10.1039/c7sc00739f,crossref,"<p>Herein, we have developed a white-light-emitting system based on the formation of discrete lanthanide-based self-assembled complexes using a newly-designed ligand. We demonstrate that fine tuning of the lanthanide ions molar ratio in the self-assemblies combined with the intrinsic blue fluorescence of the ligand allows for the successful emission of pure white light with CIE coordinates of (0.33, 0.34).</p>","herein, we have developed a white-light-emitting system based on the formation of discrete lanthanide-based self-assembled complexes using a newly-designed ligand. we demonstrate that fine tuning of the lanthanide ions molar ratio in the self-assemblies combined with the intrinsic blue fluorescence of the ligand allows for the successful emission of pure white light with cie coordinates of (0.33, 0.34)."
http://orkg.org/orkg/resource/R110993,"Anilido-oxazoline-ligated rare-earth metal complexes: synthesis, characterization and highly cis-1,4-selective polymerization of isoprene",10.1039/c8dt04647f,crossref,"<p>Anilido-oxazoline-ligated rare-earth metal complexes show strong fluorescence emissions and good catalytic performance on isoprene polymerization with high <italic>cis</italic>-1,4-selectivity.</p>","anilido-oxazoline-ligated rare-earth metal complexes show strong fluorescence emissions and good catalytic performance on isoprene polymerization with high cis -1,4-selectivity."
http://orkg.org/orkg/resource/R111056,Do Political Attitudes Matter for Epistemic Decisions of Scientists?,10.1007/s13164-020-00504-7,crossref,"""<jats:title>Abstract</jats:title><jats:p>The epistemic attitudes of scientists, such as epistemic tolerance and authoritarianism, play important roles in the discourse about rivaling theories. Epistemic tolerance stands for the mental attitude of an epistemic agent, e.g., a scientist, who is open to opposing views, while epistemic authoritarianism represents the tendency to uncritically accept views of authorities. Another relevant epistemic factor when it comes to the epistemic decisions of scientists is the skepticism towards the scientific method. However, the question is whether these epistemic attitudes are influenced by their sociopolitical counterparts, such as the researcher’s degree of conservatism. To empirically investigate the interplay between epistemic and sociopolitical attitudes of scientists, we conducted a survey with researchers (<jats:italic>N</jats:italic>\u2009=\u2009655) across different disciplines. We propose scales for measuring epistemic tolerance and epistemic authoritarianism, as well as a scale for detecting the participants'\xa0readiness to question the scientific method. Furthermore, we investigate the relationship between epistemic tolerance and epistemic authoritarianism on the one hand, and career stage and sociopolitical views on the other hand. Interestingly, our study found only small correlations between the\xa0participants' degree of conservatism and their epistemic attitudes. This suggests that political views, against common argumentation, actually do not play an important role in one’s scientific decisions. Moreover, social scientists scored higher on the epistemic tolerance and lower on the epistemic authoritarianism scale than natural scientists. Finally, the results indicate that natural scientists question the scientific method less than social scientists.</jats:p>""",""" abstract the epistemic attitudes of scientists, such as epistemic tolerance and authoritarianism, play important roles in the discourse about rivaling theories. epistemic tolerance stands for the mental attitude of an epistemic agent, e.g., a scientist, who is open to opposing views, while epistemic authoritarianism represents the tendency to uncritically accept views of authorities. another relevant epistemic factor when it comes to the epistemic decisions of scientists is the skepticism towards the scientific method. however, the question is whether these epistemic attitudes are influenced by their sociopolitical counterparts, such as the researcher’s degree of conservatism. to empirically investigate the interplay between epistemic and sociopolitical attitudes of scientists, we conducted a survey with researchers ( n \u2009=\u2009655) across different disciplines. we propose scales for measuring epistemic tolerance and epistemic authoritarianism, as well as a scale for detecting the participants'\xa0readiness to question the scientific method. furthermore, we investigate the relationship between epistemic tolerance and epistemic authoritarianism on the one hand, and career stage and sociopolitical views on the other hand. interestingly, our study found only small correlations between the\xa0participants' degree of conservatism and their epistemic attitudes. this suggests that political views, against common argumentation, actually do not play an important role in one’s scientific decisions. moreover, social scientists scored higher on the epistemic tolerance and lower on the epistemic authoritarianism scale than natural scientists. finally, the results indicate that natural scientists question the scientific method less than social scientists. """
http://orkg.org/orkg/resource/R111072,One-step synthesis of α/β cyano-aqua cobinamides from vitamin B12 with Zn(II) or Cu(II) salts in methanol,10.1142/s1088424611003446,crossref,<jats:p> This short communication describes the screening of various metal salts for the preparation of cyano-aqua cobinamides from vitamin B12 in methanol. ZnCl<jats:sub>2</jats:sub> and Cu(NO<jats:sub>3</jats:sub>)<jats:sub>2</jats:sub>·3H<jats:sub>2</jats:sub>O have been identified as most active for this purpose and represent useful alternatives to the widely applied Ce(III) method that requires excess cyanide. </jats:p>,this short communication describes the screening of various metal salts for the preparation of cyano-aqua cobinamides from vitamin b12 in methanol. zncl 2 and cu(no 3 ) 2 ·3h 2 o have been identified as most active for this purpose and represent useful alternatives to the widely applied ce(iii) method that requires excess cyanide.
http://orkg.org/orkg/resource/R111114,Development of optimized substitution ratio for wheatcassava-african yam bean flour composite for Nigerian bread industries,10.4314/njt.v39i4.33,crossref,"<jats:p>An optimization study of the mix ratio for substitution of Wheat flour with Cassava and African Yam Bean flours (AYB) was carried out and reported in this paper. The aim was to obtain a mix ratio that would optimise selected physical properties of the bread. Wheat flour was substituted with Cassava and African Yam Bean flours at different levels: 80% to 100% of wheat, 0% to 10% of cassava flour and 0% to 10% for AYB flour. The experiment was conducted in mixture design which was generated and analysed by Design-Expert Software 11 version. The Composite dough was prepared in different mix ratios according to the design matrix and subsequentlybaked under the same conditions and analysed for the following loaf quality attributes: Loaf Specific Volume, Bread Crumb Hardness and Crumb Colour Index as response variables. The objective functions were to maximize Loaf Specific Volume, minimize Wheat flour, Bread Crumb Hardness and Crumb Colour Index to obtain the most suitable substitution ratio acceptable to\xa0 consumers. Predictive models for the response variables were developed with the coefficient of determination (R2 ) of 0.991 for Loaf Specific Volume (LSV) while that of Bread Crumb Hardness (BCH) and Crumb Colour Index (CCI) were 0.834 and 0.895 respectively at 95% confidence interval (CI).The predicted optimal substitution ratio was obtained as follows: 88% Wheat flour, 10% Cassava flour, and 2% AYB flour. At this formulation, the predicted Loaf Specific Volume was 2.11cm3 /g, Bread Crumb Hardness was 25.12N, and Crumb Colour Index was 18.88.The study shows that addition of 2% of AYB flour in the formulation would help to optimise the LSV, BCH and the CCI of the Wheat-Cassava flour bread at the mix ratio of 88:10. Application of the results of this study in bread industries will reduce the cost of bread in Nigeria, which is influenced by the rising cost of imported wheat. This is a significant development because wheat flour was the sole baking flour in Nigeria before wheat substitution initiative.&#x0D;\nKeywords: Bread, wheat, Cassava, African Yam Bean, Flour, Dough, Loaf Specific Volume, Crumb Hardness, Crumb Colour Index </jats:p>","an optimization study of the mix ratio for substitution of wheat flour with cassava and african yam bean flours (ayb) was carried out and reported in this paper. the aim was to obtain a mix ratio that would optimise selected physical properties of the bread. wheat flour was substituted with cassava and african yam bean flours at different levels: 80% to 100% of wheat, 0% to 10% of cassava flour and 0% to 10% for ayb flour. the experiment was conducted in mixture design which was generated and analysed by design-expert software 11 version. the composite dough was prepared in different mix ratios according to the design matrix and subsequentlybaked under the same conditions and analysed for the following loaf quality attributes: loaf specific volume, bread crumb hardness and crumb colour index as response variables. the objective functions were to maximize loaf specific volume, minimize wheat flour, bread crumb hardness and crumb colour index to obtain the most suitable substitution ratio acceptable to\xa0 consumers. predictive models for the response variables were developed with the coefficient of determination (r2 ) of 0.991 for loaf specific volume (lsv) while that of bread crumb hardness (bch) and crumb colour index (cci) were 0.834 and 0.895 respectively at 95% confidence interval (ci).the predicted optimal substitution ratio was obtained as follows: 88% wheat flour, 10% cassava flour, and 2% ayb flour. at this formulation, the predicted loaf specific volume was 2.11cm3 /g, bread crumb hardness was 25.12n, and crumb colour index was 18.88.the study shows that addition of 2% of ayb flour in the formulation would help to optimise the lsv, bch and the cci of the wheat-cassava flour bread at the mix ratio of 88:10. application of the results of this study in bread industries will reduce the cost of bread in nigeria, which is influenced by the rising cost of imported wheat. this is a significant development because wheat flour was the sole baking flour in nigeria before wheat substitution initiative.&#x0d;\nkeywords: bread, wheat, cassava, african yam bean, flour, dough, loaf specific volume, crumb hardness, crumb colour index"
http://orkg.org/orkg/resource/R111426,Trabajo y Educación en la Seguridad Societal,10.29378/plurais.2447-9373.2019.v4.n3.9-35,crossref,"<jats:p>La interrelación entre trabajo, educación y seguridad societal es un objeto de estudio e investigación con novedad original, que es investigado bajo el enfoque heurístico del modelo emergente de cuádruple hélice para la innovación en el dominio del ciberespacio; teniendo como actores a la sociedad civil, a los pueblos con su poder creador, a las instituciones de educación universitaria, a los gobiernos o administraciones de los Estados-nación, y a \xa0los sectores de la economía real y de la economía especulativa\xa0 globalizante. Este artículo es un reporte preliminar de investigación, que presenta el estado del arte del trabajo y la educación como procesos categoriales que, objetiva o subjetivamente, impactan a la seguridad societal, y tienen efectos en la seguridad de los Estados-nación y\xa0 la Paz internacional.</jats:p>","la interrelación entre trabajo, educación y seguridad societal es un objeto de estudio e investigación con novedad original, que es investigado bajo el enfoque heurístico del modelo emergente de cuádruple hélice para la innovación en el dominio del ciberespacio; teniendo como actores a la sociedad civil, a los pueblos con su poder creador, a las instituciones de educación universitaria, a los gobiernos o administraciones de los estados-nación, y a \xa0los sectores de la economía real y de la economía especulativa\xa0 globalizante. este artículo es un reporte preliminar de investigación, que presenta el estado del arte del trabajo y la educación como procesos categoriales que, objetiva o subjetivamente, impactan a la seguridad societal, y tienen efectos en la seguridad de los estados-nación y\xa0 la paz internacional."
http://orkg.org/orkg/resource/R111393,"Clinical Evaluation of Self-Collected Saliva by Quantitative Reverse Transcription-PCR (RT-qPCR), Direct RT-qPCR, Reverse Transcription–Loop-Mediated Isothermal Amplification, and a Rapid Antigen Test To Diagnose COVID-19",10.1128/jcm.01438-20,crossref,"<jats:p>The clinical performances of six molecular diagnostic tests and a rapid antigen test for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) were clinically evaluated for the diagnosis of coronavirus disease 2019 (COVID-19) in self-collected saliva. Saliva samples from 103 patients with laboratory-confirmed COVID-19 (15 asymptomatic and 88 symptomatic) were collected on the day of hospital admission. SARS-CoV-2 RNA in saliva was detected using a quantitative reverse transcription-PCR (RT-qPCR) laboratory-developed test (LDT), a cobas SARS-CoV-2 high-throughput system, three direct RT-qPCR kits, and reverse transcription–loop-mediated isothermal amplification (RT-LAMP).</jats:p>","the clinical performances of six molecular diagnostic tests and a rapid antigen test for severe acute respiratory syndrome coronavirus 2 (sars-cov-2) were clinically evaluated for the diagnosis of coronavirus disease 2019 (covid-19) in self-collected saliva. saliva samples from 103 patients with laboratory-confirmed covid-19 (15 asymptomatic and 88 symptomatic) were collected on the day of hospital admission. sars-cov-2 rna in saliva was detected using a quantitative reverse transcription-pcr (rt-qpcr) laboratory-developed test (ldt), a cobas sars-cov-2 high-throughput system, three direct rt-qpcr kits, and reverse transcription–loop-mediated isothermal amplification (rt-lamp)."
http://orkg.org/orkg/resource/R111455,"The mitochondrial genome and phylogenetic characteristics of the Thick-billed Green-Pigeon, Treron curvirostra: the first sequence for the genus",10.3897/zookeys.1041.60150,crossref,"<jats:p>Members of the genus <jats:italic>Treron</jats:italic> (Columbidae) are widely distributed in southern Asia and the Indo-Malayan Region but their relationships are poorly understood. Better knowledge of the systematic status of this genus may help studies of historical biogeography and taxonomy. The complete mitochondrial genome of <jats:italic>T. curvirostra</jats:italic> was characterized, a first for the genus. It is 17,414 base pairs in length, containing two rRNAs, 22 tRNAs, 13 protein coding genes (PCGs), and one D-loop with a primary structure that is similar to that found in most members of Columbidae. Most PCGs start with the common ATG codon but are terminated by different codons. The highest value of the Ka/Ks ratio within 13 PCGs was found in ATP8 with 0.1937, suggesting that PCGs of the mitochondrial genome tend to be conservative in Columbidae. Moreover, the phylogenetic relationships within Columbidae, which was based on sequences of 13 PCGs, showed that (<jats:italic>T. curvirostra</jats:italic> + <jats:italic>Hemiphaga novaeseelandiae</jats:italic>) were clustered in one clade, suggesting a potentially close relationship between <jats:italic>Treron</jats:italic> and <jats:italic>Hemiphaga.</jats:italic> However, the monophyly of the subfamilies of Columbidae recognized by the Interagency Taxonomic Information System could not be corroborated. Hence, the position of the genus <jats:italic>Treron</jats:italic> in the classification of Columbidae may have to be revised.</jats:p>","members of the genus treron (columbidae) are widely distributed in southern asia and the indo-malayan region but their relationships are poorly understood. better knowledge of the systematic status of this genus may help studies of historical biogeography and taxonomy. the complete mitochondrial genome of t. curvirostra was characterized, a first for the genus. it is 17,414 base pairs in length, containing two rrnas, 22 trnas, 13 protein coding genes (pcgs), and one d-loop with a primary structure that is similar to that found in most members of columbidae. most pcgs start with the common atg codon but are terminated by different codons. the highest value of the ka/ks ratio within 13 pcgs was found in atp8 with 0.1937, suggesting that pcgs of the mitochondrial genome tend to be conservative in columbidae. moreover, the phylogenetic relationships within columbidae, which was based on sequences of 13 pcgs, showed that ( t. curvirostra + hemiphaga novaeseelandiae ) were clustered in one clade, suggesting a potentially close relationship between treron and hemiphaga. however, the monophyly of the subfamilies of columbidae recognized by the interagency taxonomic information system could not be corroborated. hence, the position of the genus treron in the classification of columbidae may have to be revised."
http://orkg.org/orkg/resource/R111328,Hybrid apomicts trapped in the ecological niches of their sexual ancestors,10.1073/pnas.1423447112,crossref,"<jats:title>Significance</jats:title>\n          <jats:p>\n            Ecological-niche differentiation in diploid sexual–polyploid asexual complexes has been observed within and among many taxa, yet the relative contributions of reproductive system and ploidy are not fully understood. Here, we assess niche characteristics of sexual diploid, apomictic (asexual) diploid, and triploid\n            <jats:italic>Boechera</jats:italic>\n            (Brassicaceae) lineages. We find strong evidence for widespread hybridization and, to a lesser degree, ploidy variation as factors that together overcome the adaptive disadvantages of apomictic (i.e., asexual) reproduction. When controlling for ploidy, we find only modest evidence for putatively asexually driven ecological-niche divergence among reproductive systems, a finding that contradicts the well-supported patterns of geographic parthenogenesis.\n          </jats:p>","significance \n \n ecological-niche differentiation in diploid sexual–polyploid asexual complexes has been observed within and among many taxa, yet the relative contributions of reproductive system and ploidy are not fully understood. here, we assess niche characteristics of sexual diploid, apomictic (asexual) diploid, and triploid\n boechera \n (brassicaceae) lineages. we find strong evidence for widespread hybridization and, to a lesser degree, ploidy variation as factors that together overcome the adaptive disadvantages of apomictic (i.e., asexual) reproduction. when controlling for ploidy, we find only modest evidence for putatively asexually driven ecological-niche divergence among reproductive systems, a finding that contradicts the well-supported patterns of geographic parthenogenesis.\n"
http://orkg.org/orkg/resource/R111735,Brain connectivity dynamics during social interaction reflect social network structure,10.1073/pnas.1616130114,crossref,"<jats:title>Significance</jats:title>\n          <jats:p>We examine brain dynamics during a common social experience—social exclusion—to determine whether cohesive networks in the brain support navigation of the social world and contribute to the shape of friendship networks. Specifically, exclusion is associated with increased cohesion within brain networks that support understanding what other people think and feel. Furthermore, using social network analysis, we find that variability in brain dynamics is associated with the shape of participants’ friendship networks. Bringing together findings related to brain network dynamics and social network dynamics illuminates ways that psychological processes may shape and be shaped by social environments.</jats:p>","significance \n we examine brain dynamics during a common social experience—social exclusion—to determine whether cohesive networks in the brain support navigation of the social world and contribute to the shape of friendship networks. specifically, exclusion is associated with increased cohesion within brain networks that support understanding what other people think and feel. furthermore, using social network analysis, we find that variability in brain dynamics is associated with the shape of participants’ friendship networks. bringing together findings related to brain network dynamics and social network dynamics illuminates ways that psychological processes may shape and be shaped by social environments."
http://orkg.org/orkg/resource/R111748,The Coupled Brains of Captivated Audiences: An Investigation of the Collective Brain Dynamics of an Audience Watching 					a Suspenseful Film,10.1027/1864-1105/a000271,crossref,"<jats:p> Abstract. Suspense not only creates a strong psychological tension within individuals, but it does so reliably across viewers who become collectively engaged with the story. Despite its prevalence in media psychology, limited work has examined suspense from a media neuroscience perspective, and thus the biological underpinnings of suspense remain unknown. Here we examine continuous brain responses of 494 viewers watching a suspenseful movie. To create a time-resolved measure of the degree to which a movie aligns audience-wide brain responses, we computed dynamic inter-subject correlations of functional magnetic resonance imaging (fMRI) time series among all viewers using sliding-window analysis. In parallel, we captured in-the-moment reports of suspense in an independent sample via continuous response measurement (CRM). We found that dynamic inter-subject correlations over the course of the movie tracked well with the reported suspense in the CRM sample, particularly in regions associated with emotional salience and higher cognitive processes. These results are compatible with theoretical views on motivated attention and psychological tension. The finding that fMRI-based audience response measurement relates to audience reports of suspense creates new opportunities for research on the mechanisms of suspense and other entertainment phenomena and has applied potential for measuring audience responses in a nonreactive and objective fashion. </jats:p>","abstract. suspense not only creates a strong psychological tension within individuals, but it does so reliably across viewers who become collectively engaged with the story. despite its prevalence in media psychology, limited work has examined suspense from a media neuroscience perspective, and thus the biological underpinnings of suspense remain unknown. here we examine continuous brain responses of 494 viewers watching a suspenseful movie. to create a time-resolved measure of the degree to which a movie aligns audience-wide brain responses, we computed dynamic inter-subject correlations of functional magnetic resonance imaging (fmri) time series among all viewers using sliding-window analysis. in parallel, we captured in-the-moment reports of suspense in an independent sample via continuous response measurement (crm). we found that dynamic inter-subject correlations over the course of the movie tracked well with the reported suspense in the crm sample, particularly in regions associated with emotional salience and higher cognitive processes. these results are compatible with theoretical views on motivated attention and psychological tension. the finding that fmri-based audience response measurement relates to audience reports of suspense creates new opportunities for research on the mechanisms of suspense and other entertainment phenomena and has applied potential for measuring audience responses in a nonreactive and objective fashion."
http://orkg.org/orkg/resource/R113122,UCFrame: A Use Case Framework for Crowd-Centric Requirement Acquisition,10.1145/2894784.2894795,crossref,"<jats:p>To build needed mobile applications in specific domains, requirements should be collected and analyzed in holistic approach. However, resource is limited for small vendor groups to perform holistic requirement acquisition and elicitation. The rise of crowdsourcing and crowdfunding gives small vendor groups new opportunities to build needed mobile applications for the crowd. By finding prior stakeholders and gathering requirements effectively from the crowd, mobile application projects can establish sound foundation in early phase of software process. Therefore, integration of crowd-based requirement engineering into software process is important for small vendor groups. Conventional requirement acquisition and elicitation methods are analyst-centric. Very little discussion is in adapting requirement acquisition tools for crowdcentric context. In this study, several tool features of use case documentation are revised in crowd-centric context. These features constitute a use case-based framework, called UCFrame, for crowd-centric requirement acquisition. An instantiation of UCFrame is also presented to demonstrate the effectiveness of UCFrame in collecting crowd requirements for building two mobile applications.</jats:p>","to build needed mobile applications in specific domains, requirements should be collected and analyzed in holistic approach. however, resource is limited for small vendor groups to perform holistic requirement acquisition and elicitation. the rise of crowdsourcing and crowdfunding gives small vendor groups new opportunities to build needed mobile applications for the crowd. by finding prior stakeholders and gathering requirements effectively from the crowd, mobile application projects can establish sound foundation in early phase of software process. therefore, integration of crowd-based requirement engineering into software process is important for small vendor groups. conventional requirement acquisition and elicitation methods are analyst-centric. very little discussion is in adapting requirement acquisition tools for crowdcentric context. in this study, several tool features of use case documentation are revised in crowd-centric context. these features constitute a use case-based framework, called ucframe, for crowd-centric requirement acquisition. an instantiation of ucframe is also presented to demonstrate the effectiveness of ucframe in collecting crowd requirements for building two mobile applications."
http://orkg.org/orkg/resource/R170112,Estimating genetic kin relationships in prehistoric populations,10.1371/journal.pone.0195491,crossref,"<jats:title>Abstract</jats:title><jats:p>Archaeogenomic research has proven to be a valuable tool to trace migrations of historic and prehistoric individuals and groups, whereas relationships within a group or burial site have not been investigated to a large extent. Knowing the genetic kinship of historic and prehistoric individuals would give important insights into social structures of ancient and historic cultures. Most archaeogenetic research concerning kinship has been restricted to uniparental markers, while studies using genome-wide information were mainly focused on comparisons between populations. Applications which infer the degree of relationship based on modern-day DNA information typically require diploid genotype data. Low concentration of endogenous DNA, fragmentation and other post-mortem damage to ancient DNA (aDNA) makes the application of such tools unfeasible for most archaeological samples. To infer family relationships for degraded samples, we developed the software READ (Relationship Estimation from Ancient DNA). We show that our heuristic approach can successfully infer up to second degree relationships with as little as 0.1x shotgun coverage per genome for pairs of individuals. We uncover previously unknown relationships among prehistoric individuals by applying READ to published aDNA data from several human remains excavated from different cultural contexts. In particular, we find a group of five closely related males from the same Corded Ware culture site in modern-day Germany, suggesting patrilocality, which highlights the possibility to uncover social structures of ancient populations by applying READ to genome-wide aDNA data.</jats:p>","abstract archaeogenomic research has proven to be a valuable tool to trace migrations of historic and prehistoric individuals and groups, whereas relationships within a group or burial site have not been investigated to a large extent. knowing the genetic kinship of historic and prehistoric individuals would give important insights into social structures of ancient and historic cultures. most archaeogenetic research concerning kinship has been restricted to uniparental markers, while studies using genome-wide information were mainly focused on comparisons between populations. applications which infer the degree of relationship based on modern-day dna information typically require diploid genotype data. low concentration of endogenous dna, fragmentation and other post-mortem damage to ancient dna (adna) makes the application of such tools unfeasible for most archaeological samples. to infer family relationships for degraded samples, we developed the software read (relationship estimation from ancient dna). we show that our heuristic approach can successfully infer up to second degree relationships with as little as 0.1x shotgun coverage per genome for pairs of individuals. we uncover previously unknown relationships among prehistoric individuals by applying read to published adna data from several human remains excavated from different cultural contexts. in particular, we find a group of five closely related males from the same corded ware culture site in modern-day germany, suggesting patrilocality, which highlights the possibility to uncover social structures of ancient populations by applying read to genome-wide adna data."
http://orkg.org/orkg/resource/R171488,Socioeconomic disparities and sexual dimorphism in neurotoxic effects of ambient fine particles on youth IQ: A longitudinal analysis,10.1371/journal.pone.0188731,crossref,"<jats:title>Abstract</jats:title><jats:p>Mounting evidence indicates that early-life exposure to particulate air pollutants pose threats to children’s cognitive development, but studies about the neurotoxic effects associated with exposures during adolescence remain unclear. We examined whether exposure to ambient fine particles (PM<jats:sub>2.5</jats:sub>) at residential locations affects intelligence quotient (IQ) during pre-/early-adolescence (ages 9-11) and emerging adulthood (ages 18-20) in a demographically-diverse population (N = 1,360) residing in Southern California. Increased ambient PM<jats:sub>2.5</jats:sub>levels were associated with decreased IQ scores. This association was more evident for Performance IQ (PIQ), but less for Verbal IQ, assessed by the Wechsler Abbreviated Scale of Intelligence. For each inter-quartile (7.73 μg/m<jats:sup>3</jats:sup>) increase in one-year PM<jats:sub>2.5</jats:sub>preceding each assessment, the average PIQ score decreased by 3.08 points (95% confidence interval = [−6.04, −0.12]) accounting for within-family/within-individual correlations, demographic characteristics, family socioeconomic status (SES), parents’ cognitive abilities, neighborhood characteristics, and other spatial confounders. The adverse effect was 150% greater in low SES families and 89% stronger in males, compared to their counterparts. Better understanding of the social disparities and sexual dimorphism in the adverse PM<jats:sub>2.5</jats:sub>-IQ effects may help elucidate the underlying mechanisms and shed light on prevention strategies.</jats:p>","abstract mounting evidence indicates that early-life exposure to particulate air pollutants pose threats to children’s cognitive development, but studies about the neurotoxic effects associated with exposures during adolescence remain unclear. we examined whether exposure to ambient fine particles (pm 2.5 ) at residential locations affects intelligence quotient (iq) during pre-/early-adolescence (ages 9-11) and emerging adulthood (ages 18-20) in a demographically-diverse population (n = 1,360) residing in southern california. increased ambient pm 2.5 levels were associated with decreased iq scores. this association was more evident for performance iq (piq), but less for verbal iq, assessed by the wechsler abbreviated scale of intelligence. for each inter-quartile (7.73 μg/m 3 ) increase in one-year pm 2.5 preceding each assessment, the average piq score decreased by 3.08 points (95% confidence interval = [−6.04, −0.12]) accounting for within-family/within-individual correlations, demographic characteristics, family socioeconomic status (ses), parents’ cognitive abilities, neighborhood characteristics, and other spatial confounders. the adverse effect was 150% greater in low ses families and 89% stronger in males, compared to their counterparts. better understanding of the social disparities and sexual dimorphism in the adverse pm 2.5 -iq effects may help elucidate the underlying mechanisms and shed light on prevention strategies."
http://orkg.org/orkg/resource/R172170,Are women more generous than men? A meta-analysis,10.1007/s40881-021-00105-9,crossref,"<jats:title>Abstract</jats:title><jats:p>We perform a meta analysis of gender differences in the standard windfall gains dictator game (DG) by collecting raw data from 53 studies with 117 conditions, giving us 15,016 unique individual observations. We find that women on average give 4 percentage points more than men (Cohen’s <jats:inline-formula><jats:alternatives><jats:tex-math>$$d=0.16$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">\n                  <mml:mrow>\n                    <mml:mi>d</mml:mi>\n                    <mml:mo>=</mml:mo>\n                    <mml:mn>0.16</mml:mn>\n                  </mml:mrow>\n                </mml:math></jats:alternatives></jats:inline-formula>), and that this difference decreases to <jats:inline-formula><jats:alternatives><jats:tex-math>$$3.1\\%$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">\n                  <mml:mrow>\n                    <mml:mn>3.1</mml:mn>\n                    <mml:mo>%</mml:mo>\n                  </mml:mrow>\n                </mml:math></jats:alternatives></jats:inline-formula> points (Cohen’s <jats:inline-formula><jats:alternatives><jats:tex-math>$$d=0.13$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">\n                  <mml:mrow>\n                    <mml:mi>d</mml:mi>\n                    <mml:mo>=</mml:mo>\n                    <mml:mn>0.13</mml:mn>\n                  </mml:mrow>\n                </mml:math></jats:alternatives></jats:inline-formula>) if we exclude studies where dictators can only give all or nothing. The gender difference is larger if the recipient in the DG is a charity, compared to the standard DG with an anonymous individual as the recipient (a 10.9 versus a <jats:inline-formula><jats:alternatives><jats:tex-math>$$2.3\\%$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">\n                  <mml:mrow>\n                    <mml:mn>2.3</mml:mn>\n                    <mml:mo>%</mml:mo>\n                  </mml:mrow>\n                </mml:math></jats:alternatives></jats:inline-formula> points gender difference). These effect sizes imply that many individual studies on gender differences are underpowered; the median power in our sample of standard DG studies is only <jats:inline-formula><jats:alternatives><jats:tex-math>$$9\\%$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">\n                  <mml:mrow>\n                    <mml:mn>9</mml:mn>\n                    <mml:mo>%</mml:mo>\n                  </mml:mrow>\n                </mml:math></jats:alternatives></jats:inline-formula> to detect the meta-analytic gender difference at the <jats:inline-formula><jats:alternatives><jats:tex-math>$$5\\%$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">\n                  <mml:mrow>\n                    <mml:mn>5</mml:mn>\n                    <mml:mo>%</mml:mo>\n                  </mml:mrow>\n                </mml:math></jats:alternatives></jats:inline-formula> significance level. Moving forward on this topic, sample sizes should thus be substantially larger than what has been the norm in the past.</jats:p>","abstract we perform a meta analysis of gender differences in the standard windfall gains dictator game (dg) by collecting raw data from 53 studies with 117 conditions, giving us 15,016 unique individual observations. we find that women on average give 4 percentage points more than men (cohen’s $$d=0.16$$ \n \n d \n = \n 0.16 \n \n ), and that this difference decreases to $$3.1\\%$$ \n \n 3.1 \n % \n \n points (cohen’s $$d=0.13$$ \n \n d \n = \n 0.13 \n \n ) if we exclude studies where dictators can only give all or nothing. the gender difference is larger if the recipient in the dg is a charity, compared to the standard dg with an anonymous individual as the recipient (a 10.9 versus a $$2.3\\%$$ \n \n 2.3 \n % \n \n points gender difference). these effect sizes imply that many individual studies on gender differences are underpowered; the median power in our sample of standard dg studies is only $$9\\%$$ \n \n 9 \n % \n \n to detect the meta-analytic gender difference at the $$5\\%$$ \n \n 5 \n % \n \n significance level. moving forward on this topic, sample sizes should thus be substantially larger than what has been the norm in the past."
http://orkg.org/orkg/resource/R171846,Investigation of the material combination 20MnCr5 and X45CrSi9-3 in the Tailored Forming of shafts with bearing seats,10.1007/s11740-022-01119-w,crossref,"<jats:title>Abstract</jats:title><jats:p>The Tailored Forming process chain is used to manufacture hybrid components and consists of a joining process or Additive Manufacturing for various materials (e.g. deposition welding), subsequent hot forming, machining and heat treatment. In this way, components can be produced with materials adapted to the load case. For this paper, hybrid shafts are produced by deposition welding of a cladding made of X45CrSi9-3 onto a workpiece made from 20MnCr5. The hybrid shafts are then formed by means of cross-wedge rolling. It is investigated, how the thickness of the cladding and the type of cooling after hot forming (in air or in water) affect the properties of the cladding. The hybrid shafts are formed without layer separation. However, slight core loosening occurres in the area of the bearing seat due to the Mannesmann effect. The microhardness of the cladding is only slightly effected by the cooling strategy, while the microhardness of the base material is significantly higher in water cooled shafts. The microstructure of the cladding after both cooling strategies consists mainly of martensite. In the base material, air cooling results in a mainly ferritic microstructure with grains of ferrite-pearlite. Quenching in water results in a microstructure containing mainly martensite.</jats:p>","abstract the tailored forming process chain is used to manufacture hybrid components and consists of a joining process or additive manufacturing for various materials (e.g. deposition welding), subsequent hot forming, machining and heat treatment. in this way, components can be produced with materials adapted to the load case. for this paper, hybrid shafts are produced by deposition welding of a cladding made of x45crsi9-3 onto a workpiece made from 20mncr5. the hybrid shafts are then formed by means of cross-wedge rolling. it is investigated, how the thickness of the cladding and the type of cooling after hot forming (in air or in water) affect the properties of the cladding. the hybrid shafts are formed without layer separation. however, slight core loosening occurres in the area of the bearing seat due to the mannesmann effect. the microhardness of the cladding is only slightly effected by the cooling strategy, while the microhardness of the base material is significantly higher in water cooled shafts. the microstructure of the cladding after both cooling strategies consists mainly of martensite. in the base material, air cooling results in a mainly ferritic microstructure with grains of ferrite-pearlite. quenching in water results in a microstructure containing mainly martensite."
http://orkg.org/orkg/resource/R175027,Working with mental models to learn and visualize a new reaction mechanism,10.1039/c9rp00060g,crossref,"<p>Creating and using models are essential skills in chemistry. Novices and experts alike rely on conceptual models to build their own personal mental models for predicting and explaining molecular processes. There is evidence that chemistry students lack rich mental models of the molecular level; their mental models of reaction mechanisms have often been described as static and not process-oriented. Our goal in this study was to characterize the various mental models students may have when learning a new reaction mechanism and to explore how they use them in different situations. We explored the characteristics of first year organic chemistry students’ (<italic>N</italic> = 7) mental models of epoxide-opening reaction mechanisms by qualitative analysis of transcripts and written answers following an audio-recorded interview discussion. We discovered that individual learners relied on a combination of both static (with a focus on symbolism and patterns) and dynamic (reactivity as process or as particles in motion) working mental models, and that different working mental models were used depending on the task. Static working mental models were typically used to reason generally about the reaction mechanism and products that the participants provided. Dynamic working mental models were commonly used when participants were prompted to describe how they pictured the reaction happening, and in attempting to describe the structure of a transition state. Implications for research, teaching, and learning from these findings are described herein.</p>","creating and using models are essential skills in chemistry. novices and experts alike rely on conceptual models to build their own personal mental models for predicting and explaining molecular processes. there is evidence that chemistry students lack rich mental models of the molecular level; their mental models of reaction mechanisms have often been described as static and not process-oriented. our goal in this study was to characterize the various mental models students may have when learning a new reaction mechanism and to explore how they use them in different situations. we explored the characteristics of first year organic chemistry students’ ( n = 7) mental models of epoxide-opening reaction mechanisms by qualitative analysis of transcripts and written answers following an audio-recorded interview discussion. we discovered that individual learners relied on a combination of both static (with a focus on symbolism and patterns) and dynamic (reactivity as process or as particles in motion) working mental models, and that different working mental models were used depending on the task. static working mental models were typically used to reason generally about the reaction mechanism and products that the participants provided. dynamic working mental models were commonly used when participants were prompted to describe how they pictured the reaction happening, and in attempting to describe the structure of a transition state. implications for research, teaching, and learning from these findings are described herein."
http://orkg.org/orkg/resource/R172818,Automated Learning of Social Ontologies,,crossref,"<jats:p>Learned social ontologies can be viewed as products of a social fermentation process, i.e. a process between users who belong in communities of common interests (CoI), in open, collaborative, and communicative environments. In such a setting, social fermentation ensures the automatic encapsulation of agreement and trust of shared knowledge that participating stakeholders provide during an ontology learning task. This chapter discusses the requirements for the automated learning of social ontologies and presents a working method and results of preliminary work. Furthermore, due to its importance for the exploitation of the learned ontologies, it introduces a model for representing the interlinking of agreement, trust and the learned domain conceptualizations that are extracted from social content. The motivation behind this work is an effort towards supporting the design of methods for learning ontologies from social content i.e. methods that aim to learn not only domain conceptualizations but also the degree that agents (software and human) may trust these conceptualizations or not.</jats:p>","learned social ontologies can be viewed as products of a social fermentation process, i.e. a process between users who belong in communities of common interests (coi), in open, collaborative, and communicative environments. in such a setting, social fermentation ensures the automatic encapsulation of agreement and trust of shared knowledge that participating stakeholders provide during an ontology learning task. this chapter discusses the requirements for the automated learning of social ontologies and presents a working method and results of preliminary work. furthermore, due to its importance for the exploitation of the learned ontologies, it introduces a model for representing the interlinking of agreement, trust and the learned domain conceptualizations that are extracted from social content. the motivation behind this work is an effort towards supporting the design of methods for learning ontologies from social content i.e. methods that aim to learn not only domain conceptualizations but also the degree that agents (software and human) may trust these conceptualizations or not."
http://orkg.org/orkg/resource/R172966,Acceptance of Workplace Bullying Behaviors and Job Satisfaction: Moderated Mediation Analysis With Coping Self-Efficacy and Exposure to Bullying,,crossref,"<jats:p> Previous research explored workplace climate as a factor of workplace bullying and coping with workplace bullying, but these concepts were not closely related to workplace bullying behaviors (WBBs). To examine whether the perceived exposure to bullying mediates the relationship between the climate of accepting WBBs and job satisfaction under the condition of different levels of WBBs coping self-efficacy beliefs, we performed moderated mediation analysis. The Negative Acts Questionnaire – Revised was given to 329 employees from Serbia for assessing perceived exposure to bullying. Leaving the original scale items, the instruction of the original Negative Acts Questionnaire – Revised was modified for assessing (1) the climate of accepting WBBs and (2) WBBs coping self-efficacy beliefs. There was a significant negative relationship between exposure to bullying and job satisfaction. WBB acceptance climate was positively related to exposure to workplace bullying and negatively related to job satisfaction. WBB acceptance climate had an indirect relationship with job satisfaction through bullying exposure, and the relationship between WBB acceptance and exposure to bullying was weaker among those who believed that they were more efficient in coping with workplace bullying. Workplace bullying could be sustained by WBB acceptance climate which threatens the job-related outcomes. WBBs coping self-efficacy beliefs have some buffering effects. </jats:p>","previous research explored workplace climate as a factor of workplace bullying and coping with workplace bullying, but these concepts were not closely related to workplace bullying behaviors (wbbs). to examine whether the perceived exposure to bullying mediates the relationship between the climate of accepting wbbs and job satisfaction under the condition of different levels of wbbs coping self-efficacy beliefs, we performed moderated mediation analysis. the negative acts questionnaire – revised was given to 329 employees from serbia for assessing perceived exposure to bullying. leaving the original scale items, the instruction of the original negative acts questionnaire – revised was modified for assessing (1) the climate of accepting wbbs and (2) wbbs coping self-efficacy beliefs. there was a significant negative relationship between exposure to bullying and job satisfaction. wbb acceptance climate was positively related to exposure to workplace bullying and negatively related to job satisfaction. wbb acceptance climate had an indirect relationship with job satisfaction through bullying exposure, and the relationship between wbb acceptance and exposure to bullying was weaker among those who believed that they were more efficient in coping with workplace bullying. workplace bullying could be sustained by wbb acceptance climate which threatens the job-related outcomes. wbbs coping self-efficacy beliefs have some buffering effects."
http://orkg.org/orkg/resource/R172633,SEDE: An ontology for scholarly event description,10.1177/0165551509358487,crossref,"<jats:p> Scholarly events are important scientific communication channels. Our research goal is to satisfy scientists’ basic information needs by collecting, archiving and providing access to scholarly event information. Furthermore, we aim to satisfy users’ in-depth information needs by excavating scholarly meaningful information through reasoning about knowledge. A prerequisite to accomplishing this end is to define a description base for scholarly events to enable software agents to crawl and extract scholarly event data, and to facilitate unified access to this data. The collected data may then be mined for non-obvious knowledge. We present the design and implementation of an ontology for scholarly event description (SEDE) to achieve the research goal, and the application use case scenarios in scholarly event information space. The scenarios presented highlight the characteristics of the SEDE ontology. </jats:p>","scholarly events are important scientific communication channels. our research goal is to satisfy scientists’ basic information needs by collecting, archiving and providing access to scholarly event information. furthermore, we aim to satisfy users’ in-depth information needs by excavating scholarly meaningful information through reasoning about knowledge. a prerequisite to accomplishing this end is to define a description base for scholarly events to enable software agents to crawl and extract scholarly event data, and to facilitate unified access to this data. the collected data may then be mined for non-obvious knowledge. we present the design and implementation of an ontology for scholarly event description (sede) to achieve the research goal, and the application use case scenarios in scholarly event information space. the scenarios presented highlight the characteristics of the sede ontology."
http://orkg.org/orkg/resource/R172672,Named Entity Recognition with Bidirectional LSTM-CNNs,,crossref,"<jats:p> Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information. </jats:p>","named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. in this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional lstm and cnn architecture, eliminating the need for most feature engineering. we also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the conll-2003 dataset and surpasses the previously reported state of the art performance on the ontonotes 5.0 dataset by 2.13 f1 points. by using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an f1 score of 91.62 on conll-2003 and 86.28 on ontonotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
http://orkg.org/orkg/resource/R172784,Dictating the Risk: Experimental Evidence on Giving in Risky Environments,10.1257/aer.103.1.415,crossref,"<jats:p> We study if and how social preferences extend to risky environments. We provide experimental evidence from different versions of dictator games with risky outcomes and establish that preferences that are exclusively based on ex post or on ex ante comparisons cannot generate the observed behavioral patterns. The more money decision-makers transfer in the standard dictator game, the more likely they are to equalize payoff chances under risk. Risk to the recipient does, however, generally decrease the transferred amount. Ultimately, a utility function with a combination of ex post and ex ante fairness concerns may best describe behavior. (JEL C72, D63, D64, D81) </jats:p>","we study if and how social preferences extend to risky environments. we provide experimental evidence from different versions of dictator games with risky outcomes and establish that preferences that are exclusively based on ex post or on ex ante comparisons cannot generate the observed behavioral patterns. the more money decision-makers transfer in the standard dictator game, the more likely they are to equalize payoff chances under risk. risk to the recipient does, however, generally decrease the transferred amount. ultimately, a utility function with a combination of ex post and ex ante fairness concerns may best describe behavior. (jel c72, d63, d64, d81)"
http://orkg.org/orkg/resource/R172579,WISARD·a radical step forward in image recognition,10.1108/eb007637,crossref,"<jats:p>The WISARD recognition system invented at Brunei University has been developed into an industrialised product by Computer Recognition Systems under licence from the British Technology Group. Using statistical pattern classification it already shows great potential in rapid sorting, and research indicates that it will track objects with positional feedback, rather like the human eye.</jats:p>","the wisard recognition system invented at brunei university has been developed into an industrialised product by computer recognition systems under licence from the british technology group. using statistical pattern classification it already shows great potential in rapid sorting, and research indicates that it will track objects with positional feedback, rather like the human eye."
http://orkg.org/orkg/resource/R172941,Feeling Left Out: Underserved Audiences in Science Communication,10.17645/mac.v8i1.2480,crossref,"<jats:p>Science communication only reaches certain segments of society. Various underserved audiences are detached from it and feel left out, which is a challenge for democratic societies that build on informed participation in deliberative processes. While only recently researchers and practitioners have addressed the question on the detailed composition of the not reached groups, even less is known about the emotional impact on underserved audiences: feelings and emotions can play an important role in how science communication is received, and “feeling left out” can be an important aspect of exclusion. In this exploratory study, we provide insights from interviews and focus groups with three different underserved audiences in Germany. We found that on the one hand, material exclusion factors such as available infrastructure or financial means as well as specifically attributable factors such as language skills, are influencing the audience composition of science communication. On the other hand, emotional exclusion factors such as fear, habitual distance, and self- as well as outside-perception also play an important role. Therefore, simply addressing material aspects can only be part of establishing more inclusive science communication practices. Rather, being aware of emotions and feelings can serve as a point of leverage for science communication in reaching out to underserved audiences.</jats:p>","science communication only reaches certain segments of society. various underserved audiences are detached from it and feel left out, which is a challenge for democratic societies that build on informed participation in deliberative processes. while only recently researchers and practitioners have addressed the question on the detailed composition of the not reached groups, even less is known about the emotional impact on underserved audiences: feelings and emotions can play an important role in how science communication is received, and “feeling left out” can be an important aspect of exclusion. in this exploratory study, we provide insights from interviews and focus groups with three different underserved audiences in germany. we found that on the one hand, material exclusion factors such as available infrastructure or financial means as well as specifically attributable factors such as language skills, are influencing the audience composition of science communication. on the other hand, emotional exclusion factors such as fear, habitual distance, and self- as well as outside-perception also play an important role. therefore, simply addressing material aspects can only be part of establishing more inclusive science communication practices. rather, being aware of emotions and feelings can serve as a point of leverage for science communication in reaching out to underserved audiences."
http://orkg.org/orkg/resource/R175438,3D Human Motion Editing and Synthesis: A Survey,10.1155/2014/104535,crossref,"<jats:p>The ways to compute the kinematics and dynamic quantities of human bodies in motion have been studied in many biomedical papers. This paper presents a comprehensive survey of 3D human motion editing and synthesis techniques. Firstly, four types of methods for 3D human motion synthesis are introduced and compared. Secondly, motion capture data representation, motion editing, and motion synthesis are reviewed successively. Finally, future research directions are suggested.</jats:p>","the ways to compute the kinematics and dynamic quantities of human bodies in motion have been studied in many biomedical papers. this paper presents a comprehensive survey of 3d human motion editing and synthesis techniques. firstly, four types of methods for 3d human motion synthesis are introduced and compared. secondly, motion capture data representation, motion editing, and motion synthesis are reviewed successively. finally, future research directions are suggested."
http://orkg.org/orkg/resource/R175090,CUBIC: a new TCP-friendly high-speed TCP variant,10.1145/1400097.1400105,crossref,"<jats:p>CUBIC is a congestion control protocol for TCP (transmission control protocol) and the current default TCP algorithm in Linux. The protocol modifies the linear window growth function of existing TCP standards to be a cubic function in order to improve the scalability of TCP over fast and long distance networks. It also achieves more equitable bandwidth allocations among flows with different RTTs (round trip times) by making the window growth to be independent of RTT -- thus those flows grow their congestion window at the same rate. During steady state, CUBIC increases the window size aggressively when the window is far from the saturation point, and the slowly when it is close to the saturation point. This feature allows CUBIC to be very scalable when the bandwidth and delay product of the network is large, and at the same time, be highly stable and also fair to standard TCP flows. The implementation of CUBIC in Linux has gone through several upgrades. This paper documents its design, implementation, performance and evolution as the default TCP algorithm of Linux.</jats:p>","cubic is a congestion control protocol for tcp (transmission control protocol) and the current default tcp algorithm in linux. the protocol modifies the linear window growth function of existing tcp standards to be a cubic function in order to improve the scalability of tcp over fast and long distance networks. it also achieves more equitable bandwidth allocations among flows with different rtts (round trip times) by making the window growth to be independent of rtt -- thus those flows grow their congestion window at the same rate. during steady state, cubic increases the window size aggressively when the window is far from the saturation point, and the slowly when it is close to the saturation point. this feature allows cubic to be very scalable when the bandwidth and delay product of the network is large, and at the same time, be highly stable and also fair to standard tcp flows. the implementation of cubic in linux has gone through several upgrades. this paper documents its design, implementation, performance and evolution as the default tcp algorithm of linux."
http://orkg.org/orkg/resource/R175297,Ontology learning from text: A look back and into the future,10.1145/2333112.2333115,crossref,"<jats:p>Ontologies are often viewed as the answer to the need for interoperable semantics in modern information systems. The explosion of textual information on the Read/Write Web coupled with the increasing demand for ontologies to power the Semantic Web have made (semi-)automatic ontology learning from text a very promising research area. This together with the advanced state in related areas, such as natural language processing, have fueled research into ontology learning over the past decade. This survey looks at how far we have come since the turn of the millennium and discusses the remaining challenges that will define the research directions in this area in the near future.</jats:p>","ontologies are often viewed as the answer to the need for interoperable semantics in modern information systems. the explosion of textual information on the read/write web coupled with the increasing demand for ontologies to power the semantic web have made (semi-)automatic ontology learning from text a very promising research area. this together with the advanced state in related areas, such as natural language processing, have fueled research into ontology learning over the past decade. this survey looks at how far we have come since the turn of the millennium and discusses the remaining challenges that will define the research directions in this area in the near future."
http://orkg.org/orkg/resource/R175356,Genomic Island Prediction via Chi-Square Test and Random Forest Algorithm,10.1155/2021/9969751,crossref,"<jats:p>Genomic islands are related to microbial adaptation and carry different genomic characteristics from the host. Therefore, many methods have been proposed to detect genomic islands from the rest of the genome by evaluating its sequence composition. Many sequence features have been proposed, but many of them have not been applied to the identification of genomic islands. In this paper, we present a scheme to predict genomic islands using the chi-square test and random forest algorithm. We extract seven kinds of sequence features and select the important features with the chi-square test. All the selected features are then input into the random forest to predict the genome islands. Three experiments and comparison show that the proposed method achieves the best performance. This understanding can be useful to design more powerful method for the genomic island prediction.</jats:p>","genomic islands are related to microbial adaptation and carry different genomic characteristics from the host. therefore, many methods have been proposed to detect genomic islands from the rest of the genome by evaluating its sequence composition. many sequence features have been proposed, but many of them have not been applied to the identification of genomic islands. in this paper, we present a scheme to predict genomic islands using the chi-square test and random forest algorithm. we extract seven kinds of sequence features and select the important features with the chi-square test. all the selected features are then input into the random forest to predict the genome islands. three experiments and comparison show that the proposed method achieves the best performance. this understanding can be useful to design more powerful method for the genomic island prediction."
http://orkg.org/orkg/resource/R176009,Situational Awareness: Detecting Critical Dependencies and Devices in a Network,10.1007/978-3-319-60774-0_17,crossref,"<jats:title>Abstract</jats:title><jats:p>Large-scale networks consisting of thousands of connected devices are like a living organism, constantly changing and evolving. It is very difficult for a human administrator to orient in such environment and to react to emerging security threats. With such motivation, this PhD proposal aims to find new methods for automatic identification of devices, the services they provide, their dependencies and importance. The main focus of the proposal is to find novel approaches to building cyber situational awareness in an unknown network for the purpose of computer security incident response. Our research is at the initial phase and will contribute to a PhD thesis in four years.</jats:p>","abstract large-scale networks consisting of thousands of connected devices are like a living organism, constantly changing and evolving. it is very difficult for a human administrator to orient in such environment and to react to emerging security threats. with such motivation, this phd proposal aims to find new methods for automatic identification of devices, the services they provide, their dependencies and importance. the main focus of the proposal is to find novel approaches to building cyber situational awareness in an unknown network for the purpose of computer security incident response. our research is at the initial phase and will contribute to a phd thesis in four years."
http://orkg.org/orkg/resource/R175456,A deep learning framework for character motion synthesis and editing,10.1145/2897824.2925975,crossref,"<jats:p>We present a framework to synthesize character movements based on high level parameters, such that the produced movements respect the manifold of human motion, trained on a large motion capture dataset. The learned motion manifold, which is represented by the hidden units of a convolutional autoencoder, represents motion data in sparse components which can be combined to produce a wide range of complex movements. To map from high level parameters to the motion manifold, we stack a deep feedforward neural network on top of the trained autoencoder. This network is trained to produce realistic motion sequences from parameters such as a curve over the terrain that the character should follow, or a target location for punching and kicking. The feedforward control network and the motion manifold are trained independently, allowing the user to easily switch between feedforward networks according to the desired interface, without re-training the motion manifold. Once motion is generated it can be edited by performing optimization in the space of the motion manifold. This allows for imposing kinematic constraints, or transforming the style of the motion, while ensuring the edited motion remains natural. As a result, the system can produce smooth, high quality motion sequences without any manual pre-processing of the training data.</jats:p>","we present a framework to synthesize character movements based on high level parameters, such that the produced movements respect the manifold of human motion, trained on a large motion capture dataset. the learned motion manifold, which is represented by the hidden units of a convolutional autoencoder, represents motion data in sparse components which can be combined to produce a wide range of complex movements. to map from high level parameters to the motion manifold, we stack a deep feedforward neural network on top of the trained autoencoder. this network is trained to produce realistic motion sequences from parameters such as a curve over the terrain that the character should follow, or a target location for punching and kicking. the feedforward control network and the motion manifold are trained independently, allowing the user to easily switch between feedforward networks according to the desired interface, without re-training the motion manifold. once motion is generated it can be edited by performing optimization in the space of the motion manifold. this allows for imposing kinematic constraints, or transforming the style of the motion, while ensuring the edited motion remains natural. as a result, the system can produce smooth, high quality motion sequences without any manual pre-processing of the training data."
http://orkg.org/orkg/resource/R178149,A Similarity-Inclusive Link Prediction Based Recommender System Approach,10.5755/j01.eie.25.6.24828,crossref,"<jats:p>Despite being a challenging research field with many unresolved problems, recommender systems are getting more popular in recent years. These systems rely on the personal preferences of users on items given in the form of ratings and return the preferable items based on choices of like-minded users. In this study, a graph-based recommender system using link prediction techniques incorporating similarity metrics is proposed. A graph-based recommender system that has ratings of users on items can be represented as a bipartite graph, where vertices correspond to users and items and edges to ratings. Recommendation generation in a bipartite graph is a link prediction problem. In current literature, modified link prediction approaches are used to distinguish between fundamental relational dualities of like vs. dislike and similar vs. dissimilar. However, the similarity relationship between users/items is mostly disregarded in the complex domain. The proposed model utilizes user-user and item-item cosine similarity value with the relational dualities in order to improve coverage and hits rate of the system by carefully incorporating similarities. On the standard MovieLens Hetrec and MovieLens datasets, the proposed similarity-inclusive link prediction method performed empirically well compared to other methods operating in the complex domain. The experimental results show that the proposed recommender system can be a plausible alternative to overcome the deficiencies in recommender systems.</jats:p>","despite being a challenging research field with many unresolved problems, recommender systems are getting more popular in recent years. these systems rely on the personal preferences of users on items given in the form of ratings and return the preferable items based on choices of like-minded users. in this study, a graph-based recommender system using link prediction techniques incorporating similarity metrics is proposed. a graph-based recommender system that has ratings of users on items can be represented as a bipartite graph, where vertices correspond to users and items and edges to ratings. recommendation generation in a bipartite graph is a link prediction problem. in current literature, modified link prediction approaches are used to distinguish between fundamental relational dualities of like vs. dislike and similar vs. dissimilar. however, the similarity relationship between users/items is mostly disregarded in the complex domain. the proposed model utilizes user-user and item-item cosine similarity value with the relational dualities in order to improve coverage and hits rate of the system by carefully incorporating similarities. on the standard movielens hetrec and movielens datasets, the proposed similarity-inclusive link prediction method performed empirically well compared to other methods operating in the complex domain. the experimental results show that the proposed recommender system can be a plausible alternative to overcome the deficiencies in recommender systems."
http://orkg.org/orkg/resource/R178376,SARS-CoV-2 viral load as a predictor for disease severity in outpatients and hospitalised patients with COVID-19: A prospective cohort study,10.1371/journal.pone.0258421,crossref,"<jats:sec id=""sec001"">\n<jats:title>Introduction</jats:title>\n<jats:p>We aimed to examine if severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) polymerase chain reaction (PCR) cycle quantification (C<jats:sub>q</jats:sub>) value, as a surrogate for SARS-CoV-2 viral load, could predict hospitalisation and disease severity in adult patients with coronavirus disease 2019 (COVID-19).</jats:p>\n</jats:sec>\n<jats:sec id=""sec002"">\n<jats:title>Methods</jats:title>\n<jats:p>We performed a prospective cohort study of adult patients with PCR positive SARS-CoV-2 airway samples including all out-patients registered at the Department of Infectious Diseases, Odense University Hospital (OUH) March 9-March 17 2020, and all hospitalised patients at OUH March 10-April 21 2020. To identify associations between C<jats:sub>q</jats:sub>-values and a) hospital admission and b) a severe outcome, logistic regression analyses were used to compute odds ratios (OR) and 95% Confidence Intervals (CI), adjusting for confounding factors (aOR).</jats:p>\n</jats:sec>\n<jats:sec id=""sec003"">\n<jats:title>Results</jats:title>\n<jats:p>We included 87 non-hospitalised and 82 hospitalised patients. The median baseline C<jats:sub>q</jats:sub>-value was 25.5 (interquartile range 22.3–29.0). We found a significant association between increasing C<jats:sub>q</jats:sub>-value and hospital-admission in univariate analysis (OR 1.11, 95% CI 1.04–1.19). However, this was due to an association between time from symptom onset to testing and C<jats:sub>q</jats:sub>-values, and no association was found in the adjusted analysis (aOR 1.08, 95% CI 0.94–1.23). In hospitalised patients, a significant association between lower C<jats:sub>q</jats:sub>-values and higher risk of severe disease was found (aOR 0.89, 95% CI 0.81–0.98), independent of timing of testing.</jats:p>\n</jats:sec>\n<jats:sec id=""sec004"">\n<jats:title>Conclusions</jats:title>\n<jats:p>SARS-CoV-2 PCR C<jats:sub>q</jats:sub>-values in outpatients correlated with time after symptom onset, but was not a predictor of hospitalisation. However, in hospitalised patients lower C<jats:sub>q</jats:sub>-values were associated with higher risk of severe disease.</jats:p>\n</jats:sec>","\n introduction \n we aimed to examine if severe acute respiratory syndrome coronavirus 2 (sars-cov-2) polymerase chain reaction (pcr) cycle quantification (c q ) value, as a surrogate for sars-cov-2 viral load, could predict hospitalisation and disease severity in adult patients with coronavirus disease 2019 (covid-19). \n \n \n methods \n we performed a prospective cohort study of adult patients with pcr positive sars-cov-2 airway samples including all out-patients registered at the department of infectious diseases, odense university hospital (ouh) march 9-march 17 2020, and all hospitalised patients at ouh march 10-april 21 2020. to identify associations between c q -values and a) hospital admission and b) a severe outcome, logistic regression analyses were used to compute odds ratios (or) and 95% confidence intervals (ci), adjusting for confounding factors (aor). \n \n \n results \n we included 87 non-hospitalised and 82 hospitalised patients. the median baseline c q -value was 25.5 (interquartile range 22.3–29.0). we found a significant association between increasing c q -value and hospital-admission in univariate analysis (or 1.11, 95% ci 1.04–1.19). however, this was due to an association between time from symptom onset to testing and c q -values, and no association was found in the adjusted analysis (aor 1.08, 95% ci 0.94–1.23). in hospitalised patients, a significant association between lower c q -values and higher risk of severe disease was found (aor 0.89, 95% ci 0.81–0.98), independent of timing of testing. \n \n \n conclusions \n sars-cov-2 pcr c q -values in outpatients correlated with time after symptom onset, but was not a predictor of hospitalisation. however, in hospitalised patients lower c q -values were associated with higher risk of severe disease. \n"
http://orkg.org/orkg/resource/R178334,Traffic Flow Prediction for Smart Traffic Lights Using Machine Learning Algorithms,10.3390/technologies10010005,crossref,"<jats:p>Nowadays, many cities have problems with traffic congestion at certain peak hours, which produces more pollution, noise and stress for citizens. Neural networks (NN) and machine-learning (ML) approaches are increasingly used to solve real-world problems, overcoming analytical and statistical methods, due to their ability to deal with dynamic behavior over time and with a large number of parameters in massive data. In this paper, machine-learning (ML) and deep-learning (DL) algorithms are proposed for predicting traffic flow at an intersection, thus laying the groundwork for adaptive traffic control, either by remote control of traffic lights or by applying an algorithm that adjusts the timing according to the predicted flow. Therefore, this work only focuses on traffic flow prediction. Two public datasets are used to train, validate and test the proposed ML and DL models. The first one contains the number of vehicles sampled every five minutes at six intersections for 56 days using different sensors. For this research, four of the six intersections are used to train the ML and DL models. The Multilayer Perceptron Neural Network (MLP-NN) obtained better results (R-Squared and EV score of 0.93) and took less training time, followed closely by Gradient Boosting then Recurrent Neural Networks (RNNs), with good metrics results but the longer training time, and finally Random Forest, Linear Regression and Stochastic Gradient. All ML and DL algorithms scored good performance metrics, indicating that they are feasible for implementation on smart traffic light controllers.</jats:p>","nowadays, many cities have problems with traffic congestion at certain peak hours, which produces more pollution, noise and stress for citizens. neural networks (nn) and machine-learning (ml) approaches are increasingly used to solve real-world problems, overcoming analytical and statistical methods, due to their ability to deal with dynamic behavior over time and with a large number of parameters in massive data. in this paper, machine-learning (ml) and deep-learning (dl) algorithms are proposed for predicting traffic flow at an intersection, thus laying the groundwork for adaptive traffic control, either by remote control of traffic lights or by applying an algorithm that adjusts the timing according to the predicted flow. therefore, this work only focuses on traffic flow prediction. two public datasets are used to train, validate and test the proposed ml and dl models. the first one contains the number of vehicles sampled every five minutes at six intersections for 56 days using different sensors. for this research, four of the six intersections are used to train the ml and dl models. the multilayer perceptron neural network (mlp-nn) obtained better results (r-squared and ev score of 0.93) and took less training time, followed closely by gradient boosting then recurrent neural networks (rnns), with good metrics results but the longer training time, and finally random forest, linear regression and stochastic gradient. all ml and dl algorithms scored good performance metrics, indicating that they are feasible for implementation on smart traffic light controllers."
http://orkg.org/orkg/resource/R178482,"Viral load dynamics and disease severity in patients infected with SARS-CoV-2 in Zhejiang province, China, January-March 2020: retrospective cohort study",10.1136/bmj.m1443,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Objective</jats:title><jats:p>To evaluate viral loads at different stages of disease progression in patients infected with the 2019 severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) during the first four months of the epidemic in Zhejiang province, China.</jats:p></jats:sec><jats:sec><jats:title>Design</jats:title><jats:p>Retrospective cohort study.</jats:p></jats:sec><jats:sec><jats:title>Setting</jats:title><jats:p>A designated hospital for patients with covid-19 in Zhejiang province, China.</jats:p></jats:sec><jats:sec><jats:title>Participants</jats:title><jats:p>96 consecutively admitted patients with laboratory confirmed SARS-CoV-2 infection: 22 with mild disease and 74 with severe disease. Data were collected from 19 January 2020 to 20 March 2020.</jats:p></jats:sec><jats:sec><jats:title>Main outcome measures</jats:title><jats:p>Ribonucleic acid (RNA) viral load measured in respiratory, stool, serum, and urine samples. Cycle threshold values, a measure of nucleic acid concentration, were plotted onto the standard curve constructed on the basis of the standard product. Epidemiological, clinical, and laboratory characteristics and treatment and outcomes data were obtained through data collection forms from electronic medical records, and the relation between clinical data and disease severity was analysed.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>3497 respiratory, stool, serum, and urine samples were collected from patients after admission and evaluated for SARS-CoV-2 RNA viral load. Infection was confirmed in all patients by testing sputum and saliva samples. RNA was detected in the stool of 55 (59%) patients and in the serum of 39 (41%) patients. The urine sample from one patient was positive for SARS-CoV-2. The median duration of virus in stool (22 days, interquartile range 17-31 days) was significantly longer than in respiratory (18 days, 13-29 days; P=0.02) and serum samples (16 days, 11-21 days; P&lt;0.001). The median duration of virus in the respiratory samples of patients with severe disease (21 days, 14-30 days) was significantly longer than in patients with mild disease (14 days, 10-21 days; P=0.04). In the mild group, the viral loads peaked in respiratory samples in the second week from disease onset, whereas viral load continued to be high during the third week in the severe group. Virus duration was longer in patients older than 60 years and in male patients.</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>The duration of SARS-CoV-2 is significantly longer in stool samples than in respiratory and serum samples, highlighting the need to strengthen the management of stool samples in the prevention and control of the epidemic, and the virus persists longer with higher load and peaks later in the respiratory tissue of patients with severe disease.</jats:p></jats:sec>","abstract objective to evaluate viral loads at different stages of disease progression in patients infected with the 2019 severe acute respiratory syndrome coronavirus 2 (sars-cov-2) during the first four months of the epidemic in zhejiang province, china. design retrospective cohort study. setting a designated hospital for patients with covid-19 in zhejiang province, china. participants 96 consecutively admitted patients with laboratory confirmed sars-cov-2 infection: 22 with mild disease and 74 with severe disease. data were collected from 19 january 2020 to 20 march 2020. main outcome measures ribonucleic acid (rna) viral load measured in respiratory, stool, serum, and urine samples. cycle threshold values, a measure of nucleic acid concentration, were plotted onto the standard curve constructed on the basis of the standard product. epidemiological, clinical, and laboratory characteristics and treatment and outcomes data were obtained through data collection forms from electronic medical records, and the relation between clinical data and disease severity was analysed. results 3497 respiratory, stool, serum, and urine samples were collected from patients after admission and evaluated for sars-cov-2 rna viral load. infection was confirmed in all patients by testing sputum and saliva samples. rna was detected in the stool of 55 (59%) patients and in the serum of 39 (41%) patients. the urine sample from one patient was positive for sars-cov-2. the median duration of virus in stool (22 days, interquartile range 17-31 days) was significantly longer than in respiratory (18 days, 13-29 days; p=0.02) and serum samples (16 days, 11-21 days; p&lt;0.001). the median duration of virus in the respiratory samples of patients with severe disease (21 days, 14-30 days) was significantly longer than in patients with mild disease (14 days, 10-21 days; p=0.04). in the mild group, the viral loads peaked in respiratory samples in the second week from disease onset, whereas viral load continued to be high during the third week in the severe group. virus duration was longer in patients older than 60 years and in male patients. conclusion the duration of sars-cov-2 is significantly longer in stool samples than in respiratory and serum samples, highlighting the need to strengthen the management of stool samples in the prevention and control of the epidemic, and the virus persists longer with higher load and peaks later in the respiratory tissue of patients with severe disease."
http://orkg.org/orkg/resource/R184047,Theoretical energies for the n = 1 and 2 states of the helium isoelectronic sequence up to Z = 100,10.1139/p88-100,crossref,"<jats:p> The unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s<jats:sup>2</jats:sup>\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>3</jats:sup>S<jats:sub>1</jats:sub>, 1s2p\u2002<jats:sup>1</jats:sup>P<jats:sub>1</jats:sub>, and 1s2p\u2002<jats:sup>3</jats:sup>P<jats:sub>0,1,2</jats:sub> staters of helium-like ions. Detailed tabulations are presented for all ions in the range 2\u2002≤\u2002Z\u2002≤\u2002100 and are compared with a wide range of experimental data up to <jats:sup>34</jats:sup>Kr<jats:sup>+</jats:sup>. The results for <jats:sup>90</jats:sup>U<jats:sup>+</jats:sup> significantly alter the recent Lamb shift measurement of Munger and Gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002eV, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002eV. The improved agreement is due to the inclusion of higher order two-electron corrections in the present work. </jats:p>","the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \u2002 1 s 0 , 1s2s\u2002 1 s 0 , 1s2s\u2002 3 s 1 , 1s2p\u2002 1 p 1 , and 1s2p\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\u2002≤\u2002z\u2002≤\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002ev, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
http://orkg.org/orkg/resource/R184054,Theoretical energies for the <i>n</i> = 1 and 2 states of the helium isoelectronic sequence up to <i>Z</i> = 100,10.1139/p88-100,crossref,"<jats:p> The unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s<jats:sup>2</jats:sup>\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>3</jats:sup>S<jats:sub>1</jats:sub>, 1s2p\u2002<jats:sup>1</jats:sup>P<jats:sub>1</jats:sub>, and 1s2p\u2002<jats:sup>3</jats:sup>P<jats:sub>0,1,2</jats:sub> staters of helium-like ions. Detailed tabulations are presented for all ions in the range 2\u2002≤\u2002Z\u2002≤\u2002100 and are compared with a wide range of experimental data up to <jats:sup>34</jats:sup>Kr<jats:sup>+</jats:sup>. The results for <jats:sup>90</jats:sup>U<jats:sup>+</jats:sup> significantly alter the recent Lamb shift measurement of Munger and Gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002eV, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002eV. The improved agreement is due to the inclusion of higher order two-electron corrections in the present work. </jats:p>","the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \u2002 1 s 0 , 1s2s\u2002 1 s 0 , 1s2s\u2002 3 s 1 , 1s2p\u2002 1 p 1 , and 1s2p\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\u2002≤\u2002z\u2002≤\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002ev, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
http://orkg.org/orkg/resource/R184009,"Market Access, Production Diversity, and Diet Diversity: Evidence From India",10.1177/0379572120920061,crossref,"<jats:sec><jats:title>Background:</jats:title><jats:p> Recent literature, largely from Africa, shows mixed effects of own-production on diet diversity. However, the role of own-production, relative to markets, in influencing food consumption becomes more pronounced as market integration increases. </jats:p></jats:sec><jats:sec><jats:title>Objective:</jats:title><jats:p> This paper investigates the relative importance of two factors - production diversity and household market integration - for the intake of a nutritious diet by women and households in rural India. </jats:p></jats:sec><jats:sec><jats:title>Methods:</jats:title><jats:p> Data analysis is based on primary data from an extensive agriculture-nutrition survey of 3600 Indian households that was collected in 2017. Dietary diversity scores are constructed for women and households is based on 24-hour and 7-day recall periods. Household market integration is measured as monthly household expenditure on key non-staple food groups. We measure production diversity in two ways - field-level and on-farm production diversity - in order to account for the cereal centric rice-wheat cropping system found in our study locations. The analysis is based on Ordinary Least Squares regressions where we control for a variety of village, household, and individual level covariates that affect food consumption, and village fixed effects. Robustness checks are done by way of using a Poisson regression specifications and 7-day recall period. </jats:p></jats:sec><jats:sec><jats:title>Results:</jats:title><jats:p> Conventional measures of field-level production diversity, like the number of crops or food groups grown, have no significant association with diet diversity. In contrast, it is on-farm production diversity (the field-level cultivation of pulses and on-farm livestock management, and kitchen gardens in the longer run) that is significantly associated with improved dietary diversity scores, thus suggesting the importance of non-staples in improving both individual and household dietary diversity. Furthermore, market purchases of non-staples like pulses and dairy products are associated with a significantly higher dietary diversity. Other significant determinants of dietary diversity include women’s literacy and awareness of nutrition. These results mostly remain robust to changes in the recall period of the diet diversity measure and the nature of the empirical specification. </jats:p></jats:sec><jats:sec><jats:title>Conclusions:</jats:title><jats:p> This study contributes to the scarce empirical evidence related to diets in India. Additionally, our results indicate some key intervention areas - promoting livestock rearing, strengthening households’ market integration (for purchase of non-staples) and increasing women’s awareness about nutrition. These are more impactful than raising production diversity. </jats:p></jats:sec>","background: recent literature, largely from africa, shows mixed effects of own-production on diet diversity. however, the role of own-production, relative to markets, in influencing food consumption becomes more pronounced as market integration increases. objective: this paper investigates the relative importance of two factors - production diversity and household market integration - for the intake of a nutritious diet by women and households in rural india. methods: data analysis is based on primary data from an extensive agriculture-nutrition survey of 3600 indian households that was collected in 2017. dietary diversity scores are constructed for women and households is based on 24-hour and 7-day recall periods. household market integration is measured as monthly household expenditure on key non-staple food groups. we measure production diversity in two ways - field-level and on-farm production diversity - in order to account for the cereal centric rice-wheat cropping system found in our study locations. the analysis is based on ordinary least squares regressions where we control for a variety of village, household, and individual level covariates that affect food consumption, and village fixed effects. robustness checks are done by way of using a poisson regression specifications and 7-day recall period. results: conventional measures of field-level production diversity, like the number of crops or food groups grown, have no significant association with diet diversity. in contrast, it is on-farm production diversity (the field-level cultivation of pulses and on-farm livestock management, and kitchen gardens in the longer run) that is significantly associated with improved dietary diversity scores, thus suggesting the importance of non-staples in improving both individual and household dietary diversity. furthermore, market purchases of non-staples like pulses and dairy products are associated with a significantly higher dietary diversity. other significant determinants of dietary diversity include women’s literacy and awareness of nutrition. these results mostly remain robust to changes in the recall period of the diet diversity measure and the nature of the empirical specification. conclusions: this study contributes to the scarce empirical evidence related to diets in india. additionally, our results indicate some key intervention areas - promoting livestock rearing, strengthening households’ market integration (for purchase of non-staples) and increasing women’s awareness about nutrition. these are more impactful than raising production diversity."
http://orkg.org/orkg/resource/R182393,The association between crop and income diversity and farmer intra-household dietary diversity in India,10.1007/s12571-020-01012-3,crossref,"<jats:title>Abstract</jats:title><jats:p>This paper investigates the associations between crop and income diversity and dietary diversity among men, women, adolescents, and children of farmer households in India. We examine crop, income, and dietary data collected from 1106 farmer households across Gujarat and Haryana, two states that represent different livelihood transition pathways in India. Regression results suggest that crop diversity had a positive association with dietary diversity among adults (both men and women) in both states, and among adolescents and children in Haryana. Higher family education and annual income were the two most important factors associated with higher dietary diversity score (DDS) in Gujarat whereas, higher family education, greater crop diversity, and increased distance traveled to markets were the most important factors associated with higher individual DDS in Haryana. Specifically, for children, crop diversity emerged as one of the most important factors associated with dietary diversity in both states. Interestingly, we find that even in these two relatively prosperous states, the pathways to dietary diversity vary across sites and within households, suggesting that policies to improve dietary diversity should be tailored to a given location and context.</jats:p>","abstract this paper investigates the associations between crop and income diversity and dietary diversity among men, women, adolescents, and children of farmer households in india. we examine crop, income, and dietary data collected from 1106 farmer households across gujarat and haryana, two states that represent different livelihood transition pathways in india. regression results suggest that crop diversity had a positive association with dietary diversity among adults (both men and women) in both states, and among adolescents and children in haryana. higher family education and annual income were the two most important factors associated with higher dietary diversity score (dds) in gujarat whereas, higher family education, greater crop diversity, and increased distance traveled to markets were the most important factors associated with higher individual dds in haryana. specifically, for children, crop diversity emerged as one of the most important factors associated with dietary diversity in both states. interestingly, we find that even in these two relatively prosperous states, the pathways to dietary diversity vary across sites and within households, suggesting that policies to improve dietary diversity should be tailored to a given location and context."
http://orkg.org/orkg/resource/R184006,A GeoSPARQL Compliance Benchmark,10.3390/ijgi10070487,crossref,"<jats:p>GeoSPARQL is an important standard for the geospatial linked data community, given that it defines a vocabulary for representing geospatial data in RDF, defines an extension to SPARQL for processing geospatial data, and provides support for both qualitative and quantitative spatial reasoning. However, what the community is missing is a comprehensive and objective way to measure the extent of GeoSPARQL support in GeoSPARQL-enabled RDF triplestores. To fill this gap, we developed the GeoSPARQL compliance benchmark. We propose a series of tests that check for the compliance of RDF triplestores with the GeoSPARQL standard, in order to test how many of the requirements outlined in the standard a tested system supports. This topic is of concern because the support of GeoSPARQL varies greatly between different triplestore implementations, and the extent of support is of great importance for different users. In order to showcase the benchmark and its applicability, we present a comparison of the benchmark results of several triplestores, providing an insight into their current GeoSPARQL support and the overall GeoSPARQL support in the geospatial linked data domain.</jats:p>","geosparql is an important standard for the geospatial linked data community, given that it defines a vocabulary for representing geospatial data in rdf, defines an extension to sparql for processing geospatial data, and provides support for both qualitative and quantitative spatial reasoning. however, what the community is missing is a comprehensive and objective way to measure the extent of geosparql support in geosparql-enabled rdf triplestores. to fill this gap, we developed the geosparql compliance benchmark. we propose a series of tests that check for the compliance of rdf triplestores with the geosparql standard, in order to test how many of the requirements outlined in the standard a tested system supports. this topic is of concern because the support of geosparql varies greatly between different triplestore implementations, and the extent of support is of great importance for different users. in order to showcase the benchmark and its applicability, we present a comparison of the benchmark results of several triplestores, providing an insight into their current geosparql support and the overall geosparql support in the geospatial linked data domain."
http://orkg.org/orkg/resource/R184042,Innovation under pressure: Implications for data privacy during the Covid-19 pandemic,,crossref,"<jats:p> The global Covid-19 pandemic has resulted in social and economic disruption unprecedented in the modern era. Many countries have introduced severe measures to contain the virus, including travel restrictions, public event bans, non-essential business closures and remote work policies. While digital technologies help governments and organizations to enforce protection measures, such as contact tracing, their rushed deployment and adoption also raises profound concerns about surveillance, privacy and data protection. This article presents two critical cases on digital surveillance technologies implemented during the Covid-19 pandemic and delineates the privacy implications thereof. We explain the contextual nature of privacy trade-offs during a pandemic and explore how regulatory and technical responses are needed to protect privacy in such circumstances. By providing a multi-disciplinary conversation on the value of privacy and data protection during a global pandemic, this article reflects on the implications digital solutions have for the future and raises the question of whether there is a way to have expedited privacy assessments that could anticipate and help mitigate adverse privacy implications these may have on society. </jats:p>","the global covid-19 pandemic has resulted in social and economic disruption unprecedented in the modern era. many countries have introduced severe measures to contain the virus, including travel restrictions, public event bans, non-essential business closures and remote work policies. while digital technologies help governments and organizations to enforce protection measures, such as contact tracing, their rushed deployment and adoption also raises profound concerns about surveillance, privacy and data protection. this article presents two critical cases on digital surveillance technologies implemented during the covid-19 pandemic and delineates the privacy implications thereof. we explain the contextual nature of privacy trade-offs during a pandemic and explore how regulatory and technical responses are needed to protect privacy in such circumstances. by providing a multi-disciplinary conversation on the value of privacy and data protection during a global pandemic, this article reflects on the implications digital solutions have for the future and raises the question of whether there is a way to have expedited privacy assessments that could anticipate and help mitigate adverse privacy implications these may have on society."
http://orkg.org/orkg/resource/R182127,"Crop diversity is associated with higher child diet diversity in Ethiopia, particularly among low-income households, but not in Vietnam",10.1017/S1368980021003281,crossref,"<jats:title>Abstract</jats:title><jats:sec id=""S1368980021003281_as1""><jats:title>Objectives:</jats:title><jats:p>To examine associations of household crop diversity with school-aged child dietary diversity in Vietnam and Ethiopia and mechanisms underlying these associations.</jats:p></jats:sec><jats:sec id=""S1368980021003281_as2""><jats:title>Design:</jats:title><jats:p>We created a child diet diversity score (DDS) using data on seven food groups consumed in the last 24 h. Generalised estimating equations were used to model associations of household-level crop diversity, measured as a count of crop species richness (CSR) and of plant crop nutritional functional richness (CNFR), with DDS. We examined effect modification by household wealth and subsistence orientation, and mediation by the farm’s market orientation.</jats:p></jats:sec><jats:sec id=""S1368980021003281_as3""><jats:title>Setting:</jats:title><jats:p>Two survey years of longitudinal data from the Young Lives cohort.</jats:p></jats:sec><jats:sec id=""S1368980021003281_as4""><jats:title>Participants:</jats:title><jats:p>Children (aged 5 years in 2006 and 8 years in 2009) from rural farming households in Ethiopia (<jats:italic>n</jats:italic> 1012) and Vietnam (<jats:italic>n</jats:italic> 1083).</jats:p></jats:sec><jats:sec id=""S1368980021003281_as5""><jats:title>Results:</jats:title><jats:p>There was a small, positive association between household CNFR and DDS in Ethiopia (CNFR–DDS, <jats:italic>β</jats:italic> = 0·13; (95 % CI 0·07, 0·19)), but not in Vietnam. Associations of crop diversity and child diet diversity were strongest among poor households in Ethiopia and among subsistence-oriented households in Vietnam. Agricultural earnings positively mediated the crop diversity–diet diversity association in Ethiopia.</jats:p></jats:sec><jats:sec id=""S1368980021003281_as6""><jats:title>Discussion:</jats:title><jats:p>Children from households that are poorer and those that rely more on their own agricultural production for food may benefit most from increased crop diversity.</jats:p></jats:sec>","abstract objectives: to examine associations of household crop diversity with school-aged child dietary diversity in vietnam and ethiopia and mechanisms underlying these associations. design: we created a child diet diversity score (dds) using data on seven food groups consumed in the last 24 h. generalised estimating equations were used to model associations of household-level crop diversity, measured as a count of crop species richness (csr) and of plant crop nutritional functional richness (cnfr), with dds. we examined effect modification by household wealth and subsistence orientation, and mediation by the farm’s market orientation. setting: two survey years of longitudinal data from the young lives cohort. participants: children (aged 5 years in 2006 and 8 years in 2009) from rural farming households in ethiopia ( n 1012) and vietnam ( n 1083). results: there was a small, positive association between household cnfr and dds in ethiopia (cnfr–dds, β = 0·13; (95 % ci 0·07, 0·19)), but not in vietnam. associations of crop diversity and child diet diversity were strongest among poor households in ethiopia and among subsistence-oriented households in vietnam. agricultural earnings positively mediated the crop diversity–diet diversity association in ethiopia. discussion: children from households that are poorer and those that rely more on their own agricultural production for food may benefit most from increased crop diversity."
http://orkg.org/orkg/resource/R184028,"A Literature Review on the GDPR, COVID-19 and the Ethical Considerations of Data Protection During a Time of Crisis",,crossref,"""<jats:title>Summary</jats:title><jats:p>\n          Objective: This survey article presents a literature review of relevant publications aiming to explore whether the EU's General Data Protection Regulation (GDPR) has held true during a time of crisis and the implications that arose during the COVID-19 outbreak.</jats:p><jats:p>\n          Method and Results: Based on the approach taken and the screening of the relevant articles, the results focus on three themes: a critique on GDPR; the ethics surrounding the use of digital health technologies, namely in the form of mobile applications; and the possibility of cross border transfers of said data outside of Europe. Within this context, the article reviews the arising themes, considers the use of data through mobile health applications, and discusses whether data protection may require a revision when balancing societal and personal interests.</jats:p><jats:p>\n          Conclusions: In summary, although it is clear that the GDPR has been applied through a mixed and complex experience with data handling during the pandemic, the COVID-19 pandemic has indeed shown that it was a test the GDPR was designed and prepared to undertake. The article suggests that further review and research is needed to first ensure that an understanding of the state of the art in data protection during the pandemic is maintained and second to subsequently explore and carefully create a specific framework for the ethical considerations involved. The paper echoes the literature reviewed and calls for the creation of a unified and harmonised network or database to enable the secure data sharing across borders.</jats:p>""",""" summary \n objective: this survey article presents a literature review of relevant publications aiming to explore whether the eu's general data protection regulation (gdpr) has held true during a time of crisis and the implications that arose during the covid-19 outbreak. \n method and results: based on the approach taken and the screening of the relevant articles, the results focus on three themes: a critique on gdpr; the ethics surrounding the use of digital health technologies, namely in the form of mobile applications; and the possibility of cross border transfers of said data outside of europe. within this context, the article reviews the arising themes, considers the use of data through mobile health applications, and discusses whether data protection may require a revision when balancing societal and personal interests. \n conclusions: in summary, although it is clear that the gdpr has been applied through a mixed and complex experience with data handling during the pandemic, the covid-19 pandemic has indeed shown that it was a test the gdpr was designed and prepared to undertake. the article suggests that further review and research is needed to first ensure that an understanding of the state of the art in data protection during the pandemic is maintained and second to subsequently explore and carefully create a specific framework for the ethical considerations involved. the paper echoes the literature reviewed and calls for the creation of a unified and harmonised network or database to enable the secure data sharing across borders. """
http://orkg.org/orkg/resource/R182148,"Farm production, market access and dietary diversity in Malawi",10.1017/s1368980016002135,crossref,"<jats:title>Abstract</jats:title><jats:sec id=""S1368980016002135_abs1"" sec-type=""general""><jats:title>Objective</jats:title><jats:p>The association between farm production diversity and dietary diversity in rural smallholder households was recently analysed. Most existing studies build on household-level dietary diversity indicators calculated from 7d food consumption recalls. Herein, this association is revisited with individual-level 24 h recall data. The robustness of the results is tested by comparing household- and individual-level estimates. The role of other factors that may influence dietary diversity, such as market access and agricultural technology, is also analysed.</jats:p></jats:sec><jats:sec id=""S1368980016002135_abs2"" sec-type=""general""><jats:title>Design</jats:title><jats:p>A survey of smallholder farm households was carried out in Malawi in 2014. Dietary diversity scores are calculated from 24 h recall data. Production diversity scores are calculated from farm production data covering a period of 12 months. Individual- and household-level regression models are developed and estimated.</jats:p></jats:sec><jats:sec id=""S1368980016002135_abs3"" sec-type=""general""><jats:title>Setting</jats:title><jats:p>Data were collected in sixteen districts of central and southern Malawi.</jats:p></jats:sec><jats:sec id=""S1368980016002135_abs4"" sec-type=""subjects""><jats:title>Subjects</jats:title><jats:p>Smallholder farm households (<jats:italic>n</jats:italic>408), young children (<jats:italic>n</jats:italic>519) and mothers (<jats:italic>n</jats:italic>408).</jats:p></jats:sec><jats:sec id=""S1368980016002135_abs5"" sec-type=""results""><jats:title>Results</jats:title><jats:p>Farm production diversity is positively associated with dietary diversity. However, the estimated effects are small. Access to markets for buying food and selling farm produce and use of chemical fertilizers are shown to be more important for dietary diversity than diverse farm production. Results with household- and individual-level dietary data are very similar.</jats:p></jats:sec><jats:sec id=""S1368980016002135_abs6"" sec-type=""conclusions""><jats:title>Conclusions</jats:title><jats:p>Further increasing production diversity may not be the most effective strategy to improve diets in smallholder farm households. Improving access to markets, productivity-enhancing inputs and technologies seems to be more promising.</jats:p></jats:sec>","abstract objective the association between farm production diversity and dietary diversity in rural smallholder households was recently analysed. most existing studies build on household-level dietary diversity indicators calculated from 7d food consumption recalls. herein, this association is revisited with individual-level 24 h recall data. the robustness of the results is tested by comparing household- and individual-level estimates. the role of other factors that may influence dietary diversity, such as market access and agricultural technology, is also analysed. design a survey of smallholder farm households was carried out in malawi in 2014. dietary diversity scores are calculated from 24 h recall data. production diversity scores are calculated from farm production data covering a period of 12 months. individual- and household-level regression models are developed and estimated. setting data were collected in sixteen districts of central and southern malawi. subjects smallholder farm households ( n 408), young children ( n 519) and mothers ( n 408). results farm production diversity is positively associated with dietary diversity. however, the estimated effects are small. access to markets for buying food and selling farm produce and use of chemical fertilizers are shown to be more important for dietary diversity than diverse farm production. results with household- and individual-level dietary data are very similar. conclusions further increasing production diversity may not be the most effective strategy to improve diets in smallholder farm households. improving access to markets, productivity-enhancing inputs and technologies seems to be more promising."
http://orkg.org/orkg/resource/R182241,COVID-19 Disruptions Disproportionately Affect Female Academics,10.1257/pandp.20211017,crossref,"""<jats:p> The rapid spread of the COVID-19 pandemic and subsequent countermeasures disrupted economic activity around the world. We investigate the effects of COVID-19 disruptions on the gender gap in academia. We administer a global survey of academics to collect nuanced data on the respondents' circumstances, such as the number and ages of children and time use. All academics report substantial increases in childcare and housework burdens, but women experienced significantly larger increases than men. Female academics with children report a disproportionate reduction in research time, both relative to childless men and women and to male academics with children. </jats:p>""",""" the rapid spread of the covid-19 pandemic and subsequent countermeasures disrupted economic activity around the world. we investigate the effects of covid-19 disruptions on the gender gap in academia. we administer a global survey of academics to collect nuanced data on the respondents' circumstances, such as the number and ages of children and time use. all academics report substantial increases in childcare and housework burdens, but women experienced significantly larger increases than men. female academics with children report a disproportionate reduction in research time, both relative to childless men and women and to male academics with children. """
http://orkg.org/orkg/resource/R184044,"Big data, privacy and COVID-19 – learning from humanitarian expertise in data protection",,crossref,"<jats:title>Abstract</jats:title><jats:p>The COVID-19 pandemic leads governments around the world to resort to tracking technology and other data-driven tools in order to monitor and curb the spread of SARS-CoV-2. Such large-scale incursion into privacy and data protection is unthinkable during times of normalcy. However, in times of a pandemic the use of location data provided by telecom operators and/or technology companies becomes a viable option. Importantly, legal regulations hardly protect people’s privacy against governmental and corporate misuse. Established privacy regimes are focused on individual consent, and most human rights treaties know derogations from privacy and data protection norms for states of emergency. This leaves little safeguards nor remedies to guarantee individual and collective autonomy. However, the challenge of responsible data use during a crisis is not novel. The humanitarian sector has more than a decade of experience to offer. International organisations and humanitarian actors have developed detailed guidelines on how to use data responsibly under extreme circumstances. This article briefly addresses the legal gap of data protection and privacy during this global crisis. Then it outlines the state of the art in humanitarian practice and academia on data protection and data responsibility during crisis.</jats:p>","abstract the covid-19 pandemic leads governments around the world to resort to tracking technology and other data-driven tools in order to monitor and curb the spread of sars-cov-2. such large-scale incursion into privacy and data protection is unthinkable during times of normalcy. however, in times of a pandemic the use of location data provided by telecom operators and/or technology companies becomes a viable option. importantly, legal regulations hardly protect people’s privacy against governmental and corporate misuse. established privacy regimes are focused on individual consent, and most human rights treaties know derogations from privacy and data protection norms for states of emergency. this leaves little safeguards nor remedies to guarantee individual and collective autonomy. however, the challenge of responsible data use during a crisis is not novel. the humanitarian sector has more than a decade of experience to offer. international organisations and humanitarian actors have developed detailed guidelines on how to use data responsibly under extreme circumstances. this article briefly addresses the legal gap of data protection and privacy during this global crisis. then it outlines the state of the art in humanitarian practice and academia on data protection and data responsibility during crisis."
http://orkg.org/orkg/resource/R182384,Production diversity and dietary diversity in smallholder farm households,10.1073/pnas.1510982112,crossref,"<jats:title>Significance</jats:title>\n          <jats:p>Given that hunger and malnutrition are still widespread problems in many developing countries, the question of how to make agriculture and food systems more nutrition-sensitive is of high relevance for research and policy. Many of the undernourished people in Africa and Asia are small-scale subsistence farmers. Diversifying production on these farms is often perceived as a promising strategy to improve dietary quality and diversity. This hypothesis is tested with data from smallholder farm households in Indonesia, Kenya, Ethiopia, and Malawi. Higher farm production diversity significantly contributes to dietary diversity in some situations, but not in all. Improving small farmers’ access to markets seems to be a more effective strategy to improve nutrition than promoting production diversity on subsistence farms.</jats:p>","significance \n given that hunger and malnutrition are still widespread problems in many developing countries, the question of how to make agriculture and food systems more nutrition-sensitive is of high relevance for research and policy. many of the undernourished people in africa and asia are small-scale subsistence farmers. diversifying production on these farms is often perceived as a promising strategy to improve dietary quality and diversity. this hypothesis is tested with data from smallholder farm households in indonesia, kenya, ethiopia, and malawi. higher farm production diversity significantly contributes to dietary diversity in some situations, but not in all. improving small farmers’ access to markets seems to be a more effective strategy to improve nutrition than promoting production diversity on subsistence farms."
http://orkg.org/orkg/resource/R184000,Agricultural Food Production Diversity and Dietary Diversity among Female Small Holder Farmers in a Region of the Ecuadorian Andes Experiencing Nutrition Transition,10.3390/nu12082454,crossref,"<jats:p>Some rural areas of Ecuador, including the Imbabura Province of the Andes Highlands, are experiencing a double burden of malnutrition where micronutrient deficiencies persist at the same time obesity is increasing as many traditional home-grown foods are being replaced with more commercially prepared convenience foods. Thus, the relationships among agricultural food production diversity (FPD), dietary diversity (DD), and household food insecurity (HFI) of the rural small holder farmers need further study. Therefore, we examined these associations in small holder farmers residing in this Province in the Andes Highlands (elevation &gt; 2500 m). Non-pregnant maternal home managers (n = 558, x age = 44.1, SD = 16.5 y) were interviewed regarding the number of different agricultural food crops cultivated and domestic animals raised in their family farm plots. DD was determined using the Minimum Dietary Diversity for Women Score (MDD-W) based on the number of 10 different food groups consumed, and household food insecurity (HFI) was determined using the 8-item Household Food Insecurity Experience Scale. The women reported consuming an average of 53% of their total food from what they cultivated or raised. Women with higher DD [MMD-W score ≥ 5 food groups (79% of total sample)] were on farms that cultivated a greater variety of crops (x = 8.7 vs. 6.7), raised more animals (x = 17.9 vs. 12.7, p &lt; 0.05), and reported lower HFI and significantly higher intakes of energy, protein, iron, zinc, and vitamin A (all p &lt; 0.05). Multiple regression analyses demonstrated that FPD was only modestly related to DD, which together with years of education, per capita family income, and HFI accounted for 26% of DD variance. In rural areas of the Imbabura Province, small holder farmers still rely heavily on consumption of self-cultivated foods, but greater diversity of crops grown in family farm plots is only weakly associated with greater DD and lower HFI among the female caretakers.</jats:p>","some rural areas of ecuador, including the imbabura province of the andes highlands, are experiencing a double burden of malnutrition where micronutrient deficiencies persist at the same time obesity is increasing as many traditional home-grown foods are being replaced with more commercially prepared convenience foods. thus, the relationships among agricultural food production diversity (fpd), dietary diversity (dd), and household food insecurity (hfi) of the rural small holder farmers need further study. therefore, we examined these associations in small holder farmers residing in this province in the andes highlands (elevation &gt; 2500 m). non-pregnant maternal home managers (n = 558, x age = 44.1, sd = 16.5 y) were interviewed regarding the number of different agricultural food crops cultivated and domestic animals raised in their family farm plots. dd was determined using the minimum dietary diversity for women score (mdd-w) based on the number of 10 different food groups consumed, and household food insecurity (hfi) was determined using the 8-item household food insecurity experience scale. the women reported consuming an average of 53% of their total food from what they cultivated or raised. women with higher dd [mmd-w score ≥ 5 food groups (79% of total sample)] were on farms that cultivated a greater variety of crops (x = 8.7 vs. 6.7), raised more animals (x = 17.9 vs. 12.7, p &lt; 0.05), and reported lower hfi and significantly higher intakes of energy, protein, iron, zinc, and vitamin a (all p &lt; 0.05). multiple regression analyses demonstrated that fpd was only modestly related to dd, which together with years of education, per capita family income, and hfi accounted for 26% of dd variance. in rural areas of the imbabura province, small holder farmers still rely heavily on consumption of self-cultivated foods, but greater diversity of crops grown in family farm plots is only weakly associated with greater dd and lower hfi among the female caretakers."
http://orkg.org/orkg/resource/R185065,Fe XXV Drake 1/2,10.1139/p88-100,crossref,"<jats:p> The unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s<jats:sup>2</jats:sup>\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>3</jats:sup>S<jats:sub>1</jats:sub>, 1s2p\u2002<jats:sup>1</jats:sup>P<jats:sub>1</jats:sub>, and 1s2p\u2002<jats:sup>3</jats:sup>P<jats:sub>0,1,2</jats:sub> staters of helium-like ions. Detailed tabulations are presented for all ions in the range 2\u2002≤\u2002Z\u2002≤\u2002100 and are compared with a wide range of experimental data up to <jats:sup>34</jats:sup>Kr<jats:sup>+</jats:sup>. The results for <jats:sup>90</jats:sup>U<jats:sup>+</jats:sup> significantly alter the recent Lamb shift measurement of Munger and Gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002eV, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002eV. The improved agreement is due to the inclusion of higher order two-electron corrections in the present work. </jats:p>","the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \u2002 1 s 0 , 1s2s\u2002 1 s 0 , 1s2s\u2002 3 s 1 , 1s2p\u2002 1 p 1 , and 1s2p\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\u2002≤\u2002z\u2002≤\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002ev, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
http://orkg.org/orkg/resource/R185098,Theoretical energies for the <i>n</i> = 1 and 2 states of the helium isoelectronic sequence up to <i>Z</i> = 100,10.1139/p88-100,crossref,"<jats:p> The unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s<jats:sup>2</jats:sup>\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>3</jats:sup>S<jats:sub>1</jats:sub>, 1s2p\u2002<jats:sup>1</jats:sup>P<jats:sub>1</jats:sub>, and 1s2p\u2002<jats:sup>3</jats:sup>P<jats:sub>0,1,2</jats:sub> staters of helium-like ions. Detailed tabulations are presented for all ions in the range 2\u2002≤\u2002Z\u2002≤\u2002100 and are compared with a wide range of experimental data up to <jats:sup>34</jats:sup>Kr<jats:sup>+</jats:sup>. The results for <jats:sup>90</jats:sup>U<jats:sup>+</jats:sup> significantly alter the recent Lamb shift measurement of Munger and Gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002eV, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002eV. The improved agreement is due to the inclusion of higher order two-electron corrections in the present work. </jats:p>","the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \u2002 1 s 0 , 1s2s\u2002 1 s 0 , 1s2s\u2002 3 s 1 , 1s2p\u2002 1 p 1 , and 1s2p\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\u2002≤\u2002z\u2002≤\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002ev, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
http://orkg.org/orkg/resource/R185172,Theoretical energies for the <i>n</i> = 1 and 2 states of the helium isoelectronic sequence up to <i>Z</i> = 100,10.1139/p88-100,crossref,"<jats:p> The unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s<jats:sup>2</jats:sup>\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>3</jats:sup>S<jats:sub>1</jats:sub>, 1s2p\u2002<jats:sup>1</jats:sup>P<jats:sub>1</jats:sub>, and 1s2p\u2002<jats:sup>3</jats:sup>P<jats:sub>0,1,2</jats:sub> staters of helium-like ions. Detailed tabulations are presented for all ions in the range 2\u2002≤\u2002Z\u2002≤\u2002100 and are compared with a wide range of experimental data up to <jats:sup>34</jats:sup>Kr<jats:sup>+</jats:sup>. The results for <jats:sup>90</jats:sup>U<jats:sup>+</jats:sup> significantly alter the recent Lamb shift measurement of Munger and Gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002eV, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002eV. The improved agreement is due to the inclusion of higher order two-electron corrections in the present work. </jats:p>","the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \u2002 1 s 0 , 1s2s\u2002 1 s 0 , 1s2s\u2002 3 s 1 , 1s2p\u2002 1 p 1 , and 1s2p\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\u2002≤\u2002z\u2002≤\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002ev, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
http://orkg.org/orkg/resource/R186134,Exploiting Declarative Mapping Rules for Generating GraphQL Servers with Morph-GraphQL,10.1142/s0218194020400070,crossref,"<jats:p> In the last decade, REST has become the most common approach to provide web services, yet it was not originally designed to handle typical modern applications (e.g. mobile apps). GraphQL was proposed to reduce the number of queries and data exchanged in comparison with REST. Since its release in 2015, it has gained momentum as an alternative approach to REST. However, generating and maintaining GraphQL resolvers is not a simple task. First, a domain expert has to analyze a dataset, design the corresponding GraphQL schema and map the dataset to the schema. Then, a software engineer (e.g. GraphQL developer) implements the corresponding GraphQL resolvers in a specific programming language. In this paper, we present an approach to exploit the information from mappings rules (relation between target and source schema) and generate a GraphQL server. These mapping rules construct a virtual knowledge graph which is accessed by the generated GraphQL resolvers. These resolvers translate the input GraphQL queries into the queries supported by the underlying dataset. Domain experts or software developers may benefit from our approach: a domain expert does not need to involve software developers to implement the resolvers, and software developers can generate the initial version of the resolvers to be implemented. We implemented our approach in the Morph-GraphQL framework and evaluated it using the LinGBM benchmark. </jats:p>","in the last decade, rest has become the most common approach to provide web services, yet it was not originally designed to handle typical modern applications (e.g. mobile apps). graphql was proposed to reduce the number of queries and data exchanged in comparison with rest. since its release in 2015, it has gained momentum as an alternative approach to rest. however, generating and maintaining graphql resolvers is not a simple task. first, a domain expert has to analyze a dataset, design the corresponding graphql schema and map the dataset to the schema. then, a software engineer (e.g. graphql developer) implements the corresponding graphql resolvers in a specific programming language. in this paper, we present an approach to exploit the information from mappings rules (relation between target and source schema) and generate a graphql server. these mapping rules construct a virtual knowledge graph which is accessed by the generated graphql resolvers. these resolvers translate the input graphql queries into the queries supported by the underlying dataset. domain experts or software developers may benefit from our approach: a domain expert does not need to involve software developers to implement the resolvers, and software developers can generate the initial version of the resolvers to be implemented. we implemented our approach in the morph-graphql framework and evaluated it using the lingbm benchmark."
http://orkg.org/orkg/resource/R185211,Theoretical energies for the <i>n</i> = 1 and 2 states of the helium isoelectronic sequence up to <i>Z</i> = 100,10.1139/p88-100,crossref,"<jats:p> The unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s<jats:sup>2</jats:sup>\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>1</jats:sup>S<jats:sub>0</jats:sub>, 1s2s\u2002<jats:sup>3</jats:sup>S<jats:sub>1</jats:sub>, 1s2p\u2002<jats:sup>1</jats:sup>P<jats:sub>1</jats:sub>, and 1s2p\u2002<jats:sup>3</jats:sup>P<jats:sub>0,1,2</jats:sub> staters of helium-like ions. Detailed tabulations are presented for all ions in the range 2\u2002≤\u2002Z\u2002≤\u2002100 and are compared with a wide range of experimental data up to <jats:sup>34</jats:sup>Kr<jats:sup>+</jats:sup>. The results for <jats:sup>90</jats:sup>U<jats:sup>+</jats:sup> significantly alter the recent Lamb shift measurement of Munger and Gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002eV, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002eV. The improved agreement is due to the inclusion of higher order two-electron corrections in the present work. </jats:p>","the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \u2002 1 s 0 , 1s2s\u2002 1 s 0 , 1s2s\u2002 3 s 1 , 1s2p\u2002 1 p 1 , and 1s2p\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\u2002≤\u2002z\u2002≤\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\u2002±\u20028.3 to 71.0\u2002±\u20028.3\u2002ev, in comparison with a revised theoretical value of 74.3\u2002±\u20020.4\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
http://orkg.org/orkg/resource/R185392,In-Pero: Exploiting Deep Learning Embeddings of Protein Sequences to Predict the Localisation of Peroxisomal Proteins,10.3390/ijms22126409,crossref,"<jats:p>Peroxisomes are ubiquitous membrane-bound organelles, and aberrant localisation of peroxisomal proteins contributes to the pathogenesis of several disorders. Many computational methods focus on assigning protein sequences to subcellular compartments, but there are no specific tools tailored for the sub-localisation (matrix vs. membrane) of peroxisome proteins. We present here In-Pero, a new method for predicting protein sub-peroxisomal cellular localisation. In-Pero combines standard machine learning approaches with recently proposed multi-dimensional deep-learning representations of the protein amino-acid sequence. It showed a classification accuracy above 0.9 in predicting peroxisomal matrix and membrane proteins. The method is trained and tested using a double cross-validation approach on a curated data set comprising 160 peroxisomal proteins with experimental evidence for sub-peroxisomal localisation. We further show that the proposed approach can be easily adapted (In-Mito) to the prediction of mitochondrial protein localisation obtaining performances for certain classes of proteins (matrix and inner-membrane) superior to existing tools.</jats:p>","peroxisomes are ubiquitous membrane-bound organelles, and aberrant localisation of peroxisomal proteins contributes to the pathogenesis of several disorders. many computational methods focus on assigning protein sequences to subcellular compartments, but there are no specific tools tailored for the sub-localisation (matrix vs. membrane) of peroxisome proteins. we present here in-pero, a new method for predicting protein sub-peroxisomal cellular localisation. in-pero combines standard machine learning approaches with recently proposed multi-dimensional deep-learning representations of the protein amino-acid sequence. it showed a classification accuracy above 0.9 in predicting peroxisomal matrix and membrane proteins. the method is trained and tested using a double cross-validation approach on a curated data set comprising 160 peroxisomal proteins with experimental evidence for sub-peroxisomal localisation. we further show that the proposed approach can be easily adapted (in-mito) to the prediction of mitochondrial protein localisation obtaining performances for certain classes of proteins (matrix and inner-membrane) superior to existing tools."
http://orkg.org/orkg/resource/R186740,Systematic Dissection of the Evolutionarily Conserved WetA Developmental Regulator across a Genus of Filamentous Fungi,10.1128/mbio.01130-18,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            Asexual sporulation is fundamental to the ecology and lifestyle of filamentous fungi and can facilitate both plant and human infection. In\n            <jats:italic>Aspergillus</jats:italic>\n            , the production of asexual spores is primarily governed by the BrlA→AbaA→WetA regulatory cascade. The final step in this cascade is controlled by the WetA protein and governs not only the morphological differentiation of spores but also the production and deposition of diverse metabolites into spores. While WetA is conserved across the genus\n            <jats:italic>Aspergillus</jats:italic>\n            , the structure and degree of conservation of the\n            <jats:italic>wetA</jats:italic>\n            gene regulatory network (GRN) remain largely unknown. We carried out comparative transcriptome analyses of comparisons between\n            <jats:italic>wetA</jats:italic>\n            null mutant and wild-type asexual spores in three representative species spanning the diversity of the genus\n            <jats:italic>Aspergillus</jats:italic>\n            :\n            <jats:named-content content-type=""genus-species"">A.\xa0nidulans</jats:named-content>\n            ,\n            <jats:named-content content-type=""genus-species"">A.\xa0flavus</jats:named-content>\n            , and\n            <jats:named-content content-type=""genus-species"">A.\xa0fumigatus</jats:named-content>\n            . We discovered that WetA regulates asexual sporulation in all three species via a negative-feedback loop that represses BrlA, the cascade’s first step. Furthermore, data from chromatin immunoprecipitation sequencing (ChIP-seq) experiments in\n            <jats:named-content content-type=""genus-species"">A.\xa0nidulans</jats:named-content>\n            asexual spores suggest that WetA is a DNA-binding protein that interacts with a novel regulatory motif. Several global regulators known to bridge spore production and the production of secondary metabolites show species-specific regulatory patterns in our data. These results suggest that the BrlA→AbaA→WetA cascade’s regulatory role in cellular and chemical asexual spore development is functionally conserved but that the\n            <jats:italic>wetA</jats:italic>\n            -associated GRN has diverged during\n            <jats:italic>Aspergillus</jats:italic>\n            evolution.\n          </jats:p>\n          <jats:p>\n            <jats:bold>IMPORTANCE</jats:bold>\n            The formation of resilient spores is a key factor contributing to the survival and fitness of many microorganisms, including fungi. In the fungal genus\n            <jats:italic>Aspergillus</jats:italic>\n            , spore formation is controlled by a complex gene regulatory network that also impacts a variety of other processes, including secondary metabolism. To gain mechanistic insights into how fungal spore formation is controlled across\n            <jats:italic>Aspergillus</jats:italic>\n            , we dissected the gene regulatory network downstream of a major regulator of spore maturation (WetA) in three species that span the diversity of the genus: the genetic model\n            <jats:named-content content-type=""genus-species"">A.\xa0nidulans</jats:named-content>\n            , the human pathogen\n            <jats:named-content content-type=""genus-species"">A.\xa0fumigatus</jats:named-content>\n            , and the aflatoxin producer\n            <jats:named-content content-type=""genus-species"">A.\xa0flavus</jats:named-content>\n            . Our data show that WetA regulates asexual sporulation in all three species via a negative-feedback loop and likely binds a novel regulatory element that we term the WetA response element (WRE). These results shed light on how gene regulatory networks in microorganisms control important biological processes and evolve across diverse species.\n          </jats:p>","abstract \n \n asexual sporulation is fundamental to the ecology and lifestyle of filamentous fungi and can facilitate both plant and human infection. in\n aspergillus \n , the production of asexual spores is primarily governed by the brla→abaa→weta regulatory cascade. the final step in this cascade is controlled by the weta protein and governs not only the morphological differentiation of spores but also the production and deposition of diverse metabolites into spores. while weta is conserved across the genus\n aspergillus \n , the structure and degree of conservation of the\n weta \n gene regulatory network (grn) remain largely unknown. we carried out comparative transcriptome analyses of comparisons between\n weta \n null mutant and wild-type asexual spores in three representative species spanning the diversity of the genus\n aspergillus \n :\n a.\xa0nidulans \n ,\n a.\xa0flavus \n , and\n a.\xa0fumigatus \n . we discovered that weta regulates asexual sporulation in all three species via a negative-feedback loop that represses brla, the cascade’s first step. furthermore, data from chromatin immunoprecipitation sequencing (chip-seq) experiments in\n a.\xa0nidulans \n asexual spores suggest that weta is a dna-binding protein that interacts with a novel regulatory motif. several global regulators known to bridge spore production and the production of secondary metabolites show species-specific regulatory patterns in our data. these results suggest that the brla→abaa→weta cascade’s regulatory role in cellular and chemical asexual spore development is functionally conserved but that the\n weta \n -associated grn has diverged during\n aspergillus \n evolution.\n \n \n importance \n the formation of resilient spores is a key factor contributing to the survival and fitness of many microorganisms, including fungi. in the fungal genus\n aspergillus \n , spore formation is controlled by a complex gene regulatory network that also impacts a variety of other processes, including secondary metabolism. to gain mechanistic insights into how fungal spore formation is controlled across\n aspergillus \n , we dissected the gene regulatory network downstream of a major regulator of spore maturation (weta) in three species that span the diversity of the genus: the genetic model\n a.\xa0nidulans \n , the human pathogen\n a.\xa0fumigatus \n , and the aflatoxin producer\n a.\xa0flavus \n . our data show that weta regulates asexual sporulation in all three species via a negative-feedback loop and likely binds a novel regulatory element that we term the weta response element (wre). these results shed light on how gene regulatory networks in microorganisms control important biological processes and evolve across diverse species.\n"
http://orkg.org/orkg/resource/R186093,Assessing Business-IT Allignment Maturity,10.4018/978-1-59140-140-7.ch004,crossref,"<jats:p>Strategic alignment focuses on the activities that management performs to achieve cohesive goals across the IT (Information Technology) and other functional organizations (e.g., finance, marketing, H/R, R&amp;D, manufacturing). Therefore, alignment addresses both how IT is in harmony with the business, and how the business should, or could, be in harmony with IT. Alignment evolves into a relationship where the function of IT and other business functions adapt their strategies together. Achieving alignment is evolutionary and dynamic. It requires strong support from senior management, good working relationships, strong leadership, appropriate prioritization, trust, and effective communication, as well as a thorough understanding of the business and technical environments. The strategic alignment maturity assessment provides organizations with a vehicle to evaluate these activities. Knowing the maturity of its strategic choices and alignment practices make it possible for a firm to see where it stands and how it can improve. This chapter discusses an approach for assessing the maturity of the business-IT alignment. Once maturity is understood, an organization can identify opportunities for enhancing the harmonious relationship of business and IT.</jats:p>","strategic alignment focuses on the activities that management performs to achieve cohesive goals across the it (information technology) and other functional organizations (e.g., finance, marketing, h/r, r&amp;d, manufacturing). therefore, alignment addresses both how it is in harmony with the business, and how the business should, or could, be in harmony with it. alignment evolves into a relationship where the function of it and other business functions adapt their strategies together. achieving alignment is evolutionary and dynamic. it requires strong support from senior management, good working relationships, strong leadership, appropriate prioritization, trust, and effective communication, as well as a thorough understanding of the business and technical environments. the strategic alignment maturity assessment provides organizations with a vehicle to evaluate these activities. knowing the maturity of its strategic choices and alignment practices make it possible for a firm to see where it stands and how it can improve. this chapter discusses an approach for assessing the maturity of the business-it alignment. once maturity is understood, an organization can identify opportunities for enhancing the harmonious relationship of business and it."
http://orkg.org/orkg/resource/R186167,Scalable SPARQL querying of large RDF graphs,10.14778/3402707.3402747,crossref,"<jats:p>The generation of RDF data has accelerated to the point where many data sets need to be partitioned across multiple machines in order to achieve reasonable performance when querying the data. Although tremendous progress has been made in the Semantic Web community for achieving high performance data management on a single node, current solutions that allow the data to be partitioned across multiple machines are highly inefficient. In this paper, we introduce a scalable RDF data management system that is up to three orders of magnitude more efficient than popular multi-node RDF data management systems. In so doing, we introduce techniques for (1) leveraging state-of-the-art single node RDF-store technology (2) partitioning the data across nodes in a manner that helps accelerate query processing through locality optimizations and (3) decomposing SPARQL queries into high performance fragments that take advantage of how data is partitioned in a cluster.</jats:p>","the generation of rdf data has accelerated to the point where many data sets need to be partitioned across multiple machines in order to achieve reasonable performance when querying the data. although tremendous progress has been made in the semantic web community for achieving high performance data management on a single node, current solutions that allow the data to be partitioned across multiple machines are highly inefficient. in this paper, we introduce a scalable rdf data management system that is up to three orders of magnitude more efficient than popular multi-node rdf data management systems. in so doing, we introduce techniques for (1) leveraging state-of-the-art single node rdf-store technology (2) partitioning the data across nodes in a manner that helps accelerate query processing through locality optimizations and (3) decomposing sparql queries into high performance fragments that take advantage of how data is partitioned in a cluster."
http://orkg.org/orkg/resource/R186234,PredictRoute: A Network Path Prediction Toolkit,10.1145/3460090,crossref,"""<jats:p>Accurate prediction of network paths between arbitrary hosts on the Internet is of vital importance for network operators, cloud providers, and academic researchers. We present PredictRoute, a system that predicts network paths between hosts on the Internet using historical knowledge of the data and control plane. In addition to feeding on freely available traceroutes and BGP routing tables, PredictRoute optimally explores network paths towards chosen BGP prefixes. PredictRoute's strategy for exploring network paths discovers 4X more autonomous system (AS) hops than other well-known strategies used in practice today. Using a corpus of traceroutes, PredictRoute trains probabilistic models of routing towards prefixes on the Internet to predict network paths and their likelihood. PredictRoute's AS-path predictions differ from the measured path by at most 1 hop, 75% of the time. We expose PredictRoute's path prediction capability via a REST API to facilitate its inclusion in other applications and studies. We additionally demonstrate the utility of PredictRoute in improving real-world applications for circumventing Internet censorship and preserving anonymity online.</jats:p>""",""" accurate prediction of network paths between arbitrary hosts on the internet is of vital importance for network operators, cloud providers, and academic researchers. we present predictroute, a system that predicts network paths between hosts on the internet using historical knowledge of the data and control plane. in addition to feeding on freely available traceroutes and bgp routing tables, predictroute optimally explores network paths towards chosen bgp prefixes. predictroute's strategy for exploring network paths discovers 4x more autonomous system (as) hops than other well-known strategies used in practice today. using a corpus of traceroutes, predictroute trains probabilistic models of routing towards prefixes on the internet to predict network paths and their likelihood. predictroute's as-path predictions differ from the measured path by at most 1 hop, 75% of the time. we expose predictroute's path prediction capability via a rest api to facilitate its inclusion in other applications and studies. we additionally demonstrate the utility of predictroute in improving real-world applications for circumventing internet censorship and preserving anonymity online. """
http://orkg.org/orkg/resource/R186224,Building an AS-topology model that captures route diversity,10.1145/1151659.1159937,crossref,"<jats:p>An understanding of the topological structure of the Internet is needed for quite a number of networking tasks, e. g., making decisions about peering relationships, choice of upstream providers, inter-domain traffic engineering. One essential component of these tasks is the ability to predict routes in the Internet. However, the Internet is composed of a large number of independent autonomous systems (ASes) resulting in complex interactions, and until now no model of the Internet has succeeded in producing predictions of acceptable accuracy.We demonstrate that there are two limitations of prior models: (i) they have all assumed that an Autonomous System (AS) is an atomic structure - it is not, and (ii) models have tended to oversimplify the relationships between ASes. Our approach uses multiple quasi-routers to capture route diversity within the ASes, and is deliberately agnostic regarding the types of relationships between ASes. The resulting model ensures that its routing is consistent with the observed routes. Exploiting a large number of observation points, we show that our model provides accurate predictions for unobserved routes, a first step towards developing structural mod-els of the Internet that enable real applications.</jats:p>","an understanding of the topological structure of the internet is needed for quite a number of networking tasks, e. g., making decisions about peering relationships, choice of upstream providers, inter-domain traffic engineering. one essential component of these tasks is the ability to predict routes in the internet. however, the internet is composed of a large number of independent autonomous systems (ases) resulting in complex interactions, and until now no model of the internet has succeeded in producing predictions of acceptable accuracy.we demonstrate that there are two limitations of prior models: (i) they have all assumed that an autonomous system (as) is an atomic structure - it is not, and (ii) models have tended to oversimplify the relationships between ases. our approach uses multiple quasi-routers to capture route diversity within the ases, and is deliberately agnostic regarding the types of relationships between ases. the resulting model ensures that its routing is consistent with the observed routes. exploiting a large number of observation points, we show that our model provides accurate predictions for unobserved routes, a first step towards developing structural mod-els of the internet that enable real applications."
http://orkg.org/orkg/resource/R188105,KG-COVID-19: A Framework to Produce Customized Knowledge Graphs for COVID-19 Response,,crossref,"<jats:title>SUMMARY</jats:title><jats:p>Integrated, up-to-date data about SARS-CoV-2 and coronavirus disease 2019 (COVID-19) is crucial for the ongoing response to the COVID-19 pandemic by the biomedical research community. While rich biological knowledge exists for SARS-CoV-2 and related viruses (SARS-CoV, MERS-CoV), integrating this knowledge is difficult and time consuming, since much of it is in siloed databases or in textual format. Furthermore, the data required by the research community varies drastically for different tasks - the optimal data for a machine learning task, for example, is much different from the data used to populate a browsable user interface for clinicians. To address these challenges, we created KG-COVID-19, a flexible framework that ingests and integrates biomedical data to produce knowledge graphs (KGs) for COVID-19 response. This KG framework can also be applied to other problems in which siloed biomedical data must be quickly integrated for different research applications, including future pandemics.</jats:p><jats:sec><jats:title>BIGGER PICTURE</jats:title><jats:p>An effective response to the COVID-19 pandemic relies on integration of many different types of data available about SARS-CoV-2 and related viruses. KG-COVID-19 is a framework for producing knowledge graphs that can be customized for downstream applications including machine learning tasks, hypothesis-based querying, and browsable user interface to enable researchers to explore COVID-19 data and discover relationships.</jats:p></jats:sec>","summary integrated, up-to-date data about sars-cov-2 and coronavirus disease 2019 (covid-19) is crucial for the ongoing response to the covid-19 pandemic by the biomedical research community. while rich biological knowledge exists for sars-cov-2 and related viruses (sars-cov, mers-cov), integrating this knowledge is difficult and time consuming, since much of it is in siloed databases or in textual format. furthermore, the data required by the research community varies drastically for different tasks - the optimal data for a machine learning task, for example, is much different from the data used to populate a browsable user interface for clinicians. to address these challenges, we created kg-covid-19, a flexible framework that ingests and integrates biomedical data to produce knowledge graphs (kgs) for covid-19 response. this kg framework can also be applied to other problems in which siloed biomedical data must be quickly integrated for different research applications, including future pandemics. bigger picture an effective response to the covid-19 pandemic relies on integration of many different types of data available about sars-cov-2 and related viruses. kg-covid-19 is a framework for producing knowledge graphs that can be customized for downstream applications including machine learning tasks, hypothesis-based querying, and browsable user interface to enable researchers to explore covid-19 data and discover relationships."
http://orkg.org/orkg/resource/R186589,Low-Dimensional Model for Bike-Sharing Demand Forecasting that Explicitly Accounts for Weather Data,https://doi.org/10.1177/0361198120932160,crossref,"<jats:p> With the increasing availability of big, transport-related datasets, detailed data-driven mobility analysis is becoming possible. Trips with their origins, destinations, and travel times are now collected in publicly available databases, allowing for detailed demand forecasting with methods exploiting big and accurate data. In this paper, we predict the demand pattern of New York City bikes with a low-dimensional approach utilizing three-level data clustering. We use historical demand data along with temperature and precipitation to first aggregate and then decompose data to obtain meaningful clusters. The core of this approach lies in the proposed clustering technique, which reduces the dimension of the problem and, differently from other machine learning techniques, requires limited assumptions on the model or its parameters. The proposed method allows, for the given temperature and precipitation method, to obtain expected vector of movement (mean number and direction of trips) for each zone. In this paper, we synthesize more than 17 million trips into daily and zonal vectors of movement, which combined with weather data allow forecasting of the trip demand. The method allows us to predict the demand with over 75% accuracy, as shown in series of experiments in which various settings and parameterizations are validated against 25% holdout data. </jats:p>","with the increasing availability of big, transport-related datasets, detailed data-driven mobility analysis is becoming possible. trips with their origins, destinations, and travel times are now collected in publicly available databases, allowing for detailed demand forecasting with methods exploiting big and accurate data. in this paper, we predict the demand pattern of new york city bikes with a low-dimensional approach utilizing three-level data clustering. we use historical demand data along with temperature and precipitation to first aggregate and then decompose data to obtain meaningful clusters. the core of this approach lies in the proposed clustering technique, which reduces the dimension of the problem and, differently from other machine learning techniques, requires limited assumptions on the model or its parameters. the proposed method allows, for the given temperature and precipitation method, to obtain expected vector of movement (mean number and direction of trips) for each zone. in this paper, we synthesize more than 17 million trips into daily and zonal vectors of movement, which combined with weather data allow forecasting of the trip demand. the method allows us to predict the demand with over 75% accuracy, as shown in series of experiments in which various settings and parameterizations are validated against 25% holdout data."
http://orkg.org/orkg/resource/R187017,The Infectious Disease Ontology in the age of COVID-19,,crossref,"<p>Background: Effective response to public health emergencies, such as we are now experiencing with COVID-19, requires data sharing across multiple disciplines and data systems. Ontologies offer a powerful data sharing tool, and this holds especially for those ontologies built on the design principles of the Open Biomedical Ontologies Foundry. These principles are exemplified by the Infectious Disease Ontology (IDO), a suite of interoperable ontology modules aiming to provide coverage of all aspects of the infectious disease domain. At its center is IDO Core, a disease- and pathogen-neutral ontology covering just those types of entities and relations that are relevant to infectious diseases generally. IDO Core is extended by disease and pathogen-specific ontology modules.Results: To assist the integration and analysis of COVID-19 data, and viral infectious disease data more generally, we have recently developed three new IDO extensions: IDO Virus (VIDO); the Coronavirus Infectious Disease Ontology (CIDO); and an extension of CIDO focusing on COVID-19 (IDO-COVID-19). Reflecting the fact that viruses lack cellular parts, we have introduced to IDO Core the term acellular structure to cover viruses and other acellular entities studied by virologists. We now distinguish between infectious agents – organisms with an infectious disposition – and infectious structures – acellular structures with an infectious disposition. This in turn has led to various updates and refinements of IDO Core’s content. We believe that our work on VIDO, CIDO, and IDO-COVID-19 can serve as a model for yielding greater conformance with ontology building best practices.Conclusions: IDO provides a simple recipe for building new pathogen-specific ontologies in a way that allows data about novel diseases to be easily compared, along multiple dimensions, with data represented by existing disease ontologies. The IDO strategy, moreover, supports ontology coordination, providing a powerful method of data integration and sharing that allows physicians, researchers, and public health organizations to respond rapidly and efficiently to current and future public health crises.</p>","background: effective response to public health emergencies, such as we are now experiencing with covid-19, requires data sharing across multiple disciplines and data systems. ontologies offer a powerful data sharing tool, and this holds especially for those ontologies built on the design principles of the open biomedical ontologies foundry. these principles are exemplified by the infectious disease ontology (ido), a suite of interoperable ontology modules aiming to provide coverage of all aspects of the infectious disease domain. at its center is ido core, a disease- and pathogen-neutral ontology covering just those types of entities and relations that are relevant to infectious diseases generally. ido core is extended by disease and pathogen-specific ontology modules.results: to assist the integration and analysis of covid-19 data, and viral infectious disease data more generally, we have recently developed three new ido extensions: ido virus (vido); the coronavirus infectious disease ontology (cido); and an extension of cido focusing on covid-19 (ido-covid-19). reflecting the fact that viruses lack cellular parts, we have introduced to ido core the term acellular structure to cover viruses and other acellular entities studied by virologists. we now distinguish between infectious agents – organisms with an infectious disposition – and infectious structures – acellular structures with an infectious disposition. this in turn has led to various updates and refinements of ido core’s content. we believe that our work on vido, cido, and ido-covid-19 can serve as a model for yielding greater conformance with ontology building best practices.conclusions: ido provides a simple recipe for building new pathogen-specific ontologies in a way that allows data about novel diseases to be easily compared, along multiple dimensions, with data represented by existing disease ontologies. the ido strategy, moreover, supports ontology coordination, providing a powerful method of data integration and sharing that allows physicians, researchers, and public health organizations to respond rapidly and efficiently to current and future public health crises."
http://orkg.org/orkg/resource/R188068,Gender Inequality in Research Productivity During the COVID-19 Pandemic,10.1287/msom.2021.0991,crossref,"<jats:p> Problem definition: We study the disproportionate impact of the lockdown as a result of the COVID-19 outbreak on female and male academic research productivity in social science. Academic/practical relevance: The lockdown has caused substantial disruptions to academic activities, requiring people to work from home. How this disruption affects productivity and the related gender equity is an important operations and societal question. Methodology: We collect data from the largest open-access preprint repository for social science on 41,858 research preprints in 18 disciplines produced by 76,832 authors across 25 countries over a span of two years. We use a difference-in-differences approach leveraging the exogenous pandemic shock. Results: Our results indicate that, in the 10 weeks after the lockdown in the United States, although total research productivity increased by 35%, female academics’ productivity dropped by 13.2% relative to that of male academics. We also show that this intensified productivity gap is more pronounced for assistant professors and for academics in top-ranked universities and is found in six other countries. Managerial implications: Our work points out the fairness issue in productivity caused by the lockdown, a finding that universities will find helpful when evaluating faculty productivity. It also helps organizations realize the potential unintended consequences that can arise from telecommuting. </jats:p>","problem definition: we study the disproportionate impact of the lockdown as a result of the covid-19 outbreak on female and male academic research productivity in social science. academic/practical relevance: the lockdown has caused substantial disruptions to academic activities, requiring people to work from home. how this disruption affects productivity and the related gender equity is an important operations and societal question. methodology: we collect data from the largest open-access preprint repository for social science on 41,858 research preprints in 18 disciplines produced by 76,832 authors across 25 countries over a span of two years. we use a difference-in-differences approach leveraging the exogenous pandemic shock. results: our results indicate that, in the 10 weeks after the lockdown in the united states, although total research productivity increased by 35%, female academics’ productivity dropped by 13.2% relative to that of male academics. we also show that this intensified productivity gap is more pronounced for assistant professors and for academics in top-ranked universities and is found in six other countries. managerial implications: our work points out the fairness issue in productivity caused by the lockdown, a finding that universities will find helpful when evaluating faculty productivity. it also helps organizations realize the potential unintended consequences that can arise from telecommuting."
http://orkg.org/orkg/resource/R186659,Built Environment Factors Affecting Bike Sharing Ridership: Data-Driven Approach for Multiple Cities,https://doi.org/10.1177/0361198119849908,crossref,"<jats:p> Identification of factors influencing ridership is necessary for policy-making, as well as, when examining transferability and aspects of performance and reliability. In this work, a data-driven method is formulated to correlate arrivals and departures of station-based bike sharing systems with built environment factors in multiple cities. Ridership data from stations of multiple cities are pooled in one data set regardless of their geographic boundaries. The method bundles the collection, analysis, and processing of data, as well as, the model’s estimation using statistical and machine learning techniques. The method was applied on a national level in six cities in Germany, and also on an international level in three cities in Europe and North America. The results suggest that the model’s performance did not depend on clustering cities by size but by the relative daily distribution of the rentals. Selected statistically significant factors were identified to vary temporally (e.g., nightclubs were significant during the night). The most influencing variables were related to the city population, distance to city center, leisure-related establishments, and transport-related infrastructure. This data-driven method can help as a support decision-making tool to implement or expand bike sharing systems. </jats:p>","identification of factors influencing ridership is necessary for policy-making, as well as, when examining transferability and aspects of performance and reliability. in this work, a data-driven method is formulated to correlate arrivals and departures of station-based bike sharing systems with built environment factors in multiple cities. ridership data from stations of multiple cities are pooled in one data set regardless of their geographic boundaries. the method bundles the collection, analysis, and processing of data, as well as, the model’s estimation using statistical and machine learning techniques. the method was applied on a national level in six cities in germany, and also on an international level in three cities in europe and north america. the results suggest that the model’s performance did not depend on clustering cities by size but by the relative daily distribution of the rentals. selected statistically significant factors were identified to vary temporally (e.g., nightclubs were significant during the night). the most influencing variables were related to the city population, distance to city center, leisure-related establishments, and transport-related infrastructure. this data-driven method can help as a support decision-making tool to implement or expand bike sharing systems."
http://orkg.org/orkg/resource/R186691,Soil Organic Matter Prediction Model with Satellite Hyperspectral Image Based on Optimized Denoising Method,10.3390/rs13122273,crossref,"<jats:p>In order to improve the signal-to-noise ratio of the hyperspectral sensors and exploit the potential of satellite hyperspectral data for predicting soil properties, we took MingShui County as the study area, which the study area is approximately 1481 km2, and we selected Gaofen-5 (GF-5) satellite hyperspectral image of the study area to explore an applicable and accurate denoising method that can effectively improve the prediction accuracy of soil organic matter (SOM) content. First, fractional-order derivative (FOD) processing is performed on the original reflectance (OR) to evaluate the optimal FOD. Second, singular value decomposition (SVD), Fourier transform (FT) and discrete wavelet transform (DWT) are used to denoise the OR and optimal FOD reflectance. Third, the spectral indexes of the reflectance under different denoising methods are extracted by optimal band combination algorithm, and the input variables of different denoising methods are selected by the recursive feature elimination (RFE) algorithm. Finally, the SOM content is predicted by a random forest prediction model. The results reveal that 0.6-order reflectance describes more useful details in satellite hyperspectral data. Five spectral indexes extracted from the reflectance under different denoising methods have a strong correlation with the SOM content, which is helpful for realizing high-accuracy SOM predictions. All three denoising methods can reduce the noise in hyperspectral data, and the accuracies of the different denoising methods are ranked DWT &gt; FT &gt; SVD, where 0.6-order-DWT has the highest accuracy (R2 = 0.84, RMSE = 3.36 g kg−1, and RPIQ = 1.71). This paper is relatively novel, in that GF-5 satellite hyperspectral data based on different denoising methods are used to predict SOM, and the results provide a highly robust and novel method for mapping the spatial distribution of SOM content at the regional scale.</jats:p>","in order to improve the signal-to-noise ratio of the hyperspectral sensors and exploit the potential of satellite hyperspectral data for predicting soil properties, we took mingshui county as the study area, which the study area is approximately 1481 km2, and we selected gaofen-5 (gf-5) satellite hyperspectral image of the study area to explore an applicable and accurate denoising method that can effectively improve the prediction accuracy of soil organic matter (som) content. first, fractional-order derivative (fod) processing is performed on the original reflectance (or) to evaluate the optimal fod. second, singular value decomposition (svd), fourier transform (ft) and discrete wavelet transform (dwt) are used to denoise the or and optimal fod reflectance. third, the spectral indexes of the reflectance under different denoising methods are extracted by optimal band combination algorithm, and the input variables of different denoising methods are selected by the recursive feature elimination (rfe) algorithm. finally, the som content is predicted by a random forest prediction model. the results reveal that 0.6-order reflectance describes more useful details in satellite hyperspectral data. five spectral indexes extracted from the reflectance under different denoising methods have a strong correlation with the som content, which is helpful for realizing high-accuracy som predictions. all three denoising methods can reduce the noise in hyperspectral data, and the accuracies of the different denoising methods are ranked dwt &gt; ft &gt; svd, where 0.6-order-dwt has the highest accuracy (r2 = 0.84, rmse = 3.36 g kg−1, and rpiq = 1.71). this paper is relatively novel, in that gf-5 satellite hyperspectral data based on different denoising methods are used to predict som, and the results provide a highly robust and novel method for mapping the spatial distribution of som content at the regional scale."
http://orkg.org/orkg/resource/R191162,Is Higher Viral Load in SARS-CoV-2 Associated with Death?,10.4269/ajtmh.20-0954,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>There is no proven prognostic marker or adequate number of studies in patients hospitalized for coronavirus disease 2019 (COVID-19).</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>We conducted a retrospective cohort study of patients hospitalized with COVID-19 from March 14 to June 17, 2020, at São Paulo Hospital. SARS-CoV-2 viral load was assessed using the cycle threshold (Ct) values obtained from an RT-PCR assay applied to the nasopharyngeal swab samples. Disease severity and patient outcomes were compared.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Among the 875 patients, 50.1% (439/875) had mild, 30.4% (266/875) moderate, and 19.5% (170/875) severe disease. A Ct value of &lt;25 (472/875) indicated a high viral load, which was independently associated with mortality (OR: 0,34; 95% CI: 0,217–0,533; <jats:italic>p</jats:italic> &lt; 0.0001).</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>Admission SARS-CoV-2 viral load is an important surrogate biomarker of infectivity and is independently associated with mortality among patients hospitalized with COVID-19.</jats:p></jats:sec>","abstract background there is no proven prognostic marker or adequate number of studies in patients hospitalized for coronavirus disease 2019 (covid-19). methods we conducted a retrospective cohort study of patients hospitalized with covid-19 from march 14 to june 17, 2020, at são paulo hospital. sars-cov-2 viral load was assessed using the cycle threshold (ct) values obtained from an rt-pcr assay applied to the nasopharyngeal swab samples. disease severity and patient outcomes were compared. results among the 875 patients, 50.1% (439/875) had mild, 30.4% (266/875) moderate, and 19.5% (170/875) severe disease. a ct value of &lt;25 (472/875) indicated a high viral load, which was independently associated with mortality (or: 0,34; 95% ci: 0,217–0,533; p &lt; 0.0001). conclusions admission sars-cov-2 viral load is an important surrogate biomarker of infectivity and is independently associated with mortality among patients hospitalized with covid-19."
http://orkg.org/orkg/resource/R188125,"COVID-19 Knowledge Graph: a computable, multi-modal, cause-and-effect knowledge model of COVID-19 pathophysiology",,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Summary</jats:title><jats:p>The past few weeks have witnessed a worldwide mobilization of the research community in response to the novel coronavirus (COVID-19). This global response has led to a burst of publications on the pathophysiology of the virus, yet without coordinated efforts to organize this knowledge, it can remain hidden away from individual research groups. By extracting and formalizing this knowledge in a structured and computable form, as in the form of a knowledge graph, researchers can readily reason and analyze this information on a much larger scale. Here, we present the COVID-19 Knowledge Graph, an expansive cause-and-effect network constructed from scientific literature on the new coronavirus that aims to provide a comprehensive view of its pathophysiology. To make this resource available to the research community and facilitate its exploration and analysis, we also implemented a web application and released the KG in multiple standard formats.</jats:p></jats:sec><jats:sec><jats:title>Availability</jats:title><jats:p>The COVID-19 Knowledge Graph is publicly available under CC-0 license at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://github.com/covid19kg"">https://github.com/covid19kg</jats:ext-link> and <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://bikmi.covid19-knowledgespace.de"">https://bikmi.covid19-knowledgespace.de</jats:ext-link>.</jats:p></jats:sec><jats:sec><jats:title>Contact</jats:title><jats:p><jats:email>alpha.tom.kodamullil@scai.fraunhofer.de</jats:email></jats:p></jats:sec><jats:sec><jats:title>Supplementary information</jats:title><jats:p>Supplementary data are available online.</jats:p></jats:sec>","abstract summary the past few weeks have witnessed a worldwide mobilization of the research community in response to the novel coronavirus (covid-19). this global response has led to a burst of publications on the pathophysiology of the virus, yet without coordinated efforts to organize this knowledge, it can remain hidden away from individual research groups. by extracting and formalizing this knowledge in a structured and computable form, as in the form of a knowledge graph, researchers can readily reason and analyze this information on a much larger scale. here, we present the covid-19 knowledge graph, an expansive cause-and-effect network constructed from scientific literature on the new coronavirus that aims to provide a comprehensive view of its pathophysiology. to make this resource available to the research community and facilitate its exploration and analysis, we also implemented a web application and released the kg in multiple standard formats. availability the covid-19 knowledge graph is publicly available under cc-0 license at https://github.com/covid19kg and https://bikmi.covid19-knowledgespace.de . contact alpha.tom.kodamullil@scai.fraunhofer.de supplementary information supplementary data are available online."
http://orkg.org/orkg/resource/R191238,SARS-CoV-2 Viral Load on Admission Is Associated With 30-Day Mortality,10.1093/ofid/ofaa535,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) viral load on admission was associated with a significantly increased 30-day mortality (odds ratio [OR], 4.20; 95% CI, 1.62–10.86), and anti-SARS-CoV-2 nucleocapisid IgG seropositivity on admission trended toward a reduced 30-day mortality (OR, 0.43; 95% CI, 0.15–1.26). Reporting of quantitative SARS-CoV-2 viral load and serologic assays may offer prognostic clinical information.</jats:p>","abstract \n severe acute respiratory syndrome coronavirus 2 (sars-cov-2) viral load on admission was associated with a significantly increased 30-day mortality (odds ratio [or], 4.20; 95% ci, 1.62–10.86), and anti-sars-cov-2 nucleocapisid igg seropositivity on admission trended toward a reduced 30-day mortality (or, 0.43; 95% ci, 0.15–1.26). reporting of quantitative sars-cov-2 viral load and serologic assays may offer prognostic clinical information."
http://orkg.org/orkg/resource/R187227,A Causality Mining and Knowledge Graph Based Method of Root Cause Diagnosis for Performance Anomaly in Cloud Applications,10.3390/app10062166,crossref,"<jats:p>With the development of cloud computing technology, the microservice architecture (MSA) has become a prevailing application architecture in cloud-native applications. Many user-oriented services are supported by many microservices, and the dependencies between services are more complicated than those of a traditional monolithic architecture application. In such a situation, if an anomalous change happens in the performance metric of a microservice, it will cause other related services to be downgraded or even to fail, which would probably cause large losses to dependent businesses. Therefore, in the operation and maintenance job of cloud applications, it is critical to mine the causality of the problem and find its root cause as soon as possible. In this paper, we propose an approach for mining causality and diagnosing the root cause that uses knowledge graph technology and a causal search algorithm. We verified the proposed method on a classic cloud-native application and found that the method is effective. After applying our method on most of the services of a cloud-native application, both precision and recall were over 80%.</jats:p>","with the development of cloud computing technology, the microservice architecture (msa) has become a prevailing application architecture in cloud-native applications. many user-oriented services are supported by many microservices, and the dependencies between services are more complicated than those of a traditional monolithic architecture application. in such a situation, if an anomalous change happens in the performance metric of a microservice, it will cause other related services to be downgraded or even to fail, which would probably cause large losses to dependent businesses. therefore, in the operation and maintenance job of cloud applications, it is critical to mine the causality of the problem and find its root cause as soon as possible. in this paper, we propose an approach for mining causality and diagnosing the root cause that uses knowledge graph technology and a causal search algorithm. we verified the proposed method on a classic cloud-native application and found that the method is effective. after applying our method on most of the services of a cloud-native application, both precision and recall were over 80%."
http://orkg.org/orkg/resource/R188122,COVID-19 knowledge graph from semantic integration of biomedical literature and databases,,crossref,"<jats:title>Abstract</jats:title>\n               <jats:sec>\n                  <jats:title>Summary</jats:title>\n                  <jats:p>The global response to the COVID-19 pandemic has led to a rapid increase of scientific literature on this deadly disease. Extracting knowledge from biomedical literature and integrating it with relevant information from curated biological databases is essential to gain insight into COVID-19 etiology, diagnosis and treatment. We used Semantic Web technology RDF to integrate COVID-19 knowledge mined from literature by iTextMine, PubTator and SemRep with relevant biological databases and formalized the knowledge in a standardized and computable COVID-19 Knowledge Graph (KG). We published the COVID-19 KG via a SPARQL endpoint to support federated queries on the Semantic Web and developed a knowledge portal with browsing and searching interfaces. We also developed a RESTful API to support programmatic access and provided RDF dumps for download.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Availability and implementation</jats:title>\n                  <jats:p>The COVID-19 Knowledge Graph is publicly available under CC-BY 4.0 license at https://research.bioinformatics.udel.edu/covid19kg/.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Supplementary information</jats:title>\n                  <jats:p>Supplementary data are available at Bioinformatics online.</jats:p>\n               </jats:sec>","abstract \n \n summary \n the global response to the covid-19 pandemic has led to a rapid increase of scientific literature on this deadly disease. extracting knowledge from biomedical literature and integrating it with relevant information from curated biological databases is essential to gain insight into covid-19 etiology, diagnosis and treatment. we used semantic web technology rdf to integrate covid-19 knowledge mined from literature by itextmine, pubtator and semrep with relevant biological databases and formalized the knowledge in a standardized and computable covid-19 knowledge graph (kg). we published the covid-19 kg via a sparql endpoint to support federated queries on the semantic web and developed a knowledge portal with browsing and searching interfaces. we also developed a restful api to support programmatic access and provided rdf dumps for download. \n \n \n availability and implementation \n the covid-19 knowledge graph is publicly available under cc-by 4.0 license at https://research.bioinformatics.udel.edu/covid19kg/. \n \n \n supplementary information \n supplementary data are available at bioinformatics online. \n"
http://orkg.org/orkg/resource/R188141,Publishing speed and acceptance rates of open access megajournals,10.1108/oir-04-2018-0151,crossref,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>The purpose of this paper is to look at two particular aspects of open access megajournals, a new type of scholarly journals. Such journals only review for scientific soundness and leave the judgment of scientific impact to the readers. The two leading journals currently each publish more than 20,000 articles per year. The publishing speed of such journals and acceptance rates of such journals are the topics of the study.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>Submission, acceptance and publication dates for a sample of articles in 12 megajournals were manually extracted from the articles. Information about acceptance rates was obtained using web searches of journal home pages, editorials, blogs, etc.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>The time from submission to publication varies a lot, with engineering megajournals publishing much more rapidly. But on average it takes almost half a year to get published, particularly in the high-volume biomedical journals. As some of the journals have grown in publication volume, the average review time has increased by almost two months. Acceptance rates have slightly decreased over the past five years, and are now in the range of 50–55 percent.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>This is the first empirical study of how long it takes to get published in megajournals and it highlights a clear increase of around two months in publishing. Currently, the review process in the biomedical megajournals takes as long as in regular more selective journals in the same fields. Possible explanations could be increasing difficulties in finding willing and motivated reviewers and in a higher share of submissions from developing countries.</jats:p></jats:sec>","purpose the purpose of this paper is to look at two particular aspects of open access megajournals, a new type of scholarly journals. such journals only review for scientific soundness and leave the judgment of scientific impact to the readers. the two leading journals currently each publish more than 20,000 articles per year. the publishing speed of such journals and acceptance rates of such journals are the topics of the study. design/methodology/approach submission, acceptance and publication dates for a sample of articles in 12 megajournals were manually extracted from the articles. information about acceptance rates was obtained using web searches of journal home pages, editorials, blogs, etc. findings the time from submission to publication varies a lot, with engineering megajournals publishing much more rapidly. but on average it takes almost half a year to get published, particularly in the high-volume biomedical journals. as some of the journals have grown in publication volume, the average review time has increased by almost two months. acceptance rates have slightly decreased over the past five years, and are now in the range of 50–55 percent. originality/value this is the first empirical study of how long it takes to get published in megajournals and it highlights a clear increase of around two months in publishing. currently, the review process in the biomedical megajournals takes as long as in regular more selective journals in the same fields. possible explanations could be increasing difficulties in finding willing and motivated reviewers and in a higher share of submissions from developing countries."
http://orkg.org/orkg/resource/R187558,The Pandemic Penalty: The Gendered Effects of COVID-19 on Scientific Productivity,10.1177/23780231211006977,crossref,"<jats:p> Academia serves as a valuable case for studying the effects of social forces on workplace productivity, using a concrete measure of output: scholarly papers. Many academics, especially women, have experienced unprecedented challenges to scholarly productivity during the coronavirus disease 2019 (COVID-19) pandemic. The authors analyze the gender composition of more than 450,000 authorships in the arXiv and bioRxiv scholarly preprint repositories from before and during the COVID-19 pandemic. This analysis reveals that the underrepresentation of women scientists in the last authorship position necessary for retention and promotion in the sciences is growing more inequitable. The authors find differences between the arXiv and bioRxiv repositories in how gender affects first, middle, and sole authorship submission rates before and during the pandemic. A review of existing research and theory outlines potential mechanisms underlying this widening gender gap in productivity during COVID-19. The authors aggregate recommendations for institutional change that could ameliorate challenges to women’s productivity during the pandemic and beyond. </jats:p>","academia serves as a valuable case for studying the effects of social forces on workplace productivity, using a concrete measure of output: scholarly papers. many academics, especially women, have experienced unprecedented challenges to scholarly productivity during the coronavirus disease 2019 (covid-19) pandemic. the authors analyze the gender composition of more than 450,000 authorships in the arxiv and biorxiv scholarly preprint repositories from before and during the covid-19 pandemic. this analysis reveals that the underrepresentation of women scientists in the last authorship position necessary for retention and promotion in the sciences is growing more inequitable. the authors find differences between the arxiv and biorxiv repositories in how gender affects first, middle, and sole authorship submission rates before and during the pandemic. a review of existing research and theory outlines potential mechanisms underlying this widening gender gap in productivity during covid-19. the authors aggregate recommendations for institutional change that could ameliorate challenges to women’s productivity during the pandemic and beyond."
http://orkg.org/orkg/resource/R187573,Mapping of ImageNet and Wikidata for Knowledge Graphs Enabled Computer Vision,10.52825/bis.v1i.65,crossref,"<jats:p>Knowledge graphs are used as a source of prior knowledge in numerous computer vision tasks. However, such an approach requires to have a mapping between ground truth data labels and the target knowledge graph. We linked the ILSVRC 2012 dataset (often simply referred to as ImageNet) labels to Wikidata entities. This enables using rich knowledge graph structure and contextual information for several computer vision tasks, traditionally benchmarked with ImageNet and its variations. For instance, in few-shot learning classification scenarios with neural networks, this mapping can be leveraged for weight initialisation, which can improve the final performance metrics value. We mapped all 1000 ImageNet labels – 461 were already directly linked with the exact match property (P2888), 467 have exact match candidates, and 72 cannot be matched directly. For these 72 labels, we discuss different problem categories stemming from the inability of finding an exact match. Semantically close non-exact match candidates are presented as well. The mapping is publicly available athttps://github.com/DominikFilipiak/imagenet-to-wikidata-mapping.</jats:p>","knowledge graphs are used as a source of prior knowledge in numerous computer vision tasks. however, such an approach requires to have a mapping between ground truth data labels and the target knowledge graph. we linked the ilsvrc 2012 dataset (often simply referred to as imagenet) labels to wikidata entities. this enables using rich knowledge graph structure and contextual information for several computer vision tasks, traditionally benchmarked with imagenet and its variations. for instance, in few-shot learning classification scenarios with neural networks, this mapping can be leveraged for weight initialisation, which can improve the final performance metrics value. we mapped all 1000 imagenet labels – 461 were already directly linked with the exact match property (p2888), 467 have exact match candidates, and 72 cannot be matched directly. for these 72 labels, we discuss different problem categories stemming from the inability of finding an exact match. semantically close non-exact match candidates are presented as well. the mapping is publicly available athttps://github.com/dominikfilipiak/imagenet-to-wikidata-mapping."
http://orkg.org/orkg/resource/R187629,Gender Disparity in the Authorship of Biomedical Research Publications During the COVID-19 Pandemic: Retrospective Observational Study,10.2196/25379,crossref,"<jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Gender imbalances in academia have been evident historically and persist today. For the past 60 years, we have witnessed the increase of participation of women in biomedical disciplines, showing that the gender gap is shrinking. However, preliminary evidence suggests that women, including female researchers, are disproportionately affected by the COVID-19 pandemic in terms of unequal distribution of childcare, elderly care, and other kinds of domestic and emotional labor. Sudden lockdowns and abrupt shifts in daily routines have had disproportionate consequences on their productivity, which is reflected by a sudden drop in research output in biomedical research, consequently affecting the number of female authors of scientific publications.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Objective</jats:title>\n            <jats:p>The objective of this study is to test the hypothesis that the COVID-19 pandemic has had a disproportionate adverse effect on the productivity of female researchers in the biomedical field in terms of authorship of scientific publications.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Methods</jats:title>\n            <jats:p>This is a retrospective observational bibliometric study. We investigated the proportion of male and female researchers who published scientific papers during the COVID-19 pandemic, using bibliometric data from biomedical preprint servers and selected Springer-Nature journals. We used the ordinary least squares regression model to estimate the expected proportions over time by correcting for temporal trends. We also used a set of statistical methods, such as the Kolmogorov-Smirnov test and regression discontinuity design, to test the validity of the results.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>A total of 78,950 papers from the bioRxiv and medRxiv repositories and from 62 selected Springer-Nature journals by 346,354 unique authors were analyzed. The acquired data set consisted of papers that were published between January 1, 2019, and August 2, 2020.  The proportion of female first authors publishing in the biomedical field during the pandemic dropped by 9.1%, on average, across disciplines (expected arithmetic mean yest=0.39; observed arithmetic mean y=0.35; standard error of the estimate, Sest=0.007; standard error of the observation, σx=0.004). The impact was particularly pronounced for papers related to COVID-19 research, where the proportion of female scientists in the first author position dropped by 28% (yest=0.39; y=0.28; Sest=0.007; σx=0.007). When looking at the last authors, the proportion of women dropped by 7.9%, on average (yest=0.25; y=0.23; Sest=0.005; σx=0.003), while the proportion of women writing about COVID-19 as the last author decreased by 18.8% (yest=0.25; y=0.21; Sest=0.005; σx=0.007). Further, by geocoding authors’ affiliations, we showed that the gender disparities became even more apparent when disaggregated by country, up to 35% in some cases.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>Our findings document a decrease in the number of publications by female authors in the biomedical field during the global pandemic. This effect was particularly pronounced for papers related to COVID-19, indicating that women are producing fewer publications related to COVID-19 research. This sudden increase in the gender gap was persistent across the 10 countries with the highest number of researchers. These results should be used to inform the scientific community of this worrying trend in COVID-19 research and the disproportionate effect that the pandemic has had on female academics.</jats:p>\n          </jats:sec>","\n background \n gender imbalances in academia have been evident historically and persist today. for the past 60 years, we have witnessed the increase of participation of women in biomedical disciplines, showing that the gender gap is shrinking. however, preliminary evidence suggests that women, including female researchers, are disproportionately affected by the covid-19 pandemic in terms of unequal distribution of childcare, elderly care, and other kinds of domestic and emotional labor. sudden lockdowns and abrupt shifts in daily routines have had disproportionate consequences on their productivity, which is reflected by a sudden drop in research output in biomedical research, consequently affecting the number of female authors of scientific publications. \n \n \n objective \n the objective of this study is to test the hypothesis that the covid-19 pandemic has had a disproportionate adverse effect on the productivity of female researchers in the biomedical field in terms of authorship of scientific publications. \n \n \n methods \n this is a retrospective observational bibliometric study. we investigated the proportion of male and female researchers who published scientific papers during the covid-19 pandemic, using bibliometric data from biomedical preprint servers and selected springer-nature journals. we used the ordinary least squares regression model to estimate the expected proportions over time by correcting for temporal trends. we also used a set of statistical methods, such as the kolmogorov-smirnov test and regression discontinuity design, to test the validity of the results. \n \n \n results \n a total of 78,950 papers from the biorxiv and medrxiv repositories and from 62 selected springer-nature journals by 346,354 unique authors were analyzed. the acquired data set consisted of papers that were published between january 1, 2019, and august 2, 2020. the proportion of female first authors publishing in the biomedical field during the pandemic dropped by 9.1%, on average, across disciplines (expected arithmetic mean yest=0.39; observed arithmetic mean y=0.35; standard error of the estimate, sest=0.007; standard error of the observation, σx=0.004). the impact was particularly pronounced for papers related to covid-19 research, where the proportion of female scientists in the first author position dropped by 28% (yest=0.39; y=0.28; sest=0.007; σx=0.007). when looking at the last authors, the proportion of women dropped by 7.9%, on average (yest=0.25; y=0.23; sest=0.005; σx=0.003), while the proportion of women writing about covid-19 as the last author decreased by 18.8% (yest=0.25; y=0.21; sest=0.005; σx=0.007). further, by geocoding authors’ affiliations, we showed that the gender disparities became even more apparent when disaggregated by country, up to 35% in some cases. \n \n \n conclusions \n our findings document a decrease in the number of publications by female authors in the biomedical field during the global pandemic. this effect was particularly pronounced for papers related to covid-19, indicating that women are producing fewer publications related to covid-19 research. this sudden increase in the gender gap was persistent across the 10 countries with the highest number of researchers. these results should be used to inform the scientific community of this worrying trend in covid-19 research and the disproportionate effect that the pandemic has had on female academics. \n"
http://orkg.org/orkg/resource/R188183,Assessing the effect of article processing charges on the geographic diversity of authors using Elsevier’s ‘Mirror Journal’ system,10.31222/osf.io/s7cx4,crossref,"<p>Journals publishing open access (OA) articles often require that authors pay article processing charges (APC). Researchers in the Global South often cite APCs as a major financial obstacle to OA publishing, especially in widely-recognized or prestigious outlets. Consequently, it has been hypothesized that authors from the Global South will be underrepresented in journals charging APCs. We tested this hypothesis using &amp;gt;37,000 articles from Elsevier’s ‘Mirror journal’ system, in which a hybrid ‘Parent’ journal and its Gold-OA ‘Mirror’ share editorial boards and standards for acceptance. Most articles were non-OA; 45% of articles had lead authors based in either the United States of America (USA) or China. After correcting for the effect of this dominance and differences in sample size, we found that OA articles published in Parent and Mirror journals had lead authors with similar Geographic Diversity. However, Author Geographic Diversity of OA articles was significantly lower than that of non-OA articles. Most OA articles were written by authors in high-income countries, and there were no articles in Mirror journals by authors in low-income countries. Our results for Elsevier’s Mirror-Parent system are consistent with the hypothesis that APCs are a barrier to OA publication for scientists from the Global South.</p>","journals publishing open access (oa) articles often require that authors pay article processing charges (apc). researchers in the global south often cite apcs as a major financial obstacle to oa publishing, especially in widely-recognized or prestigious outlets. consequently, it has been hypothesized that authors from the global south will be underrepresented in journals charging apcs. we tested this hypothesis using &amp;gt;37,000 articles from elsevier’s ‘mirror journal’ system, in which a hybrid ‘parent’ journal and its gold-oa ‘mirror’ share editorial boards and standards for acceptance. most articles were non-oa; 45% of articles had lead authors based in either the united states of america (usa) or china. after correcting for the effect of this dominance and differences in sample size, we found that oa articles published in parent and mirror journals had lead authors with similar geographic diversity. however, author geographic diversity of oa articles was significantly lower than that of non-oa articles. most oa articles were written by authors in high-income countries, and there were no articles in mirror journals by authors in low-income countries. our results for elsevier’s mirror-parent system are consistent with the hypothesis that apcs are a barrier to oa publication for scientists from the global south."
http://orkg.org/orkg/resource/R191750,Effect of Data Scaling Methods on Machine Learning Algorithms and Model Performance,10.3390/technologies9030052,crossref,"<jats:p>Heart disease, one of the main reasons behind the high mortality rate around the world, requires a sophisticated and expensive diagnosis process. In the recent past, much literature has demonstrated machine learning approaches as an opportunity to efficiently diagnose heart disease patients. However, challenges associated with datasets such as missing data, inconsistent data, and mixed data (containing inconsistent missing data both as numerical and categorical) are often obstacles in medical diagnosis. This inconsistency led to a higher probability of misprediction and a misled result. Data preprocessing steps like feature reduction, data conversion, and data scaling are employed to form a standard dataset—such measures play a crucial role in reducing inaccuracy in final prediction. This paper aims to evaluate eleven machine learning (ML) algorithms—Logistic Regression (LR), Linear Discriminant Analysis (LDA), K-Nearest Neighbors (KNN), Classification and Regression Trees (CART), Naive Bayes (NB), Support Vector Machine (SVM), XGBoost (XGB), Random Forest Classifier (RF), Gradient Boost (GB), AdaBoost (AB), Extra Tree Classifier (ET)—and six different data scaling methods—Normalization (NR), Standscale (SS), MinMax (MM), MaxAbs (MA), Robust Scaler (RS), and Quantile Transformer (QT) on a dataset comprising of information of patients with heart disease. The result shows that CART, along with RS or QT, outperforms all other ML algorithms with 100% accuracy, 100% precision, 99% recall, and 100% F1 score. The study outcomes demonstrate that the model’s performance varies depending on the data scaling method.</jats:p>","heart disease, one of the main reasons behind the high mortality rate around the world, requires a sophisticated and expensive diagnosis process. in the recent past, much literature has demonstrated machine learning approaches as an opportunity to efficiently diagnose heart disease patients. however, challenges associated with datasets such as missing data, inconsistent data, and mixed data (containing inconsistent missing data both as numerical and categorical) are often obstacles in medical diagnosis. this inconsistency led to a higher probability of misprediction and a misled result. data preprocessing steps like feature reduction, data conversion, and data scaling are employed to form a standard dataset—such measures play a crucial role in reducing inaccuracy in final prediction. this paper aims to evaluate eleven machine learning (ml) algorithms—logistic regression (lr), linear discriminant analysis (lda), k-nearest neighbors (knn), classification and regression trees (cart), naive bayes (nb), support vector machine (svm), xgboost (xgb), random forest classifier (rf), gradient boost (gb), adaboost (ab), extra tree classifier (et)—and six different data scaling methods—normalization (nr), standscale (ss), minmax (mm), maxabs (ma), robust scaler (rs), and quantile transformer (qt) on a dataset comprising of information of patients with heart disease. the result shows that cart, along with rs or qt, outperforms all other ml algorithms with 100% accuracy, 100% precision, 99% recall, and 100% f1 score. the study outcomes demonstrate that the model’s performance varies depending on the data scaling method."
http://orkg.org/orkg/resource/R191025,Multiplexed single-cell transcriptional response profiling to define cancer vulnerabilities and therapeutic mechanism of action,10.1038/s41467-020-17440-w,crossref,"<jats:title>Abstract</jats:title><jats:p>Assays to study cancer cell responses to pharmacologic or genetic perturbations are typically restricted to using simple phenotypic readouts such as proliferation rate. Information-rich assays, such as gene-expression profiling, have generally not permitted efficient profiling of a given perturbation across multiple cellular contexts. Here, we develop MIX-Seq, a method for multiplexed transcriptional profiling of post-perturbation responses across a mixture of samples with single-cell resolution, using SNP-based computational demultiplexing of single-cell RNA-sequencing data. We show that MIX-Seq can be used to profile responses to chemical or genetic perturbations across pools of 100 or more cancer cell lines. We combine it with Cell Hashing to further multiplex additional experimental conditions, such as post-treatment time points or drug doses. Analyzing the high-content readout of scRNA-seq reveals both shared and context-specific transcriptional response components that can identify drug mechanism of action and enable prediction of long-term cell viability from short-term transcriptional responses to treatment.</jats:p>","abstract assays to study cancer cell responses to pharmacologic or genetic perturbations are typically restricted to using simple phenotypic readouts such as proliferation rate. information-rich assays, such as gene-expression profiling, have generally not permitted efficient profiling of a given perturbation across multiple cellular contexts. here, we develop mix-seq, a method for multiplexed transcriptional profiling of post-perturbation responses across a mixture of samples with single-cell resolution, using snp-based computational demultiplexing of single-cell rna-sequencing data. we show that mix-seq can be used to profile responses to chemical or genetic perturbations across pools of 100 or more cancer cell lines. we combine it with cell hashing to further multiplex additional experimental conditions, such as post-treatment time points or drug doses. analyzing the high-content readout of scrna-seq reveals both shared and context-specific transcriptional response components that can identify drug mechanism of action and enable prediction of long-term cell viability from short-term transcriptional responses to treatment."
http://orkg.org/orkg/resource/R187786,The impact of the open-access status on journal indices: a review of medical journals,10.12688/f1000research.17979.1,crossref,"<ns4:p><ns4:bold>Background:</ns4:bold> Over the past few decades, there has been an increase in the number of open access (OA) journals in almost all disciplines. This increase in OA journals was accompanied an increase in funding to support such movements. Medical fields are among the highest funded fields, which further promoted its journals to move toward OA publishing. Here, we aim to compare OA and non-OA journals in terms of citation metrics and other indices.</ns4:p><ns4:p> <ns4:bold>Methods:</ns4:bold> We collected data on the included journals from Scopus Source List on 1<ns4:sup>st</ns4:sup> November 2018.\xa0 We filtered the list for medical journals only. For each journal, we extracted data regarding citation metrics, scholarly output, and wither the journal is OA or non-OA.</ns4:p><ns4:p> <ns4:bold>Results:</ns4:bold> On the 2017 Scopus list of journals, there was 5835 medical journals. Upon analyzing the difference between medical OA and non-OA journals, we found that OA journals had a significantly higher CiteScore (p&lt; 0.001), percent cited (p&lt; 0.001), and source normalized impact per paper (SNIP) (p&lt; 0.001), whereas non-OA journals had higher scholarly output (p&lt; 0.001). Among the five largest journal publishers, Springer Nature published the highest frequency of OA articles (31.5%), while Wiley-Blackwell had the lowest frequency among its medical journals (4.4%).</ns4:p><ns4:p> <ns4:bold>Conclusion:</ns4:bold> Among medical journals, although non-OA journals still have higher output in terms of articles per year, OA journals have higher citation metrics.</ns4:p>","background: over the past few decades, there has been an increase in the number of open access (oa) journals in almost all disciplines. this increase in oa journals was accompanied an increase in funding to support such movements. medical fields are among the highest funded fields, which further promoted its journals to move toward oa publishing. here, we aim to compare oa and non-oa journals in terms of citation metrics and other indices. methods: we collected data on the included journals from scopus source list on 1 st november 2018.\xa0 we filtered the list for medical journals only. for each journal, we extracted data regarding citation metrics, scholarly output, and wither the journal is oa or non-oa. results: on the 2017 scopus list of journals, there was 5835 medical journals. upon analyzing the difference between medical oa and non-oa journals, we found that oa journals had a significantly higher citescore (p&lt; 0.001), percent cited (p&lt; 0.001), and source normalized impact per paper (snip) (p&lt; 0.001), whereas non-oa journals had higher scholarly output (p&lt; 0.001). among the five largest journal publishers, springer nature published the highest frequency of oa articles (31.5%), while wiley-blackwell had the lowest frequency among its medical journals (4.4%). conclusion: among medical journals, although non-oa journals still have higher output in terms of articles per year, oa journals have higher citation metrics."
http://orkg.org/orkg/resource/R187737,News media attention in Climate Action: latent topics and open access,10.1007/s11192-021-04095-7,crossref,"<jats:title>Abstract</jats:title><jats:p>In this study we investigated whether open access could assist the broader dissemination of scientific research in Climate Action (Sustainable Development Goal 13) via news outlets. We did this by comparing (i) the share of open and non-open access documents in different Climate Action topics, and their news counts, and (ii) the mean of news counts for open access and non-open access documents. The data set of this study comprised 70,206 articles and reviews in Sustainable Development Goal 13, published during 2014–2018, retrieved from SciVal. The number of news mentions for each document was obtained from Altmetrics Details Page API using their DOIs, whereas the open access statuses were obtained using Unpaywall.org. The analysis in this paper was done using a combination of (Latent Dirichlet allocation) topic modelling, descriptive statistics, and regression analysis. The covariates included in the regression analysis were features related to authors, country, journal, institution, funding, readability, news source category and topic. Using topic modelling, we identified 10 topics, with topics 4 (meteorology) [21%], 5 (adaption, mitigation, and legislation) [18%] and 8 (ecosystems and biodiversity) [14%] accounting for 53% of the research in Sustainable Development Goal 13. Additionally, the results of regression analysis showed that while keeping all the variables constant in the model, open access papers in Climate Action had a news count advantage (8.8%) in comparison to non-open access papers. Our findings also showed that while a higher share of open access documents in topics such as topic 9 (Human vulnerability to risks) might not assist with its broader dissemination, in some others such as topic 5 (adaption, mitigation, and legislation), even a lower share of open access documents might accelerate its broad communication via news outlets.</jats:p>","abstract in this study we investigated whether open access could assist the broader dissemination of scientific research in climate action (sustainable development goal 13) via news outlets. we did this by comparing (i) the share of open and non-open access documents in different climate action topics, and their news counts, and (ii) the mean of news counts for open access and non-open access documents. the data set of this study comprised 70,206 articles and reviews in sustainable development goal 13, published during 2014–2018, retrieved from scival. the number of news mentions for each document was obtained from altmetrics details page api using their dois, whereas the open access statuses were obtained using unpaywall.org. the analysis in this paper was done using a combination of (latent dirichlet allocation) topic modelling, descriptive statistics, and regression analysis. the covariates included in the regression analysis were features related to authors, country, journal, institution, funding, readability, news source category and topic. using topic modelling, we identified 10 topics, with topics 4 (meteorology) [21%], 5 (adaption, mitigation, and legislation) [18%] and 8 (ecosystems and biodiversity) [14%] accounting for 53% of the research in sustainable development goal 13. additionally, the results of regression analysis showed that while keeping all the variables constant in the model, open access papers in climate action had a news count advantage (8.8%) in comparison to non-open access papers. our findings also showed that while a higher share of open access documents in topics such as topic 9 (human vulnerability to risks) might not assist with its broader dissemination, in some others such as topic 5 (adaption, mitigation, and legislation), even a lower share of open access documents might accelerate its broad communication via news outlets."
http://orkg.org/orkg/resource/R187739,All the research that’s fit to print: Open access and the news media,10.1162/qss_a_00139,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>The goal of the open access (OA) movement is to help everyone access scholarly research, not just those who can afford to. However, most studies looking at whether OA has met this goal have focused on whether other scholars are making use of OA research. Few have considered how the broader public, including the news media, uses OA research. I sought to answer whether the news media mentions OA articles more or less than paywalled articles by looking at articles published from 2010 through 2018 in journals across all four quartiles of the Journal Impact Factor using data obtained through Altmetric.com and Web of Science. Gold, green and hybrid OA articles all had a positive correlation with the number of news mentions received. News mentions for OA articles did see a dip in 2018, although they remained higher than those for paywalled articles.</jats:p>","abstract \n the goal of the open access (oa) movement is to help everyone access scholarly research, not just those who can afford to. however, most studies looking at whether oa has met this goal have focused on whether other scholars are making use of oa research. few have considered how the broader public, including the news media, uses oa research. i sought to answer whether the news media mentions oa articles more or less than paywalled articles by looking at articles published from 2010 through 2018 in journals across all four quartiles of the journal impact factor using data obtained through altmetric.com and web of science. gold, green and hybrid oa articles all had a positive correlation with the number of news mentions received. news mentions for oa articles did see a dip in 2018, although they remained higher than those for paywalled articles."
http://orkg.org/orkg/resource/R187745,The Impact of Open Access Mandates on Invention,10.1162/rest_a_00926,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>How do barriers to the diffusion of academic research affect innovation? In 2008, the National Institutes of Health (NIH) mandated free online availability of funded research. This policy caused a 50 percentage point increase in free access to funded articles. We introduce a novel measure, in-text patent citations, to study how this mandate affected industry use of academic science. After 2008, patents cite NIH-funded research 12% to 27% more often. Nonfunded research, funded research in journals unaffected by the mandate, and academic citations see no change. These estimates are consistent with a model of search for useful knowledge. Inefficiency caused by academic publishing may be substantial.</jats:p>","abstract \n how do barriers to the diffusion of academic research affect innovation? in 2008, the national institutes of health (nih) mandated free online availability of funded research. this policy caused a 50 percentage point increase in free access to funded articles. we introduce a novel measure, in-text patent citations, to study how this mandate affected industry use of academic science. after 2008, patents cite nih-funded research 12% to 27% more often. nonfunded research, funded research in journals unaffected by the mandate, and academic citations see no change. these estimates are consistent with a model of search for useful knowledge. inefficiency caused by academic publishing may be substantial."
http://orkg.org/orkg/resource/R187749,An analysis of COVID-19 article dissemination by Twitter compared to citation rates,10.1101/2020.06.22.20137505,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Background</jats:title><jats:p>The COVID-19 pandemic has resulted in over 1,000,000 cases across 181 countries worldwide. The global impact of COVID-19 has resulted in a surge of related research. Researchers have turned to social media platforms, namely Twitter, to disseminate their articles. The online database Altmetric is a tool which tracks the social media metrics of articles and is complementary to traditional, citation-based metrics. Citation-based metrics may fail to portray dissemination accurately, due to the lengthy publication process. Altmetrics are not subject to this time-lag, suggesting that they may be an effective marker of research dissemination during the COVID-19 pandemic.</jats:p></jats:sec><jats:sec><jats:title>Objectives</jats:title><jats:p>To assess the dissemination of COVID-19 articles as measured by Twitter dissemination, compared to traditional citation-based metrics, and determine article characteristics associated with tweet rates.</jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p>COVID-19 articles obtained from LitCovid published between January 1st to March 18th, 2020 were screened for inclusion. The following article characteristics were extracted independently, in single: Topic (General Info, Mechanism, Diagnosis, Transmission, Treatment, Prevention, Case Report, and Epidemic Forecasting), open access status (open access and subscription-based), continent of corresponding author (Asia, Australia, Africa, North America, South America, and Europe), tweets, and citations. A sign test was used to compare the tweet rate and citation rate per day. A negative binomial regression analysis was conducted to evaluate the association between tweet rate and article characteristics of interest.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>1328 articles were included in the analysis. Tweet rates were found to be significantly higher than citation rates for COVID-19 articles, with a median tweet rate of 1.09 (IQR 6.83) tweets per day and median citation rate of 0.00 (IQR 0.00) citations per day, resulting in a median of differences of 1.09 (95% CI 0.86-1.33, <jats:italic>P</jats:italic> &lt; .001). 2018 journal impact factors were positively correlated with tweet rate (<jats:italic>P</jats:italic> &lt; .001). The topics Diagnosis (<jats:italic>P</jats:italic> = .01), Transmission (<jats:italic>P</jats:italic> &lt; .001), Treatment (<jats:italic>P</jats:italic> = .01), and Epidemic Forecasting (<jats:italic>P</jats:italic> &lt; .001) were positively correlated with tweet rate, relative to Case Report. The following continents of the corresponding author were negatively correlated with tweet rate, Africa (<jats:italic>P</jats:italic> &lt; .001), Australia (<jats:italic>P</jats:italic> = .03), and South America (<jats:italic>P</jats:italic> &lt; .001), relative to Asia. Open access journals were negatively correlated with tweet rate, relative to subscription-based journals (<jats:italic>P</jats:italic> &lt; .001).</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>COVID-19 articles had significantly higher tweets rates compared to citation rates. This study further identified article characteristics that are correlated with the dissemination of articles on Twitter, such as 2018 journal impact factor, continent of the corresponding author, topic, and open access status. This highlights the importance of altmetrics in periods of rapidly expanding research, such as the COVID-19 pandemic to localize highly disseminated articles.</jats:p></jats:sec>","abstract background the covid-19 pandemic has resulted in over 1,000,000 cases across 181 countries worldwide. the global impact of covid-19 has resulted in a surge of related research. researchers have turned to social media platforms, namely twitter, to disseminate their articles. the online database altmetric is a tool which tracks the social media metrics of articles and is complementary to traditional, citation-based metrics. citation-based metrics may fail to portray dissemination accurately, due to the lengthy publication process. altmetrics are not subject to this time-lag, suggesting that they may be an effective marker of research dissemination during the covid-19 pandemic. objectives to assess the dissemination of covid-19 articles as measured by twitter dissemination, compared to traditional citation-based metrics, and determine article characteristics associated with tweet rates. methods covid-19 articles obtained from litcovid published between january 1st to march 18th, 2020 were screened for inclusion. the following article characteristics were extracted independently, in single: topic (general info, mechanism, diagnosis, transmission, treatment, prevention, case report, and epidemic forecasting), open access status (open access and subscription-based), continent of corresponding author (asia, australia, africa, north america, south america, and europe), tweets, and citations. a sign test was used to compare the tweet rate and citation rate per day. a negative binomial regression analysis was conducted to evaluate the association between tweet rate and article characteristics of interest. results 1328 articles were included in the analysis. tweet rates were found to be significantly higher than citation rates for covid-19 articles, with a median tweet rate of 1.09 (iqr 6.83) tweets per day and median citation rate of 0.00 (iqr 0.00) citations per day, resulting in a median of differences of 1.09 (95% ci 0.86-1.33, p &lt; .001). 2018 journal impact factors were positively correlated with tweet rate ( p &lt; .001). the topics diagnosis ( p = .01), transmission ( p &lt; .001), treatment ( p = .01), and epidemic forecasting ( p &lt; .001) were positively correlated with tweet rate, relative to case report. the following continents of the corresponding author were negatively correlated with tweet rate, africa ( p &lt; .001), australia ( p = .03), and south america ( p &lt; .001), relative to asia. open access journals were negatively correlated with tweet rate, relative to subscription-based journals ( p &lt; .001). conclusions covid-19 articles had significantly higher tweets rates compared to citation rates. this study further identified article characteristics that are correlated with the dissemination of articles on twitter, such as 2018 journal impact factor, continent of the corresponding author, topic, and open access status. this highlights the importance of altmetrics in periods of rapidly expanding research, such as the covid-19 pandemic to localize highly disseminated articles."
http://orkg.org/orkg/resource/R187751,Revisiting an open access monograph experiment: measuring citations and tweets 5 years later,10.1007/s11192-016-2160-6,crossref,"<jats:title>Abstract</jats:title><jats:p>An experiment run in 2009 could not assess whether making monographs available in open access enhanced scholarly impact. This paper revisits the experiment, drawing on additional citation data and tweets. It attempts to answer the following research question: does open access have a positive influence on the number of citations and tweets a monograph receives, taking into account the influence of scholarly field and language? The correlation between monograph citations and tweets is also investigated. The number of citations and tweets measured in 2014 reveal a slight open access advantage, but the influence of language or subject should also be taken into account. However, Twitter usage and citation behaviour hardly overlap.</jats:p>","abstract an experiment run in 2009 could not assess whether making monographs available in open access enhanced scholarly impact. this paper revisits the experiment, drawing on additional citation data and tweets. it attempts to answer the following research question: does open access have a positive influence on the number of citations and tweets a monograph receives, taking into account the influence of scholarly field and language? the correlation between monograph citations and tweets is also investigated. the number of citations and tweets measured in 2014 reveal a slight open access advantage, but the influence of language or subject should also be taken into account. however, twitter usage and citation behaviour hardly overlap."
http://orkg.org/orkg/resource/R187759,Do open access journal articles experience a citation advantage? Results and methodological reflections of an application of multiple measures to an analysis by WoS subject areas,10.1007/s11192-020-03734-9,crossref,"<jats:title>Abstract</jats:title><jats:p>This study is one of the first that uses the recently introduced open access (OA) labels in the Web of Science (WoS) metadata to investigate whether OA articles published in Directory of Open Access Journals (DOAJ) listed journals experience a citation advantage in comparison to subscription journal articles, specifically those of which no self-archived versions are available. Bibliometric data on all articles and reviews indexed in WoS, and published from 2013 to 2015, were analysed. In addition to normalised citation score (NCS), we used two additional measures of citation advantage: whether an article was cited at all; and whether an article is among the most frequently cited percentile of articles within its respective subject area (pptopX\xa0%). For each WoS subject area, the strength of the relationship between access status (whether an article was published in an OA journal) and each of these three measures was calculated. We found that OA journal articles experience a citation advantage in very few subject areas and, in most of these subject areas, the citation advantage was found on only a single measure of citation advantage, namely whether the article was cited at all. Our results lead us to conclude that access status accounts for little of the variability in the number of citations an article accumulates. The methodology and the calculations that were used in this study are described in detail and we believe that the lessons we learnt, and the recommendations we make, will be of much use to future researchers interested in using the WoS OA labels, and to the field of citation advantage in general.</jats:p>","abstract this study is one of the first that uses the recently introduced open access (oa) labels in the web of science (wos) metadata to investigate whether oa articles published in directory of open access journals (doaj) listed journals experience a citation advantage in comparison to subscription journal articles, specifically those of which no self-archived versions are available. bibliometric data on all articles and reviews indexed in wos, and published from 2013 to 2015, were analysed. in addition to normalised citation score (ncs), we used two additional measures of citation advantage: whether an article was cited at all; and whether an article is among the most frequently cited percentile of articles within its respective subject area (pptopx\xa0%). for each wos subject area, the strength of the relationship between access status (whether an article was published in an oa journal) and each of these three measures was calculated. we found that oa journal articles experience a citation advantage in very few subject areas and, in most of these subject areas, the citation advantage was found on only a single measure of citation advantage, namely whether the article was cited at all. our results lead us to conclude that access status accounts for little of the variability in the number of citations an article accumulates. the methodology and the calculations that were used in this study are described in detail and we believe that the lessons we learnt, and the recommendations we make, will be of much use to future researchers interested in using the wos oa labels, and to the field of citation advantage in general."
http://orkg.org/orkg/resource/R187763,The open access advantage for studies of human electrophysiology: Impact on citations and Altmetrics,10.1016/j.ijpsycho.2021.03.006,crossref,"<p>Barriers to accessing scientific findings contribute to knowledge inequalities based on financial resources and decrease the transparency and rigor of scientific research. Recent initiatives aim to improve access to research as well as methodological rigor via transparency and openness. We sought to determine the impact of such initiatives on open access publishing in the sub-area of human electrophysiology and the impact of open access on the attention articles received in the scholarly literature and other outlets. Data for 35,144 articles across 967 journals from the last 20 years were examined. Approximately 35% of articles were open access, and the rate of publication of open-access articles increased over time. Open access articles showed 9 to 21% more PubMed and CrossRef citations and 39% more Altmetric mentions than closed access articles. Green open access articles (i.e., author archived) did not differ from non-green open access articles (i.e., publisher archived) with respect to citations and were related to higher Altmetric mentions. These findings demonstrate that open-access publishing is increasing in popularity in the sub-area of human electrophysiology and that open-access articles enjoy the “open access advantage” in citations similar to the larger scientific literature. The benefit of the open access advantage may motivate researchers to make their publications open access and pursue publication outlets that support it. In consideration of the direct connection between citations and journal impact factor, journal editors may improve the accessibility and impact of published articles by encouraging authors to self-archive manuscripts on preprint servers.</p>","barriers to accessing scientific findings contribute to knowledge inequalities based on financial resources and decrease the transparency and rigor of scientific research. recent initiatives aim to improve access to research as well as methodological rigor via transparency and openness. we sought to determine the impact of such initiatives on open access publishing in the sub-area of human electrophysiology and the impact of open access on the attention articles received in the scholarly literature and other outlets. data for 35,144 articles across 967 journals from the last 20 years were examined. approximately 35% of articles were open access, and the rate of publication of open-access articles increased over time. open access articles showed 9 to 21% more pubmed and crossref citations and 39% more altmetric mentions than closed access articles. green open access articles (i.e., author archived) did not differ from non-green open access articles (i.e., publisher archived) with respect to citations and were related to higher altmetric mentions. these findings demonstrate that open-access publishing is increasing in popularity in the sub-area of human electrophysiology and that open-access articles enjoy the “open access advantage” in citations similar to the larger scientific literature. the benefit of the open access advantage may motivate researchers to make their publications open access and pursue publication outlets that support it. in consideration of the direct connection between citations and journal impact factor, journal editors may improve the accessibility and impact of published articles by encouraging authors to self-archive manuscripts on preprint servers."
http://orkg.org/orkg/resource/R187771,What happens when a journal converts to open access? A bibliometric analysis,10.1007/s11192-021-03972-5,crossref,"<jats:title>Abstract</jats:title><jats:p>In recent years, increased stakeholder pressure to transition research to Open Access has led to many journals converting, or ‘flipping’, from a closed access (CA) to an open access (OA) publishing model. Changing the publishing model can influence the decision of authors to submit their papers to a journal, and increased article accessibility may influence citation behaviour. In this paper we aimed to understand how flipping a journal to an OA model influences the journal’s future publication volumes and citation impact. We analysed two independent sets of journals that had flipped to an OA model, one from the Directory of Open Access Journals (DOAJ) and one from the Open Access Directory (OAD), and compared their development with two respective control groups of similar journals. For bibliometric analyses, journals were matched to the Scopus database. We assessed changes in the number of articles published over time, as well as two citation metrics at the journal and article level: the normalised impact factor (IF) and the average relative citations (ARC), respectively. Our results show that overall, journals that flipped to an OA model increased their publication output compared to journals that remained closed. Mean normalised IF and ARC also generally increased following the flip to an OA model, at a greater rate than was observed in the control groups. However, the changes appear to vary largely by scientific discipline. Overall, these results indicate that flipping to an OA publishing model can bring positive changes to a journal.</jats:p>","abstract in recent years, increased stakeholder pressure to transition research to open access has led to many journals converting, or ‘flipping’, from a closed access (ca) to an open access (oa) publishing model. changing the publishing model can influence the decision of authors to submit their papers to a journal, and increased article accessibility may influence citation behaviour. in this paper we aimed to understand how flipping a journal to an oa model influences the journal’s future publication volumes and citation impact. we analysed two independent sets of journals that had flipped to an oa model, one from the directory of open access journals (doaj) and one from the open access directory (oad), and compared their development with two respective control groups of similar journals. for bibliometric analyses, journals were matched to the scopus database. we assessed changes in the number of articles published over time, as well as two citation metrics at the journal and article level: the normalised impact factor (if) and the average relative citations (arc), respectively. our results show that overall, journals that flipped to an oa model increased their publication output compared to journals that remained closed. mean normalised if and arc also generally increased following the flip to an oa model, at a greater rate than was observed in the control groups. however, the changes appear to vary largely by scientific discipline. overall, these results indicate that flipping to an oa publishing model can bring positive changes to a journal."
http://orkg.org/orkg/resource/R187766,Do articles in open access journals have more frequent altmetric activity than articles in subscription-based journals? An investigation of the research output of Finnish universities,10.1007/s11192-019-03301-x,crossref,"<jats:title>Abstract</jats:title><jats:p>Scientific articles available in Open Access (OA) have been found to attract more citations and online attention to the extent that it has become common to speak about OA Altmetrics Advantage. This research investigates how the OA Altmetrics Advantage holds for a specific case of research articles, namely the research outputs from universities in Finland. Furthermore, this research examines disciplinary and platform specific differences in that (dis)advantage. The new methodological approaches developed in this research focus on relative visibility, i.e. how often articles in OA journals receive at least one mention on the investigated online platforms, and relative receptivity, i.e. how frequently articles in OA journals gain mentions in comparison to articles in subscription-based journals. The results show significant disciplinary and platform specific differences in the OA advantage, with articles in OA journals within for instance veterinary sciences, social and economic geography and psychology receiving more citations and attention on social media platforms, while the opposite was found for articles in OA journals within medicine and health sciences. The results strongly support field- and platform-specific considerations when assessing the influence of journal OA status on altmetrics. The new methodological approaches used in this research will serve future comparative research into OA advantage of scientific articles over time and between countries.</jats:p>","abstract scientific articles available in open access (oa) have been found to attract more citations and online attention to the extent that it has become common to speak about oa altmetrics advantage. this research investigates how the oa altmetrics advantage holds for a specific case of research articles, namely the research outputs from universities in finland. furthermore, this research examines disciplinary and platform specific differences in that (dis)advantage. the new methodological approaches developed in this research focus on relative visibility, i.e. how often articles in oa journals receive at least one mention on the investigated online platforms, and relative receptivity, i.e. how frequently articles in oa journals gain mentions in comparison to articles in subscription-based journals. the results show significant disciplinary and platform specific differences in the oa advantage, with articles in oa journals within for instance veterinary sciences, social and economic geography and psychology receiving more citations and attention on social media platforms, while the opposite was found for articles in oa journals within medicine and health sciences. the results strongly support field- and platform-specific considerations when assessing the influence of journal oa status on altmetrics. the new methodological approaches used in this research will serve future comparative research into oa advantage of scientific articles over time and between countries."
http://orkg.org/orkg/resource/R187768,Impact of cytopathology authors work: Comparative analysis based on Open-access cytopathology publications versus non-Open-access conventional publications,10.25259/cytojournal_32_2021,crossref,"<jats:sec id=""st1"">\n<jats:title>Objectives:</jats:title>\n<jats:p>Open access (OA) is based on a set of principles and a range of practices through which fruits of research are distributed online, free of cost, or other access barriers. According to the 2001 definition, OA publications are without barriers to copy or reuse with free access to readers. Some studies have reported higher rates of citation for OA publications. In this study, we analyzed the citation rates of OA and traditional nonOA (with or without free access) publications for authors publishing in the subspecialty of cytopathology during 2010–2015.</jats:p>\n</jats:sec>\n<jats:sec id=""st2"">\n<jats:title>Material and Methods:</jats:title>\n<jats:p>We observed and compared citation patterns for authors who had published in both OA and traditional non-OA, peer-reviewed, scientific, cytopathology journals. Thirty authors were randomly selected with criteria of publishing a total of at least five cytopathology articles over 2010–2015. Number of citations per article (CPA) (during 2010–2015) for OA publications (in CytoJournal and Journal of Cytology) and non-OA publications (in Diagnostic Cytopathology, Cytopathology, Acta Cytologica, Journal of American of Cytopathology, and Indian Journal of Pathology and Microbiology) was collected and compared statistically using two-tailed Student’s <jats:italic>t</jats:italic>-est. The data were collected manually through science citation analysis sites, mostly Google Scholar.</jats:p>\n</jats:sec>\n<jats:sec id=""st3"">\n<jats:title>Results:</jats:title>\n<jats:p>Thirty authors published 579 cytopathology articles in OA and non-OA journals. Average CPA for OA publications was 26.64. This was 11.35 higher than the average CPA) of non-OA conventional with subscription cytopathology journals (74% increase) and 11.76 higher than the average CPA of conventional cytopathology non-OA journal articles with free access (79% increase). These differences were statistically significantly with <jats:italic>P</jats:italic> &lt; 0.05.</jats:p>\n</jats:sec>\n<jats:sec id=""st4"">\n<jats:title>Conclusion:</jats:title>\n<jats:p>We observed that the cytopathology publications in the OA journal attained a higher rate of CPA than the publications in the traditional non-OA journals in the field of cytopathology during 2010–2015.</jats:p>\n</jats:sec>","\n objectives: \n open access (oa) is based on a set of principles and a range of practices through which fruits of research are distributed online, free of cost, or other access barriers. according to the 2001 definition, oa publications are without barriers to copy or reuse with free access to readers. some studies have reported higher rates of citation for oa publications. in this study, we analyzed the citation rates of oa and traditional nonoa (with or without free access) publications for authors publishing in the subspecialty of cytopathology during 2010–2015. \n \n \n material and methods: \n we observed and compared citation patterns for authors who had published in both oa and traditional non-oa, peer-reviewed, scientific, cytopathology journals. thirty authors were randomly selected with criteria of publishing a total of at least five cytopathology articles over 2010–2015. number of citations per article (cpa) (during 2010–2015) for oa publications (in cytojournal and journal of cytology) and non-oa publications (in diagnostic cytopathology, cytopathology, acta cytologica, journal of american of cytopathology, and indian journal of pathology and microbiology) was collected and compared statistically using two-tailed student’s t -est. the data were collected manually through science citation analysis sites, mostly google scholar. \n \n \n results: \n thirty authors published 579 cytopathology articles in oa and non-oa journals. average cpa for oa publications was 26.64. this was 11.35 higher than the average cpa) of non-oa conventional with subscription cytopathology journals (74% increase) and 11.76 higher than the average cpa of conventional cytopathology non-oa journal articles with free access (79% increase). these differences were statistically significantly with p &lt; 0.05. \n \n \n conclusion: \n we observed that the cytopathology publications in the oa journal attained a higher rate of cpa than the publications in the traditional non-oa journals in the field of cytopathology during 2010–2015. \n"
http://orkg.org/orkg/resource/R187780,The Impact of Open-Access Status on Journal Indices: Respiratory and Pulmonology Journals,10.2174/1573398x15666190214154531,crossref,"<jats:sec>\n<jats:title>Background:</jats:title>\n<jats:p>Open access (OA) publishing is rapidly emerging in almost all disciplines,\nwith variable intensity and effect on the discipline itself. The move toward OA is also observed in the\nfield of respiratory and pulmonology, where both OA data repositories and OA journals are rapidly\nemerging.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title>Objective:</jats:title>\n<jats:p>we aim to study the open-access status of respiratory and pulmonology journals and the\nimpact of the open-access status on journal indices.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title>Methods:</jats:title>\n<jats:p> We collected journal’s data from Scopus Source List on 1st of November 2018. We filtered\nthe list for respiratory and pulmonology journals. Open Access Journals covered by Scopus are\nrecognized as Open Access if the journal is listed in the Directory of Open Access Journals (DOAJ)\nand/or the Directory of Open Access Scholarly Resources (ROAD). For each journal, we used\nseveral metrics to measure its strength, and then we compared these metrics between OA and non-\nOA journals.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title>Results:</jats:title>\n<jats:p>There were 125 respiratory and pulmonology journals, a number that has increased by\n12.6% since 2011. Moreover, the percentage of OA journals has increased from 21.6% to 26.4%\nduring the same period. Non-OA journals have significantly higher scholarly output (p= 0.033), but\nOA journals have significantly higher percentage of citation (p= 0.05).</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title>Conclusion:</jats:title>\n<jats:p>Publishing in OA journals will yield a higher citation percentage compared to non-OA\njournals. Although this should not be the only reason to publish in an OA journal, it is still an\nimportant factor to decide where to publish.</jats:p>\n</jats:sec>","\n background: \n open access (oa) publishing is rapidly emerging in almost all disciplines,\nwith variable intensity and effect on the discipline itself. the move toward oa is also observed in the\nfield of respiratory and pulmonology, where both oa data repositories and oa journals are rapidly\nemerging. \n \n \n objective: \n we aim to study the open-access status of respiratory and pulmonology journals and the\nimpact of the open-access status on journal indices. \n \n \n methods: \n we collected journal’s data from scopus source list on 1st of november 2018. we filtered\nthe list for respiratory and pulmonology journals. open access journals covered by scopus are\nrecognized as open access if the journal is listed in the directory of open access journals (doaj)\nand/or the directory of open access scholarly resources (road). for each journal, we used\nseveral metrics to measure its strength, and then we compared these metrics between oa and non-\noa journals. \n \n \n results: \n there were 125 respiratory and pulmonology journals, a number that has increased by\n12.6% since 2011. moreover, the percentage of oa journals has increased from 21.6% to 26.4%\nduring the same period. non-oa journals have significantly higher scholarly output (p= 0.033), but\noa journals have significantly higher percentage of citation (p= 0.05). \n \n \n conclusion: \n publishing in oa journals will yield a higher citation percentage compared to non-oa\njournals. although this should not be the only reason to publish in an oa journal, it is still an\nimportant factor to decide where to publish. \n"
http://orkg.org/orkg/resource/R192123,"Tracking Urban Expansion Using Random Forests for the Classification of Landsat Imagery (1986–2015) and Predicting Urban/Built-Up Areas for 2025: A Study of the Kumasi Metropolis, Ghana",10.3390/land10010044,crossref,"<jats:p>Kumasi is a nodal city and functions as the administrative and economic capital of the Ashanti region in Ghana. Rapid urbanization has been experienced inducing the transformation of various Land Use Land Cover (LULC) types into urban/built-up areas in Kumasi. This paper aims at tracking spatio-temporal LULC changes utilizing Landsat imagery from 1986, 2013 and 2015 of Kumasi. The unique contribution of this research is its focus on urban expansion analysis and the utilization of Random Forest (RF) Classifier for satellite image classification. Change detection, urban land modelling and urban expansion in the sub-metropolitan zones, buffers, density decay curve and correlation analysis were methodologies adopted for our study. The classifier yielded better accuracy compared to earlier works in Ghana. The evaluation of LULC changes indicated that urban/built-up areas are continually increasing at the expense of agricultural and forestlands. The urban/built-up areas occupied 4622.49 hectares (ha) (23.78%), 13,447.50 ha (69.18%) and 14,004.60 ha (72.05%) in 1986, 2013 and 2015, respectively of the 19,438 ha area of Kumasi. Projection indicated that urban/built-up areas will occupy 15,490 ha (79.70%) in 2025. The urban expansion was statistically significant. The results revealed the importance of spatial modeling for environmental management and city planning.</jats:p>","kumasi is a nodal city and functions as the administrative and economic capital of the ashanti region in ghana. rapid urbanization has been experienced inducing the transformation of various land use land cover (lulc) types into urban/built-up areas in kumasi. this paper aims at tracking spatio-temporal lulc changes utilizing landsat imagery from 1986, 2013 and 2015 of kumasi. the unique contribution of this research is its focus on urban expansion analysis and the utilization of random forest (rf) classifier for satellite image classification. change detection, urban land modelling and urban expansion in the sub-metropolitan zones, buffers, density decay curve and correlation analysis were methodologies adopted for our study. the classifier yielded better accuracy compared to earlier works in ghana. the evaluation of lulc changes indicated that urban/built-up areas are continually increasing at the expense of agricultural and forestlands. the urban/built-up areas occupied 4622.49 hectares (ha) (23.78%), 13,447.50 ha (69.18%) and 14,004.60 ha (72.05%) in 1986, 2013 and 2015, respectively of the 19,438 ha area of kumasi. projection indicated that urban/built-up areas will occupy 15,490 ha (79.70%) in 2025. the urban expansion was statistically significant. the results revealed the importance of spatial modeling for environmental management and city planning."
http://orkg.org/orkg/resource/R191871,"Analysis on Land-Use Change and Its Driving Mechanism in Xilingol, China, during 2000–2020 Using the Google Earth Engine",10.3390/rs13245134,crossref,"<jats:p>Large-scale, long time-series, and high-precision land-use mapping is the basis for assessing the evolution and sustainability of ecosystems in Xilingol, the Inner Mongolia Autonomous Region, China. Based on Google Earth Engine (GEE) and Landsat satellite remote-sensing images, the random forest (RF) classification algorithm was applied to create a yearly land-use/land-cover change (LULC) dataset in Xilingol during the past 20 years (2000–2020) and to examine the spatiotemporal characteristics, dynamic changes, and driving mechanisms of LULC using principal component analysis and multiple linear stepwise regression methods. The main findings are summarized as follows. (1) The RF classification algorithm supported by the GEE platform enables fast and accurate acquisition of the LULC dataset, and the overall accuracy is 0.88 ± 0.01. (2) The ecological condition across Xilingol has improved significantly in the last 20 years (2000–2020), and the area of vegetation (grassland and woodland) has increased. Specifically, the area of high-coverage grass and woodland increases (+13.26%, +1.19%), while the area of water and moderate- and low-coverage grass decreases (−15.96%, −7.23%, and −3.27%). Cropland increases first and then decreases (−34.85%) and is mainly distributed in the southeast. The area of deserted land decreases in the south and increases in the center and north, but the total area still decreases (−13.74%). The built-up land expands rapidly (+108.45%). (3) In addition, our results suggest that regional socioeconomic development factors are the primary causes of changes in built-up land, and climate-related factors are the primary causes of water changes, but the correlations between other land-use types and relevant factors are not significant (cropland and grassland). We conclude that the GEE+RF method is capable of automated, long time-series, and high-accuracy land-use mapping, and further changes in climatic, environmental, and socioeconomic development factors, i.e., climate warming and rotational grazing, might have significant implications on regional land surface morphology and landscape dynamics.</jats:p>","large-scale, long time-series, and high-precision land-use mapping is the basis for assessing the evolution and sustainability of ecosystems in xilingol, the inner mongolia autonomous region, china. based on google earth engine (gee) and landsat satellite remote-sensing images, the random forest (rf) classification algorithm was applied to create a yearly land-use/land-cover change (lulc) dataset in xilingol during the past 20 years (2000–2020) and to examine the spatiotemporal characteristics, dynamic changes, and driving mechanisms of lulc using principal component analysis and multiple linear stepwise regression methods. the main findings are summarized as follows. (1) the rf classification algorithm supported by the gee platform enables fast and accurate acquisition of the lulc dataset, and the overall accuracy is 0.88 ± 0.01. (2) the ecological condition across xilingol has improved significantly in the last 20 years (2000–2020), and the area of vegetation (grassland and woodland) has increased. specifically, the area of high-coverage grass and woodland increases (+13.26%, +1.19%), while the area of water and moderate- and low-coverage grass decreases (−15.96%, −7.23%, and −3.27%). cropland increases first and then decreases (−34.85%) and is mainly distributed in the southeast. the area of deserted land decreases in the south and increases in the center and north, but the total area still decreases (−13.74%). the built-up land expands rapidly (+108.45%). (3) in addition, our results suggest that regional socioeconomic development factors are the primary causes of changes in built-up land, and climate-related factors are the primary causes of water changes, but the correlations between other land-use types and relevant factors are not significant (cropland and grassland). we conclude that the gee+rf method is capable of automated, long time-series, and high-accuracy land-use mapping, and further changes in climatic, environmental, and socioeconomic development factors, i.e., climate warming and rotational grazing, might have significant implications on regional land surface morphology and landscape dynamics."
http://orkg.org/orkg/resource/R188500,The cost-effectiveness of the article-processing-charge-funded model across countries in different scientific blocks: the case of Elsevier's hybrid open access journals,10.47989/irpaper897,crossref,"""<jats:p>Introduction. The present study investigated the cost-effectiveness of article-processing-charge-funded model across the world countries in terms of its citation value proportional to the article processing charges. Method. Using a comparative citation analysis method at the macro level, it explored a sample of articles in forty-seven Elsevier hybrid open access journals that had been following the model since 2007. Analysis. The contributing countries' open access citation advantages were calculated based on the percentage of their open access citation surplus proportional to that of their non-open access articles. Their relative open access citation cost-effectiveness was obtained based on their open access citation counts proportional to the article processing charges, normalised by those of non-open access papers. The countries were categorised into four scientific blocks using Rand's categorization of countries' scientific development. Descriptive and inferential statistics were used to analyse the data in SPSS. Results. The results supported the citation advantage of the article-processing-charge-funded papers, encompassing the majority of the contributing countries in the four scientific development blocks. The articles showed relative cost-effectiveness over the years and for most countries in all the scientific development blocks. Conclusions. Publishing article-processing-charge-funded papers is relatively cost-effective, implying higher visibility and influence in exchange for the money paid.</jats:p>""",""" introduction. the present study investigated the cost-effectiveness of article-processing-charge-funded model across the world countries in terms of its citation value proportional to the article processing charges. method. using a comparative citation analysis method at the macro level, it explored a sample of articles in forty-seven elsevier hybrid open access journals that had been following the model since 2007. analysis. the contributing countries' open access citation advantages were calculated based on the percentage of their open access citation surplus proportional to that of their non-open access articles. their relative open access citation cost-effectiveness was obtained based on their open access citation counts proportional to the article processing charges, normalised by those of non-open access papers. the countries were categorised into four scientific blocks using rand's categorization of countries' scientific development. descriptive and inferential statistics were used to analyse the data in spss. results. the results supported the citation advantage of the article-processing-charge-funded papers, encompassing the majority of the contributing countries in the four scientific development blocks. the articles showed relative cost-effectiveness over the years and for most countries in all the scientific development blocks. conclusions. publishing article-processing-charge-funded papers is relatively cost-effective, implying higher visibility and influence in exchange for the money paid. """
http://orkg.org/orkg/resource/R188503,Scrutinising what Open Access Journals Mean for Global Inequalities,10.1007/s12109-020-09771-9,crossref,"<jats:title>Abstract</jats:title><jats:p>In the current article, we tested our hypothesis by which high-impact journals tend to have higher Article Processing Charges (APCs) by comparing journal IF metrics with the OA publishing fees they charge. Our study engaged with both journals in Science, Technology, Engineering and Mathematics (STEM) fields and the Humanities and Social Sciences (HSS) and included Hybrid, Diamond and No OA journals. The overall findings demonstrate a positive relationship between APCs and journals with high IF for two of the subject areas we examined but not for the third, which could be mediated by the characteristics and market environment of the publishers. We also found significant differences between the analysed research fields in terms of APC policies, as well as differences in the relationship between APCs and the IF across periodicals. The study and analysis conducted reinforces our concerns that Hybrid OA models are likely to perpetuate inequalities in knowledge production.</jats:p>","abstract in the current article, we tested our hypothesis by which high-impact journals tend to have higher article processing charges (apcs) by comparing journal if metrics with the oa publishing fees they charge. our study engaged with both journals in science, technology, engineering and mathematics (stem) fields and the humanities and social sciences (hss) and included hybrid, diamond and no oa journals. the overall findings demonstrate a positive relationship between apcs and journals with high if for two of the subject areas we examined but not for the third, which could be mediated by the characteristics and market environment of the publishers. we also found significant differences between the analysed research fields in terms of apc policies, as well as differences in the relationship between apcs and the if across periodicals. the study and analysis conducted reinforces our concerns that hybrid oa models are likely to perpetuate inequalities in knowledge production."
http://orkg.org/orkg/resource/R188796,Gold Route Open Access Journals in Engineering and Technology: Analysis of Research Impact and Implications for Scholarly Communication,10.29173/istl43,crossref,"<jats:p>Abstract&#x0D;\nThe study examined the research impact of gold open access (OA) journals in engineering and technology. A total of 37 gold route journals that request article processing charges (APC) and 66 other OA journals without APC in the field were found in the Directory of Open Access Journals (DOAJ) and used for the study. The publishers, APC charges and the year each journal was added to DOAJ were identified and included in this study\xa0 An informetric approach was used to extract research impact indicators (citations, paper productivity, cite/paper, cite/journal and h-index) of journals. Findings revealed that the publishers of the APC journals were dominated by commercial publishing companies producing over 60% of the journals. Article processing charges ranged from 7.6 USD to 3471.5 USD while the average article processing charges for APC journals was 727 USD. Gold route open access (APC) journals performed better than open access non-APC OA journals since they produced 51.2% of the papers, 71.8% of the total citations and 65.1% of the total h-index. However, findings showed a weak positive correlation between articles processing charges (APC) of the gold route journals and their research impact; while there is high significant positive correlation between research impact of gold route (APC journals) and non-APC open access journals.</jats:p>","abstract&#x0d;\nthe study examined the research impact of gold open access (oa) journals in engineering and technology. a total of 37 gold route journals that request article processing charges (apc) and 66 other oa journals without apc in the field were found in the directory of open access journals (doaj) and used for the study. the publishers, apc charges and the year each journal was added to doaj were identified and included in this study\xa0 an informetric approach was used to extract research impact indicators (citations, paper productivity, cite/paper, cite/journal and h-index) of journals. findings revealed that the publishers of the apc journals were dominated by commercial publishing companies producing over 60% of the journals. article processing charges ranged from 7.6 usd to 3471.5 usd while the average article processing charges for apc journals was 727 usd. gold route open access (apc) journals performed better than open access non-apc oa journals since they produced 51.2% of the papers, 71.8% of the total citations and 65.1% of the total h-index. however, findings showed a weak positive correlation between articles processing charges (apc) of the gold route journals and their research impact; while there is high significant positive correlation between research impact of gold route (apc journals) and non-apc open access journals."
http://orkg.org/orkg/resource/R191301,Impact of Severe Acute Respiratory Syndrome Coronavirus 2 Viral Load on Risk of Intubation and Mortality Among Hospitalized Patients With Coronavirus Disease 2019,10.1093/cid/ciaa851,crossref,"<jats:title>Abstract</jats:title>\n               <jats:sec>\n                  <jats:title>Background</jats:title>\n                  <jats:p>Patients hospitalized with coronavirus disease 2019 (COVID-19) frequently require mechanical ventilation and have high mortality rates. However, the impact of viral burden on these outcomes is unknown.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Methods</jats:title>\n                  <jats:p>We conducted a retrospective cohort study of patients hospitalized with COVID-19 from 30 March 2020 to 30 April 2020 at 2 hospitals in New York City. Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) viral load was assessed using cycle threshold (Ct) values from a reverse transcription-polymerase chain reaction assay applied to nasopharyngeal swab samples. We compared characteristics and outcomes of patients with high, medium, and low admission viral loads and assessed whether viral load was independently associated with intubation and in-hospital mortality.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Results</jats:title>\n                  <jats:p>We evaluated 678 patients with COVID-19. Higher viral load was associated with increased age, comorbidities, smoking status, and recent chemotherapy. In-hospital mortality was 35.0% (Ct\u2005&amp;lt;25; n\u2005=\u2005220), 17.6% (Ct 25–30; n\u2005=\u2005216), and 6.2% (Ct\u2005&amp;gt;30; n\u2005=\u2005242) with high, medium, and low viral loads, respectively (P &amp;lt; .001). The risk of intubation was also higher in patients with a high viral load (29.1%) compared with those with a medium (20.8%) or low viral load (14.9%; P\u2005&amp;lt;\u2005.001). High viral load was independently associated with mortality (adjusted odds ratio [aOR], 6.05; 95% confidence interval [CI], 2.92–12.52) and intubation (aOR, 2.73; 95% CI, 1.68–4.44).</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Conclusions</jats:title>\n                  <jats:p>Admission SARS-CoV-2 viral load among hospitalized patients with COVID-19 independently correlates with the risk of intubation and in-hospital mortality. Providing this information to clinicians could potentially be used to guide patient care.</jats:p>\n               </jats:sec>","abstract \n \n background \n patients hospitalized with coronavirus disease 2019 (covid-19) frequently require mechanical ventilation and have high mortality rates. however, the impact of viral burden on these outcomes is unknown. \n \n \n methods \n we conducted a retrospective cohort study of patients hospitalized with covid-19 from 30 march 2020 to 30 april 2020 at 2 hospitals in new york city. severe acute respiratory syndrome coronavirus 2 (sars-cov-2) viral load was assessed using cycle threshold (ct) values from a reverse transcription-polymerase chain reaction assay applied to nasopharyngeal swab samples. we compared characteristics and outcomes of patients with high, medium, and low admission viral loads and assessed whether viral load was independently associated with intubation and in-hospital mortality. \n \n \n results \n we evaluated 678 patients with covid-19. higher viral load was associated with increased age, comorbidities, smoking status, and recent chemotherapy. in-hospital mortality was 35.0% (ct\u2005&amp;lt;25; n\u2005=\u2005220), 17.6% (ct 25–30; n\u2005=\u2005216), and 6.2% (ct\u2005&amp;gt;30; n\u2005=\u2005242) with high, medium, and low viral loads, respectively (p &amp;lt; .001). the risk of intubation was also higher in patients with a high viral load (29.1%) compared with those with a medium (20.8%) or low viral load (14.9%; p\u2005&amp;lt;\u2005.001). high viral load was independently associated with mortality (adjusted odds ratio [aor], 6.05; 95% confidence interval [ci], 2.92–12.52) and intubation (aor, 2.73; 95% ci, 1.68–4.44). \n \n \n conclusions \n admission sars-cov-2 viral load among hospitalized patients with covid-19 independently correlates with the risk of intubation and in-hospital mortality. providing this information to clinicians could potentially be used to guide patient care. \n"
http://orkg.org/orkg/resource/R188829,The impact of open access citation and social media on leading top Information Science journals,10.22201/iibi.24488321xe.2018.77.57874,crossref,"<jats:p>El objetivo de esta investigación fue verificar las posibles ventajas que el acceso abierto puede ofrecer al área de la Ciencia de la Información. Para ello, se analizaron los indicadores bibliométricos de citas y los datos de altmetría en 16 revistas científicas híbridas, seleccionadas mediante el Journal Citation Reports y filtradas con base en sus respectivos factores de impacto. La recolección de datos fue realizada en Web of Science, Google Scholar, Altmetric.com y Mendeley. Esta verificación se realizó en dos periodos de tiempo diferentes para examinar si hubo alguna influencia del acceso abierto en el tiempo. Los resultados indican que el acceso abierto puede ofrecer una ventaja en el número de citas y menciones en las redes sociales para el conjunto de artículos de las revistas analizadas aquí, y que esta ventaja es mayor para los casos en que los autores pagaron el cargo por procesamiento del artículo a garantizar la disponibilidad inmediata del artículo en acceso abierto al momento de la publicación. En la conclusión se afirma que no es sólo el acceso abierto el que provoca una mayor cantidad de citas a un artículo, aunque ayuda, sino la calidad del propio artículo.</jats:p>","el objetivo de esta investigación fue verificar las posibles ventajas que el acceso abierto puede ofrecer al área de la ciencia de la información. para ello, se analizaron los indicadores bibliométricos de citas y los datos de altmetría en 16 revistas científicas híbridas, seleccionadas mediante el journal citation reports y filtradas con base en sus respectivos factores de impacto. la recolección de datos fue realizada en web of science, google scholar, altmetric.com y mendeley. esta verificación se realizó en dos periodos de tiempo diferentes para examinar si hubo alguna influencia del acceso abierto en el tiempo. los resultados indican que el acceso abierto puede ofrecer una ventaja en el número de citas y menciones en las redes sociales para el conjunto de artículos de las revistas analizadas aquí, y que esta ventaja es mayor para los casos en que los autores pagaron el cargo por procesamiento del artículo a garantizar la disponibilidad inmediata del artículo en acceso abierto al momento de la publicación. en la conclusión se afirma que no es sólo el acceso abierto el que provoca una mayor cantidad de citas a un artículo, aunque ayuda, sino la calidad del propio artículo."
http://orkg.org/orkg/resource/R191188,Epidemiological Correlates of Polymerase Chain Reaction Cycle Threshold Values in the Detection of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2),10.1093/cid/ciaa1469,crossref,"<jats:title>Abstract</jats:title>\n               <jats:sec>\n                  <jats:title>Background</jats:title>\n                  <jats:p>Detection of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection has principally been performed through the use of real-time reverse-transcription polymerase chain reaction testing. Results of such tests can be reported as cycle threshold (Ct) values, which may provide semi-quantitative or indirect measurements of viral load. Previous reports have examined temporal trends in Ct values over the course of a SARS-CoV-2 infection.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Methods</jats:title>\n                  <jats:p>Using testing data collected during a prospective household transmission investigation of outpatient and mild coronavirus disease 2019 cases, we examined the relationships between Ct values of the viral RNA N1 target and demographic, clinical, and epidemiological characteristics collected through participant interviews and daily symptom diaries.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Results</jats:title>\n                  <jats:p>We found that Ct values are lowest (corresponding to a higher viral RNA concentration) soon after symptom onset and are significantly correlated with the time elapsed since onset (P\u2005&amp;lt;\u2005.001); within 7 days after symptom onset, the median Ct value was 26.5, compared with a median Ct value of 35.0 occurring 21 days after onset. Ct values were significantly lower among participants under 18 years of age (P\u2005=\u2005.01) and those reporting upper respiratory symptoms at the time of sample collection (P\u2005=\u2005.001), and were higher among participants reporting no symptoms (P\u2005=\u2005.05).</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Conclusions</jats:title>\n                  <jats:p>These results emphasize the importance of early testing for SARS-CoV-2 among individuals with symptoms of respiratory illness, and allow cases to be identified and isolated when their viral shedding may be highest.</jats:p>\n               </jats:sec>","abstract \n \n background \n detection of severe acute respiratory syndrome coronavirus 2 (sars-cov-2) infection has principally been performed through the use of real-time reverse-transcription polymerase chain reaction testing. results of such tests can be reported as cycle threshold (ct) values, which may provide semi-quantitative or indirect measurements of viral load. previous reports have examined temporal trends in ct values over the course of a sars-cov-2 infection. \n \n \n methods \n using testing data collected during a prospective household transmission investigation of outpatient and mild coronavirus disease 2019 cases, we examined the relationships between ct values of the viral rna n1 target and demographic, clinical, and epidemiological characteristics collected through participant interviews and daily symptom diaries. \n \n \n results \n we found that ct values are lowest (corresponding to a higher viral rna concentration) soon after symptom onset and are significantly correlated with the time elapsed since onset (p\u2005&amp;lt;\u2005.001); within 7 days after symptom onset, the median ct value was 26.5, compared with a median ct value of 35.0 occurring 21 days after onset. ct values were significantly lower among participants under 18 years of age (p\u2005=\u2005.01) and those reporting upper respiratory symptoms at the time of sample collection (p\u2005=\u2005.001), and were higher among participants reporting no symptoms (p\u2005=\u2005.05). \n \n \n conclusions \n these results emphasize the importance of early testing for sars-cov-2 among individuals with symptoms of respiratory illness, and allow cases to be identified and isolated when their viral shedding may be highest. \n"
http://orkg.org/orkg/resource/R191800,"Analysis of Land Use and Land Cover Using Machine Learning Algorithms on Google Earth Engine for Munneru River Basin, India",10.3390/su132413758,crossref,"<jats:p>The growing human population accelerates alterations in land use and land cover (LULC) over time, putting tremendous strain on natural resources. Monitoring and assessing LULC change over large areas is critical in a variety of fields, including natural resource management and climate change research. LULC change has emerged as a critical concern for policymakers and environmentalists. As the need for the reliable estimation of LULC maps from remote sensing data grows, it is critical to comprehend how different machine learning classifiers perform. The primary goal of the present study was to classify LULC on the Google Earth Engine platform using three different machine learning algorithms—namely, support vector machine (SVM), random forest (RF), and classification and regression trees (CART)—and to compare their performance using accuracy assessments. The LULC of the study area was classified via supervised classification. For improved classification accuracy, NDVI (normalized difference vegetation index) and NDWI (normalized difference water index) indices were also derived and included. For the years 2016, 2018, and 2020, multitemporal Sentinel-2 and Landsat-8 data with spatial resolutions of 10 m and 30 m were used for the LULC classification. ‘Water bodies’, ‘forest’, ‘barren land’, ‘vegetation’, and ‘built-up’ were the major land use classes. The average overall accuracy of SVM, RF, and CART classifiers for Landsat-8 images was 90.88%, 94.85%, and 82.88%, respectively, and 93.8%, 95.8%, and 86.4% for Sentinel-2 images. These results indicate that RF classifiers outperform both SVM and CART classifiers in terms of accuracy.</jats:p>","the growing human population accelerates alterations in land use and land cover (lulc) over time, putting tremendous strain on natural resources. monitoring and assessing lulc change over large areas is critical in a variety of fields, including natural resource management and climate change research. lulc change has emerged as a critical concern for policymakers and environmentalists. as the need for the reliable estimation of lulc maps from remote sensing data grows, it is critical to comprehend how different machine learning classifiers perform. the primary goal of the present study was to classify lulc on the google earth engine platform using three different machine learning algorithms—namely, support vector machine (svm), random forest (rf), and classification and regression trees (cart)—and to compare their performance using accuracy assessments. the lulc of the study area was classified via supervised classification. for improved classification accuracy, ndvi (normalized difference vegetation index) and ndwi (normalized difference water index) indices were also derived and included. for the years 2016, 2018, and 2020, multitemporal sentinel-2 and landsat-8 data with spatial resolutions of 10 m and 30 m were used for the lulc classification. ‘water bodies’, ‘forest’, ‘barren land’, ‘vegetation’, and ‘built-up’ were the major land use classes. the average overall accuracy of svm, rf, and cart classifiers for landsat-8 images was 90.88%, 94.85%, and 82.88%, respectively, and 93.8%, 95.8%, and 86.4% for sentinel-2 images. these results indicate that rf classifiers outperform both svm and cart classifiers in terms of accuracy."
http://orkg.org/orkg/resource/R193018,Comparison of Upper Respiratory Viral Load Distributions in Asymptomatic and Symptomatic Children Diagnosed with SARS-CoV-2 Infection in Pediatric Hospital Testing Programs,10.1128/JCM.02593-20,crossref,<jats:p>The distribution of upper respiratory viral loads (VL) in asymptomatic children infected with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is unknown. We assessed PCR cycle threshold (Ct) values and estimated VL in infected asymptomatic children diagnosed in nine pediatric hospital testing programs. Records for asymptomatic and symptomatic patients with positive clinical SARS-CoV-2 tests were reviewed. Ct values were (i) adjusted by centering each value around the institutional median Ct value from symptomatic children tested with that assay and (ii) converted to estimated VL (numbers of copies per milliliter) using internal or manufacturer data.</jats:p>,the distribution of upper respiratory viral loads (vl) in asymptomatic children infected with severe acute respiratory syndrome coronavirus 2 (sars-cov-2) is unknown. we assessed pcr cycle threshold (ct) values and estimated vl in infected asymptomatic children diagnosed in nine pediatric hospital testing programs. records for asymptomatic and symptomatic patients with positive clinical sars-cov-2 tests were reviewed. ct values were (i) adjusted by centering each value around the institutional median ct value from symptomatic children tested with that assay and (ii) converted to estimated vl (numbers of copies per milliliter) using internal or manufacturer data.
http://orkg.org/orkg/resource/R189153,Urban Land Use and Land Cover Classification Using Multisource Remote Sensing Images and Social Media Data,10.3390/rs11222719,crossref,"<jats:p>Land use and land cover (LULC) are diverse and complex in urban areas. Remotely sensed images are commonly used for land cover classification but hardly identifies urban land use and functional areas because of the semantic gap (i.e., different definitions of similar or identical buildings). Social media data, “marks” left by people using mobile phones, have great potential to overcome this semantic gap. Multisource remote sensing data are also expected to be useful in distinguishing different LULC types. This study examined the capability of combined multisource remote sensing images and social media data in urban LULC classification. Multisource remote sensing images included a Chinese ZiYuan-3 (ZY-3) high-resolution image, a Landsat 8 Operational Land Imager (OLI) multispectral image, and a Sentinel-1A synthetic aperture radar (SAR) image. Social media data consisted of the hourly spatial distribution of WeChat users, which is a ubiquitous messaging and payment platform in China. LULC was classified into 10 types, namely, vegetation, bare land, road, water, urban village, greenhouses, residential, commercial, industrial, and educational buildings. A method that integrates object-based image analysis, decision trees, and random forests was used for LULC classification. The overall accuracy and kappa value attained by the combination of multisource remote sensing images and WeChat data were 87.55% and 0.84, respectively. They further improved to 91.55% and 0.89, respectively, by integrating the textural and spatial features extracted from the ZY-3 image. The ZY-3 high-resolution image was essential for urban LULC classification because it is necessary for the accurate delineation of land parcels. The addition of Landsat 8 OLI, Sentinel-1A SAR, or WeChat data also made an irreplaceable contribution to the classification of different LULC types. The Landsat 8 OLI image helped distinguish between the urban village, residential buildings, commercial buildings, and roads, while the Sentinel-1A SAR data reduced the confusion between commercial buildings, greenhouses, and water. Rendering the spatial and temporal dynamics of population density, the WeChat data improved the classification accuracies of an urban village, greenhouses, and commercial buildings.</jats:p>","land use and land cover (lulc) are diverse and complex in urban areas. remotely sensed images are commonly used for land cover classification but hardly identifies urban land use and functional areas because of the semantic gap (i.e., different definitions of similar or identical buildings). social media data, “marks” left by people using mobile phones, have great potential to overcome this semantic gap. multisource remote sensing data are also expected to be useful in distinguishing different lulc types. this study examined the capability of combined multisource remote sensing images and social media data in urban lulc classification. multisource remote sensing images included a chinese ziyuan-3 (zy-3) high-resolution image, a landsat 8 operational land imager (oli) multispectral image, and a sentinel-1a synthetic aperture radar (sar) image. social media data consisted of the hourly spatial distribution of wechat users, which is a ubiquitous messaging and payment platform in china. lulc was classified into 10 types, namely, vegetation, bare land, road, water, urban village, greenhouses, residential, commercial, industrial, and educational buildings. a method that integrates object-based image analysis, decision trees, and random forests was used for lulc classification. the overall accuracy and kappa value attained by the combination of multisource remote sensing images and wechat data were 87.55% and 0.84, respectively. they further improved to 91.55% and 0.89, respectively, by integrating the textural and spatial features extracted from the zy-3 image. the zy-3 high-resolution image was essential for urban lulc classification because it is necessary for the accurate delineation of land parcels. the addition of landsat 8 oli, sentinel-1a sar, or wechat data also made an irreplaceable contribution to the classification of different lulc types. the landsat 8 oli image helped distinguish between the urban village, residential buildings, commercial buildings, and roads, while the sentinel-1a sar data reduced the confusion between commercial buildings, greenhouses, and water. rendering the spatial and temporal dynamics of population density, the wechat data improved the classification accuracies of an urban village, greenhouses, and commercial buildings."
http://orkg.org/orkg/resource/R189179,Land cover classification combining Sentinel-1 and Landsat 8 imagery driven by Markov random field with amendment reliability factors,10.1186/s13638-020-01713-5,crossref,"<jats:title>Abstract</jats:title><jats:p>Reliability factors in Markov random field (MRF) could be used to improve classification performance for synthetic aperture radar (SAR) and optical images; however, insufficient utilization of reliability factors based on characteristics of different sources leaves more room for classification improvement. To solve this problem, a Markov random field (MRF) with amendment reliability factors classification algorithm (MRF-ARF) is proposed. The ARF is constructed based on the coarse label field of urban region, and different controlling factors are utilized for different sensor data. Then, ARF is involved into the data energy of MRF, to classify the sand, vegetation, farmland, and urban regions, with the gray level co-occurrence matrix textures of Sentinel-1 imagery and the spectral values of the Landsat 8 imagery. In the experiments, Sentinel-1 and Landsat-8 images are used with overall accuracy and Kappa coefficient to evaluate the proposed algorithm with other algorithms. Results show that the overall accuracy of the proposed algorithm has the superiority of about 20% in overall precision and at least 0.2 in Kappa coefficient than the comparison algorithms. Thus, the problem of insufficient utilization of different sensors data could be solved.</jats:p>","abstract reliability factors in markov random field (mrf) could be used to improve classification performance for synthetic aperture radar (sar) and optical images; however, insufficient utilization of reliability factors based on characteristics of different sources leaves more room for classification improvement. to solve this problem, a markov random field (mrf) with amendment reliability factors classification algorithm (mrf-arf) is proposed. the arf is constructed based on the coarse label field of urban region, and different controlling factors are utilized for different sensor data. then, arf is involved into the data energy of mrf, to classify the sand, vegetation, farmland, and urban regions, with the gray level co-occurrence matrix textures of sentinel-1 imagery and the spectral values of the landsat 8 imagery. in the experiments, sentinel-1 and landsat-8 images are used with overall accuracy and kappa coefficient to evaluate the proposed algorithm with other algorithms. results show that the overall accuracy of the proposed algorithm has the superiority of about 20% in overall precision and at least 0.2 in kappa coefficient than the comparison algorithms. thus, the problem of insufficient utilization of different sensors data could be solved."
http://orkg.org/orkg/resource/R189166,Improving Urban Land Cover Classification with Combined Use of Sentinel-2 and Sentinel-1 Imagery,10.3390/ijgi10080533,crossref,"<jats:p>Accurate land cover mapping is important for urban planning and management. Remote sensing data have been widely applied for urban land cover mapping. However, obtaining land cover classification via optical remote sensing data alone is difficult due to spectral confusion. To reduce the confusion between dark impervious surface and water, the Sentinel-1A Synthetic Aperture Rader (SAR) data are synergistically combined with the Sentinel-2B Multispectral Instrument (MSI) data. The novel support vector machine with composite kernels (SVM-CK) approach, which can exploit the spatial information, is proposed to process the combination of Sentinel-2B MSI and Sentinel-1A SAR data. The classification based on the fusion of Sentinel-2B and Sentinel-1A data yields an overall accuracy (OA) of 92.12% with a kappa coefficient (KA) of 0.89, superior to the classification results using Sentinel-2B MSI imagery and Sentinel-1A SAR imagery separately. The results indicate that the inclusion of Sentinel-1A SAR data to Sentinel-2B MSI data can improve the classification performance by reducing the confusion between built-up area and water. This study shows that the land cover classification can be improved by fusing Sentinel-2B and Sentinel-1A imagery.</jats:p>","accurate land cover mapping is important for urban planning and management. remote sensing data have been widely applied for urban land cover mapping. however, obtaining land cover classification via optical remote sensing data alone is difficult due to spectral confusion. to reduce the confusion between dark impervious surface and water, the sentinel-1a synthetic aperture rader (sar) data are synergistically combined with the sentinel-2b multispectral instrument (msi) data. the novel support vector machine with composite kernels (svm-ck) approach, which can exploit the spatial information, is proposed to process the combination of sentinel-2b msi and sentinel-1a sar data. the classification based on the fusion of sentinel-2b and sentinel-1a data yields an overall accuracy (oa) of 92.12% with a kappa coefficient (ka) of 0.89, superior to the classification results using sentinel-2b msi imagery and sentinel-1a sar imagery separately. the results indicate that the inclusion of sentinel-1a sar data to sentinel-2b msi data can improve the classification performance by reducing the confusion between built-up area and water. this study shows that the land cover classification can be improved by fusing sentinel-2b and sentinel-1a imagery."
http://orkg.org/orkg/resource/R192299,Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network,10.1007/s10489-020-01829-7,crossref,"<jats:title>Abstract</jats:title><jats:p>Chest X-ray is the first imaging technique that plays an important role in the diagnosis of COVID-19 disease. Due to the high availability of large-scale annotated image datasets, great success has been achieved using convolutional neural networks (<jats:italic>CNN</jats:italic> s) for image recognition and classification. However, due to the limited availability of annotated medical images, the classification of medical images remains the biggest challenge in medical diagnosis. Thanks to transfer learning, an effective mechanism that can provide a promising solution by transferring knowledge from generic object recognition tasks to domain-specific tasks. In this paper, we validate and a deep <jats:italic>CNN</jats:italic>, called Decompose, Transfer, and Compose (<jats:italic>DeTraC</jats:italic>), for the classification of COVID-19 chest X-ray images. <jats:italic>DeTraC</jats:italic> can deal with any irregularities in the image dataset by investigating its class boundaries using a class decomposition mechanism. The experimental results showed the capability of <jats:italic>DeTraC</jats:italic> in the detection of COVID-19 cases from a comprehensive image dataset collected from several hospitals around the world. High accuracy of 93.1% (with a sensitivity of 100%) was achieved by <jats:italic>DeTraC</jats:italic> in the detection of COVID-19 X-ray images from normal, and severe acute respiratory syndrome cases.</jats:p>","abstract chest x-ray is the first imaging technique that plays an important role in the diagnosis of covid-19 disease. due to the high availability of large-scale annotated image datasets, great success has been achieved using convolutional neural networks ( cnn s) for image recognition and classification. however, due to the limited availability of annotated medical images, the classification of medical images remains the biggest challenge in medical diagnosis. thanks to transfer learning, an effective mechanism that can provide a promising solution by transferring knowledge from generic object recognition tasks to domain-specific tasks. in this paper, we validate and a deep cnn , called decompose, transfer, and compose ( detrac ), for the classification of covid-19 chest x-ray images. detrac can deal with any irregularities in the image dataset by investigating its class boundaries using a class decomposition mechanism. the experimental results showed the capability of detrac in the detection of covid-19 cases from a comprehensive image dataset collected from several hospitals around the world. high accuracy of 93.1% (with a sensitivity of 100%) was achieved by detrac in the detection of covid-19 x-ray images from normal, and severe acute respiratory syndrome cases."
http://orkg.org/orkg/resource/R191251,Neural User Response Generator: Fake News Detection with Collective User Intelligence,10.24963/ijcai.2018/533,crossref,"<jats:p>Fake news on social media is a major challenge and studies have shown that fake news can propagate exponentially quickly in early stages. Therefore, we focus on early detection of fake news, and consider that only news article text is available at the time of detection, since additional information such as user responses and propagation patterns can be obtained only after the news spreads. However, we find historical user responses to previous articles are available and can be treated as soft semantic labels, that enrich the binary label of an article, by providing insights into why the article must be labeled as fake. We propose a novel Two-Level Convolutional Neural Network with User Response Generator (TCNN-URG) where TCNN captures semantic information from article text by representing it at the sentence and word level, and URG learns a generative model of user response to article text from historical user responses which it can use to generate responses to new articles in order to assist fake news detection. We conduct experiments on one available dataset and a larger dataset collected by ourselves. Experimental results show that TCNN-URG outperforms the baselines based on prior approaches that detect fake news from article text alone.</jats:p>","fake news on social media is a major challenge and studies have shown that fake news can propagate exponentially quickly in early stages. therefore, we focus on early detection of fake news, and consider that only news article text is available at the time of detection, since additional information such as user responses and propagation patterns can be obtained only after the news spreads. however, we find historical user responses to previous articles are available and can be treated as soft semantic labels, that enrich the binary label of an article, by providing insights into why the article must be labeled as fake. we propose a novel two-level convolutional neural network with user response generator (tcnn-urg) where tcnn captures semantic information from article text by representing it at the sentence and word level, and urg learns a generative model of user response to article text from historical user responses which it can use to generate responses to new articles in order to assist fake news detection. we conduct experiments on one available dataset and a larger dataset collected by ourselves. experimental results show that tcnn-urg outperforms the baselines based on prior approaches that detect fake news from article text alone."
http://orkg.org/orkg/resource/R191339,SARS-CoV-2 viral load predicts COVID-19 mortality,10.1016/s2213-2600(20)30354-4,crossref,"<jats:title>Abstract</jats:title><jats:p>The need for reliable and widely available SARS-CoV-2 testing is well recognized, but it will be equally necessary to develop quantitative methods that determine viral load in order to guide patient triage and medical decision making. We are the first to report that SARS-CoV-2 viral load at the time of presentation is an independent predictor of COVID-19 mortality in a large patient cohort (n=1,145). Viral loads should be used to identify higher-risk patients that may require more aggressive care and should be included as a key biomarker in the development of predictive algorithms.</jats:p>","abstract the need for reliable and widely available sars-cov-2 testing is well recognized, but it will be equally necessary to develop quantitative methods that determine viral load in order to guide patient triage and medical decision making. we are the first to report that sars-cov-2 viral load at the time of presentation is an independent predictor of covid-19 mortality in a large patient cohort (n=1,145). viral loads should be used to identify higher-risk patients that may require more aggressive care and should be included as a key biomarker in the development of predictive algorithms."
http://orkg.org/orkg/resource/R191344,Unsupervised Fake News Detection on Social Media: A Generative Approach,10.1609/aaai.v33i01.33015644,crossref,"<jats:p>Social media has become one of the main channels for people to access and consume news, due to the rapidness and low cost of news dissemination on it. However, such properties of social media also make it a hotbed of fake news dissemination, bringing negative impacts on both individuals and society. Therefore, detecting fake news has become a crucial problem attracting tremendous research effort. Most existing methods of fake news detection are supervised, which require an extensive amount of time and labor to build a reliably annotated dataset. In search of an alternative, in this paper, we investigate if we could detect fake news in an unsupervised manner. We treat truths of news and users’ credibility as latent random variables, and exploit users’ engagements on social media to identify their opinions towards the authenticity of news. We leverage a Bayesian network model to capture the conditional dependencies among the truths of news, the users’ opinions, and the users’ credibility. To solve the inference problem, we propose an efficient collapsed Gibbs sampling approach to infer the truths of news and the users’ credibility without any labelled data. Experiment results on two datasets show that the proposed method significantly outperforms the compared unsupervised methods.</jats:p>","social media has become one of the main channels for people to access and consume news, due to the rapidness and low cost of news dissemination on it. however, such properties of social media also make it a hotbed of fake news dissemination, bringing negative impacts on both individuals and society. therefore, detecting fake news has become a crucial problem attracting tremendous research effort. most existing methods of fake news detection are supervised, which require an extensive amount of time and labor to build a reliably annotated dataset. in search of an alternative, in this paper, we investigate if we could detect fake news in an unsupervised manner. we treat truths of news and users’ credibility as latent random variables, and exploit users’ engagements on social media to identify their opinions towards the authenticity of news. we leverage a bayesian network model to capture the conditional dependencies among the truths of news, the users’ opinions, and the users’ credibility. to solve the inference problem, we propose an efficient collapsed gibbs sampling approach to infer the truths of news and the users’ credibility without any labelled data. experiment results on two datasets show that the proposed method significantly outperforms the compared unsupervised methods."
http://orkg.org/orkg/resource/R192222,"Monitoring and Modeling the Patterns and Trends of Urban Growth Using Urban Sprawl Matrix and CA-Markov Model: A Case Study of Karachi, Pakistan",10.3390/land10070700,crossref,"<jats:p>Understanding the spatial growth of cities is crucial for proactive planning and sustainable urbanization. The largest and most densely inhabited megapolis of Pakistan, Karachi, has experienced massive spatial growth not only in the core areas of the city, but also in the city’s suburbs and outskirts over the past decades. In this study, the land use/land cover (LULC) in Karachi was classified using Landsat data and the random forest algorithm from the Google Earth Engine cloud platform for the years 1990, 2000, 2010, and 2020. Land use/land cover classification maps as well as an urban sprawl matrix technique were used to analyze the geographical patterns and trends of urban sprawl. Six urban classes, namely, the primary urban core, secondary urban core, sub-urban fringe, scatter settlement, urban open space, and non-urban area, were determined for the exploration of urban landscape changes. Future scenarios of LULC for 2030 were predicted using a CA–Markov model. The study found that the built-up area had expanded in a considerably unpredictable manner, primarily at the expense of agricultural land. The increase in mangroves and grassland and shrub land proved the effectiveness of afforestation programs in improving vegetation coverage in the study area. The investigation of urban landscape alteration revealed that the primary urban core expanded from the core districts, namely, the Central, South, and East districts, and a new urban secondary core emerged in Malir in 2020. The CA–Markov model showed that the total urban built-up area could potentially increase from 584.78 km2 in 2020 to 652.59 km2 in 2030. The integrated method combining remote sensing, GIS, and an urban sprawl matrix has proven invaluable for the investigation of urban sprawl in a rapidly growing city.</jats:p>","understanding the spatial growth of cities is crucial for proactive planning and sustainable urbanization. the largest and most densely inhabited megapolis of pakistan, karachi, has experienced massive spatial growth not only in the core areas of the city, but also in the city’s suburbs and outskirts over the past decades. in this study, the land use/land cover (lulc) in karachi was classified using landsat data and the random forest algorithm from the google earth engine cloud platform for the years 1990, 2000, 2010, and 2020. land use/land cover classification maps as well as an urban sprawl matrix technique were used to analyze the geographical patterns and trends of urban sprawl. six urban classes, namely, the primary urban core, secondary urban core, sub-urban fringe, scatter settlement, urban open space, and non-urban area, were determined for the exploration of urban landscape changes. future scenarios of lulc for 2030 were predicted using a ca–markov model. the study found that the built-up area had expanded in a considerably unpredictable manner, primarily at the expense of agricultural land. the increase in mangroves and grassland and shrub land proved the effectiveness of afforestation programs in improving vegetation coverage in the study area. the investigation of urban landscape alteration revealed that the primary urban core expanded from the core districts, namely, the central, south, and east districts, and a new urban secondary core emerged in malir in 2020. the ca–markov model showed that the total urban built-up area could potentially increase from 584.78 km2 in 2020 to 652.59 km2 in 2030. the integrated method combining remote sensing, gis, and an urban sprawl matrix has proven invaluable for the investigation of urban sprawl in a rapidly growing city."
http://orkg.org/orkg/resource/R189394,A Hierarchical Multi-Task Approach for Learning Embeddings from Semantic Tasks,10.1609/aaai.v33i01.33016949,crossref,"<jats:p>Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.</jats:p>","much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various natural language processing (nlp) down-stream applications. however, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. in this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. the model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. this model achieves state-of-the-art results on a number of tasks, namely named entity recognition, entity mention detection and relation extraction without hand-engineered features or external nlp tools like syntactic parsers. the hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. we show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information."
http://orkg.org/orkg/resource/R189450,FNG-IE: an improved graph-based method for keyword extraction from scholarly big-data,10.7717/peerj-cs.389,crossref,"<jats:p>Keyword extraction is essential in determining influenced keywords from huge documents as the research repositories are becoming massive in volume day by day. The research community is drowning in data and starving for information. The keywords are the words that describe the theme of the whole document in a precise way by consisting of just a few words. Furthermore, many state-of-the-art approaches are available for keyword extraction from a huge collection of documents and are classified into three types, the statistical approaches, machine learning, and graph-based methods. The machine learning approaches require a large training dataset that needs to be developed manually by domain experts, which sometimes is difficult to produce while determining influenced keywords. However, this research focused on enhancing state-of-the-art graph-based methods to extract keywords when the training dataset is unavailable. This research first converted the handcrafted dataset, collected from impact factor journals into <jats:italic>n</jats:italic>-grams combinations, ranging from unigram to pentagram and also enhanced traditional graph-based approaches. The experiment was conducted on a handcrafted dataset, and all methods were applied on it. Domain experts performed the user study to evaluate the results. The results were observed from every method and were evaluated with the user study using precision, recall and f-measure as evaluation matrices. The results showed that the proposed method (FNG-IE) performed well and scored near the machine learning approaches score.</jats:p>","keyword extraction is essential in determining influenced keywords from huge documents as the research repositories are becoming massive in volume day by day. the research community is drowning in data and starving for information. the keywords are the words that describe the theme of the whole document in a precise way by consisting of just a few words. furthermore, many state-of-the-art approaches are available for keyword extraction from a huge collection of documents and are classified into three types, the statistical approaches, machine learning, and graph-based methods. the machine learning approaches require a large training dataset that needs to be developed manually by domain experts, which sometimes is difficult to produce while determining influenced keywords. however, this research focused on enhancing state-of-the-art graph-based methods to extract keywords when the training dataset is unavailable. this research first converted the handcrafted dataset, collected from impact factor journals into n -grams combinations, ranging from unigram to pentagram and also enhanced traditional graph-based approaches. the experiment was conducted on a handcrafted dataset, and all methods were applied on it. domain experts performed the user study to evaluate the results. the results were observed from every method and were evaluated with the user study using precision, recall and f-measure as evaluation matrices. the results showed that the proposed method (fng-ie) performed well and scored near the machine learning approaches score."
http://orkg.org/orkg/resource/R189477,"DESIGN ANTHROPOLOGY, ALGORITHMIC BIAS, BEHAVIORAL CAPITAL, AND THE CREATOR ECONOMY",10.17730/0888-4552.44.2.33,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>As algorithms become increasingly responsible for discovering information, how we choose to design them will have a significant impact on our collective lived experience. One example is how algorithmic bias affects the estimated 50 million people that make up the creator economy. This group of independent creators is financially dependent on recommender systems to suggest their content. Currently, most recommender system designs produce rich-get-richer dynamics, resulting in structural inequalities that favor some over others. This article details a design anthropology approach for creating a new model of sociality and business that rewards behavioral capital.</jats:p>","abstract \n as algorithms become increasingly responsible for discovering information, how we choose to design them will have a significant impact on our collective lived experience. one example is how algorithmic bias affects the estimated 50 million people that make up the creator economy. this group of independent creators is financially dependent on recommender systems to suggest their content. currently, most recommender system designs produce rich-get-richer dynamics, resulting in structural inequalities that favor some over others. this article details a design anthropology approach for creating a new model of sociality and business that rewards behavioral capital."
http://orkg.org/orkg/resource/R192303,COVID-19 detection from chest X-Ray images using Deep Learning and Convolutional Neural Networks,10.1101/2020.05.22.20110817,crossref,"<jats:title>A<jats:sc>bstract</jats:sc></jats:title><jats:p>The COVID-19 pandemic in 2020 has highlighted the need to pull all available resources towards the mitigation of the devastating effects of such “Black Swan” events. Towards that end, we investigated the option to employ technology in order to assist the diagnosis of patients infected by the virus. As such, several state-of-the-art pre-trained convolutional neural networks were evaluated as of their ability to detect infected patients from chest X-Ray images. A dataset was created as a mix of publicly available X-ray images from patients with confirmed COVID-19 disease, common bacterial pneumonia and healthy individuals. To mitigate the small number of samples, we employed transfer learning, which transfers knowledge extracted by pre-trained models to the model to be trained. The experimental results demonstrate that the classification performance can reach an accuracy of 95% for the best two models.</jats:p>","a bstract the covid-19 pandemic in 2020 has highlighted the need to pull all available resources towards the mitigation of the devastating effects of such “black swan” events. towards that end, we investigated the option to employ technology in order to assist the diagnosis of patients infected by the virus. as such, several state-of-the-art pre-trained convolutional neural networks were evaluated as of their ability to detect infected patients from chest x-ray images. a dataset was created as a mix of publicly available x-ray images from patients with confirmed covid-19 disease, common bacterial pneumonia and healthy individuals. to mitigate the small number of samples, we employed transfer learning, which transfers knowledge extracted by pre-trained models to the model to be trained. the experimental results demonstrate that the classification performance can reach an accuracy of 95% for the best two models."
http://orkg.org/orkg/resource/R193267,Malaria Diagnosis Using a Lightweight Deep Convolutional Neural Network,10.1155/2022/4176982,crossref,"<jats:p>The applications of AI in the healthcare sector are increasing day by day. The application of convolutional neural network (CNN) and mask-region-based CNN (Mask-RCCN) to the medical domain has really revolutionized medical image analysis. CNNs have been prominently used for identification, classification, and feature extraction tasks, and they have delivered a great performance at these tasks. In our study, we propose a lightweight CNN, which requires less time to train, for identifying malaria parasitic red blood cells and distinguishing them from healthy red blood cells. To compare the accuracy of our model, we used transfer learning on two models, namely, the VGG-19 and the Inception v3. We train our model in three different configurations depending on the proportion of data being fed to the model for training. For all three configurations, our proposed model is able to achieve an accuracy of around 96%, which is higher than both the other models that we trained for the same three configurations. It shows that our model is able to perform better along with low computational requirements. Therefore, it can be used more efficiently and can be easily deployed for detecting malaria cells.</jats:p>","the applications of ai in the healthcare sector are increasing day by day. the application of convolutional neural network (cnn) and mask-region-based cnn (mask-rccn) to the medical domain has really revolutionized medical image analysis. cnns have been prominently used for identification, classification, and feature extraction tasks, and they have delivered a great performance at these tasks. in our study, we propose a lightweight cnn, which requires less time to train, for identifying malaria parasitic red blood cells and distinguishing them from healthy red blood cells. to compare the accuracy of our model, we used transfer learning on two models, namely, the vgg-19 and the inception v3. we train our model in three different configurations depending on the proportion of data being fed to the model for training. for all three configurations, our proposed model is able to achieve an accuracy of around 96%, which is higher than both the other models that we trained for the same three configurations. it shows that our model is able to perform better along with low computational requirements. therefore, it can be used more efficiently and can be easily deployed for detecting malaria cells."
http://orkg.org/orkg/resource/R189587,Effects on the Mechanical Properties of Nacre-Like Bio-Hybrid Membranes with Inter-Penetrating Petal Structure Based on Magadiite,10.3390/ma12010173,crossref,"<jats:p>Rigid biological systems are increasingly becoming a source of inspiration for the fabrication of the advanced functional materials due to their diverse hierarchical structures and remarkable engineering properties. As a bionic biomaterial with a clear layered structure, excellent mechanical properties, and interesting rainbow colors, nacre has become one of the most attractive models for novel artificial materials design. In this research paper, the tough and strong nacre-like bio-hybrid membranes with an interpenetrating petals structure were fabricated from chitosan (CS) and magadiite (MAG) clay nanosheets through the gel-casting self-assembling method. The analyses from X-ray diffraction (XRD), scanning electron microscope (SEM), and observations of water droplets on membranes indicated that the nacre-like hybrid membranes had a layered compact structure. Fourier transforms infrared spectroscopy (FTIR) analyses suggested that the CS molecular chains formed chemical bonds and hydrogen bonds with MAG layers. The inter-penetrating petal layered structure had a good effect on the mechanical properties of a nacre-like bio-hybrid membranes and the tensile strength of the hybrid membranes could reach at 78.6 MPa. However, the transmission analyses of the results showed that the hybrid membranes still had a certain visible light transmittance. Finally, the hybrid membranes possessed an intriguing efficient fire-shielding property during exposure to the flame of alcohol burner. Consequently, the great biocompatibility and excellent mechanical properties of the bio-hybrid membranes with the special interpenetrating petals structure provides a great opportunity for these composites to be widely applied in biomaterial research.</jats:p>","rigid biological systems are increasingly becoming a source of inspiration for the fabrication of the advanced functional materials due to their diverse hierarchical structures and remarkable engineering properties. as a bionic biomaterial with a clear layered structure, excellent mechanical properties, and interesting rainbow colors, nacre has become one of the most attractive models for novel artificial materials design. in this research paper, the tough and strong nacre-like bio-hybrid membranes with an interpenetrating petals structure were fabricated from chitosan (cs) and magadiite (mag) clay nanosheets through the gel-casting self-assembling method. the analyses from x-ray diffraction (xrd), scanning electron microscope (sem), and observations of water droplets on membranes indicated that the nacre-like hybrid membranes had a layered compact structure. fourier transforms infrared spectroscopy (ftir) analyses suggested that the cs molecular chains formed chemical bonds and hydrogen bonds with mag layers. the inter-penetrating petal layered structure had a good effect on the mechanical properties of a nacre-like bio-hybrid membranes and the tensile strength of the hybrid membranes could reach at 78.6 mpa. however, the transmission analyses of the results showed that the hybrid membranes still had a certain visible light transmittance. finally, the hybrid membranes possessed an intriguing efficient fire-shielding property during exposure to the flame of alcohol burner. consequently, the great biocompatibility and excellent mechanical properties of the bio-hybrid membranes with the special interpenetrating petals structure provides a great opportunity for these composites to be widely applied in biomaterial research."
http://orkg.org/orkg/resource/R191870,Monitoring Land Use/Cover Change Using Remotely Sensed Data in Guangzhou of China,10.3390/su13052944,crossref,"<jats:p>Land use/cover change (LUCC) has a crucial influence on ecosystem function, environmental change and decision support. Rapid and precise monitoring of land use/cover change information is essential for utilization and management of land resources. The objectives of this study were to monitor land use/cover change of Guangzhou of China from 1986 to 2018 using remotely sensed data, and analyze the correlation between artificial surface expansion and the gross domestic product (GDP) growth. Supervised classification was performed using Random Forest classifier, and the overall accuracy (OA) ranged from 86.42% to 96.58% and kappa coefficient (K) ranged from 0.8079 to 0.9499. The results show that the built-up area of Guangzhou of China from 1986 to 2018 continued to increase. However, the vegetation area continued to decrease during 32 years. The built-up area increased by 1315.56 km2 (increased by 439.34%) with an average growth of 41.11 km2/year. The vegetation area reduced by 1290.78 km2 (reduced by 19.99%) with an average reduction of 40.34 km2/year. Research has shown that the reduced vegetation area was mainly converted into built-up area. The area of water bodies and bare lands was relatively stable and had a little change. The results indicate that the GDP had a strong positive correlation with built-up area (R2 = 0.98). However, there is a strong negative correlation between the GDP and vegetation area (R2 = 0.97) in Guangzhou City, China. As a consequence, the increase of built-up area was at the cost of the reduction of vegetation area.</jats:p>","land use/cover change (lucc) has a crucial influence on ecosystem function, environmental change and decision support. rapid and precise monitoring of land use/cover change information is essential for utilization and management of land resources. the objectives of this study were to monitor land use/cover change of guangzhou of china from 1986 to 2018 using remotely sensed data, and analyze the correlation between artificial surface expansion and the gross domestic product (gdp) growth. supervised classification was performed using random forest classifier, and the overall accuracy (oa) ranged from 86.42% to 96.58% and kappa coefficient (k) ranged from 0.8079 to 0.9499. the results show that the built-up area of guangzhou of china from 1986 to 2018 continued to increase. however, the vegetation area continued to decrease during 32 years. the built-up area increased by 1315.56 km2 (increased by 439.34%) with an average growth of 41.11 km2/year. the vegetation area reduced by 1290.78 km2 (reduced by 19.99%) with an average reduction of 40.34 km2/year. research has shown that the reduced vegetation area was mainly converted into built-up area. the area of water bodies and bare lands was relatively stable and had a little change. the results indicate that the gdp had a strong positive correlation with built-up area (r2 = 0.98). however, there is a strong negative correlation between the gdp and vegetation area (r2 = 0.97) in guangzhou city, china. as a consequence, the increase of built-up area was at the cost of the reduction of vegetation area."
http://orkg.org/orkg/resource/R191315,Quantitative Detection and Viral Load Analysis of SARS-CoV-2 in Infected Patients,10.1093/cid/ciaa345,crossref,"<jats:title>Abstract</jats:title>\n               <jats:sec>\n                  <jats:title>Background</jats:title>\n                  <jats:p>Coronavirus disease 2019 (COVID-19) has become a public health emergency. The widely used reverse transcription–polymerase chain reaction (RT-PCR) method has limitations for clinical diagnosis and treatment.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Methods</jats:title>\n                  <jats:p>A total of 323 samples from 76 COVID-19–confirmed patients were analyzed by droplet digital PCR (ddPCR) and RT-PCR based 2 target genes (ORF1ab and N). Nasal swabs, throat swabs, sputum, blood, and urine were collected. Clinical and imaging data were obtained for clinical staging.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Results</jats:title>\n                  <jats:p>In 95 samples that tested positive by both methods, the cycle threshold (Ct) of RT-PCR was highly correlated with the copy number of ddPCR (ORF1ab gene, R2\u2005=\u20050.83; N gene, R2\u2005=\u20050.87). Four (4/161) negative and 41 (41/67) single-gene positive samples tested by RT-PCR were positive according to ddPCR with viral loads ranging from 11.1 to 123.2 copies/test. The viral load of respiratory samples was then compared and the average viral load in sputum (17\u2005429\u2005±\u20056920 copies/test) was found to be significantly higher than in throat swabs (2552\u2005±\u20051965 copies/test, P\u2005&amp;lt;\u2005.001) and nasal swabs (651\u2005±\u2005501 copies/test, P\u2005&amp;lt;\u2005.001). Furthermore, the viral loads in the early and progressive stages were significantly higher than that in the recovery stage (46\u2005800\u2005±\u200517\u2005272 vs 1252\u2005±\u20051027, P\u2005&amp;lt;\u2005.001) analyzed by sputum samples.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Conclusions</jats:title>\n                  <jats:p>Quantitative monitoring of viral load in lower respiratory tract samples helps to evaluate disease progression, especially in cases of low viral load.</jats:p>\n               </jats:sec>","abstract \n \n background \n coronavirus disease 2019 (covid-19) has become a public health emergency. the widely used reverse transcription–polymerase chain reaction (rt-pcr) method has limitations for clinical diagnosis and treatment. \n \n \n methods \n a total of 323 samples from 76 covid-19–confirmed patients were analyzed by droplet digital pcr (ddpcr) and rt-pcr based 2 target genes (orf1ab and n). nasal swabs, throat swabs, sputum, blood, and urine were collected. clinical and imaging data were obtained for clinical staging. \n \n \n results \n in 95 samples that tested positive by both methods, the cycle threshold (ct) of rt-pcr was highly correlated with the copy number of ddpcr (orf1ab gene, r2\u2005=\u20050.83; n gene, r2\u2005=\u20050.87). four (4/161) negative and 41 (41/67) single-gene positive samples tested by rt-pcr were positive according to ddpcr with viral loads ranging from 11.1 to 123.2 copies/test. the viral load of respiratory samples was then compared and the average viral load in sputum (17\u2005429\u2005±\u20056920 copies/test) was found to be significantly higher than in throat swabs (2552\u2005±\u20051965 copies/test, p\u2005&amp;lt;\u2005.001) and nasal swabs (651\u2005±\u2005501 copies/test, p\u2005&amp;lt;\u2005.001). furthermore, the viral loads in the early and progressive stages were significantly higher than that in the recovery stage (46\u2005800\u2005±\u200517\u2005272 vs 1252\u2005±\u20051027, p\u2005&amp;lt;\u2005.001) analyzed by sputum samples. \n \n \n conclusions \n quantitative monitoring of viral load in lower respiratory tract samples helps to evaluate disease progression, especially in cases of low viral load. \n"
http://orkg.org/orkg/resource/R191038,Digital Methods for Hashtag Engagement Research,10.1177/2056305120940697,crossref,"<jats:p> This article seeks to contribute to the field of digital research by critically accounting for the relationship between hashtags and their forms of grammatization—the platform techno-materialization process of online activity. We approach hashtags as sociotechnical formations that serve social media research not only as criteria in corpus selection but also displaying the complexity of the online engagement and its entanglement with the technicity of web platforms. Therefore, the study of hashtag engagement requires a grasping of the functioning of the platform itself (technicity) along with the platform grammatization. In this respect, we propose the three-layered (3L) perspective for addressing hashtag engagement. The first contemplates potential differences between high-visibility and ordinary hashtag usage culture, its related actors, and content. The second focuses on hashtagging activity and the repurposing of how hashtags can be differently embedded into social media databases. The last layer looks particularly into the images and texts to which hashtags are brought to relation. To operationalize the 3L framework, we draw on the case of the “impeachment-cum-coup” of Brazilian president Dilma Rousseff. When cross-read, the three layers add value to one another, providing also difference visions of the high-visibility and ordinary groups. </jats:p>","this article seeks to contribute to the field of digital research by critically accounting for the relationship between hashtags and their forms of grammatization—the platform techno-materialization process of online activity. we approach hashtags as sociotechnical formations that serve social media research not only as criteria in corpus selection but also displaying the complexity of the online engagement and its entanglement with the technicity of web platforms. therefore, the study of hashtag engagement requires a grasping of the functioning of the platform itself (technicity) along with the platform grammatization. in this respect, we propose the three-layered (3l) perspective for addressing hashtag engagement. the first contemplates potential differences between high-visibility and ordinary hashtag usage culture, its related actors, and content. the second focuses on hashtagging activity and the repurposing of how hashtags can be differently embedded into social media databases. the last layer looks particularly into the images and texts to which hashtags are brought to relation. to operationalize the 3l framework, we draw on the case of the “impeachment-cum-coup” of brazilian president dilma rousseff. when cross-read, the three layers add value to one another, providing also difference visions of the high-visibility and ordinary groups."
http://orkg.org/orkg/resource/R189856,Gastrointestinal symptoms of 95 cases with SARS-CoV-2 infection,10.1136/gutjnl-2020-321013,crossref,"<jats:sec><jats:title>Objective</jats:title><jats:p>To study the GI symptoms in severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infected patients.</jats:p></jats:sec><jats:sec><jats:title>Design</jats:title><jats:p>We analysed epidemiological, demographic, clinical and laboratory data of 95 cases with SARS-CoV-2 caused coronavirus disease 2019. Real-time reverse transcriptase PCR was used to detect the presence of SARS-CoV-2 in faeces and GI tissues.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Among the 95 patients, 58 cases exhibited GI symptoms of which 11 (11.6%) occurred on admission and 47 (49.5%) developed during hospitalisation. Diarrhoea (24.2%), anorexia (17.9%) and nausea (17.9%) were the main symptoms with five (5.3%), five (5.3%) and three (3.2%) cases occurred on the illness onset, respectively. A substantial proportion of patients developed diarrhoea during hospitalisation, potentially aggravated by various drugs including antibiotics. Faecal samples of 65 hospitalised patients were tested for the presence of SARS-CoV-2, including 42 with and 23 without GI symptoms, of which 22 (52.4%) and 9 (39.1%) were positive, respectively. Six patients with GI symptoms were subjected to endoscopy, revealing oesophageal bleeding with erosions and ulcers in one severe patient. SARS-CoV-2 RNA was detected in oesophagus, stomach, duodenum and rectum specimens for both two severe patients. In contrast, only duodenum was positive in one of the four non-severe patients.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>GI tract may be a potential transmission route and target organ of SARS-CoV-2.</jats:p></jats:sec>","objective to study the gi symptoms in severe acute respiratory syndrome coronavirus 2 (sars-cov-2) infected patients. design we analysed epidemiological, demographic, clinical and laboratory data of 95 cases with sars-cov-2 caused coronavirus disease 2019. real-time reverse transcriptase pcr was used to detect the presence of sars-cov-2 in faeces and gi tissues. results among the 95 patients, 58 cases exhibited gi symptoms of which 11 (11.6%) occurred on admission and 47 (49.5%) developed during hospitalisation. diarrhoea (24.2%), anorexia (17.9%) and nausea (17.9%) were the main symptoms with five (5.3%), five (5.3%) and three (3.2%) cases occurred on the illness onset, respectively. a substantial proportion of patients developed diarrhoea during hospitalisation, potentially aggravated by various drugs including antibiotics. faecal samples of 65 hospitalised patients were tested for the presence of sars-cov-2, including 42 with and 23 without gi symptoms, of which 22 (52.4%) and 9 (39.1%) were positive, respectively. six patients with gi symptoms were subjected to endoscopy, revealing oesophageal bleeding with erosions and ulcers in one severe patient. sars-cov-2 rna was detected in oesophagus, stomach, duodenum and rectum specimens for both two severe patients. in contrast, only duodenum was positive in one of the four non-severe patients. conclusions gi tract may be a potential transmission route and target organ of sars-cov-2."
http://orkg.org/orkg/resource/R189870,"Epidemiological, clinical and virological characteristics of 74 cases of coronavirus-infected disease 2019 (COVID-19) with gastrointestinal symptoms",10.1136/gutjnl-2020-320926,crossref,"<jats:sec><jats:title>Objective</jats:title><jats:p>The SARS-CoV-2-infected disease (COVID-19) outbreak is a major threat to human beings. Previous studies mainly focused on Wuhan and typical symptoms. We analysed 74 confirmed COVID-19 cases with GI symptoms in the Zhejiang province to determine epidemiological, clinical and virological characteristics.</jats:p></jats:sec><jats:sec><jats:title>Design</jats:title><jats:p>COVID-19 hospital patients were admitted in the Zhejiang province from 17 January 2020 to 8 February 2020. Epidemiological, demographic, clinical, laboratory, management and outcome data of patients with GI symptoms were analysed using multivariate analysis for risk of severe/critical type. Bioinformatics were used to analyse features of SARS-CoV-2 from Zhejiang province.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Among enrolled 651 patients, 74 (11.4%) presented with at least one GI symptom (nausea, vomiting or diarrhoea), average age of 46.14 years, 4-day incubation period and 10.8% had pre-existing liver disease. Of patients with COVID-19 with GI symptoms, 17 (22.97%) and 23 (31.08%) had severe/critical types and family clustering, respectively, significantly higher than those without GI symptoms, 47 (8.14%) and 118 (20.45%). Of patients with COVID-19 with GI symptoms, 29 (39.19%), 23 (31.08%), 8 (10.81%) and 16 (21.62%) had significantly higher rates of fever &gt;38.5°C, fatigue, shortness of breath and headache, respectively. Low-dose glucocorticoids and antibiotics were administered to 14.86% and 41.89% of patients, respectively. Sputum production and increased lactate dehydrogenase/glucose levels were risk factors for severe/critical type. Bioinformatics showed sequence mutation of SARS-CoV-2 with m<jats:sup>6</jats:sup>A methylation and changed binding capacity with ACE2.</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>We report COVID-19 cases with GI symptoms with novel features outside Wuhan. Attention to patients with COVID-19 with non-classic symptoms should increase to protect health providers.</jats:p></jats:sec>","objective the sars-cov-2-infected disease (covid-19) outbreak is a major threat to human beings. previous studies mainly focused on wuhan and typical symptoms. we analysed 74 confirmed covid-19 cases with gi symptoms in the zhejiang province to determine epidemiological, clinical and virological characteristics. design covid-19 hospital patients were admitted in the zhejiang province from 17 january 2020 to 8 february 2020. epidemiological, demographic, clinical, laboratory, management and outcome data of patients with gi symptoms were analysed using multivariate analysis for risk of severe/critical type. bioinformatics were used to analyse features of sars-cov-2 from zhejiang province. results among enrolled 651 patients, 74 (11.4%) presented with at least one gi symptom (nausea, vomiting or diarrhoea), average age of 46.14 years, 4-day incubation period and 10.8% had pre-existing liver disease. of patients with covid-19 with gi symptoms, 17 (22.97%) and 23 (31.08%) had severe/critical types and family clustering, respectively, significantly higher than those without gi symptoms, 47 (8.14%) and 118 (20.45%). of patients with covid-19 with gi symptoms, 29 (39.19%), 23 (31.08%), 8 (10.81%) and 16 (21.62%) had significantly higher rates of fever &gt;38.5°c, fatigue, shortness of breath and headache, respectively. low-dose glucocorticoids and antibiotics were administered to 14.86% and 41.89% of patients, respectively. sputum production and increased lactate dehydrogenase/glucose levels were risk factors for severe/critical type. bioinformatics showed sequence mutation of sars-cov-2 with m 6 a methylation and changed binding capacity with ace2. conclusion we report covid-19 cases with gi symptoms with novel features outside wuhan. attention to patients with covid-19 with non-classic symptoms should increase to protect health providers."
http://orkg.org/orkg/resource/R189997,Correlation between gastrointestinal symptoms and disease severity in patients with COVID-19: a systematic review and meta-analysis,10.1136/bmjgast-2020-000437,crossref,"<jats:sec><jats:title>Objective</jats:title><jats:p>To study the correlation between gastrointestinal (GI) symptoms and disease severity in patients with COVID-19.</jats:p></jats:sec><jats:sec><jats:title>Design</jats:title><jats:p>We searched six databases including three Chinese and three English databases for all the published articles on COVID-19. Studies were screened according to inclusion and exclusion criteria. The relevant data were extracted and all the statistical analyses were performed using Revman5.3.</jats:p></jats:sec><jats:sec><jats:title>Result</jats:title><jats:p>In a meta-analysis of 9 studies, comprising 3022 patients, 479 patients (13.7%, 95%\u2009CI 0.125 to 0.149) had severe disease and 624 patients (14.7%, 95%\u2009CI 0.136 to 0.159) had GI symptoms. Of 624 patients with GI symptoms, 118 patients had severe disease (20.5%, 95%\u2009CI 0.133 to 0.276) and of 2397 cases without GI symptoms, 361 patients had severe disease (18.2%, 95%\u2009CI 0.129 to 0.235). Comparing disease severity of patients with and without GI symptoms, the results indicated: I²=62%, OR=1.21, 95%\u2009CI 0.94 to 1.56, p=0.13; there was no statistically significant difference between the two groups. The funnel plot was symmetrical with no publication bias.</jats:p></jats:sec><jats:sec><jats:title>Conclusion</jats:title><jats:p>Current results are not sufficient to demonstrate a significant correlation between GI symptoms and disease severity in patients with COVID-19.</jats:p></jats:sec>","objective to study the correlation between gastrointestinal (gi) symptoms and disease severity in patients with covid-19. design we searched six databases including three chinese and three english databases for all the published articles on covid-19. studies were screened according to inclusion and exclusion criteria. the relevant data were extracted and all the statistical analyses were performed using revman5.3. result in a meta-analysis of 9 studies, comprising 3022 patients, 479 patients (13.7%, 95%\u2009ci 0.125 to 0.149) had severe disease and 624 patients (14.7%, 95%\u2009ci 0.136 to 0.159) had gi symptoms. of 624 patients with gi symptoms, 118 patients had severe disease (20.5%, 95%\u2009ci 0.133 to 0.276) and of 2397 cases without gi symptoms, 361 patients had severe disease (18.2%, 95%\u2009ci 0.129 to 0.235). comparing disease severity of patients with and without gi symptoms, the results indicated: i²=62%, or=1.21, 95%\u2009ci 0.94 to 1.56, p=0.13; there was no statistically significant difference between the two groups. the funnel plot was symmetrical with no publication bias. conclusion current results are not sufficient to demonstrate a significant correlation between gi symptoms and disease severity in patients with covid-19."
http://orkg.org/orkg/resource/R193067,Severe Acute Respiratory Syndrome Coronavirus 2 RNA in Plasma Is Associated With Intensive Care Unit Admission and Mortality in Patients Hospitalized With Coronavirus Disease 2019,10.1093/cid/ciaa1338,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>The clinical significance of severe acute respiratory syndrome coronavirus 2 RNA in the circulation is unknown. In this prospective cohort study, we detected viral RNA in the plasma of 58 of 123 (47%) patients hospitalized with coronavirus disease 2019. RNA was detected more frequently, and levels were higher, in patients who were admitted to the intensive care unit and/or died.</jats:p>","abstract \n the clinical significance of severe acute respiratory syndrome coronavirus 2 rna in the circulation is unknown. in this prospective cohort study, we detected viral rna in the plasma of 58 of 123 (47%) patients hospitalized with coronavirus disease 2019. rna was detected more frequently, and levels were higher, in patients who were admitted to the intensive care unit and/or died."
http://orkg.org/orkg/resource/R190036,Gastrointestinal Symptoms and Outcomes in Hospitalized Coronavirus Disease 2019 Patients,10.1159/000509774,crossref,"<jats:p>&lt;b&gt;&lt;i&gt;Introduction:&lt;/i&gt;&lt;/b&gt; Gastrointestinal (GI) symptoms are increasingly being recognized in coronavirus disease 2019 (COVID-19). It is unclear if the presence of GI symptoms is associated with poor outcomes in COVID-19. We aim to assess if GI symptoms could be used for prognostication in hospitalized patients with COVID-19. &lt;b&gt;&lt;i&gt;Methods:&lt;/i&gt;&lt;/b&gt; We retrospectively analyzed patients admitted to a tertiary medical center in Brooklyn, NY, from March 18, 2020, to March 31, 2020, with COVID-19. The patients’ medical charts were reviewed for the presence of GI symptoms at admission, including nausea, vomiting, diarrhea, and abdominal pain. COVID-19 patients with GI symptoms (cases) were compared with COVID-19 patients without GI symptoms (control). &lt;b&gt;&lt;i&gt;Results:&lt;/i&gt;&lt;/b&gt; A total of 150 hospitalized COVID-19 patients were included, of which 31 (20.6%) patients had at least 1 or more of the GI symptoms (cases). They were compared with the 119 COVID-19 patients without GI symptoms (controls). The average age among cases was 57.6 years (SD 17.2) and control was 63.3 years (SD 14.6). No statistically significant difference was noted in comorbidities and laboratory findings. The primary outcome was mortality, which did not differ between cases and controls (41.9 vs. 37.8%, &lt;i&gt;p&lt;/i&gt; = 0.68). No statistically significant differences were noted in secondary outcomes, including the length of stay (LOS, 7.8 vs. 7.9 days, &lt;i&gt;p&lt;/i&gt; = 0.87) and need for mechanical ventilation (29 vs. 26.9%, &lt;i&gt;p&lt;/i&gt; = 0.82). &lt;b&gt;&lt;i&gt;Discussion:&lt;/i&gt;&lt;/b&gt; In our study, the presence of GI manifestations in COVID-19 at the time of admission was not associated with increased mortality, LOS, or mechanical ventilation. </jats:p>","&lt;b&gt;&lt;i&gt;introduction:&lt;/i&gt;&lt;/b&gt; gastrointestinal (gi) symptoms are increasingly being recognized in coronavirus disease 2019 (covid-19). it is unclear if the presence of gi symptoms is associated with poor outcomes in covid-19. we aim to assess if gi symptoms could be used for prognostication in hospitalized patients with covid-19. &lt;b&gt;&lt;i&gt;methods:&lt;/i&gt;&lt;/b&gt; we retrospectively analyzed patients admitted to a tertiary medical center in brooklyn, ny, from march 18, 2020, to march 31, 2020, with covid-19. the patients’ medical charts were reviewed for the presence of gi symptoms at admission, including nausea, vomiting, diarrhea, and abdominal pain. covid-19 patients with gi symptoms (cases) were compared with covid-19 patients without gi symptoms (control). &lt;b&gt;&lt;i&gt;results:&lt;/i&gt;&lt;/b&gt; a total of 150 hospitalized covid-19 patients were included, of which 31 (20.6%) patients had at least 1 or more of the gi symptoms (cases). they were compared with the 119 covid-19 patients without gi symptoms (controls). the average age among cases was 57.6 years (sd 17.2) and control was 63.3 years (sd 14.6). no statistically significant difference was noted in comorbidities and laboratory findings. the primary outcome was mortality, which did not differ between cases and controls (41.9 vs. 37.8%, &lt;i&gt;p&lt;/i&gt; = 0.68). no statistically significant differences were noted in secondary outcomes, including the length of stay (los, 7.8 vs. 7.9 days, &lt;i&gt;p&lt;/i&gt; = 0.87) and need for mechanical ventilation (29 vs. 26.9%, &lt;i&gt;p&lt;/i&gt; = 0.82). &lt;b&gt;&lt;i&gt;discussion:&lt;/i&gt;&lt;/b&gt; in our study, the presence of gi manifestations in covid-19 at the time of admission was not associated with increased mortality, los, or mechanical ventilation."
http://orkg.org/orkg/resource/R193213,On interdomain routing security and pretty secure BGP (psBGP),,crossref,"<jats:p>\n            It is well known that the Border Gateway Protocol (BGP), the IETF standard interdomain routing protocol, is vulnerable to a variety of attacks, and that a single misconfigured or malicious BGP speaker could result in large-scale service disruption. In this paper, we present\n            <jats:italic>Pretty Secure BGP (psBGP)</jats:italic>\n            ---a proposal for securing BGP, including an architectural overview, design details for significant aspects, and preliminary security and operational analysis. psBGP differs from other security proposals (e.g., S-BGP and soBGP) in that it makes use of a single-level PKI for AS number authentication, a decentralized trust model for verifying the propriety of IP prefix origin, and a rating-based stepwise approach for AS_PATH (integrity) verification. psBGP trades off the strong security guarantees of S-BGP for presumed-simpler operation, e.g., using a PKI with a simple structure, with a small number of certificate types, and of manageable size. psBGP is designed to successfully defend against various (nonmalicious and malicious) threats from uncoordinated BGP speakers, and to be incrementally deployed with incremental benefits.\n          </jats:p>","\n it is well known that the border gateway protocol (bgp), the ietf standard interdomain routing protocol, is vulnerable to a variety of attacks, and that a single misconfigured or malicious bgp speaker could result in large-scale service disruption. in this paper, we present\n pretty secure bgp (psbgp) \n ---a proposal for securing bgp, including an architectural overview, design details for significant aspects, and preliminary security and operational analysis. psbgp differs from other security proposals (e.g., s-bgp and sobgp) in that it makes use of a single-level pki for as number authentication, a decentralized trust model for verifying the propriety of ip prefix origin, and a rating-based stepwise approach for as_path (integrity) verification. psbgp trades off the strong security guarantees of s-bgp for presumed-simpler operation, e.g., using a pki with a simple structure, with a small number of certificate types, and of manageable size. psbgp is designed to successfully defend against various (nonmalicious and malicious) threats from uncoordinated bgp speakers, and to be incrementally deployed with incremental benefits.\n"
http://orkg.org/orkg/resource/R190059,Gastrointestinal symptoms of 95 cases with SARS-CoV-2 infection,10.1136/gutjnl-2020-321013,crossref,"<jats:sec><jats:title>Objective</jats:title><jats:p>To study the GI symptoms in severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infected patients.</jats:p></jats:sec><jats:sec><jats:title>Design</jats:title><jats:p>We analysed epidemiological, demographic, clinical and laboratory data of 95 cases with SARS-CoV-2 caused coronavirus disease 2019. Real-time reverse transcriptase PCR was used to detect the presence of SARS-CoV-2 in faeces and GI tissues.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Among the 95 patients, 58 cases exhibited GI symptoms of which 11 (11.6%) occurred on admission and 47 (49.5%) developed during hospitalisation. Diarrhoea (24.2%), anorexia (17.9%) and nausea (17.9%) were the main symptoms with five (5.3%), five (5.3%) and three (3.2%) cases occurred on the illness onset, respectively. A substantial proportion of patients developed diarrhoea during hospitalisation, potentially aggravated by various drugs including antibiotics. Faecal samples of 65 hospitalised patients were tested for the presence of SARS-CoV-2, including 42 with and 23 without GI symptoms, of which 22 (52.4%) and 9 (39.1%) were positive, respectively. Six patients with GI symptoms were subjected to endoscopy, revealing oesophageal bleeding with erosions and ulcers in one severe patient. SARS-CoV-2 RNA was detected in oesophagus, stomach, duodenum and rectum specimens for both two severe patients. In contrast, only duodenum was positive in one of the four non-severe patients.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>GI tract may be a potential transmission route and target organ of SARS-CoV-2.</jats:p></jats:sec>","objective to study the gi symptoms in severe acute respiratory syndrome coronavirus 2 (sars-cov-2) infected patients. design we analysed epidemiological, demographic, clinical and laboratory data of 95 cases with sars-cov-2 caused coronavirus disease 2019. real-time reverse transcriptase pcr was used to detect the presence of sars-cov-2 in faeces and gi tissues. results among the 95 patients, 58 cases exhibited gi symptoms of which 11 (11.6%) occurred on admission and 47 (49.5%) developed during hospitalisation. diarrhoea (24.2%), anorexia (17.9%) and nausea (17.9%) were the main symptoms with five (5.3%), five (5.3%) and three (3.2%) cases occurred on the illness onset, respectively. a substantial proportion of patients developed diarrhoea during hospitalisation, potentially aggravated by various drugs including antibiotics. faecal samples of 65 hospitalised patients were tested for the presence of sars-cov-2, including 42 with and 23 without gi symptoms, of which 22 (52.4%) and 9 (39.1%) were positive, respectively. six patients with gi symptoms were subjected to endoscopy, revealing oesophageal bleeding with erosions and ulcers in one severe patient. sars-cov-2 rna was detected in oesophagus, stomach, duodenum and rectum specimens for both two severe patients. in contrast, only duodenum was positive in one of the four non-severe patients. conclusions gi tract may be a potential transmission route and target organ of sars-cov-2."
http://orkg.org/orkg/resource/R190060,Gastrointestinal symptoms of 95 cases with SARS-CoV-2 infection,10.1136/gutjnl-2020-321013,crossref,"<jats:sec><jats:title>Objective</jats:title><jats:p>To study the GI symptoms in severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infected patients.</jats:p></jats:sec><jats:sec><jats:title>Design</jats:title><jats:p>We analysed epidemiological, demographic, clinical and laboratory data of 95 cases with SARS-CoV-2 caused coronavirus disease 2019. Real-time reverse transcriptase PCR was used to detect the presence of SARS-CoV-2 in faeces and GI tissues.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Among the 95 patients, 58 cases exhibited GI symptoms of which 11 (11.6%) occurred on admission and 47 (49.5%) developed during hospitalisation. Diarrhoea (24.2%), anorexia (17.9%) and nausea (17.9%) were the main symptoms with five (5.3%), five (5.3%) and three (3.2%) cases occurred on the illness onset, respectively. A substantial proportion of patients developed diarrhoea during hospitalisation, potentially aggravated by various drugs including antibiotics. Faecal samples of 65 hospitalised patients were tested for the presence of SARS-CoV-2, including 42 with and 23 without GI symptoms, of which 22 (52.4%) and 9 (39.1%) were positive, respectively. Six patients with GI symptoms were subjected to endoscopy, revealing oesophageal bleeding with erosions and ulcers in one severe patient. SARS-CoV-2 RNA was detected in oesophagus, stomach, duodenum and rectum specimens for both two severe patients. In contrast, only duodenum was positive in one of the four non-severe patients.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>GI tract may be a potential transmission route and target organ of SARS-CoV-2.</jats:p></jats:sec>","objective to study the gi symptoms in severe acute respiratory syndrome coronavirus 2 (sars-cov-2) infected patients. design we analysed epidemiological, demographic, clinical and laboratory data of 95 cases with sars-cov-2 caused coronavirus disease 2019. real-time reverse transcriptase pcr was used to detect the presence of sars-cov-2 in faeces and gi tissues. results among the 95 patients, 58 cases exhibited gi symptoms of which 11 (11.6%) occurred on admission and 47 (49.5%) developed during hospitalisation. diarrhoea (24.2%), anorexia (17.9%) and nausea (17.9%) were the main symptoms with five (5.3%), five (5.3%) and three (3.2%) cases occurred on the illness onset, respectively. a substantial proportion of patients developed diarrhoea during hospitalisation, potentially aggravated by various drugs including antibiotics. faecal samples of 65 hospitalised patients were tested for the presence of sars-cov-2, including 42 with and 23 without gi symptoms, of which 22 (52.4%) and 9 (39.1%) were positive, respectively. six patients with gi symptoms were subjected to endoscopy, revealing oesophageal bleeding with erosions and ulcers in one severe patient. sars-cov-2 rna was detected in oesophagus, stomach, duodenum and rectum specimens for both two severe patients. in contrast, only duodenum was positive in one of the four non-severe patients. conclusions gi tract may be a potential transmission route and target organ of sars-cov-2."
http://orkg.org/orkg/resource/R190064,Gastrointestinal symptoms of 95 cases with SARS-CoV-2 infection,10.1136/gutjnl-2020-321013,crossref,"<jats:sec><jats:title>Objective</jats:title><jats:p>To study the GI symptoms in severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infected patients.</jats:p></jats:sec><jats:sec><jats:title>Design</jats:title><jats:p>We analysed epidemiological, demographic, clinical and laboratory data of 95 cases with SARS-CoV-2 caused coronavirus disease 2019. Real-time reverse transcriptase PCR was used to detect the presence of SARS-CoV-2 in faeces and GI tissues.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>Among the 95 patients, 58 cases exhibited GI symptoms of which 11 (11.6%) occurred on admission and 47 (49.5%) developed during hospitalisation. Diarrhoea (24.2%), anorexia (17.9%) and nausea (17.9%) were the main symptoms with five (5.3%), five (5.3%) and three (3.2%) cases occurred on the illness onset, respectively. A substantial proportion of patients developed diarrhoea during hospitalisation, potentially aggravated by various drugs including antibiotics. Faecal samples of 65 hospitalised patients were tested for the presence of SARS-CoV-2, including 42 with and 23 without GI symptoms, of which 22 (52.4%) and 9 (39.1%) were positive, respectively. Six patients with GI symptoms were subjected to endoscopy, revealing oesophageal bleeding with erosions and ulcers in one severe patient. SARS-CoV-2 RNA was detected in oesophagus, stomach, duodenum and rectum specimens for both two severe patients. In contrast, only duodenum was positive in one of the four non-severe patients.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>GI tract may be a potential transmission route and target organ of SARS-CoV-2.</jats:p></jats:sec>","objective to study the gi symptoms in severe acute respiratory syndrome coronavirus 2 (sars-cov-2) infected patients. design we analysed epidemiological, demographic, clinical and laboratory data of 95 cases with sars-cov-2 caused coronavirus disease 2019. real-time reverse transcriptase pcr was used to detect the presence of sars-cov-2 in faeces and gi tissues. results among the 95 patients, 58 cases exhibited gi symptoms of which 11 (11.6%) occurred on admission and 47 (49.5%) developed during hospitalisation. diarrhoea (24.2%), anorexia (17.9%) and nausea (17.9%) were the main symptoms with five (5.3%), five (5.3%) and three (3.2%) cases occurred on the illness onset, respectively. a substantial proportion of patients developed diarrhoea during hospitalisation, potentially aggravated by various drugs including antibiotics. faecal samples of 65 hospitalised patients were tested for the presence of sars-cov-2, including 42 with and 23 without gi symptoms, of which 22 (52.4%) and 9 (39.1%) were positive, respectively. six patients with gi symptoms were subjected to endoscopy, revealing oesophageal bleeding with erosions and ulcers in one severe patient. sars-cov-2 rna was detected in oesophagus, stomach, duodenum and rectum specimens for both two severe patients. in contrast, only duodenum was positive in one of the four non-severe patients. conclusions gi tract may be a potential transmission route and target organ of sars-cov-2."
http://orkg.org/orkg/resource/R191146,Overview on Digital Twin for Autonomous Electrical Vehicles Propulsion Drive System,10.3390/su14020601,crossref,"<jats:p>The significant progress in the electric automotive industry brought a higher need for new technological innovations. Digital Twin (DT) is one of the hottest trends of the fourth industrial revolution. It allows representing physical assets under various operating conditions in a low-cost and zero-risk environment. DTs are used in many different fields from aerospace to healthcare. However, one of the perspective applications of such technology is the automotive industry. This paper presents an overview of the implementation of DT technology in electric vehicles (EV) propulsion drive systems. A general review of DT technology is supplemented with main applications analysis and comparison between different simulation technologies. Primary attention is given to the adaptation of DT technology for EV propulsion drive systems.</jats:p>","the significant progress in the electric automotive industry brought a higher need for new technological innovations. digital twin (dt) is one of the hottest trends of the fourth industrial revolution. it allows representing physical assets under various operating conditions in a low-cost and zero-risk environment. dts are used in many different fields from aerospace to healthcare. however, one of the perspective applications of such technology is the automotive industry. this paper presents an overview of the implementation of dt technology in electric vehicles (ev) propulsion drive systems. a general review of dt technology is supplemented with main applications analysis and comparison between different simulation technologies. primary attention is given to the adaptation of dt technology for ev propulsion drive systems."
http://orkg.org/orkg/resource/R193118,"Chronological Changes of Viral Shedding in Adult Inpatients with COVID-19 in Wuhan, China",10.1093/cid/ciaa631,crossref,"<jats:title>Abstract</jats:title>\n               <jats:sec>\n                  <jats:title>Background</jats:title>\n                  <jats:p>In December 2019, the coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) broke out in Wuhan. Epidemiological and clinical characteristics of patients with COVID-19 have been reported, but the relationships between laboratory features and viral load has not been comprehensively described.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Methods</jats:title>\n                  <jats:p>Adult inpatients (≥18 years old) with COVID-19 who underwent multiple (≥5 times) nucleic acid tests with nasal and pharyngeal swabs were recruited from Renmin Hospital of Wuhan University, including general patients (n = 70), severe patients (n = 195), and critical patients (n = 43). Laboratory data, demographic data, and clinical data were extracted from electronic medical records. The fitted polynomial curve was used to explore the association between serial viral loads and illness severity.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Results</jats:title>\n                  <jats:p>Viral load of SARS-CoV-2 peaked within the first few days (2–4 days) after admission, then decreased rapidly along with virus rebound under treatment. Critical patients had the highest viral loads, in contrast to the general patients showing the lowest viral loads. The viral loads were higher in sputum compared with nasal and pharyngeal swab (P = .026). The positive rate of respiratory tract samples was significantly higher than that of gastrointestinal tract samples (P &amp;lt; .001). The SARS-CoV-2 viral load was negatively correlated with portion parameters of blood routine and lymphocyte subsets and was positively associated with laboratory features of cardiovascular system.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Conclusions</jats:title>\n                  <jats:p>The serial viral loads of patients revealed whole viral shedding during hospitalization and the resurgence of virus during the treatment, which could be used for early warning of illness severity, thus improve antiviral interventions.</jats:p>\n               </jats:sec>","abstract \n \n background \n in december 2019, the coronavirus disease 2019 (covid-19) caused by severe acute respiratory syndrome coronavirus 2 (sars-cov-2) broke out in wuhan. epidemiological and clinical characteristics of patients with covid-19 have been reported, but the relationships between laboratory features and viral load has not been comprehensively described. \n \n \n methods \n adult inpatients (≥18 years old) with covid-19 who underwent multiple (≥5 times) nucleic acid tests with nasal and pharyngeal swabs were recruited from renmin hospital of wuhan university, including general patients (n = 70), severe patients (n = 195), and critical patients (n = 43). laboratory data, demographic data, and clinical data were extracted from electronic medical records. the fitted polynomial curve was used to explore the association between serial viral loads and illness severity. \n \n \n results \n viral load of sars-cov-2 peaked within the first few days (2–4 days) after admission, then decreased rapidly along with virus rebound under treatment. critical patients had the highest viral loads, in contrast to the general patients showing the lowest viral loads. the viral loads were higher in sputum compared with nasal and pharyngeal swab (p = .026). the positive rate of respiratory tract samples was significantly higher than that of gastrointestinal tract samples (p &amp;lt; .001). the sars-cov-2 viral load was negatively correlated with portion parameters of blood routine and lymphocyte subsets and was positively associated with laboratory features of cardiovascular system. \n \n \n conclusions \n the serial viral loads of patients revealed whole viral shedding during hospitalization and the resurgence of virus during the treatment, which could be used for early warning of illness severity, thus improve antiviral interventions. \n"
http://orkg.org/orkg/resource/R191883,Effect of normalization methods on the performance of supervised learning algorithms applied to HTSeq-FPKM-UQ data sets: 7SK RNA expression as a predictor of survival in patients with colon adenocarcinoma,10.1093/bib/bbx153,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Motivation: One of the main challenges in machine learning (ML) is choosing an appropriate normalization method. Here, we examine the effect of various normalization methods on analyzing FPKM upper quartile (FPKM-UQ) RNA sequencing data sets. We collect the HTSeq-FPKM-UQ files of patients with colon adenocarcinoma from TCGA-COAD project. We compare three most common normalization methods: scaling, standardizing using z-score and vector normalization by visualizing the normalized data set and evaluating the performance of 12 supervised learning algorithms on the normalized data set. Additionally, for each of these normalization methods, we use two different normalization strategies: normalizing samples (files) or normalizing features (genes). Results: Regardless of normalization methods, a support vector machine (SVM) model with the radial basis function kernel had the maximum accuracy (78%) in predicting the vital status of the patients. However, the fitting time of SVM depended on the normalization methods, and it reached its minimum fitting time when files were normalized to the unit length. Furthermore, among all 12 learning algorithms and 6 different normalization techniques, the Bernoulli naive Bayes model after standardizing files had the best performance in terms of maximizing the accuracy as well as minimizing the fitting time. We also investigated the effect of dimensionality reduction methods on the performance of the supervised ML algorithms. Reducing the dimension of the data set did not increase the maximum accuracy of 78%. However, it leaded to discovery of the 7SK RNA gene expression as a predictor of survival in patients with colon adenocarcinoma with accuracy of 78%.</jats:p>","abstract \n motivation: one of the main challenges in machine learning (ml) is choosing an appropriate normalization method. here, we examine the effect of various normalization methods on analyzing fpkm upper quartile (fpkm-uq) rna sequencing data sets. we collect the htseq-fpkm-uq files of patients with colon adenocarcinoma from tcga-coad project. we compare three most common normalization methods: scaling, standardizing using z-score and vector normalization by visualizing the normalized data set and evaluating the performance of 12 supervised learning algorithms on the normalized data set. additionally, for each of these normalization methods, we use two different normalization strategies: normalizing samples (files) or normalizing features (genes). results: regardless of normalization methods, a support vector machine (svm) model with the radial basis function kernel had the maximum accuracy (78%) in predicting the vital status of the patients. however, the fitting time of svm depended on the normalization methods, and it reached its minimum fitting time when files were normalized to the unit length. furthermore, among all 12 learning algorithms and 6 different normalization techniques, the bernoulli naive bayes model after standardizing files had the best performance in terms of maximizing the accuracy as well as minimizing the fitting time. we also investigated the effect of dimensionality reduction methods on the performance of the supervised ml algorithms. reducing the dimension of the data set did not increase the maximum accuracy of 78%. however, it leaded to discovery of the 7sk rna gene expression as a predictor of survival in patients with colon adenocarcinoma with accuracy of 78%."
http://orkg.org/orkg/resource/R190487,Structured reviews for data and knowledge-driven research,10.1093/database/baaa015,crossref,"<jats:title>Abstract</jats:title>\n               <jats:sec>\n                  <jats:title />\n                  <jats:p>Hypothesis generation is a critical step in research and a cornerstone in the rare disease field. Research is most efficient when those hypotheses are based on the entirety of knowledge known to date. Systematic review articles are commonly used in biomedicine to summarize existing knowledge and contextualize experimental data. But the information contained within review articles is typically only expressed as free-text, which is difficult to use computationally. Researchers struggle to navigate, collect and remix prior knowledge as it is scattered in several silos without seamless integration and access. This lack of a structured information framework hinders research by both experimental and computational scientists. To better organize knowledge and data, we built a structured review article that is specifically focused on NGLY1 Deficiency, an ultra-rare genetic disease first reported in 2012. We represented this structured review as a knowledge graph and then stored this knowledge graph in a Neo4j database to simplify dissemination, querying and visualization of the network. Relative to free-text, this structured review better promotes the principles of findability, accessibility, interoperability and reusability (FAIR). In collaboration with domain experts in NGLY1 Deficiency, we demonstrate how this resource can improve the efficiency and comprehensiveness of hypothesis generation. We also developed a read–write interface that allows domain experts to contribute FAIR structured knowledge to this community resource. In contrast to traditional free-text review articles, this structured review exists as a living knowledge graph that is curated by humans and accessible to computational analyses. Finally, we have generalized this workflow into modular and repurposable components that can be applied to other domain areas. This NGLY1 Deficiency-focused network is publicly available at http://ngly1graph.org/.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Availability and implementation</jats:title>\n                  <jats:p>Database URL: http://ngly1graph.org/. Network data files are at: https://github.com/SuLab/ngly1-graph and source code at: https://github.com/SuLab/bioknowledge-reviewer.</jats:p>\n               </jats:sec>\n               <jats:sec>\n                  <jats:title>Contact</jats:title>\n                  <jats:p>asu@scripps.edu</jats:p>\n               </jats:sec>","abstract \n \n \n hypothesis generation is a critical step in research and a cornerstone in the rare disease field. research is most efficient when those hypotheses are based on the entirety of knowledge known to date. systematic review articles are commonly used in biomedicine to summarize existing knowledge and contextualize experimental data. but the information contained within review articles is typically only expressed as free-text, which is difficult to use computationally. researchers struggle to navigate, collect and remix prior knowledge as it is scattered in several silos without seamless integration and access. this lack of a structured information framework hinders research by both experimental and computational scientists. to better organize knowledge and data, we built a structured review article that is specifically focused on ngly1 deficiency, an ultra-rare genetic disease first reported in 2012. we represented this structured review as a knowledge graph and then stored this knowledge graph in a neo4j database to simplify dissemination, querying and visualization of the network. relative to free-text, this structured review better promotes the principles of findability, accessibility, interoperability and reusability (fair). in collaboration with domain experts in ngly1 deficiency, we demonstrate how this resource can improve the efficiency and comprehensiveness of hypothesis generation. we also developed a read–write interface that allows domain experts to contribute fair structured knowledge to this community resource. in contrast to traditional free-text review articles, this structured review exists as a living knowledge graph that is curated by humans and accessible to computational analyses. finally, we have generalized this workflow into modular and repurposable components that can be applied to other domain areas. this ngly1 deficiency-focused network is publicly available at http://ngly1graph.org/. \n \n \n availability and implementation \n database url: http://ngly1graph.org/. network data files are at: https://github.com/sulab/ngly1-graph and source code at: https://github.com/sulab/bioknowledge-reviewer. \n \n \n contact \n asu@scripps.edu \n"
http://orkg.org/orkg/resource/R191449,SARS-CoV-2 PCR cycle threshold at hospital admission associated with patient mortality,10.1371/journal.pone.0244777,crossref,"<jats:sec id=""sec001""><jats:title>Background</jats:title><jats:p>Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) cycle threshold (Ct) has been suggested as an approximate measure of initial viral burden. The utility of cycle threshold, at admission, as a predictor of disease severity has not been thoroughly investigated.</jats:p></jats:sec><jats:sec id=""sec002""><jats:title>Methods and findings</jats:title><jats:p>We conducted a retrospective study of SARS-CoV-2 positive, hospitalized patients from 3/26/2020 to 8/5/2020 who had SARS-CoV-2 Ct data within 48 hours of admission (n = 1044). Only patients with complete survival data, discharged (n = 774) or died in hospital (n = 270), were included in our analysis. Laboratory, demographic, and clinical data were extracted from electronic medical records. Multivariable logistic regression was applied to examine the relationship of patient mortality with Ct values while adjusting for established risk factors. Ct was analyzed as continuous variable and subdivided into quartiles to better illustrate its relationship with outcome. Cumulative incidence curves were created to assess whether there was a survival difference in the setting of the competing risks of death versus patient discharge. Mean Ct at admission was higher for survivors (28.6, SD = 5.8) compared to non-survivors (24.8, SD = 6.0, P&lt;0.001). In-hospital mortality significantly differed (p&lt;0.05) by Ct quartile. After adjusting for age, gender, BMI, hypertension and diabetes, increased cycle threshold was associated with decreased odds of in-hospital mortality (0.91, CI 0.89–0.94, p&lt;0.001). Compared to the 4<jats:sup>th</jats:sup>Quartile, patients with Ct values in the 1st Quartile (Ct &lt;22.9) and 2nd Quartile (Ct 23.0–27.3) had an adjusted odds ratio of in-hospital mortality of 3.8 and 2.6 respectively (p&lt;0.001). The discriminative ability of Ct to predict inpatient mortality was found to be limited, possessing an area under the curve (AUC) of 0.68 (CI 0.63–0.71).</jats:p></jats:sec><jats:sec id=""sec003""><jats:title>Conclusion</jats:title><jats:p>SARS-CoV-2 Ct was found to be an independent predictor of patient mortality. However, further study is needed on how to best clinically utilize such information given the result variation due to specimen quality, phase of disease, and the limited discriminative ability of the test.</jats:p></jats:sec>","background severe acute respiratory syndrome coronavirus 2 (sars-cov-2) cycle threshold (ct) has been suggested as an approximate measure of initial viral burden. the utility of cycle threshold, at admission, as a predictor of disease severity has not been thoroughly investigated. methods and findings we conducted a retrospective study of sars-cov-2 positive, hospitalized patients from 3/26/2020 to 8/5/2020 who had sars-cov-2 ct data within 48 hours of admission (n = 1044). only patients with complete survival data, discharged (n = 774) or died in hospital (n = 270), were included in our analysis. laboratory, demographic, and clinical data were extracted from electronic medical records. multivariable logistic regression was applied to examine the relationship of patient mortality with ct values while adjusting for established risk factors. ct was analyzed as continuous variable and subdivided into quartiles to better illustrate its relationship with outcome. cumulative incidence curves were created to assess whether there was a survival difference in the setting of the competing risks of death versus patient discharge. mean ct at admission was higher for survivors (28.6, sd = 5.8) compared to non-survivors (24.8, sd = 6.0, p&lt;0.001). in-hospital mortality significantly differed (p&lt;0.05) by ct quartile. after adjusting for age, gender, bmi, hypertension and diabetes, increased cycle threshold was associated with decreased odds of in-hospital mortality (0.91, ci 0.89–0.94, p&lt;0.001). compared to the 4 th quartile, patients with ct values in the 1st quartile (ct &lt;22.9) and 2nd quartile (ct 23.0–27.3) had an adjusted odds ratio of in-hospital mortality of 3.8 and 2.6 respectively (p&lt;0.001). the discriminative ability of ct to predict inpatient mortality was found to be limited, possessing an area under the curve (auc) of 0.68 (ci 0.63–0.71). conclusion sars-cov-2 ct was found to be an independent predictor of patient mortality. however, further study is needed on how to best clinically utilize such information given the result variation due to specimen quality, phase of disease, and the limited discriminative ability of the test."
http://orkg.org/orkg/resource/R193556,Elevated wildlife-vehicle collision rates during the COVID-19 pandemic,10.1038/s41598-021-99233-9,crossref,"<jats:title>Abstract</jats:title><jats:p>Wildlife-vehicle collisions threaten both humans and wildlife, but we still lack information about the relationship between traffic volume and wildlife-vehicle collisions. The COVID-19 pandemic allowed us to investigate the effects of traffic volume on wildlife-vehicle collisions in the United States. We observed decreased traffic nationwide, particularly in densely populated states with low or high disease burdens. Despite reduced traffic, total collisions were unchanged; wildlife-vehicle collisions did decline at the start of the pandemic, but increased as the pandemic progressed, ultimately exceeding collisions in the previous year. As a result, nationwide collision rates were higher during the pandemic. We suggest that increased wildlife road use offsets the effects of decreased traffic volume on wildlife-vehicle collisions. Thus, decreased traffic volume will not always reduce wildlife-vehicle collisions.</jats:p>","abstract wildlife-vehicle collisions threaten both humans and wildlife, but we still lack information about the relationship between traffic volume and wildlife-vehicle collisions. the covid-19 pandemic allowed us to investigate the effects of traffic volume on wildlife-vehicle collisions in the united states. we observed decreased traffic nationwide, particularly in densely populated states with low or high disease burdens. despite reduced traffic, total collisions were unchanged; wildlife-vehicle collisions did decline at the start of the pandemic, but increased as the pandemic progressed, ultimately exceeding collisions in the previous year. as a result, nationwide collision rates were higher during the pandemic. we suggest that increased wildlife road use offsets the effects of decreased traffic volume on wildlife-vehicle collisions. thus, decreased traffic volume will not always reduce wildlife-vehicle collisions."
http://orkg.org/orkg/resource/R193573,Impact of COVID-19 Lockdown on Wildlife-Vehicle Collisions in NW of Spain,,crossref,"<jats:p>Wildlife–vehicle collisions (WVCs) in many places have a significant impact on wildlife management and road safety. The COVID-19 lockdown enabled the study of the specific impact that traffic has on these events. WVC variation in the Asturias and Cantabria regions (NW of Spain) because of the COVID-19 lockdown reached a maximum reduction of −64.77% during strict confinement but it was minimal or nonexistent during “soft” confinement. The global average value was −30.22% compared with the WVCs registered in the same period in 2019, but only −4.69% considering the average throughout the period 2010–2019. There are huge differences between conventional roads, where the traffic reduction was greater, and highways, where the traffic reduction was lesser during the COVID-19 lockdown. The results depend on the season, the day of the week and the time of day, but mainly on the traffic reduction occurring. The results obtained highlight the need to include the traffic factor in WVC reduction strategies.</jats:p>","wildlife–vehicle collisions (wvcs) in many places have a significant impact on wildlife management and road safety. the covid-19 lockdown enabled the study of the specific impact that traffic has on these events. wvc variation in the asturias and cantabria regions (nw of spain) because of the covid-19 lockdown reached a maximum reduction of −64.77% during strict confinement but it was minimal or nonexistent during “soft” confinement. the global average value was −30.22% compared with the wvcs registered in the same period in 2019, but only −4.69% considering the average throughout the period 2010–2019. there are huge differences between conventional roads, where the traffic reduction was greater, and highways, where the traffic reduction was lesser during the covid-19 lockdown. the results depend on the season, the day of the week and the time of day, but mainly on the traffic reduction occurring. the results obtained highlight the need to include the traffic factor in wvc reduction strategies."
http://orkg.org/orkg/resource/R193614,DOLPHIN: An efficient algorithm for mining distance-based outliers in very large datasets,10.1145/1497577.1497581,crossref,"<jats:p>In this work a novel distance-based outlier detection algorithm, named DOLPHIN, working on disk-resident datasets and whose I/O cost corresponds to the cost of sequentially reading the input dataset file twice, is presented.</jats:p>\n          <jats:p>It is both theoretically and empirically shown that the main memory usage of DOLPHIN amounts to a small fraction of the dataset and that DOLPHIN has linear time performance with respect to the dataset size. DOLPHIN gains efficiency by naturally merging together in a unified schema three strategies, namely the selection policy of objects to be maintained in main memory, usage of pruning rules, and similarity search techniques. Importantly, similarity search is accomplished by the algorithm without the need of preliminarily indexing the whole dataset, as other methods do.</jats:p>\n          <jats:p>The algorithm is simple to implement and it can be used with any type of data, belonging to either metric or nonmetric spaces. Moreover, a modification to the basic method allows DOLPHIN to deal with the scenario in which the available buffer of main memory is smaller than its standard requirements. DOLPHIN has been compared with state-of-the-art distance-based outlier detection algorithms, showing that it is much more efficient.</jats:p>","in this work a novel distance-based outlier detection algorithm, named dolphin, working on disk-resident datasets and whose i/o cost corresponds to the cost of sequentially reading the input dataset file twice, is presented. \n it is both theoretically and empirically shown that the main memory usage of dolphin amounts to a small fraction of the dataset and that dolphin has linear time performance with respect to the dataset size. dolphin gains efficiency by naturally merging together in a unified schema three strategies, namely the selection policy of objects to be maintained in main memory, usage of pruning rules, and similarity search techniques. importantly, similarity search is accomplished by the algorithm without the need of preliminarily indexing the whole dataset, as other methods do. \n the algorithm is simple to implement and it can be used with any type of data, belonging to either metric or nonmetric spaces. moreover, a modification to the basic method allows dolphin to deal with the scenario in which the available buffer of main memory is smaller than its standard requirements. dolphin has been compared with state-of-the-art distance-based outlier detection algorithms, showing that it is much more efficient."
http://orkg.org/orkg/resource/R193559,"Animal-vehicle collisions during the COVID-19 lockdown in early 2020 in the Krakow metropolitan region, Poland",,crossref,"<jats:title>Abstract</jats:title><jats:p>The interrelations between human activity and animal populations are of increasing interest due to the emergence of the novel COVID-19 and the consequent pandemic across the world. Anthropogenic impacts of the pandemic on animals in urban-suburban environments are largely unknown. In this study, the temporal and spatial patterns of urban animal response to the COVID-19 lockdown were assessed using animal-vehicle collisions (AVC) data. We collected AVC data over two 6-month periods in 2019 and 2020 (January to June) from the largest metropolis in southern Poland, which included lockdown months. Furthermore, we used traffic data to understand the impact of lockdown on AVC in the urban area. Our analysis of 1063 AVC incidents revealed that COVID-19 related lockdown decreased AVC rates in suburban areas. However, in the urban area, even though traffic volume had significantly reduced, AVC did not decrease significantly, suggesting that lockdown did not influence the collision rates in the urban area. Our results suggest that there is a need to focus on understanding the effects of changes in traffic volume on both human behaviour and wildlife space use on the resulting impacts on AVC in the urban area.</jats:p>","abstract the interrelations between human activity and animal populations are of increasing interest due to the emergence of the novel covid-19 and the consequent pandemic across the world. anthropogenic impacts of the pandemic on animals in urban-suburban environments are largely unknown. in this study, the temporal and spatial patterns of urban animal response to the covid-19 lockdown were assessed using animal-vehicle collisions (avc) data. we collected avc data over two 6-month periods in 2019 and 2020 (january to june) from the largest metropolis in southern poland, which included lockdown months. furthermore, we used traffic data to understand the impact of lockdown on avc in the urban area. our analysis of 1063 avc incidents revealed that covid-19 related lockdown decreased avc rates in suburban areas. however, in the urban area, even though traffic volume had significantly reduced, avc did not decrease significantly, suggesting that lockdown did not influence the collision rates in the urban area. our results suggest that there is a need to focus on understanding the effects of changes in traffic volume on both human behaviour and wildlife space use on the resulting impacts on avc in the urban area."
http://orkg.org/orkg/resource/R193951,Supplier Selection and Order Allocation under a Carbon Emission Trading Scheme: A Case Study from China,10.3390/ijerph17010111,crossref,"<jats:p>In implementing carbon emission trading schemes (ETSs), the cost of carbon embedded in raw materials further complicates supplier selection and order allocation. Firms have to make decisions by comprehensively considering the cost and the important intangible performance of suppliers. This paper uses an analytic network process–integer programming (ANP–IP) model based on a multiple-criteria decision-making (MCDM) approach to solve the above issues by first evaluating and then optimizing them. The carbon embedded in components, which can be used to reflect the carbon competitiveness of a supplier, is integrated into the ANP–IP model. In addition, an international large-scale electronic equipment manufacturer in China is used to validate the model. Different scenarios involving different carbon prices are designed to analyze whether China’s current ETS drives firms to choose more low-carbon suppliers. The results show that current carbon constraints are not stringent enough to drive firms to select low-carbon suppliers. A more stringent ETS with a higher carbon price could facilitate the creation of a low-carbon supply chain. The analysis of the firm’s total cost and of the total cost composition indicates that the impact of a more stringent ETS on the firm results mainly from indirect costs instead of direct costs. The indirect cost is caused by the suppliers’ transfer of part of the low-carbon investment in the product, and arises from buying carbon permits with high carbon prices. Implications revealed by the model analysis are discussed to provide guidance to suppliers regarding the balance between soft competitiveness and low-carbon production capability and to provide guidance to the firm on how to cooperate with suppliers to achieve a mutually beneficial situation.</jats:p>","in implementing carbon emission trading schemes (etss), the cost of carbon embedded in raw materials further complicates supplier selection and order allocation. firms have to make decisions by comprehensively considering the cost and the important intangible performance of suppliers. this paper uses an analytic network process–integer programming (anp–ip) model based on a multiple-criteria decision-making (mcdm) approach to solve the above issues by first evaluating and then optimizing them. the carbon embedded in components, which can be used to reflect the carbon competitiveness of a supplier, is integrated into the anp–ip model. in addition, an international large-scale electronic equipment manufacturer in china is used to validate the model. different scenarios involving different carbon prices are designed to analyze whether china’s current ets drives firms to choose more low-carbon suppliers. the results show that current carbon constraints are not stringent enough to drive firms to select low-carbon suppliers. a more stringent ets with a higher carbon price could facilitate the creation of a low-carbon supply chain. the analysis of the firm’s total cost and of the total cost composition indicates that the impact of a more stringent ets on the firm results mainly from indirect costs instead of direct costs. the indirect cost is caused by the suppliers’ transfer of part of the low-carbon investment in the product, and arises from buying carbon permits with high carbon prices. implications revealed by the model analysis are discussed to provide guidance to suppliers regarding the balance between soft competitiveness and low-carbon production capability and to provide guidance to the firm on how to cooperate with suppliers to achieve a mutually beneficial situation."
http://orkg.org/orkg/resource/R193470,An Empirical Evaluation of Convolutional Networks for Malaria Diagnosis,,crossref,"<jats:p>Malaria is a globally widespread disease caused by parasitic protozoa transmitted to humans by infected female mosquitoes of Anopheles. It is caused in humans only by the parasite Plasmodium, further classified into four different species. Identifying malaria parasites is possible by analysing digital microscopic blood smears, which is tedious, time-consuming and error prone. So, automation of the process has assumed great importance as it helps the laborious manual process of review and diagnosis. This work focuses on deep learning-based models, by comparing off-the-shelf architectures for classifying healthy and parasite-affected cells, by investigating the four-class classification on the Plasmodium falciparum stages of life and, finally, by evaluating the robustness of the models with cross-dataset experiments on two different datasets. The main contributions to the research in this field can be resumed as follows: (i) comparing off-the-shelf architectures in the task of classifying healthy and parasite-affected cells, (ii) investigating the four-class classification on the P. falciparum stages of life and (iii) evaluating the robustness of the models with cross-dataset experiments. Eleven well-known convolutional neural networks on two public datasets have been exploited. The results show that the networks have great accuracy in binary classification, even though they lack few samples per class. Moreover, the cross-dataset experiments exhibit the need for some further regulations. In particular, ResNet-18 achieved up to 97.68% accuracy in the binary classification, while DenseNet-201 reached 99.40% accuracy on the multiclass classification. The cross-dataset experiments exhibit the limitations of deep learning approaches in such a scenario, even though combining the two datasets permitted DenseNet-201 to reach 97.45% accuracy. Naturally, this needs further investigation to improve the robustness. In general, DenseNet-201 seems to offer the most stable and robust performance, offering as a crucial candidate to further developments and modifications. Moreover, the mobile-oriented architectures showed promising and satisfactory performance in the classification of malaria parasites. The obtained results enable extensive improvements, specifically oriented to the application of object detectors for type and stage of life recognition, even in mobile environments.</jats:p>","malaria is a globally widespread disease caused by parasitic protozoa transmitted to humans by infected female mosquitoes of anopheles. it is caused in humans only by the parasite plasmodium, further classified into four different species. identifying malaria parasites is possible by analysing digital microscopic blood smears, which is tedious, time-consuming and error prone. so, automation of the process has assumed great importance as it helps the laborious manual process of review and diagnosis. this work focuses on deep learning-based models, by comparing off-the-shelf architectures for classifying healthy and parasite-affected cells, by investigating the four-class classification on the plasmodium falciparum stages of life and, finally, by evaluating the robustness of the models with cross-dataset experiments on two different datasets. the main contributions to the research in this field can be resumed as follows: (i) comparing off-the-shelf architectures in the task of classifying healthy and parasite-affected cells, (ii) investigating the four-class classification on the p. falciparum stages of life and (iii) evaluating the robustness of the models with cross-dataset experiments. eleven well-known convolutional neural networks on two public datasets have been exploited. the results show that the networks have great accuracy in binary classification, even though they lack few samples per class. moreover, the cross-dataset experiments exhibit the need for some further regulations. in particular, resnet-18 achieved up to 97.68% accuracy in the binary classification, while densenet-201 reached 99.40% accuracy on the multiclass classification. the cross-dataset experiments exhibit the limitations of deep learning approaches in such a scenario, even though combining the two datasets permitted densenet-201 to reach 97.45% accuracy. naturally, this needs further investigation to improve the robustness. in general, densenet-201 seems to offer the most stable and robust performance, offering as a crucial candidate to further developments and modifications. moreover, the mobile-oriented architectures showed promising and satisfactory performance in the classification of malaria parasites. the obtained results enable extensive improvements, specifically oriented to the application of object detectors for type and stage of life recognition, even in mobile environments."
http://orkg.org/orkg/resource/R193517,Deep malaria parasite detection in thin blood smear microscopic images,,crossref,"<jats:p>Malaria is a disease activated by a type of microscopic parasite transmitted from infected female mosquito bites to humans. Malaria is a fatal disease that is endemic in many regions of the world. Quick diagnosis of this disease will be very valuable for patients, as traditional methods require tedious work for its detection. Recently, some automated methods have been proposed that exploit hand-crafted feature extraction techniques however, their accuracies are not reliable. Deep learning approaches modernize the world with their superior performance. Convolutional Neural Networks (CNN) are vastly scalable for image classification tasks that extract features through hidden layers of the model without any handcrafting. The detection of malaria-infected red blood cells from segmented microscopic blood images using convolutional neural networks can assist in quick diagnosis, and this will be useful for regions with fewer healthcare experts. The contributions of this paper are two-fold. First, we evaluate the performance of different existing deep learning models for efficient malaria detection. Second, we propose a customized CNN model that outperforms all observed deep learning models. It exploits the bilateral filtering and image augmentation techniques for highlighting features of red blood cells before training the model. Due to image augmentation techniques, the customized CNN model is generalized and avoids over-fitting. All experimental evaluations are performed on the benchmark NIH Malaria Dataset, and the results reveal that the proposed algorithm is 96.82% accurate in detecting malaria from the microscopic blood smears.</jats:p>","malaria is a disease activated by a type of microscopic parasite transmitted from infected female mosquito bites to humans. malaria is a fatal disease that is endemic in many regions of the world. quick diagnosis of this disease will be very valuable for patients, as traditional methods require tedious work for its detection. recently, some automated methods have been proposed that exploit hand-crafted feature extraction techniques however, their accuracies are not reliable. deep learning approaches modernize the world with their superior performance. convolutional neural networks (cnn) are vastly scalable for image classification tasks that extract features through hidden layers of the model without any handcrafting. the detection of malaria-infected red blood cells from segmented microscopic blood images using convolutional neural networks can assist in quick diagnosis, and this will be useful for regions with fewer healthcare experts. the contributions of this paper are two-fold. first, we evaluate the performance of different existing deep learning models for efficient malaria detection. second, we propose a customized cnn model that outperforms all observed deep learning models. it exploits the bilateral filtering and image augmentation techniques for highlighting features of red blood cells before training the model. due to image augmentation techniques, the customized cnn model is generalized and avoids over-fitting. all experimental evaluations are performed on the benchmark nih malaria dataset, and the results reveal that the proposed algorithm is 96.82% accurate in detecting malaria from the microscopic blood smears."
http://orkg.org/orkg/resource/R193649,A Study of Neural Machine Translation from Chinese to Urdu,10.32629/jai.v2i4.82,crossref,"<jats:p>&lt;div&gt;Machine Translation (MT) is used for giving a translation from a source language to a target language. Machine translation simply translates text or speech from one language to another language, but this process is not sufficient to give the perfect translation of a text due to the requirement of identification of whole expressions and their direct counterparts. Neural Machine Translation (NMT) is one of the most standard machine translation methods, which has made great progress in the recent years especially in non-universal languages. However, local language translation software for other foreign languages is limited and needs improving. In this paper, the Chinese language is translated to the Urdu language with the help of Open Neural Machine Translation (OpenNMT) in Deep Learning. Firstly, a Chinese&lt;/div&gt;&lt;div&gt;to Urdu language sentences datasets were established and supported with Seven million sentences. After that, these datasets were trained by using the Open Neural Machine Translation (OpenNMT) method. At the final stage, the translation was compared to the desired translation with the help of the Bleu Score Method.&lt;/div&gt;</jats:p>","&lt;div&gt;machine translation (mt) is used for giving a translation from a source language to a target language. machine translation simply translates text or speech from one language to another language, but this process is not sufficient to give the perfect translation of a text due to the requirement of identification of whole expressions and their direct counterparts. neural machine translation (nmt) is one of the most standard machine translation methods, which has made great progress in the recent years especially in non-universal languages. however, local language translation software for other foreign languages is limited and needs improving. in this paper, the chinese language is translated to the urdu language with the help of open neural machine translation (opennmt) in deep learning. firstly, a chinese&lt;/div&gt;&lt;div&gt;to urdu language sentences datasets were established and supported with seven million sentences. after that, these datasets were trained by using the open neural machine translation (opennmt) method. at the final stage, the translation was compared to the desired translation with the help of the bleu score method.&lt;/div&gt;"
http://orkg.org/orkg/resource/R193662,Do blockchain and circular economy practices improve post COVID-19 supply chains? A resource-based and resource dependence perspective,10.1108/imds-09-2020-0560,crossref,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>Using the resource-based and the resource dependence theoretical approaches of the firm, the paper explores firm responses to supply chain disruptions during COVID-19. The paper explores how firms develop localization, agility and digitization (L-A-D) capabilities by applying (or not applying) their critical circular economy (CE) and blockchain technology (BCT)-related resources and capabilities that they either already possess or acquire from external agents.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>An abductive approach, applying exploratory qualitative research was conducted over a sample of 24 firms. The sample represented different industries to study their critical BCT and CE resources and capabilities and the L-A-D capabilities. Firm resources and capabilities were classified using the technology, organization and environment (TOE) framework.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>Findings show significant patterns on adoption levels of the blockchain-enabled circular economy system (BCES) and L-A-D capability development. The greater the BCES adoption capabilities, the greater the L-A-D capabilities. Organizational size and industry both influence the relationship between BCES and L-A-D. Accordingly, research propositions and a research framework are proposed.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title><jats:p>Given the limited sample size, the generalizability of the findings is limited. Our findings extend supply chain resiliency research. A series of propositions provide opportunities for future research. The resource-based view and resource-dependency theories are useful frameworks to better understanding the relationship between firm resources and supply chain resilience.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Practical implications</jats:title><jats:p>The results and discussion of this study serve as useful guidance for practitioners to create CE and BCT resources and capabilities for improving supply chain resiliency.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Social implications</jats:title><jats:p>The study shows the socio-economic and socio-environmental importance of BCES in the COVID-19 or similar crises.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>The study is one of the initial attempts that highlights the possibilities of BCES across multiple industries and their value during pandemics and disruptions.</jats:p></jats:sec>","purpose using the resource-based and the resource dependence theoretical approaches of the firm, the paper explores firm responses to supply chain disruptions during covid-19. the paper explores how firms develop localization, agility and digitization (l-a-d) capabilities by applying (or not applying) their critical circular economy (ce) and blockchain technology (bct)-related resources and capabilities that they either already possess or acquire from external agents. design/methodology/approach an abductive approach, applying exploratory qualitative research was conducted over a sample of 24 firms. the sample represented different industries to study their critical bct and ce resources and capabilities and the l-a-d capabilities. firm resources and capabilities were classified using the technology, organization and environment (toe) framework. findings findings show significant patterns on adoption levels of the blockchain-enabled circular economy system (bces) and l-a-d capability development. the greater the bces adoption capabilities, the greater the l-a-d capabilities. organizational size and industry both influence the relationship between bces and l-a-d. accordingly, research propositions and a research framework are proposed. research limitations/implications given the limited sample size, the generalizability of the findings is limited. our findings extend supply chain resiliency research. a series of propositions provide opportunities for future research. the resource-based view and resource-dependency theories are useful frameworks to better understanding the relationship between firm resources and supply chain resilience. practical implications the results and discussion of this study serve as useful guidance for practitioners to create ce and bct resources and capabilities for improving supply chain resiliency. social implications the study shows the socio-economic and socio-environmental importance of bces in the covid-19 or similar crises. originality/value the study is one of the initial attempts that highlights the possibilities of bces across multiple industries and their value during pandemics and disruptions."
http://orkg.org/orkg/resource/R193655,Research on Chinese-Urdu Machine Translation Based On Deep Learning,10.32629/jai.v3i2.279,crossref,"""<jats:p>Urdu is Pakistan 's national language. However, Chinese expertise is very negligible in Pakistan and the Asian nations. Yet fewer research has been undertaken in the area of computer translation on Chinese to Urdu. In order to solve the above problems, we designed of an electronic dictionary for Chinese-Urdu, and studied the sentence-level machine translation technology which is based on deep learning. The Design of an electronic dictionary Chinese-Urdu machine translation system we collected and constructed an electronic dictionary containing 24000 entries from Chinese to Urdu. For Sentence we used English as an intermediate language, and based on the existing parallel corpus of Chinese to English and English to Urdu, we constructed a bilingual parallel corpus containing 66000 sentences from Chinese to Urdu. The Corpus has trained by using two NMT Models (LSTM,Transformer Model) and the above two translation model were compared to the desired translation, with the help of bilingual valuation understudy (BLEU) score.\xa0 On NMT, The LSTM Model is gain of 0.067 to 0.41 in BLEU score while on Transformer model, there is gain of 0.077 to 0.52 in BLEU which is better than from LSTM Model score. Furthermore, we compared the proposed model with Google and Microsoft translation.</jats:p>""",""" urdu is pakistan 's national language. however, chinese expertise is very negligible in pakistan and the asian nations. yet fewer research has been undertaken in the area of computer translation on chinese to urdu. in order to solve the above problems, we designed of an electronic dictionary for chinese-urdu, and studied the sentence-level machine translation technology which is based on deep learning. the design of an electronic dictionary chinese-urdu machine translation system we collected and constructed an electronic dictionary containing 24000 entries from chinese to urdu. for sentence we used english as an intermediate language, and based on the existing parallel corpus of chinese to english and english to urdu, we constructed a bilingual parallel corpus containing 66000 sentences from chinese to urdu. the corpus has trained by using two nmt models (lstm,transformer model) and the above two translation model were compared to the desired translation, with the help of bilingual valuation understudy (bleu) score.\xa0 on nmt, the lstm model is gain of 0.067 to 0.41 in bleu score while on transformer model, there is gain of 0.077 to 0.52 in bleu which is better than from lstm model score. furthermore, we compared the proposed model with google and microsoft translation. """
http://orkg.org/orkg/resource/R193658,A Seq to Seq Machine Translation from Urdu to Chinese,10.32629/jai.v4i1.359,crossref,"<jats:p>Machine translation (MT) is a subtype of computational linguistics that uses to implement the translation between different natural languages (NL). Simply word to word exchanging on machine translation is not enough to give desire result. Neural machine translation is one of the standard methods of machine learning which make a huge improvement in recent time especially in local and some national languages. However these languages translation are not enough and need to focus on it. In this research we translate Urdu to Chinese language with the help of neural machine translation (NMT) in deep learning methods. First we build a monolingual corpus of Urdu and Chinese languages, after that we train our model using neural machine translation (NMT) and then compare the data-test result to accurate translation with the help of BLEU score method.</jats:p>","machine translation (mt) is a subtype of computational linguistics that uses to implement the translation between different natural languages (nl). simply word to word exchanging on machine translation is not enough to give desire result. neural machine translation is one of the standard methods of machine learning which make a huge improvement in recent time especially in local and some national languages. however these languages translation are not enough and need to focus on it. in this research we translate urdu to chinese language with the help of neural machine translation (nmt) in deep learning methods. first we build a monolingual corpus of urdu and chinese languages, after that we train our model using neural machine translation (nmt) and then compare the data-test result to accurate translation with the help of bleu score method."
http://orkg.org/orkg/resource/R193694,Assessing supply chain risk for apparel production in low cost countries using newsfeed analysis,10.1108/scm-11-2019-0423,crossref,"<jats:sec>\n<jats:title content-type=""abstract-subheading"">Purpose</jats:title>\n<jats:p>With the growth of unstructured data, opportunities to generate insights into supply chain risks in low cost countries (LCCs) are emerging. Sourcing risk has primarily focused on short-term mitigation. This paper aims to offer an approach that uses newsfeed data to assess regional supply base risk in LCC’s for the apparel sector, which managers can use to plan for future risk on a long-term planning horizon.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>\n<jats:p>This paper demonstrates that the bulk of supplier risk assessments focus on short-term responses to disruptions in developed countries, revealing a gap in assessments of long-term risks for supply base expansion in LCCs. This paper develops an approach for predicting and planning for long-term supply base risk in LCC’s to address this shortfall. A machine-based learning algorithm is developed that uses the analysis of competing hypotheses heuristic to convert data from multiple news feeds into numerical risk scores and visual maps of supply chain risk. This paper demonstrates the approach by converting large amounts of unstructured data into two measures, risk impact and risk probability, leading to visualization of country-level supply base risks for a global apparel company.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Findings</jats:title>\n<jats:p>This paper produced probability and impact scores for 23 distinct supply base risks across 10 countries in the apparel sector. The results suggest that the most significant long-term risks of supply disruption for apparel in LCC’s are human resource regulatory risks, workplace issues, inflation costs, safety violations and social welfare violations. The results suggest that apparel brands seeking suppliers in the regions of Cambodia, India, Bangladesh, Brazil and Vietnam should be aware of the significant risks in these regions that may require mitigative action.</jats:p>\n</jats:sec>\n<jats:sec>\n<jats:title content-type=""abstract-subheading"">Originality/value</jats:title>\n<jats:p>This approach establishes a novel approach for objectively projecting future global sourcing risk, and yields visually mapped outcomes that can be applied in forecasting and planning for future risks when considering sourcing locations in LCC’s.</jats:p>\n</jats:sec>","\n purpose \n with the growth of unstructured data, opportunities to generate insights into supply chain risks in low cost countries (lccs) are emerging. sourcing risk has primarily focused on short-term mitigation. this paper aims to offer an approach that uses newsfeed data to assess regional supply base risk in lcc’s for the apparel sector, which managers can use to plan for future risk on a long-term planning horizon. \n \n \n design/methodology/approach \n this paper demonstrates that the bulk of supplier risk assessments focus on short-term responses to disruptions in developed countries, revealing a gap in assessments of long-term risks for supply base expansion in lccs. this paper develops an approach for predicting and planning for long-term supply base risk in lcc’s to address this shortfall. a machine-based learning algorithm is developed that uses the analysis of competing hypotheses heuristic to convert data from multiple news feeds into numerical risk scores and visual maps of supply chain risk. this paper demonstrates the approach by converting large amounts of unstructured data into two measures, risk impact and risk probability, leading to visualization of country-level supply base risks for a global apparel company. \n \n \n findings \n this paper produced probability and impact scores for 23 distinct supply base risks across 10 countries in the apparel sector. the results suggest that the most significant long-term risks of supply disruption for apparel in lcc’s are human resource regulatory risks, workplace issues, inflation costs, safety violations and social welfare violations. the results suggest that apparel brands seeking suppliers in the regions of cambodia, india, bangladesh, brazil and vietnam should be aware of the significant risks in these regions that may require mitigative action. \n \n \n originality/value \n this approach establishes a novel approach for objectively projecting future global sourcing risk, and yields visually mapped outcomes that can be applied in forecasting and planning for future risks when considering sourcing locations in lcc’s. \n"
http://orkg.org/orkg/resource/R193691,Do blockchain and circular economy practices improve post COVID-19 supply chains? A resource-based and resource dependence perspective,10.1108/imds-09-2020-0560,crossref,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>Using the resource-based and the resource dependence theoretical approaches of the firm, the paper explores firm responses to supply chain disruptions during COVID-19. The paper explores how firms develop localization, agility and digitization (L-A-D) capabilities by applying (or not applying) their critical circular economy (CE) and blockchain technology (BCT)-related resources and capabilities that they either already possess or acquire from external agents.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>An abductive approach, applying exploratory qualitative research was conducted over a sample of 24 firms. The sample represented different industries to study their critical BCT and CE resources and capabilities and the L-A-D capabilities. Firm resources and capabilities were classified using the technology, organization and environment (TOE) framework.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>Findings show significant patterns on adoption levels of the blockchain-enabled circular economy system (BCES) and L-A-D capability development. The greater the BCES adoption capabilities, the greater the L-A-D capabilities. Organizational size and industry both influence the relationship between BCES and L-A-D. Accordingly, research propositions and a research framework are proposed.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title><jats:p>Given the limited sample size, the generalizability of the findings is limited. Our findings extend supply chain resiliency research. A series of propositions provide opportunities for future research. The resource-based view and resource-dependency theories are useful frameworks to better understanding the relationship between firm resources and supply chain resilience.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Practical implications</jats:title><jats:p>The results and discussion of this study serve as useful guidance for practitioners to create CE and BCT resources and capabilities for improving supply chain resiliency.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Social implications</jats:title><jats:p>The study shows the socio-economic and socio-environmental importance of BCES in the COVID-19 or similar crises.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>The study is one of the initial attempts that highlights the possibilities of BCES across multiple industries and their value during pandemics and disruptions.</jats:p></jats:sec>","purpose using the resource-based and the resource dependence theoretical approaches of the firm, the paper explores firm responses to supply chain disruptions during covid-19. the paper explores how firms develop localization, agility and digitization (l-a-d) capabilities by applying (or not applying) their critical circular economy (ce) and blockchain technology (bct)-related resources and capabilities that they either already possess or acquire from external agents. design/methodology/approach an abductive approach, applying exploratory qualitative research was conducted over a sample of 24 firms. the sample represented different industries to study their critical bct and ce resources and capabilities and the l-a-d capabilities. firm resources and capabilities were classified using the technology, organization and environment (toe) framework. findings findings show significant patterns on adoption levels of the blockchain-enabled circular economy system (bces) and l-a-d capability development. the greater the bces adoption capabilities, the greater the l-a-d capabilities. organizational size and industry both influence the relationship between bces and l-a-d. accordingly, research propositions and a research framework are proposed. research limitations/implications given the limited sample size, the generalizability of the findings is limited. our findings extend supply chain resiliency research. a series of propositions provide opportunities for future research. the resource-based view and resource-dependency theories are useful frameworks to better understanding the relationship between firm resources and supply chain resilience. practical implications the results and discussion of this study serve as useful guidance for practitioners to create ce and bct resources and capabilities for improving supply chain resiliency. social implications the study shows the socio-economic and socio-environmental importance of bces in the covid-19 or similar crises. originality/value the study is one of the initial attempts that highlights the possibilities of bces across multiple industries and their value during pandemics and disruptions."
http://orkg.org/orkg/resource/R193727,An abstractive approach to sentence compression,,crossref,"<jats:p>In this article we generalize the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present an experimental study showing that humans can naturally create abstractive sentences using a variety of rewrite operations, not just deletion. We next create a new corpus that is suited to the abstractive compression task and formulate a discriminative tree-to-tree transduction model that can account for structural and lexical mismatches. The model incorporates a grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression-specific loss functions.</jats:p>","in this article we generalize the sentence compression task. rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. we present an experimental study showing that humans can naturally create abstractive sentences using a variety of rewrite operations, not just deletion. we next create a new corpus that is suited to the abstractive compression task and formulate a discriminative tree-to-tree transduction model that can account for structural and lexical mismatches. the model incorporates a grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression-specific loss functions."
http://orkg.org/orkg/resource/R194816,What Will Summer Look Like? Summer Learning Loss and COVID-19 Learning Gaps,10.5860/cal.19.2.3,crossref,"<jats:p>Summer 2021 will likely look much different than previous summers due to the impact of the now more than one-year-long pandemic.Here we share research about summer learning loss and overlap that with emerging studies illustrating how COVID-19 closures and remote learning have compounded learning loss, all of which disproportionately impacts Black children, indigenous children, children of color, and all children who live in poverty.</jats:p>","summer 2021 will likely look much different than previous summers due to the impact of the now more than one-year-long pandemic.here we share research about summer learning loss and overlap that with emerging studies illustrating how covid-19 closures and remote learning have compounded learning loss, all of which disproportionately impacts black children, indigenous children, children of color, and all children who live in poverty."
http://orkg.org/orkg/resource/R193966,How Is Wildlife Affected by the COVID-19 Pandemic? Lockdown Effect on the Road Mortality of Hedgehogs,10.3390/ani11030868,crossref,"<jats:p>Globally, wildlife is affected by unprecedented changes related to the COVID-19 pandemic. In this paper, the lockdown effect on the traffic-related mortality in hedgehogs in an urban area was studied. Comparing the pre-pandemic (2018 and 2019) and pandemic (2020) years, we showed that hedgehog roadkill levels during the lockdown period were over 50% lower (which means a decrease greater than the decrease in road traffic in the same period measured by the number of accidents or the average number of vehicles per day). Based on literature data, we showed that this may mean at least tens of thousands of hedgehogs have survived on a national scale. We report the need to start intensive research on the possible demographic and genetic effects of this unique phenomenon. We also ask how stable the effect of the COVID-19 pandemic will be on wildlife and whether the lockdown (which is an anthropause) may reverse the negative trends in the decline in the number of wild species, including hedgehogs.</jats:p>","globally, wildlife is affected by unprecedented changes related to the covid-19 pandemic. in this paper, the lockdown effect on the traffic-related mortality in hedgehogs in an urban area was studied. comparing the pre-pandemic (2018 and 2019) and pandemic (2020) years, we showed that hedgehog roadkill levels during the lockdown period were over 50% lower (which means a decrease greater than the decrease in road traffic in the same period measured by the number of accidents or the average number of vehicles per day). based on literature data, we showed that this may mean at least tens of thousands of hedgehogs have survived on a national scale. we report the need to start intensive research on the possible demographic and genetic effects of this unique phenomenon. we also ask how stable the effect of the covid-19 pandemic will be on wildlife and whether the lockdown (which is an anthropause) may reverse the negative trends in the decline in the number of wild species, including hedgehogs."
http://orkg.org/orkg/resource/R194882,Electrochemical CO2 Reduction to CO Catalyzed by 2D Nanostructures,10.3390/catal10010098,crossref,"<jats:p>Electrochemical CO2 reduction towards value-added chemical feedstocks has been extensively studied in recent years to resolve the energy and environmental problems. The practical application of electrochemical CO2 reduction technology requires a cost-effective, highly efficient, and robust catalyst. To date, vigorous research have been carried out to increase the proficiency of electrocatalysts. In recent years, two-dimensional (2D) graphene and transition metal chalcogenides (TMCs) have displayed excellent activity towards CO2 reduction. This review focuses on the recent progress of 2D graphene and TMCs for selective electrochemical CO2 reduction into CO.</jats:p>","electrochemical co2 reduction towards value-added chemical feedstocks has been extensively studied in recent years to resolve the energy and environmental problems. the practical application of electrochemical co2 reduction technology requires a cost-effective, highly efficient, and robust catalyst. to date, vigorous research have been carried out to increase the proficiency of electrocatalysts. in recent years, two-dimensional (2d) graphene and transition metal chalcogenides (tmcs) have displayed excellent activity towards co2 reduction. this review focuses on the recent progress of 2d graphene and tmcs for selective electrochemical co2 reduction into co."
http://orkg.org/orkg/resource/R194718,MQALD: Evaluating the impact of modifiers in question answering over knowledge graphs,10.3233/sw-210440,crossref,"<jats:p>Question Answering (QA) over Knowledge Graphs (KG) aims to develop a system that is capable of answering users’ questions using the information coming from one or multiple Knowledge Graphs, like DBpedia, Wikidata, and so on. Question Answering systems need to translate the user’s question, written using natural language, into a query formulated through a specific data query language that is compliant with the underlying KG. This translation process is already non-trivial when trying to answer simple questions that involve a single triple pattern. It becomes even more troublesome when trying to cope with questions that require modifiers in the final query, i.e., aggregate functions, query forms, and so on. The attention over this last aspect is growing but has never been thoroughly addressed by the existing literature. Starting from the latest advances in this field, we want to further step in this direction. This work aims to provide a publicly available dataset designed for evaluating the performance of a QA system in translating articulated questions into a specific data query language. This dataset has also been used to evaluate three QA systems available at the state of the art.</jats:p>","question answering (qa) over knowledge graphs (kg) aims to develop a system that is capable of answering users’ questions using the information coming from one or multiple knowledge graphs, like dbpedia, wikidata, and so on. question answering systems need to translate the user’s question, written using natural language, into a query formulated through a specific data query language that is compliant with the underlying kg. this translation process is already non-trivial when trying to answer simple questions that involve a single triple pattern. it becomes even more troublesome when trying to cope with questions that require modifiers in the final query, i.e., aggregate functions, query forms, and so on. the attention over this last aspect is growing but has never been thoroughly addressed by the existing literature. starting from the latest advances in this field, we want to further step in this direction. this work aims to provide a publicly available dataset designed for evaluating the performance of a qa system in translating articulated questions into a specific data query language. this dataset has also been used to evaluate three qa systems available at the state of the art."
http://orkg.org/orkg/resource/R195387,FOBI: an ontology to represent food intake data and associate it with metabolomic data,10.1093/databa/baaa033,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Nutrition research can be conducted by using two complementary approaches: (i) traditional self-reporting methods or (ii) via metabolomics techniques to analyze food intake biomarkers in biofluids. However, the complexity and heterogeneity of these two very different types of data often hinder their analysis and integration. To manage this challenge, we have developed a novel ontology that describes food and their associated metabolite entities in a hierarchical way. This ontology uses a formal naming system, category definitions, properties and relations between both types of data. The ontology presented is called FOBI (Food-Biomarker Ontology) and it is composed of two interconnected sub-ontologies. One is a ’Food Ontology’ consisting of raw foods and ‘multi-component foods’ while the second is a ‘Biomarker Ontology’ containing food intake biomarkers classified by their chemical classes. These two sub-ontologies are conceptually independent but interconnected by different properties. This allows data and information regarding foods and food biomarkers to be visualized in a bidirectional way, going from metabolomics to nutritional data or vice versa. Potential applications of this ontology include the annotation of foods and biomarkers using a well-defined and consistent nomenclature, the standardized reporting of metabolomics workflows (e.g. metabolite identification, experimental design) or the application of different enrichment analysis approaches to analyze nutrimetabolomic data. Availability: FOBI is freely available in both OWL (Web Ontology Language) and OBO (Open Biomedical Ontologies) formats at the project’s Github repository (https://github.com/pcastellanoescuder/FoodBiomarkerOntology) and FOBI visualization tool is available in https://polcastellano.shinyapps.io/FOBI_Visualization_Tool/.</jats:p>","abstract \n nutrition research can be conducted by using two complementary approaches: (i) traditional self-reporting methods or (ii) via metabolomics techniques to analyze food intake biomarkers in biofluids. however, the complexity and heterogeneity of these two very different types of data often hinder their analysis and integration. to manage this challenge, we have developed a novel ontology that describes food and their associated metabolite entities in a hierarchical way. this ontology uses a formal naming system, category definitions, properties and relations between both types of data. the ontology presented is called fobi (food-biomarker ontology) and it is composed of two interconnected sub-ontologies. one is a ’food ontology’ consisting of raw foods and ‘multi-component foods’ while the second is a ‘biomarker ontology’ containing food intake biomarkers classified by their chemical classes. these two sub-ontologies are conceptually independent but interconnected by different properties. this allows data and information regarding foods and food biomarkers to be visualized in a bidirectional way, going from metabolomics to nutritional data or vice versa. potential applications of this ontology include the annotation of foods and biomarkers using a well-defined and consistent nomenclature, the standardized reporting of metabolomics workflows (e.g. metabolite identification, experimental design) or the application of different enrichment analysis approaches to analyze nutrimetabolomic data. availability: fobi is freely available in both owl (web ontology language) and obo (open biomedical ontologies) formats at the project’s github repository (https://github.com/pcastellanoescuder/foodbiomarkerontology) and fobi visualization tool is available in https://polcastellano.shinyapps.io/fobi_visualization_tool/."
http://orkg.org/orkg/resource/R195718,NAct: The Nutrition &amp; Activity Ontology for Healthy Living,10.3233/faia210377,crossref,"<jats:p>This paper presents the NAct (Nutrition &amp; Activity) Ontology, designed to drive personalised nutritional and physical activity recommendations and effectively support healthy living, through a reasoning-based AI decision support system. NAct coalesces nutritional, medical, behavioural and lifestyle indicators with potential dietary and physical activity directives. The paper presents the first version of the ontology, including its co-design and engineering methodology, along with usage examples in supporting healthy nutritional and physical activity choices. Lastly, the plan for future improvements and extensions is discussed.</jats:p>","this paper presents the nact (nutrition &amp; activity) ontology, designed to drive personalised nutritional and physical activity recommendations and effectively support healthy living, through a reasoning-based ai decision support system. nact coalesces nutritional, medical, behavioural and lifestyle indicators with potential dietary and physical activity directives. the paper presents the first version of the ontology, including its co-design and engineering methodology, along with usage examples in supporting healthy nutritional and physical activity choices. lastly, the plan for future improvements and extensions is discussed."
http://orkg.org/orkg/resource/R195906,The Automatic Detection of Dataset Names in Scientific Articles,10.3390/data6080084,crossref,"<jats:p>We study the task of recognizing named datasets in scientific articles as a Named Entity Recognition (NER) problem. Noticing that available annotated datasets were not adequate for our goals, we annotated 6000 sentences extracted from four major AI conferences, with roughly half of them containing one or more named datasets. A distinguishing feature of this set is the many sentences using enumerations, conjunctions and ellipses, resulting in long BI+ tag sequences. On all measures, the SciBERT NER tagger performed best and most robustly. Our baseline rule based tagger performed remarkably well and better than several state-of-the-art methods. The gold standard dataset, with links and offsets from each sentence to the (open access available) articles together with the annotation guidelines and all code used in the experiments, is available on GitHub.</jats:p>","we study the task of recognizing named datasets in scientific articles as a named entity recognition (ner) problem. noticing that available annotated datasets were not adequate for our goals, we annotated 6000 sentences extracted from four major ai conferences, with roughly half of them containing one or more named datasets. a distinguishing feature of this set is the many sentences using enumerations, conjunctions and ellipses, resulting in long bi+ tag sequences. on all measures, the scibert ner tagger performed best and most robustly. our baseline rule based tagger performed remarkably well and better than several state-of-the-art methods. the gold standard dataset, with links and offsets from each sentence to the (open access available) articles together with the annotation guidelines and all code used in the experiments, is available on github."
http://orkg.org/orkg/resource/R195176,Bi-bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis,10.1145/1122445.1122456,crossref,"<jats:p>Recently, Generative Adversarial Networks (GANs) have received enormous progress, which makes them able to learn complex data distributions in particular faces. More and more efficient GAN architectures have been designed and proposed to learn the different variations of faces, such as cross pose, age, expression and style. These GAN based approaches need to be reviewed, discussed, and categorized in terms of architectures, applications, and metrics. Several reviews that focus on the use and advances of GAN in general have been proposed. However, the GAN models applied to the face, that we call facial GANs, have never been addressed. In this article, we review facial GANs and their different applications. We mainly focus on architectures, problems and performance evaluation with respect to each application and used datasets. More precisely, we reviewed the progress of architectures and we discussed the contributions and limits of each. Then, we exposed the encountered problems of facial GANs and proposed solutions to handle them. Additionally, as GANs evaluation has become a notable current defiance, we investigate the state of the art quantitative and qualitative evaluation metrics and their applications. We concluded the article with a discussion on the face generation challenges and proposed open research issues.</jats:p>","recently, generative adversarial networks (gans) have received enormous progress, which makes them able to learn complex data distributions in particular faces. more and more efficient gan architectures have been designed and proposed to learn the different variations of faces, such as cross pose, age, expression and style. these gan based approaches need to be reviewed, discussed, and categorized in terms of architectures, applications, and metrics. several reviews that focus on the use and advances of gan in general have been proposed. however, the gan models applied to the face, that we call facial gans, have never been addressed. in this article, we review facial gans and their different applications. we mainly focus on architectures, problems and performance evaluation with respect to each application and used datasets. more precisely, we reviewed the progress of architectures and we discussed the contributions and limits of each. then, we exposed the encountered problems of facial gans and proposed solutions to handle them. additionally, as gans evaluation has become a notable current defiance, we investigate the state of the art quantitative and qualitative evaluation metrics and their applications. we concluded the article with a discussion on the face generation challenges and proposed open research issues."
http://orkg.org/orkg/resource/R195868,Boosting L2 Listening Comprehension Through Web-Based Listening Activities on Reduced Forms,10.4018/978-1-7998-7876-6.ch006,crossref,"""<jats:p>In the field of second language (L2) perception, there is a common adherence to quantitative methods to examine reduced forms (RFs). This chapter extends the field by reporting on an investigation that analyzed L2 listeners' perceptions of RFs in English from a qualitative perspective. RFs instruction through web-based activities was delivered to a total of 80 learners of English of varying proficiency for five weeks. Twenty participants reflected on their performance on RFs listening tasks and provided justifications for their perceptions of the target RFs. Qualitative analysis revealed that the RFs that influenced L2 learners' perceptions of RFs were linking, pause phenomena, and assimilation. The results of using such qualitative methodology highlights the important role that RFs plays in perception judgements in syllable-timed languages such as Turkish, a factor which has not always been given much prominence in previous L2 fluency quantitative research. </jats:p>""",""" in the field of second language (l2) perception, there is a common adherence to quantitative methods to examine reduced forms (rfs). this chapter extends the field by reporting on an investigation that analyzed l2 listeners' perceptions of rfs in english from a qualitative perspective. rfs instruction through web-based activities was delivered to a total of 80 learners of english of varying proficiency for five weeks. twenty participants reflected on their performance on rfs listening tasks and provided justifications for their perceptions of the target rfs. qualitative analysis revealed that the rfs that influenced l2 learners' perceptions of rfs were linking, pause phenomena, and assimilation. the results of using such qualitative methodology highlights the important role that rfs plays in perception judgements in syllable-timed languages such as turkish, a factor which has not always been given much prominence in previous l2 fluency quantitative research. """
http://orkg.org/orkg/resource/R195612,Road Traffic Injury During the COVID-19 Pandemic: Cured or a Continued Threat?,10.33137/utjph.v2i1.34737,crossref,"<jats:p>Road traffic injury, one of the leading causes of preventable morbidity and mortality in Canada, declined substantially as an indirect outcome of the first wave of the COVID-19 pandemic. Public health policies encouraging people to ‘stay at home’ and ‘practice physical distancing’ precipitated shifts in vehicle volumes and speed, transportation mode, and collision rates. Toronto data from January to June 2020 showed a decrease in road transportation, and a simultaneous decrease in road traffic collisions. However, reduced traffic volumes also led to increased vehicle speeds which can result in an increase in injury severity involving pedestrians and cyclists. As the pandemic progresses, an emphasis on safe, active transportation and equitable distribution of street infrastructure throughout the city is essential. A public health approach to road safety includes implementation of evidence-based road safety infrastructure enabled by access to timely transportation data to evaluate changes made.</jats:p>","road traffic injury, one of the leading causes of preventable morbidity and mortality in canada, declined substantially as an indirect outcome of the first wave of the covid-19 pandemic. public health policies encouraging people to ‘stay at home’ and ‘practice physical distancing’ precipitated shifts in vehicle volumes and speed, transportation mode, and collision rates. toronto data from january to june 2020 showed a decrease in road transportation, and a simultaneous decrease in road traffic collisions. however, reduced traffic volumes also led to increased vehicle speeds which can result in an increase in injury severity involving pedestrians and cyclists. as the pandemic progresses, an emphasis on safe, active transportation and equitable distribution of street infrastructure throughout the city is essential. a public health approach to road safety includes implementation of evidence-based road safety infrastructure enabled by access to timely transportation data to evaluate changes made."
http://orkg.org/orkg/resource/R195616,"Emission Variations of Primary Air Pollutants from Highway Vehicles and Implications during the COVID-19 Pandemic in Beijing, China",10.3390/ijerph18084019,crossref,"<jats:p>According to the traffic flow variation from January 2019 to August 2020, emissions of primary air pollutants from highway vehicles were calculated based on the emission factor method, which integrated the actual structure of on-road vehicles. The characteristics of on-highway traffic flow and pollution emissions were compared during various progression stages of coronavirus disease (COVID-19). The results showed that the average daily traffic volume decreased by 38.2% in 2020, with a decrease of 62% during the strict lockdown due to the impact of COVID-19. The daily emissions of primary atmospheric pollutants decreased by 29.2% in 2020 compared to the same period in 2019. As for the structure of on-highway vehicle types, the small and medium-sized passenger vehicles predominated, which accounted for 76.3% of traffic, while trucks and large passenger vehicles accounted for 19.7% and 4.0%, but contributed 58.4% and 33.9% of nitrogen oxide (NOx) emissions, respectively. According to the simulation results of the ADMS model, the average concentrations of NOx were reduced by 12.0 µg/m3 compared with the same period in 2019. As for the implication for future pollution control, it is necessary to further optimize the structure of on-highway and the road traffic vehicle types and increase the proportions of new-energy vehicles and vehicles with high emission standards.</jats:p>","according to the traffic flow variation from january 2019 to august 2020, emissions of primary air pollutants from highway vehicles were calculated based on the emission factor method, which integrated the actual structure of on-road vehicles. the characteristics of on-highway traffic flow and pollution emissions were compared during various progression stages of coronavirus disease (covid-19). the results showed that the average daily traffic volume decreased by 38.2% in 2020, with a decrease of 62% during the strict lockdown due to the impact of covid-19. the daily emissions of primary atmospheric pollutants decreased by 29.2% in 2020 compared to the same period in 2019. as for the structure of on-highway vehicle types, the small and medium-sized passenger vehicles predominated, which accounted for 76.3% of traffic, while trucks and large passenger vehicles accounted for 19.7% and 4.0%, but contributed 58.4% and 33.9% of nitrogen oxide (nox) emissions, respectively. according to the simulation results of the adms model, the average concentrations of nox were reduced by 12.0 µg/m3 compared with the same period in 2019. as for the implication for future pollution control, it is necessary to further optimize the structure of on-highway and the road traffic vehicle types and increase the proportions of new-energy vehicles and vehicles with high emission standards."
http://orkg.org/orkg/resource/R195876,The Effect of Audio and Video Modality on Perception of Reduced Forms: The Role of Web-Based Instruction – Reduced Forms in an L2 Listening Context,10.4018/978-1-7998-7876-6.ch005,crossref,"<jats:p>Reduced forms (RFs) spoken by native English speakers have been challenging on the part of the second language (L2) learners. This chapter aims to address suprasegmental features to Turkish preparatory language school students in relation to L2 listening comprehension. Considering the limited research on RFs in learning English as a L2 context, this pre-test post-test control group design study aimed to explore whether the instruction of five RFs in sentential level results in any difference in listening comprehension test performance. The five forms entail contraction, assimilation, flap, elision, and linking. A total of 343 were recruited, and RFs instruction was delivered through the web page designated for the study for five weeks, and the performance of the eight groups was measured twice throughout the study. The findings indicated that sentence level of RFs instruction through web-based learning facilitates the listening comprehension of RFs. </jats:p>","reduced forms (rfs) spoken by native english speakers have been challenging on the part of the second language (l2) learners. this chapter aims to address suprasegmental features to turkish preparatory language school students in relation to l2 listening comprehension. considering the limited research on rfs in learning english as a l2 context, this pre-test post-test control group design study aimed to explore whether the instruction of five rfs in sentential level results in any difference in listening comprehension test performance. the five forms entail contraction, assimilation, flap, elision, and linking. a total of 343 were recruited, and rfs instruction was delivered through the web page designated for the study for five weeks, and the performance of the eight groups was measured twice throughout the study. the findings indicated that sentence level of rfs instruction through web-based learning facilitates the listening comprehension of rfs."
http://orkg.org/orkg/resource/R195970,The effect of dictionary training in the teaching of English as a foreign language,10.14198/raei.2011.24.02,crossref,"<jats:p>The process of teaching any subject involves the use of all resources available to achieve efficient/effective result. When dealing with teaching foreign languages, we have noticed that such a traditional and apparently well-known tool as a dictionary is not so well-known nor used efficiently by students. Nevertheless, in our opinion, the dictionary should play an important role in the teaching-learning process. Therefore, we have carried out a research in our classroom in order to check that if students work systematically with the dictionary, they will improve not only their dictionary skills, but also their L2 skills mainly in writing, reading comprehension and vocabulary. The research was done at two different levels during a whole academic year focusing on dictionary training and dictionary activities. Our work showed quite interesting outcomes: (a) students were aware of the usefulness of dictionaries because they learnt that dictionaries helped them; (b) students’ confidence to face activities increased; (c) students improved their language use and knowledge; and (d) they really enjoyed the tasks. As a general conclusion we can state that with better dictionary use students acquire autonomous learning strategies for learning a foreign language.</jats:p>","the process of teaching any subject involves the use of all resources available to achieve efficient/effective result. when dealing with teaching foreign languages, we have noticed that such a traditional and apparently well-known tool as a dictionary is not so well-known nor used efficiently by students. nevertheless, in our opinion, the dictionary should play an important role in the teaching-learning process. therefore, we have carried out a research in our classroom in order to check that if students work systematically with the dictionary, they will improve not only their dictionary skills, but also their l2 skills mainly in writing, reading comprehension and vocabulary. the research was done at two different levels during a whole academic year focusing on dictionary training and dictionary activities. our work showed quite interesting outcomes: (a) students were aware of the usefulness of dictionaries because they learnt that dictionaries helped them; (b) students’ confidence to face activities increased; (c) students improved their language use and knowledge; and (d) they really enjoyed the tasks. as a general conclusion we can state that with better dictionary use students acquire autonomous learning strategies for learning a foreign language."
http://orkg.org/orkg/resource/R196541,A Deep Learning Sentiment Analyser for Social Media Comments in Low-Resource Languages,10.3390/electronics10101133,crossref,"<jats:p>During the pandemic, when people needed to physically distance, social media platforms have been one of the outlets where people expressed their opinions, thoughts, sentiments, and emotions regarding the pandemic situation. The core object of this research study is the sentiment analysis of peoples’ opinions expressed on Facebook regarding the current pandemic situation in low-resource languages. To do this, we have created a large-scale dataset comprising of 10,742 manually classified comments in the Albanian language. Furthermore, in this paper we report our efforts on the design and development of a sentiment analyser that relies on deep learning. As a result, we report the experimental findings obtained from our proposed sentiment analyser using various classifier models with static and contextualized word embeddings, that is, fastText and BERT, trained and validated on our collected and curated dataset. Specifically, the findings reveal that combining the BiLSTM with an attention mechanism achieved the highest performance on our sentiment analysis task, with an F1 score of 72.09%.</jats:p>","during the pandemic, when people needed to physically distance, social media platforms have been one of the outlets where people expressed their opinions, thoughts, sentiments, and emotions regarding the pandemic situation. the core object of this research study is the sentiment analysis of peoples’ opinions expressed on facebook regarding the current pandemic situation in low-resource languages. to do this, we have created a large-scale dataset comprising of 10,742 manually classified comments in the albanian language. furthermore, in this paper we report our efforts on the design and development of a sentiment analyser that relies on deep learning. as a result, we report the experimental findings obtained from our proposed sentiment analyser using various classifier models with static and contextualized word embeddings, that is, fasttext and bert, trained and validated on our collected and curated dataset. specifically, the findings reveal that combining the bilstm with an attention mechanism achieved the highest performance on our sentiment analysis task, with an f1 score of 72.09%."
http://orkg.org/orkg/resource/R196261,"THE ROBUSTNESS OF CRITICAL PERIOD EFFECTS IN SECOND LANGUAGE
ACQUISITION",10.1017/s0272263100004022,crossref,"<jats:p>This study was designed to test the Fundamental Difference Hypothesis (Bley-Vroman,\n1988), which states that, whereas children are known to learn language almost completely\nthrough (implicit) domain-specific mechanisms, adults have largely lost the ability to learn a\nlanguage without reflecting on its structure and have to use alternative mechanisms, drawing\nespecially on their problem-solving capacities, to learn a second language. The hypothesis\nimplies that only adults with a high level of verbal analytical ability will reach near-native\ncompetence in their second language, but that this ability will not be a significant predictor of\nsuccess for childhood second language acquisition. A study with 57 adult Hungarian-speaking\nimmigrants confirmed the hypothesis in the sense that very few adult immigrants scored within\nthe range of child arrivals on a grammaticality judgment test, and that the few who did had high\nlevels of verbal analytical ability; this ability was not a significant predictor for childhood\narrivals. This study replicates the findings of Johnson and Newport (1989) and provides an\nexplanation for the apparent exceptions in their study. These findings lead to a\nreconceptualization of the Critical Period Hypothesis: If the scope of this hypothesis is limited to\nimplicit learning mechanisms, then it appears that there may be no exceptions to the age effects\nthat the hypothesis seeks to explain.</jats:p>","this study was designed to test the fundamental difference hypothesis (bley-vroman,\n1988), which states that, whereas children are known to learn language almost completely\nthrough (implicit) domain-specific mechanisms, adults have largely lost the ability to learn a\nlanguage without reflecting on its structure and have to use alternative mechanisms, drawing\nespecially on their problem-solving capacities, to learn a second language. the hypothesis\nimplies that only adults with a high level of verbal analytical ability will reach near-native\ncompetence in their second language, but that this ability will not be a significant predictor of\nsuccess for childhood second language acquisition. a study with 57 adult hungarian-speaking\nimmigrants confirmed the hypothesis in the sense that very few adult immigrants scored within\nthe range of child arrivals on a grammaticality judgment test, and that the few who did had high\nlevels of verbal analytical ability; this ability was not a significant predictor for childhood\narrivals. this study replicates the findings of johnson and newport (1989) and provides an\nexplanation for the apparent exceptions in their study. these findings lead to a\nreconceptualization of the critical period hypothesis: if the scope of this hypothesis is limited to\nimplicit learning mechanisms, then it appears that there may be no exceptions to the age effects\nthat the hypothesis seeks to explain."
http://orkg.org/orkg/resource/R196166,Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies,10.1162/tacl_a_00115,crossref,"<jats:p> The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture’s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured. </jats:p>","the success of long short-term memory (lstm) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by lstms, which do not have explicit structural representations? we begin addressing this question using number agreement in english subject-verb dependencies. we probe the architecture’s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. in the strongly supervised settings, the lstm achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. the frequency of such errors rose sharply in the language-modeling setting. we conclude that lstms can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured."
http://orkg.org/orkg/resource/R196664,Deep neural networks ensemble for detecting medication mentions in tweets,10.1093/jamia/ocz156,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Objective</jats:title><jats:p>Twitter posts are now recognized as an important source of patient-generated data, providing unique insights into population health. A fundamental step toward incorporating Twitter data in pharmacoepidemiologic research is to automatically recognize medication mentions in tweets. Given that lexical searches for medication names suffer from low recall due to misspellings or ambiguity with common words, we propose a more advanced method to recognize them.</jats:p></jats:sec><jats:sec><jats:title>Materials and Methods</jats:title><jats:p>We present Kusuri, an Ensemble Learning classifier able to identify tweets mentioning drug products and dietary supplements. Kusuri (薬, “medication” in Japanese) is composed of 2 modules: first, 4 different classifiers (lexicon based, spelling variant based, pattern based, and a weakly trained neural network) are applied in parallel to discover tweets potentially containing medication names; second, an ensemble of deep neural networks encoding morphological, semantic, and long-range dependencies of important words in the tweets makes the final decision.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>On a class-balanced (50-50) corpus of 15 005 tweets, Kusuri demonstrated performances close to human annotators with an F1 score of 93.7%, the best score achieved thus far on this corpus. On a corpus made of all tweets posted by 112 Twitter users (98 959 tweets, with only 0.26% mentioning medications), Kusuri obtained an F1 score of 78.8%. To the best of our knowledge, Kusuri is the first system to achieve this score on such an extremely imbalanced dataset.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>The system identifies tweets mentioning drug names with performance high enough to ensure its usefulness, and is ready to be integrated in pharmacovigilance, toxicovigilance, or more generally, public health pipelines that depend on medication name mentions.</jats:p></jats:sec>","abstract objective twitter posts are now recognized as an important source of patient-generated data, providing unique insights into population health. a fundamental step toward incorporating twitter data in pharmacoepidemiologic research is to automatically recognize medication mentions in tweets. given that lexical searches for medication names suffer from low recall due to misspellings or ambiguity with common words, we propose a more advanced method to recognize them. materials and methods we present kusuri, an ensemble learning classifier able to identify tweets mentioning drug products and dietary supplements. kusuri (薬, “medication” in japanese) is composed of 2 modules: first, 4 different classifiers (lexicon based, spelling variant based, pattern based, and a weakly trained neural network) are applied in parallel to discover tweets potentially containing medication names; second, an ensemble of deep neural networks encoding morphological, semantic, and long-range dependencies of important words in the tweets makes the final decision. results on a class-balanced (50-50) corpus of 15 005 tweets, kusuri demonstrated performances close to human annotators with an f1 score of 93.7%, the best score achieved thus far on this corpus. on a corpus made of all tweets posted by 112 twitter users (98 959 tweets, with only 0.26% mentioning medications), kusuri obtained an f1 score of 78.8%. to the best of our knowledge, kusuri is the first system to achieve this score on such an extremely imbalanced dataset. conclusions the system identifies tweets mentioning drug names with performance high enough to ensure its usefulness, and is ready to be integrated in pharmacovigilance, toxicovigilance, or more generally, public health pipelines that depend on medication name mentions."
http://orkg.org/orkg/resource/R196605,Sentiment Analysis of Persian Movie Reviews Using Deep Learning,10.3390/e23050596,crossref,"<jats:p>Sentiment analysis aims to automatically classify the subject’s sentiment (e.g., positive, negative, or neutral) towards a particular aspect such as a topic, product, movie, news, etc. Deep learning has recently emerged as a powerful machine learning technique to tackle the growing demand for accurate sentiment analysis. However, the majority of research efforts are devoted to English-language only, while information of great importance is also available in other languages. This paper presents a novel, context-aware, deep-learning-driven, Persian sentiment analysis approach. Specifically, the proposed deep-learning-driven automated feature-engineering approach classifies Persian movie reviews as having positive or negative sentiments. Two deep learning algorithms, convolutional neural networks (CNN) and long-short-term memory (LSTM), are applied and compared with our previously proposed manual-feature-engineering-driven, SVM-based approach. Simulation results demonstrate that LSTM obtained a better performance as compared to multilayer perceptron (MLP), autoencoder, support vector machine (SVM), logistic regression and CNN algorithms.</jats:p>","sentiment analysis aims to automatically classify the subject’s sentiment (e.g., positive, negative, or neutral) towards a particular aspect such as a topic, product, movie, news, etc. deep learning has recently emerged as a powerful machine learning technique to tackle the growing demand for accurate sentiment analysis. however, the majority of research efforts are devoted to english-language only, while information of great importance is also available in other languages. this paper presents a novel, context-aware, deep-learning-driven, persian sentiment analysis approach. specifically, the proposed deep-learning-driven automated feature-engineering approach classifies persian movie reviews as having positive or negative sentiments. two deep learning algorithms, convolutional neural networks (cnn) and long-short-term memory (lstm), are applied and compared with our previously proposed manual-feature-engineering-driven, svm-based approach. simulation results demonstrate that lstm obtained a better performance as compared to multilayer perceptron (mlp), autoencoder, support vector machine (svm), logistic regression and cnn algorithms."
http://orkg.org/orkg/resource/R196615,Heterogeneous Social Linked Data Integration and Sharing for Public Transportation,10.1155/2022/6338365,crossref,"<jats:p>Solid (social linked data) technology has made significant progress in social web applications developed, such as Facebook, Twitter, and Wikipedia. Solid is based on semantic web and RDF (Resource Description Framework) technologies. Solid platforms can provide decentralized authentication, data management, and developer support in the form of libraries and web applications. However, thus far, little research has been conducted on understanding the problems involved in sharing public transportation data through Solid technology. It is challenging to provide personalized and adaptable public transportation services for citizens because the public transportation data originate from different devices and are heterogeneous in nature. A novel approach is proposed in this study, in order to provide personalized sharing of public transportation data between different users through integrating and sharing these heterogeneous data. This approach not only integrates diverse data types into a uniform data type using the semantic web but also stores these data in a personal online data store and retrieves data through SPARQL on the Solid platform; these data are visualized on the web pages using Google Maps. To the best of our knowledge, we are the first to apply Solid in public transportation. Furthermore, we conduct performance tests of the new C2RMF (CSV to RDF Mapping File) algorithm and functional and non-functional tests to demonstrate the stability and effectiveness of the approach. Our results indicate the feasibility of the proposed approach in facilitating public transportation data integration and sharing through Solid and semantic web technologies.</jats:p>","solid (social linked data) technology has made significant progress in social web applications developed, such as facebook, twitter, and wikipedia. solid is based on semantic web and rdf (resource description framework) technologies. solid platforms can provide decentralized authentication, data management, and developer support in the form of libraries and web applications. however, thus far, little research has been conducted on understanding the problems involved in sharing public transportation data through solid technology. it is challenging to provide personalized and adaptable public transportation services for citizens because the public transportation data originate from different devices and are heterogeneous in nature. a novel approach is proposed in this study, in order to provide personalized sharing of public transportation data between different users through integrating and sharing these heterogeneous data. this approach not only integrates diverse data types into a uniform data type using the semantic web but also stores these data in a personal online data store and retrieves data through sparql on the solid platform; these data are visualized on the web pages using google maps. to the best of our knowledge, we are the first to apply solid in public transportation. furthermore, we conduct performance tests of the new c2rmf (csv to rdf mapping file) algorithm and functional and non-functional tests to demonstrate the stability and effectiveness of the approach. our results indicate the feasibility of the proposed approach in facilitating public transportation data integration and sharing through solid and semantic web technologies."
http://orkg.org/orkg/resource/R196369,"THE ROBUSTNESS OF CRITICAL PERIOD EFFECTS IN SECOND LANGUAGE
ACQUISITION",10.1017/s0272263100004022,crossref,"<jats:p>This study was designed to test the Fundamental Difference Hypothesis (Bley-Vroman,\n1988), which states that, whereas children are known to learn language almost completely\nthrough (implicit) domain-specific mechanisms, adults have largely lost the ability to learn a\nlanguage without reflecting on its structure and have to use alternative mechanisms, drawing\nespecially on their problem-solving capacities, to learn a second language. The hypothesis\nimplies that only adults with a high level of verbal analytical ability will reach near-native\ncompetence in their second language, but that this ability will not be a significant predictor of\nsuccess for childhood second language acquisition. A study with 57 adult Hungarian-speaking\nimmigrants confirmed the hypothesis in the sense that very few adult immigrants scored within\nthe range of child arrivals on a grammaticality judgment test, and that the few who did had high\nlevels of verbal analytical ability; this ability was not a significant predictor for childhood\narrivals. This study replicates the findings of Johnson and Newport (1989) and provides an\nexplanation for the apparent exceptions in their study. These findings lead to a\nreconceptualization of the Critical Period Hypothesis: If the scope of this hypothesis is limited to\nimplicit learning mechanisms, then it appears that there may be no exceptions to the age effects\nthat the hypothesis seeks to explain.</jats:p>","this study was designed to test the fundamental difference hypothesis (bley-vroman,\n1988), which states that, whereas children are known to learn language almost completely\nthrough (implicit) domain-specific mechanisms, adults have largely lost the ability to learn a\nlanguage without reflecting on its structure and have to use alternative mechanisms, drawing\nespecially on their problem-solving capacities, to learn a second language. the hypothesis\nimplies that only adults with a high level of verbal analytical ability will reach near-native\ncompetence in their second language, but that this ability will not be a significant predictor of\nsuccess for childhood second language acquisition. a study with 57 adult hungarian-speaking\nimmigrants confirmed the hypothesis in the sense that very few adult immigrants scored within\nthe range of child arrivals on a grammaticality judgment test, and that the few who did had high\nlevels of verbal analytical ability; this ability was not a significant predictor for childhood\narrivals. this study replicates the findings of johnson and newport (1989) and provides an\nexplanation for the apparent exceptions in their study. these findings lead to a\nreconceptualization of the critical period hypothesis: if the scope of this hypothesis is limited to\nimplicit learning mechanisms, then it appears that there may be no exceptions to the age effects\nthat the hypothesis seeks to explain."
http://orkg.org/orkg/resource/R196372,"THE ROBUSTNESS OF CRITICAL PERIOD EFFECTS IN SECOND LANGUAGE
ACQUISITION",10.1017/s0272263100004022,crossref,"<jats:p>This study was designed to test the Fundamental Difference Hypothesis (Bley-Vroman,\n1988), which states that, whereas children are known to learn language almost completely\nthrough (implicit) domain-specific mechanisms, adults have largely lost the ability to learn a\nlanguage without reflecting on its structure and have to use alternative mechanisms, drawing\nespecially on their problem-solving capacities, to learn a second language. The hypothesis\nimplies that only adults with a high level of verbal analytical ability will reach near-native\ncompetence in their second language, but that this ability will not be a significant predictor of\nsuccess for childhood second language acquisition. A study with 57 adult Hungarian-speaking\nimmigrants confirmed the hypothesis in the sense that very few adult immigrants scored within\nthe range of child arrivals on a grammaticality judgment test, and that the few who did had high\nlevels of verbal analytical ability; this ability was not a significant predictor for childhood\narrivals. This study replicates the findings of Johnson and Newport (1989) and provides an\nexplanation for the apparent exceptions in their study. These findings lead to a\nreconceptualization of the Critical Period Hypothesis: If the scope of this hypothesis is limited to\nimplicit learning mechanisms, then it appears that there may be no exceptions to the age effects\nthat the hypothesis seeks to explain.</jats:p>","this study was designed to test the fundamental difference hypothesis (bley-vroman,\n1988), which states that, whereas children are known to learn language almost completely\nthrough (implicit) domain-specific mechanisms, adults have largely lost the ability to learn a\nlanguage without reflecting on its structure and have to use alternative mechanisms, drawing\nespecially on their problem-solving capacities, to learn a second language. the hypothesis\nimplies that only adults with a high level of verbal analytical ability will reach near-native\ncompetence in their second language, but that this ability will not be a significant predictor of\nsuccess for childhood second language acquisition. a study with 57 adult hungarian-speaking\nimmigrants confirmed the hypothesis in the sense that very few adult immigrants scored within\nthe range of child arrivals on a grammaticality judgment test, and that the few who did had high\nlevels of verbal analytical ability; this ability was not a significant predictor for childhood\narrivals. this study replicates the findings of johnson and newport (1989) and provides an\nexplanation for the apparent exceptions in their study. these findings lead to a\nreconceptualization of the critical period hypothesis: if the scope of this hypothesis is limited to\nimplicit learning mechanisms, then it appears that there may be no exceptions to the age effects\nthat the hypothesis seeks to explain."
http://orkg.org/orkg/resource/R196427,Infants' ability to consult the speaker for clues to word reference,10.1017/s0305000900008345,crossref,"""<jats:title>ABSTRACT</jats:title><jats:p>This research examines whether infants actively seek information from a speaker regarding the referent of the speaker's utterance. Forty-eight infants (in three age groups: 1;2–1;3, 1;4–1;5, and 1;6–1;7) heard novel labels for novel objects in two situations: follow-in labelling (the experimenter looked at and labelled the toy of the infant's focus) vs. discrepant labelling (the experimenter looked at and labelled a different toy than that of the infant's focus). Subsequently, half of the infants were asked comprehension questions (e.g. ‘Where's the <jats:italic>peri</jats:italic>?’). The other half were asked preference questions (e.g. ‘Where's the one you like?’), to ensure that their comprehension performance was not merely the result of preferential responding. The comprehension results revealed developmental change in both (<jats:italic>a</jats:italic>) infants' ability to establish new word-object mappings (infants aged 1;2–1;3 failed to establish stable word-object links even in follow-in labelling), and (<jats:italic>b</jats:italic>) infants' ability to pinpoint the correct referent during discrepant labelling (only infants aged 1;6–1;7 succeeded). Thus the period between 1;2 and 1;7 represents a time of change in infants' ability to establish new word-object mappings: infants are becoming increasingly adept at acquiring new labels under minimal learning conditions.</jats:p>""",""" abstract this research examines whether infants actively seek information from a speaker regarding the referent of the speaker's utterance. forty-eight infants (in three age groups: 1;2–1;3, 1;4–1;5, and 1;6–1;7) heard novel labels for novel objects in two situations: follow-in labelling (the experimenter looked at and labelled the toy of the infant's focus) vs. discrepant labelling (the experimenter looked at and labelled a different toy than that of the infant's focus). subsequently, half of the infants were asked comprehension questions (e.g. ‘where's the peri ?’). the other half were asked preference questions (e.g. ‘where's the one you like?’), to ensure that their comprehension performance was not merely the result of preferential responding. the comprehension results revealed developmental change in both ( a ) infants' ability to establish new word-object mappings (infants aged 1;2–1;3 failed to establish stable word-object links even in follow-in labelling), and ( b ) infants' ability to pinpoint the correct referent during discrepant labelling (only infants aged 1;6–1;7 succeeded). thus the period between 1;2 and 1;7 represents a time of change in infants' ability to establish new word-object mappings: infants are becoming increasingly adept at acquiring new labels under minimal learning conditions. """
http://orkg.org/orkg/resource/R196553,Domain Independent Automatic Labeling system for Large-scale Social Data using Lexicon and Web-based Augmentation,10.5755/j01.itc.49.1.23769,crossref,"<jats:p>Recently, with the large-scale adoption of social media, people have begun to express their opinion on these sites in the form of reviews. Potential consumers often forced to wade through huge amount of reviews to make informed decision. Sentiment analysis has become rapid and effective way to automatically gauge consumers’ opinion. However, such analysis often requires tedious process of manual tagging of large training examples or manually building a lexicon for the purpose of classifying reviews as positive or negative. In this paper, we present a method to automate the tedious process of labeling large textual data in an unsupervised, domain independent and scalable manner. The proposed method combines the lexicon-based and Web-based Point Wise Mutual Information (PMI) statistics to find the Semantic Orientation (SO) of opinion expressed in a review.\xa0 Based on proposed methods a system called Domain Independent Automatic Labeling System (DIALS) has been implemented, which takes collection of text from any domain as input and generates fully labeled dataset in an unsupervised and scalable manner. The result generated can be used to track and summarize online discussion and/or use to train any classifier in the next stage of development. The effectiveness of system is tested by comparing it with baseline machine learning and lexicon-based methods. Experiments on multi-domains dataset has shown that proposed method consistently shown improved recall and accuracy as compared to baseline machine learning and lexicon-based methods.\xa0\xa0\xa0</jats:p>","recently, with the large-scale adoption of social media, people have begun to express their opinion on these sites in the form of reviews. potential consumers often forced to wade through huge amount of reviews to make informed decision. sentiment analysis has become rapid and effective way to automatically gauge consumers’ opinion. however, such analysis often requires tedious process of manual tagging of large training examples or manually building a lexicon for the purpose of classifying reviews as positive or negative. in this paper, we present a method to automate the tedious process of labeling large textual data in an unsupervised, domain independent and scalable manner. the proposed method combines the lexicon-based and web-based point wise mutual information (pmi) statistics to find the semantic orientation (so) of opinion expressed in a review.\xa0 based on proposed methods a system called domain independent automatic labeling system (dials) has been implemented, which takes collection of text from any domain as input and generates fully labeled dataset in an unsupervised and scalable manner. the result generated can be used to track and summarize online discussion and/or use to train any classifier in the next stage of development. the effectiveness of system is tested by comparing it with baseline machine learning and lexicon-based methods. experiments on multi-domains dataset has shown that proposed method consistently shown improved recall and accuracy as compared to baseline machine learning and lexicon-based methods.\xa0\xa0\xa0"
http://orkg.org/orkg/resource/R196688,"Prediction of Soil Organic Carbon based on Landsat 8 Monthly NDVI Data for the Jianghan Plain in Hubei Province, China",10.3390/rs11141683,crossref,"<jats:p>High-precision maps of soil organic carbon (SOC) are beneficial for managing soil fertility and understanding the global carbon cycle. Digital soil mapping plays an important role in efficiently obtaining the spatial distribution of SOC, which contributes to precision agriculture. However, traditional soil-forming factors (i.e., terrain or climatic factors) have weak variability in low-relief areas, such as plains, and cannot reflect the spatial variation of soil attributes. Meanwhile, vegetation cover hinders the acquisition of the direct information of farmland soil. Thus, useful environmental variables should be utilized for SOC prediction and the digital mapping of such areas. SOC has an important effect on crop growth status, and remote sensing data can record the apparent spectral characteristics of crops. The normalized difference vegetation index (NDVI) is an important index reflecting crop growth and biomass. This study used NDVI time series data rather than traditional soil-forming factors to map SOC. Honghu City, located in the middle of the Jianghan Plain, was selected as the study region, and the NDVI time series data extracted from Landsat 8 were used as the auxiliary variables. SOC maps were estimated through stepwise linear regression (SLR), partial least squares regression (PLSR), support vector machine (SVM), and artificial neural network (ANN). Ordinary kriging (OK) was used as the reference model, while root mean square error of prediction (RMSEP) and coefficient of determination of prediction (R2P) were used to evaluate the model performance. Results showed that SOC had a significant positive correlation in July and August (0.17, 0.29) and a significant negative correlation in January, April, and December (−0.23, −0.27, and −0.23) with NDVI time series data. The best model for SOC prediction was generated by ANN, with the lowest RMSEP of 3.718 and highest R2P of 0.391, followed by SVM (RMSEP = 3.753, R2P = 0.361) and PLSR (RMSEP = 4.087, R2P = 0.283). The SLR model was the worst model, with the lowest R2P of 0.281 and highest RMSEP of 3.930. ANN and SVM were better than OK (RMSEP = 3.727, R2P = 0.372), whereas PLSR and SLR were worse than OK. Moreover, the prediction results using single-data NDVI or short time series NDVI showed low accuracy. The effect of the terrain factor on SOC prediction represented unsatisfactory results. All these results indicated that the NDVI time series data can be used for SOC mapping in plain areas and that the ANN model can maximally extract additional associated information between NDVI time series data and SOC. This study presented an effective method to overcome the selection of auxiliary variables for digital soil mapping in plain areas when the soil was covered with vegetation. This finding indicated that the time series characteristics of NDVI were conducive for predicting SOC in plains.</jats:p>","high-precision maps of soil organic carbon (soc) are beneficial for managing soil fertility and understanding the global carbon cycle. digital soil mapping plays an important role in efficiently obtaining the spatial distribution of soc, which contributes to precision agriculture. however, traditional soil-forming factors (i.e., terrain or climatic factors) have weak variability in low-relief areas, such as plains, and cannot reflect the spatial variation of soil attributes. meanwhile, vegetation cover hinders the acquisition of the direct information of farmland soil. thus, useful environmental variables should be utilized for soc prediction and the digital mapping of such areas. soc has an important effect on crop growth status, and remote sensing data can record the apparent spectral characteristics of crops. the normalized difference vegetation index (ndvi) is an important index reflecting crop growth and biomass. this study used ndvi time series data rather than traditional soil-forming factors to map soc. honghu city, located in the middle of the jianghan plain, was selected as the study region, and the ndvi time series data extracted from landsat 8 were used as the auxiliary variables. soc maps were estimated through stepwise linear regression (slr), partial least squares regression (plsr), support vector machine (svm), and artificial neural network (ann). ordinary kriging (ok) was used as the reference model, while root mean square error of prediction (rmsep) and coefficient of determination of prediction (r2p) were used to evaluate the model performance. results showed that soc had a significant positive correlation in july and august (0.17, 0.29) and a significant negative correlation in january, april, and december (−0.23, −0.27, and −0.23) with ndvi time series data. the best model for soc prediction was generated by ann, with the lowest rmsep of 3.718 and highest r2p of 0.391, followed by svm (rmsep = 3.753, r2p = 0.361) and plsr (rmsep = 4.087, r2p = 0.283). the slr model was the worst model, with the lowest r2p of 0.281 and highest rmsep of 3.930. ann and svm were better than ok (rmsep = 3.727, r2p = 0.372), whereas plsr and slr were worse than ok. moreover, the prediction results using single-data ndvi or short time series ndvi showed low accuracy. the effect of the terrain factor on soc prediction represented unsatisfactory results. all these results indicated that the ndvi time series data can be used for soc mapping in plain areas and that the ann model can maximally extract additional associated information between ndvi time series data and soc. this study presented an effective method to overcome the selection of auxiliary variables for digital soil mapping in plain areas when the soil was covered with vegetation. this finding indicated that the time series characteristics of ndvi were conducive for predicting soc in plains."
http://orkg.org/orkg/resource/R196677,Using Sentinel-2 Images for Soil Organic Carbon Content Mapping in Croplands of Southwestern France. The Usefulness of Sentinel-1/2 Derived Moisture Maps and Mismatches between Sentinel Images and Sampling Dates,10.3390/rs13245115,crossref,"<jats:p>In agronomy, soil organic carbon (SOC) content is important for the development and growth of crops. From an environmental monitoring viewpoint, SOC sequestration is essential for mitigating the emission of greenhouse gases into the atmosphere. SOC dynamics in cropland soils should be further studied through various approaches including remote sensing. In order to predict SOC content over croplands in southwestern France (area of 22,177 km²), this study addresses (i) the influence of the dates on which Sentinel-2 (S2) images were acquired in the springs of 2017–2018 as well as the influence of the soil sampling period of a set of samples collected between 2005 and 2018, (ii) the use of soil moisture products (SMPs) derived from Sentinel-1/2 satellites to analyze the influence of surface soil moisture on model performance when included as a covariate, and (iii) whether the spatial distribution of SOC as mapped using S2 is related to terrain-derived attributes. The influences of S2 image dates and soil sampling periods were analyzed for bare topsoil. The dates of the S2 images with the best performance (RPD ≥ 1.7) were 6 April and 26 May 2017, using soil samples collected between 2016 and 2018. The soil sampling dates were also analyzed using SMP values. Soil moisture values were extracted for each sample and integrated into partial least squares regression (PLSR) models. The use of soil moisture as a covariate had no effect on the prediction performance of the models; however, SMP values were used to select the driest dates, effectively mapping topsoil organic carbon. S2 was able to predict high SOC contents in the specific soil types located on the old terraces (mesas) shaped by rivers flowing from the southwestern Pyrénées.</jats:p>","in agronomy, soil organic carbon (soc) content is important for the development and growth of crops. from an environmental monitoring viewpoint, soc sequestration is essential for mitigating the emission of greenhouse gases into the atmosphere. soc dynamics in cropland soils should be further studied through various approaches including remote sensing. in order to predict soc content over croplands in southwestern france (area of 22,177 km²), this study addresses (i) the influence of the dates on which sentinel-2 (s2) images were acquired in the springs of 2017–2018 as well as the influence of the soil sampling period of a set of samples collected between 2005 and 2018, (ii) the use of soil moisture products (smps) derived from sentinel-1/2 satellites to analyze the influence of surface soil moisture on model performance when included as a covariate, and (iii) whether the spatial distribution of soc as mapped using s2 is related to terrain-derived attributes. the influences of s2 image dates and soil sampling periods were analyzed for bare topsoil. the dates of the s2 images with the best performance (rpd ≥ 1.7) were 6 april and 26 may 2017, using soil samples collected between 2016 and 2018. the soil sampling dates were also analyzed using smp values. soil moisture values were extracted for each sample and integrated into partial least squares regression (plsr) models. the use of soil moisture as a covariate had no effect on the prediction performance of the models; however, smp values were used to select the driest dates, effectively mapping topsoil organic carbon. s2 was able to predict high soc contents in the specific soil types located on the old terraces (mesas) shaped by rivers flowing from the southwestern pyrénées."
http://orkg.org/orkg/resource/R196695,TempoQR: Temporal Question Reasoning over Knowledge Graphs,10.1609/aaai.v36i5.20526,crossref,"<jats:p>Knowledge Graph Question Answering (KGQA) involves retrieving facts from a Knowledge Graph (KG) using natural language queries. A KG is a curated set of facts consisting of entities linked by relations. Certain facts include also temporal information forming a Temporal KG (TKG). Although many natural questions involve explicit or implicit time constraints, question answering (QA) over TKGs has been a relatively unexplored area. Existing solutions are mainly designed for simple temporal questions that can be answered directly by a single TKG fact.\n This paper puts forth a comprehensive embedding-based framework for answering complex questions over TKGs. Our method termed temporal question reasoning (TempoQR) exploits TKG embeddings to ground the question to the specific entities and time scope it refers to. It does so by augmenting the question embeddings with context, entity and time-aware information by employing three specialized modules. The first computes a textual representation of a given question, the second combines it with the entity embeddings for entities involved in the question, and the third generates question-specific time embeddings. Finally, a transformer-based encoder learns to fuse the generated temporal information with the question representation, which is used for answer predictions. Extensive experiments show that TempoQR improves accuracy by 25--45 percentage points on complex temporal questions over state-of-the-art approaches and it generalizes better to unseen question types.</jats:p>","knowledge graph question answering (kgqa) involves retrieving facts from a knowledge graph (kg) using natural language queries. a kg is a curated set of facts consisting of entities linked by relations. certain facts include also temporal information forming a temporal kg (tkg). although many natural questions involve explicit or implicit time constraints, question answering (qa) over tkgs has been a relatively unexplored area. existing solutions are mainly designed for simple temporal questions that can be answered directly by a single tkg fact.\n this paper puts forth a comprehensive embedding-based framework for answering complex questions over tkgs. our method termed temporal question reasoning (tempoqr) exploits tkg embeddings to ground the question to the specific entities and time scope it refers to. it does so by augmenting the question embeddings with context, entity and time-aware information by employing three specialized modules. the first computes a textual representation of a given question, the second combines it with the entity embeddings for entities involved in the question, and the third generates question-specific time embeddings. finally, a transformer-based encoder learns to fuse the generated temporal information with the question representation, which is used for answer predictions. extensive experiments show that tempoqr improves accuracy by 25--45 percentage points on complex temporal questions over state-of-the-art approaches and it generalizes better to unseen question types."
http://orkg.org/orkg/resource/R196704,Time-Aware Language Models as Temporal Knowledge Bases,10.1162/tacl_a_00459,crossref,"<jats:title>Abstract</jats:title>\n               <jats:p>Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for. However, most language models (LMs) are trained on snapshots of data collected at a specific moment in time. This can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum—those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. To mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. This improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. We also show that models trained with temporal context can be efficiently “refreshed” as new data arrives, without the need for retraining from scratch.</jats:p>","abstract \n many facts come with an expiration date, from the name of the president to the basketball team lebron james plays for. however, most language models (lms) are trained on snapshots of data collected at a specific moment in time. this can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. we introduce a diagnostic dataset aimed at probing lms for factual knowledge that changes over time and highlight problems with lms at either end of the spectrum—those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. to mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. this improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. we also show that models trained with temporal context can be efficiently “refreshed” as new data arrives, without the need for retraining from scratch."
http://orkg.org/orkg/resource/R196747,"Differential Effects of the COVID-19 Lockdown and Regional Fire on the Air Quality of Medellín, Colombia",,crossref,"<jats:p>Governments’ responses to the COVID-19 pandemic provide a unique opportunity to study the effects of restricted socioeconomic activity on air quality. Here, we study the changes in air pollution levels during the lockdown in Medellín and its metropolitan area, Colombia, for periods with and without enhanced regional fire activity, considering the effects of meteorology using random forest and multiple linear regression methods. The lockdown measures, which reduced mean traffic volume by 70% compared to 2016–2019, resulted in reductions for PM2.5 (50–63%), PM10 (59–64%), NO (75–76%), NO2 (43–47%), and CO (40–47%), while O3 concentration increased by 19–22%. In contrast, when fire activity was high, the effects of the lockdown on air quality were shadowed by the long-range transport of biomass burning emissions, increasing fine particulate matter and ozone. This study shows that healthier levels are achievable through significant efforts from decision-makers and society. The results highlight the need to develop integral measures that do not only consider reductions in the local emissions from transportation and industry, but also the role of fire activity in the region, as well as the difficulties of achieving reductions in ozone from measures that are effective at reducing primary pollutants.</jats:p>","governments’ responses to the covid-19 pandemic provide a unique opportunity to study the effects of restricted socioeconomic activity on air quality. here, we study the changes in air pollution levels during the lockdown in medellín and its metropolitan area, colombia, for periods with and without enhanced regional fire activity, considering the effects of meteorology using random forest and multiple linear regression methods. the lockdown measures, which reduced mean traffic volume by 70% compared to 2016–2019, resulted in reductions for pm2.5 (50–63%), pm10 (59–64%), no (75–76%), no2 (43–47%), and co (40–47%), while o3 concentration increased by 19–22%. in contrast, when fire activity was high, the effects of the lockdown on air quality were shadowed by the long-range transport of biomass burning emissions, increasing fine particulate matter and ozone. this study shows that healthier levels are achievable through significant efforts from decision-makers and society. the results highlight the need to develop integral measures that do not only consider reductions in the local emissions from transportation and industry, but also the role of fire activity in the region, as well as the difficulties of achieving reductions in ozone from measures that are effective at reducing primary pollutants."
http://orkg.org/orkg/resource/R196754,Quantification of Non-Exhaust Particulate Matter Traffic Emissions and the Impact of COVID-19 Lockdown at London Marylebone Road,10.3390/atmos12020190,crossref,"<jats:p>This research quantifies current sources of non-exhaust particulate matter traffic emissions in London using simultaneous, highly time-resolved, atmospheric particulate matter mass and chemical composition measurements. The measurement campaign ran at Marylebone Road (roadside) and Honor Oak Park (background) urban monitoring sites over a 12-month period between 1 September 2019 and 31 August 2020. The measurement data were used to determine the traffic increment (roadside–background) and covered a range of meteorological conditions, seasons, and driving styles, as well as the influence of the COVID-19 “lockdown” on non-exhaust concentrations. Non-exhaust particulate matter (PM)10 concentrations were calculated using chemical tracer scaling factors for brake wear (barium), tyre wear (zinc), and resuspension (silicon) and as average vehicle fleet non-exhaust emission factors, using a CO2 “dilution approach”. The effect of lockdown, which saw a 32% reduction in traffic volume and a 15% increase in average speed on Marylebone Road, resulted in lower PM10 and PM2.5 traffic increments and brake wear concentrations but similar tyre and resuspension concentrations, confirming that factors that determine non-exhaust emissions are complex. Brake wear was found to be the highest average non-exhaust emission source. In addition, results indicate that non-exhaust emission factors were dependent upon speed and road surface wetness conditions. Further statistical analysis incorporating a wider variability in vehicle mix, speeds, and meteorological conditions, as well as advanced source apportionment of the PM measurement data, were undertaken to enhance our understanding of these important vehicle sources.</jats:p>","this research quantifies current sources of non-exhaust particulate matter traffic emissions in london using simultaneous, highly time-resolved, atmospheric particulate matter mass and chemical composition measurements. the measurement campaign ran at marylebone road (roadside) and honor oak park (background) urban monitoring sites over a 12-month period between 1 september 2019 and 31 august 2020. the measurement data were used to determine the traffic increment (roadside–background) and covered a range of meteorological conditions, seasons, and driving styles, as well as the influence of the covid-19 “lockdown” on non-exhaust concentrations. non-exhaust particulate matter (pm)10 concentrations were calculated using chemical tracer scaling factors for brake wear (barium), tyre wear (zinc), and resuspension (silicon) and as average vehicle fleet non-exhaust emission factors, using a co2 “dilution approach”. the effect of lockdown, which saw a 32% reduction in traffic volume and a 15% increase in average speed on marylebone road, resulted in lower pm10 and pm2.5 traffic increments and brake wear concentrations but similar tyre and resuspension concentrations, confirming that factors that determine non-exhaust emissions are complex. brake wear was found to be the highest average non-exhaust emission source. in addition, results indicate that non-exhaust emission factors were dependent upon speed and road surface wetness conditions. further statistical analysis incorporating a wider variability in vehicle mix, speeds, and meteorological conditions, as well as advanced source apportionment of the pm measurement data, were undertaken to enhance our understanding of these important vehicle sources."
http://orkg.org/orkg/resource/R196778,Risk-Compensation Trends in Road Safety during COVID-19,10.3390/su14095057,crossref,"<jats:p>The COVID-19 pandemic has had a global impact, disrupting the normal trends of our everyday life. More specifically, the effects of COVID-19 on road safety are still largely unexplored. Hence, this study aims to investigate the change in road safety trends due to COVID-19 using real-time traffic parameters. Results from the extensive analyses of the 2017 to 2020 data of Interstate-4 show that traffic volume decreased by 13.6% in 2020 compared to the average of 2017–2019’s volume, whereas there is a decreasing number of crashes at the higher volume. Average speed increased by 11.3% during the COVID-19 period; however, the increase in average speed during the COVID-19 period has an insignificant relationship with crash severities. Fatal crashes increased, while total crashes decreased, during the COVID-19 period; severe crashes decreased with the total crashes. Alcohol-related crashes decreased by 22% from 2019 to 2020. Thus, the road-safety trend due to the impact of COVID-19 has evidently changed and presents a unique trend. The findings of the study suggest a larger need for a more in-depth study to analyze the impact of COVID-19 on road safety, to minimize fatalities on roads through appropriate policy measures.</jats:p>","the covid-19 pandemic has had a global impact, disrupting the normal trends of our everyday life. more specifically, the effects of covid-19 on road safety are still largely unexplored. hence, this study aims to investigate the change in road safety trends due to covid-19 using real-time traffic parameters. results from the extensive analyses of the 2017 to 2020 data of interstate-4 show that traffic volume decreased by 13.6% in 2020 compared to the average of 2017–2019’s volume, whereas there is a decreasing number of crashes at the higher volume. average speed increased by 11.3% during the covid-19 period; however, the increase in average speed during the covid-19 period has an insignificant relationship with crash severities. fatal crashes increased, while total crashes decreased, during the covid-19 period; severe crashes decreased with the total crashes. alcohol-related crashes decreased by 22% from 2019 to 2020. thus, the road-safety trend due to the impact of covid-19 has evidently changed and presents a unique trend. the findings of the study suggest a larger need for a more in-depth study to analyze the impact of covid-19 on road safety, to minimize fatalities on roads through appropriate policy measures."
http://orkg.org/orkg/resource/R196786,Deep neural networks ensemble for detecting medication mentions in tweets,10.1093/jamia/ocz156,crossref,"<jats:title>Abstract</jats:title><jats:sec><jats:title>Objective</jats:title><jats:p>Twitter posts are now recognized as an important source of patient-generated data, providing unique insights into population health. A fundamental step toward incorporating Twitter data in pharmacoepidemiologic research is to automatically recognize medication mentions in tweets. Given that lexical searches for medication names suffer from low recall due to misspellings or ambiguity with common words, we propose a more advanced method to recognize them.</jats:p></jats:sec><jats:sec><jats:title>Materials and Methods</jats:title><jats:p>We present Kusuri, an Ensemble Learning classifier able to identify tweets mentioning drug products and dietary supplements. Kusuri (薬, “medication” in Japanese) is composed of 2 modules: first, 4 different classifiers (lexicon based, spelling variant based, pattern based, and a weakly trained neural network) are applied in parallel to discover tweets potentially containing medication names; second, an ensemble of deep neural networks encoding morphological, semantic, and long-range dependencies of important words in the tweets makes the final decision.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>On a class-balanced (50-50) corpus of 15 005 tweets, Kusuri demonstrated performances close to human annotators with an F1 score of 93.7%, the best score achieved thus far on this corpus. On a corpus made of all tweets posted by 112 Twitter users (98 959 tweets, with only 0.26% mentioning medications), Kusuri obtained an F1 score of 78.8%. To the best of our knowledge, Kusuri is the first system to achieve this score on such an extremely imbalanced dataset.</jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p>The system identifies tweets mentioning drug names with performance high enough to ensure its usefulness, and is ready to be integrated in pharmacovigilance, toxicovigilance, or more generally, public health pipelines that depend on medication name mentions.</jats:p></jats:sec>","abstract objective twitter posts are now recognized as an important source of patient-generated data, providing unique insights into population health. a fundamental step toward incorporating twitter data in pharmacoepidemiologic research is to automatically recognize medication mentions in tweets. given that lexical searches for medication names suffer from low recall due to misspellings or ambiguity with common words, we propose a more advanced method to recognize them. materials and methods we present kusuri, an ensemble learning classifier able to identify tweets mentioning drug products and dietary supplements. kusuri (薬, “medication” in japanese) is composed of 2 modules: first, 4 different classifiers (lexicon based, spelling variant based, pattern based, and a weakly trained neural network) are applied in parallel to discover tweets potentially containing medication names; second, an ensemble of deep neural networks encoding morphological, semantic, and long-range dependencies of important words in the tweets makes the final decision. results on a class-balanced (50-50) corpus of 15 005 tweets, kusuri demonstrated performances close to human annotators with an f1 score of 93.7%, the best score achieved thus far on this corpus. on a corpus made of all tweets posted by 112 twitter users (98 959 tweets, with only 0.26% mentioning medications), kusuri obtained an f1 score of 78.8%. to the best of our knowledge, kusuri is the first system to achieve this score on such an extremely imbalanced dataset. conclusions the system identifies tweets mentioning drug names with performance high enough to ensure its usefulness, and is ready to be integrated in pharmacovigilance, toxicovigilance, or more generally, public health pipelines that depend on medication name mentions."
http://orkg.org/orkg/resource/R197081,Chimpanzees as an animal model for human norovirus infection and vaccine development,10.1073/pnas.1014577107,crossref,"<jats:p>Noroviruses are global agents of acute gastroenteritis, but the development of control strategies has been hampered by the absence of a robust animal model. Studies in chimpanzees have played a key role in the characterization of several fastidious hepatitis viruses, and we investigated the feasibility of such studies for the noroviruses. Seronegative chimpanzees inoculated i.v. with the human norovirus strain Norwalk virus (NV) did not show clinical signs of gastroenteritis, but the onset and duration of virus shedding in stool and serum antibody responses were similar to that observed in humans. NV RNA was detected in intestinal and liver biopsies concurrent with the detection of viral shedding in stool, and NV antigen expression was observed in cells of the small intestinal lamina propria. Two infected chimpanzees rechallenged 4, 10, or 24 mo later with NV were resistant to reinfection, and the presence of NV-specific serum antibodies correlated with protection. We evaluated the immunogenicity and efficacy of virus-like particles (VLPs) derived from NV (genogroup I, GI) and MD145 (genogroup II, GII) noroviruses as vaccines. Chimpanzees vaccinated intramuscularly with GI VLPs were protected from NV infection when challenged 2 and 18 mo after vaccination, whereas chimpanzees that received GII VLPs vaccine or a placebo were not. This study establishes the chimpanzee as a viable animal model for the study of norovirus replication and immunity, and shows that NV VLP vaccines could induce protective homologous immunity even after extended periods of time.</jats:p>","noroviruses are global agents of acute gastroenteritis, but the development of control strategies has been hampered by the absence of a robust animal model. studies in chimpanzees have played a key role in the characterization of several fastidious hepatitis viruses, and we investigated the feasibility of such studies for the noroviruses. seronegative chimpanzees inoculated i.v. with the human norovirus strain norwalk virus (nv) did not show clinical signs of gastroenteritis, but the onset and duration of virus shedding in stool and serum antibody responses were similar to that observed in humans. nv rna was detected in intestinal and liver biopsies concurrent with the detection of viral shedding in stool, and nv antigen expression was observed in cells of the small intestinal lamina propria. two infected chimpanzees rechallenged 4, 10, or 24 mo later with nv were resistant to reinfection, and the presence of nv-specific serum antibodies correlated with protection. we evaluated the immunogenicity and efficacy of virus-like particles (vlps) derived from nv (genogroup i, gi) and md145 (genogroup ii, gii) noroviruses as vaccines. chimpanzees vaccinated intramuscularly with gi vlps were protected from nv infection when challenged 2 and 18 mo after vaccination, whereas chimpanzees that received gii vlps vaccine or a placebo were not. this study establishes the chimpanzee as a viable animal model for the study of norovirus replication and immunity, and shows that nv vlp vaccines could induce protective homologous immunity even after extended periods of time."
http://orkg.org/orkg/resource/R196832,﻿Focus marking in Indian English,10.1075/eww.28.1.05lan,crossref,"<jats:p>This paper investigates the use of <jats:italic>only</jats:italic> and <jats:italic>itself</jats:italic> in Indian English, drawing on data from the Indian subcorpus of the <jats:italic>International Corpus of English</jats:italic> (ICE-India). In all varieties of English, <jats:italic>only</jats:italic> is used as an exclusive focus particle and <jats:italic>itself</jats:italic> as a reflexive pronoun and intensifier. Indian English has developed an additional use for <jats:italic>only</jats:italic> and <jats:italic>itself</jats:italic> as presentational, i.e. non-contrastive focus markers. The paper investigates the syntactic and semantic contexts of <jats:italic>itself</jats:italic> and <jats:italic>only</jats:italic> in order to capture the two lexical items’ functional extension in current Indian English. One interesting finding concerns the distribution of the two forms within the corpus: <jats:italic>Itself</jats:italic> is mainly found in written texts, while <jats:italic>only</jats:italic> is restricted to the spoken language. The paper further considers the origin and the likely future of this innovation in Indian English: Whereas it is quite clear that substrate influence is directly responsible for the innovative usage, the question whether this usage will also become accepted as part of an emerging Indian English standard remains to be settled.</jats:p>","this paper investigates the use of only and itself in indian english, drawing on data from the indian subcorpus of the international corpus of english (ice-india). in all varieties of english, only is used as an exclusive focus particle and itself as a reflexive pronoun and intensifier. indian english has developed an additional use for only and itself as presentational, i.e. non-contrastive focus markers. the paper investigates the syntactic and semantic contexts of itself and only in order to capture the two lexical items’ functional extension in current indian english. one interesting finding concerns the distribution of the two forms within the corpus: itself is mainly found in written texts, while only is restricted to the spoken language. the paper further considers the origin and the likely future of this innovation in indian english: whereas it is quite clear that substrate influence is directly responsible for the innovative usage, the question whether this usage will also become accepted as part of an emerging indian english standard remains to be settled."
http://orkg.org/orkg/resource/R196878,Polite Speech Emerges From Competing Social Goals,10.1162/opmi_a_00035,crossref,"<jats:p>Language is a remarkably efficient tool for transmitting information. Yet human speakers make statements that are inefficient, imprecise, or even contrary to their own beliefs, all in the service of being polite. What rational machinery underlies polite language use? Here, we show that polite speech emerges from the competition of three communicative goals: to convey information, to be kind, and to present oneself in a good light. We formalize this goal tradeoff using a probabilistic model of utterance production, which predicts human utterance choices in socially sensitive situations with high quantitative accuracy, and we show that our full model is superior to its variants with subsets of the three goals. This utility-theoretic approach to speech acts takes a step toward explaining the richness and subtlety of social language use.</jats:p>","language is a remarkably efficient tool for transmitting information. yet human speakers make statements that are inefficient, imprecise, or even contrary to their own beliefs, all in the service of being polite. what rational machinery underlies polite language use? here, we show that polite speech emerges from the competition of three communicative goals: to convey information, to be kind, and to present oneself in a good light. we formalize this goal tradeoff using a probabilistic model of utterance production, which predicts human utterance choices in socially sensitive situations with high quantitative accuracy, and we show that our full model is superior to its variants with subsets of the three goals. this utility-theoretic approach to speech acts takes a step toward explaining the richness and subtlety of social language use."
http://orkg.org/orkg/resource/R196862,The use of politeness strategies in the classroom context by English university students,10.17509/IJAL.V8I3.15258,crossref,"<jats:p>Politeness still becomes a major concern in English language teaching. It is considered as one way to maintain effective classroom interaction. Therefore, as one of the important actors in the class, teachers, and students need to practice politeness as a way to create effective classroom interaction. This study aims to explore the politeness strategies of English students at one of the universities in Makassar. The researcher applied a descriptive qualitative research method to explore the politeness phenomena in EFL classroom interaction. The participants of this research were two classes of English literature program consisting of 50 students. The primary sources of data were the individual student presentations which had been recorded. There were fifty transcriptions of the recording which lasted for five to seven minutes for each presentation. The transcriptions were analyzed and discussed based on the theory of politeness of Brown and Levinson (1987). The findings from this study revealed that English students used different kinds of expressions to encode their politeness in the class. Those expressions were in the forms of greetings, thanking, addressing terms, apologizing, and fillers. There were also some terms derived from students’ vernacular language which were used as a softening mechanism for their presentation. These expressions were categorized as positive and negative politeness. The findings of this study might be used as an input for teachers and students in an effort to create effective classroom interaction.</jats:p>","politeness still becomes a major concern in english language teaching. it is considered as one way to maintain effective classroom interaction. therefore, as one of the important actors in the class, teachers, and students need to practice politeness as a way to create effective classroom interaction. this study aims to explore the politeness strategies of english students at one of the universities in makassar. the researcher applied a descriptive qualitative research method to explore the politeness phenomena in efl classroom interaction. the participants of this research were two classes of english literature program consisting of 50 students. the primary sources of data were the individual student presentations which had been recorded. there were fifty transcriptions of the recording which lasted for five to seven minutes for each presentation. the transcriptions were analyzed and discussed based on the theory of politeness of brown and levinson (1987). the findings from this study revealed that english students used different kinds of expressions to encode their politeness in the class. those expressions were in the forms of greetings, thanking, addressing terms, apologizing, and fillers. there were also some terms derived from students’ vernacular language which were used as a softening mechanism for their presentation. these expressions were categorized as positive and negative politeness. the findings of this study might be used as an input for teachers and students in an effort to create effective classroom interaction."
http://orkg.org/orkg/resource/R196865,Introduction to Politeness and Impoliteness Research in Global Contexts,10.22363/2687-0088-2019-23-4-873-903,crossref,"<jats:p>Im/politeness research has been a solid and growing research field in sociolinguistics, pragmatics and discourse analysis during the last four decades. The scientific interest in this topic is not accidental and may be explained by the general pragmatic turn of modern interdisciplinary linguistic studies which are not focused on language as an abstract system, but on its functioning in various contexts and types of interaction. Knowledge of the strategies and politeness mechanisms used in various social and cultural contexts promotes mutual understanding in communication. In this introduction to the special issue on im/politeness in global contexts we will briefly position the topic of im/politeness research, and highlight advancements in im/politeness theory, method and data. We then turn to a brief synopsis of each individual paper and highlight the theoretical and methodological contributions and innovations proposed by our authors. We end with a discussion of the results and a brief outlook on future research.</jats:p>","im/politeness research has been a solid and growing research field in sociolinguistics, pragmatics and discourse analysis during the last four decades. the scientific interest in this topic is not accidental and may be explained by the general pragmatic turn of modern interdisciplinary linguistic studies which are not focused on language as an abstract system, but on its functioning in various contexts and types of interaction. knowledge of the strategies and politeness mechanisms used in various social and cultural contexts promotes mutual understanding in communication. in this introduction to the special issue on im/politeness in global contexts we will briefly position the topic of im/politeness research, and highlight advancements in im/politeness theory, method and data. we then turn to a brief synopsis of each individual paper and highlight the theoretical and methodological contributions and innovations proposed by our authors. we end with a discussion of the results and a brief outlook on future research."
http://orkg.org/orkg/resource/R196885,ATTENTION TO IRREGULAR VERBS BY BEGINNING LEARNERS OF GERMAN,10.1017/S0272263112000897,crossref,"<jats:p>This study focuses on beginning second language learners’ attention to irregular verb morphology, an area of grammar that many adults find difficult to acquire (e.g., DeKeyser, 2005; Larsen-Freeman, 2010). We measured beginning learners’ eye movements during sentence processing to investigate whether or not they actually attend to irregular verb features and, if so, whether the amount of attention that they pay to these features predicts their acquisition. On the assumption that attention facilitates learning (e.g., Gass, 1997; Robinson, 2003; Schmidt, 2001), we expected more attention (i.e., longer fixations or more frequent comparisons between verb forms) to lead to more learning of the irregular verbs. Forty beginning learners of German read 12 German sentence pairs with stem-changing verbs and 12 German sentence pairs with regular verbs while an Eyelink 1000 recorded their eye movements. The stem-changing verbs consisted of six<jats:italic>a</jats:italic>→<jats:italic>ä</jats:italic>changing verbs and six<jats:italic>e</jats:italic>→<jats:italic>i(e)</jats:italic>changing verbs. Each verb appeared in a baseline sentence in the first-person singular, which has no stem change, and a critical sentence in the second- or third-person singular, which have a stem change for the irregular but not the regular verbs, on the same screen. Productive pre- and posttests measured the effects of exposure on learning. Results indicate that learners looked longer overall at stem-changing verbs than regular verbs, revealing a late effect of verb irregularity on reading times. Longer total times had a modest, favorable effect on the subsequent production of the stem vowel. Finally, the production of only the<jats:italic>a</jats:italic>→<jats:italic>ä</jats:italic>verbs—not the<jats:italic>e</jats:italic>→<jats:italic>i(e)</jats:italic>verbs—benefited from direct visual comparisons during reading, possibly because of the umlaut in the former. We interpret the results with reference to recent theory and research on attention, noticing, and language learning and provide a more nuanced and empirically based understanding of the noticing construct.</jats:p>","this study focuses on beginning second language learners’ attention to irregular verb morphology, an area of grammar that many adults find difficult to acquire (e.g., dekeyser, 2005; larsen-freeman, 2010). we measured beginning learners’ eye movements during sentence processing to investigate whether or not they actually attend to irregular verb features and, if so, whether the amount of attention that they pay to these features predicts their acquisition. on the assumption that attention facilitates learning (e.g., gass, 1997; robinson, 2003; schmidt, 2001), we expected more attention (i.e., longer fixations or more frequent comparisons between verb forms) to lead to more learning of the irregular verbs. forty beginning learners of german read 12 german sentence pairs with stem-changing verbs and 12 german sentence pairs with regular verbs while an eyelink 1000 recorded their eye movements. the stem-changing verbs consisted of six a → ä changing verbs and six e → i(e) changing verbs. each verb appeared in a baseline sentence in the first-person singular, which has no stem change, and a critical sentence in the second- or third-person singular, which have a stem change for the irregular but not the regular verbs, on the same screen. productive pre- and posttests measured the effects of exposure on learning. results indicate that learners looked longer overall at stem-changing verbs than regular verbs, revealing a late effect of verb irregularity on reading times. longer total times had a modest, favorable effect on the subsequent production of the stem vowel. finally, the production of only the a → ä verbs—not the e → i(e) verbs—benefited from direct visual comparisons during reading, possibly because of the umlaut in the former. we interpret the results with reference to recent theory and research on attention, noticing, and language learning and provide a more nuanced and empirically based understanding of the noticing construct."
http://orkg.org/orkg/resource/R196942,Human norovirus targets enteroendocrine epithelial cells in the small intestine,10.1038/s41467-020-16491-3,crossref,"<jats:title>Abstract</jats:title><jats:p>Human noroviruses are a major cause of diarrheal illness, but pathogenesis is poorly understood. Here, we investigate the cellular tropism of norovirus in specimens from four immunocompromised patients. Abundant norovirus antigen and RNA are detected throughout the small intestinal tract in jejunal and ileal tissue from one pediatric intestinal transplant recipient with severe gastroenteritis. Negative-sense viral RNA, a marker of active viral replication, is found predominantly in intestinal epithelial cells, with chromogranin A-positive enteroendocrine cells (EECs) identified as a permissive cell type in this patient. These findings are consistent with the detection of norovirus-positive EECs in the other three immunocompromised patients. Investigation of the signaling pathways induced in EECs that mediate communication between the gut and brain may clarify mechanisms of pathogenesis and lead to the development of in vitro model systems in which to evaluate norovirus vaccines and treatment.</jats:p>","abstract human noroviruses are a major cause of diarrheal illness, but pathogenesis is poorly understood. here, we investigate the cellular tropism of norovirus in specimens from four immunocompromised patients. abundant norovirus antigen and rna are detected throughout the small intestinal tract in jejunal and ileal tissue from one pediatric intestinal transplant recipient with severe gastroenteritis. negative-sense viral rna, a marker of active viral replication, is found predominantly in intestinal epithelial cells, with chromogranin a-positive enteroendocrine cells (eecs) identified as a permissive cell type in this patient. these findings are consistent with the detection of norovirus-positive eecs in the other three immunocompromised patients. investigation of the signaling pathways induced in eecs that mediate communication between the gut and brain may clarify mechanisms of pathogenesis and lead to the development of in vitro model systems in which to evaluate norovirus vaccines and treatment."
http://orkg.org/orkg/resource/R196957,"Neural Language Models Capture Some, But Not All, Agreement Attraction Effects",10.31234/osf.io/97qcg,crossref,"<p>The number of the subject in English must match the number of the corresponding verb (dog runs but dogs run). Yet in real-time language production and comprehension, speakers often mistakenly compute agreement between the verb and a grammatically irrelevant non-subject noun phrase instead. This phenomenon, referred to as agreement attraction, is modulated by a wide range of factors; any complete computational model of grammatical planning and comprehension would be expected to derive this rich empirical picture. Recent developments in Natural Language Processing have shown that neural networks trained only on word-prediction over large corpora are capable of capturing subject-verb agreement dependencies to a significant extent, but with occasional errors. The goal of this paper is to evaluate the potential of such neural word prediction models as a foundation for a cognitive model of real-time grammatical processing. We simulate six experiments taken from the agreement attraction literature with LSTMs, one common type of neural language model. The LSTMs captured the critical human behavior in three of them, indicating that (1) some agreement attraction phenomena can be captured by a generic sequence processing model, but (2) capturing the other phenomena may require models with more language-specific mechanisms</p>","the number of the subject in english must match the number of the corresponding verb (dog runs but dogs run). yet in real-time language production and comprehension, speakers often mistakenly compute agreement between the verb and a grammatically irrelevant non-subject noun phrase instead. this phenomenon, referred to as agreement attraction, is modulated by a wide range of factors; any complete computational model of grammatical planning and comprehension would be expected to derive this rich empirical picture. recent developments in natural language processing have shown that neural networks trained only on word-prediction over large corpora are capable of capturing subject-verb agreement dependencies to a significant extent, but with occasional errors. the goal of this paper is to evaluate the potential of such neural word prediction models as a foundation for a cognitive model of real-time grammatical processing. we simulate six experiments taken from the agreement attraction literature with lstms, one common type of neural language model. the lstms captured the critical human behavior in three of them, indicating that (1) some agreement attraction phenomena can be captured by a generic sequence processing model, but (2) capturing the other phenomena may require models with more language-specific mechanisms"
http://orkg.org/orkg/resource/R197510,"Has COVID-19 Lockdown Affected on Air Quality?—Different Time Scale Case Study in Wrocław, Poland",10.3390/atmos12121549,crossref,"<jats:p>Due to the COVID-19 pandemic, there are series of negative economic consequences, however, in limiting mobility and reducing the number of vehicles, positive effects can also be observed, i.e., improvement of air quality. The paper presents an analysis of air quality measured by concentrations of NO2, NOx and PM2.5 during the most restrictive lockdown from 10 March to 31 May 2020 on the case of Wrocław. The results were compared with the reference period—2016–2019. A significant reduction in traffic volume was identified, on average by 26.3%. The greatest reduction in the concentration of NO2 and NOx was recorded at the station farthest from the city center, characterized by the lowest concentrations: 20.1% and 22.4%. Lower reduction in the average concentrations of NO2 and NOx was recorded at the municipal station (7.9% and 7.7%) and the communication station (6.7% and 10.2%). Concentrations of PMs in 2020 were on average 15% and 13.4% lower than in the reference period for the traffic station and the background station. The long-term impact of the lockdown on air quality was also examined. The analysis of the concentrations of the pollutants throughout 2020, and in the analyzed period of 2021, indicated that the reduction of concentrations and the improvement in air quality caused by the restrictions should be considered as a temporary anomaly, without affecting long-term changes and trends.</jats:p>","due to the covid-19 pandemic, there are series of negative economic consequences, however, in limiting mobility and reducing the number of vehicles, positive effects can also be observed, i.e., improvement of air quality. the paper presents an analysis of air quality measured by concentrations of no2, nox and pm2.5 during the most restrictive lockdown from 10 march to 31 may 2020 on the case of wrocław. the results were compared with the reference period—2016–2019. a significant reduction in traffic volume was identified, on average by 26.3%. the greatest reduction in the concentration of no2 and nox was recorded at the station farthest from the city center, characterized by the lowest concentrations: 20.1% and 22.4%. lower reduction in the average concentrations of no2 and nox was recorded at the municipal station (7.9% and 7.7%) and the communication station (6.7% and 10.2%). concentrations of pms in 2020 were on average 15% and 13.4% lower than in the reference period for the traffic station and the background station. the long-term impact of the lockdown on air quality was also examined. the analysis of the concentrations of the pollutants throughout 2020, and in the analyzed period of 2021, indicated that the reduction of concentrations and the improvement in air quality caused by the restrictions should be considered as a temporary anomaly, without affecting long-term changes and trends."
http://orkg.org/orkg/resource/R197091,Focus marking and semantic transfer in Indian English: The case of <i>also</i>,10.1075/eww.33.1.02fuc,crossref,"<jats:p>Focus marking in Indian English (IndE) with adverbs such as <jats:italic>only</jats:italic>, <jats:italic>also</jats:italic>, and <jats:italic>too</jats:italic> has been investigated recently by several authors. Based on the Indian and British sections of the International Corpus of English, this article argues that usage of <jats:italic>also</jats:italic> in IndE differs significantly from British English (BrE). <jats:italic>Also</jats:italic> often follows its focus immediately, has developed a presentational use, and is often used in negative contexts, corresponding to <jats:italic>either</jats:italic>. All these innovations are shown to be more frequent in spoken and informal than in written and formal language, respectively. Finally, evidence for substrate influence from Indo-European and Dravidian languages is presented.</jats:p>","focus marking in indian english (inde) with adverbs such as only , also , and too has been investigated recently by several authors. based on the indian and british sections of the international corpus of english, this article argues that usage of also in inde differs significantly from british english (bre). also often follows its focus immediately, has developed a presentational use, and is often used in negative contexts, corresponding to either . all these innovations are shown to be more frequent in spoken and informal than in written and formal language, respectively. finally, evidence for substrate influence from indo-european and dravidian languages is presented."
http://orkg.org/orkg/resource/R197147,Pathogenesis of a Genogroup II Human Norovirus in Gnotobiotic Pigs,10.1128/jvi.00809-06,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            We\nevaluated the gnotobiotic (Gn) pig as a model to study the pathogenesis\nof human norovirus (HuNoV) and to determine the target cells for viral\nreplication. Sixty-five Gn pigs were inoculated with fecal filtrates of\nthe NoV/GII/4/HS66/2001/US strain or with pig-passaged intestinal\ncontents (IC) and euthanized acutely (\n            <jats:italic>n</jats:italic>\n            = 43) or after\nconvalescence (\n            <jats:italic>n</jats:italic>\n            = 22). Age-matched Gn piglets\n(\n            <jats:italic>n</jats:italic>\n            = 14) served as mock-inoculated controls.\nSeventy-four percent (48/65) of the inoculated animals developed mild\ndiarrhea compared to 0 of 14 controls. Pigs from postinoculation days\n(PID) 1 to 4 tested positive for HuNoV by reverse\ntranscription-PCR of rectal swab fluids (29/65) and IC\n(9/43) and by antigen (Ag) enzyme-linked immunosorbent assay (ELISA)\nusing antiserum to virus-like particles of HuNoV GII/4. No\ncontrol pigs were positive. Histopathologic examination showed mild\nlesions in the proximal small intestine of only one pig (1/7).\nSeroconversion after PID 21 was detected by antibody ELISA in 13 of 22\nvirus-inoculated pigs (titers, 1:20 to 1:200) but not in controls.\nImmunofluorescent microscopy using a monoclonal antibody to HuNoV GII\ncapsid revealed patchy infection of duodenal and jejunal enterocytes of\n18 of 31 HuNoV-inoculated pigs with a few stained cells in the ileum\nand no immunofluorescence (IF) in mock-inoculated controls.\nImmunofluorescent detection of the viral nonstructural N-terminal\nprotein antigen in enterocytes confirmed translation. Transmission\nelectron microscopy of intestines from HuNoV-inoculated pigs showed\ndisrupted enterocytes, with cytoplasmic membrane vesicles containing\ncalicivirus-like particles of 25 to 40 nm in diameter. In summary,\nserial passage of HuNoV in pigs, with occurrence of mild diarrhea and\nshedding, and immunofluorescent detection of the HuNoV structural and\nnonstructural proteins in enterocytes confirm HuNoV replication in Gn\npigs.\n          </jats:p>","abstract \n \n we\nevaluated the gnotobiotic (gn) pig as a model to study the pathogenesis\nof human norovirus (hunov) and to determine the target cells for viral\nreplication. sixty-five gn pigs were inoculated with fecal filtrates of\nthe nov/gii/4/hs66/2001/us strain or with pig-passaged intestinal\ncontents (ic) and euthanized acutely (\n n \n = 43) or after\nconvalescence (\n n \n = 22). age-matched gn piglets\n(\n n \n = 14) served as mock-inoculated controls.\nseventy-four percent (48/65) of the inoculated animals developed mild\ndiarrhea compared to 0 of 14 controls. pigs from postinoculation days\n(pid) 1 to 4 tested positive for hunov by reverse\ntranscription-pcr of rectal swab fluids (29/65) and ic\n(9/43) and by antigen (ag) enzyme-linked immunosorbent assay (elisa)\nusing antiserum to virus-like particles of hunov gii/4. no\ncontrol pigs were positive. histopathologic examination showed mild\nlesions in the proximal small intestine of only one pig (1/7).\nseroconversion after pid 21 was detected by antibody elisa in 13 of 22\nvirus-inoculated pigs (titers, 1:20 to 1:200) but not in controls.\nimmunofluorescent microscopy using a monoclonal antibody to hunov gii\ncapsid revealed patchy infection of duodenal and jejunal enterocytes of\n18 of 31 hunov-inoculated pigs with a few stained cells in the ileum\nand no immunofluorescence (if) in mock-inoculated controls.\nimmunofluorescent detection of the viral nonstructural n-terminal\nprotein antigen in enterocytes confirmed translation. transmission\nelectron microscopy of intestines from hunov-inoculated pigs showed\ndisrupted enterocytes, with cytoplasmic membrane vesicles containing\ncalicivirus-like particles of 25 to 40 nm in diameter. in summary,\nserial passage of hunov in pigs, with occurrence of mild diarrhea and\nshedding, and immunofluorescent detection of the hunov structural and\nnonstructural proteins in enterocytes confirm hunov replication in gn\npigs.\n"
http://orkg.org/orkg/resource/R197177,Pathogenesis and Immune Responses in Gnotobiotic Calves after Infection with the Genogroup II.4-HS66 Strain of Human Norovirus,10.1128/jvi.01347-07,crossref,"""<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            We previously characterized the pathogenesis of two host-specific bovine enteric caliciviruses (BEC), the GIII.2 norovirus (NoV) strain CV186-OH and the phylogenetically unassigned NB strain, in gnotobiotic (Gn) calves. In this study we evaluated the Gn calf as an alternative animal model to study the pathogenesis and host immune responses to the human norovirus (HuNoV) strain GII.4-HS66. The HuNoV HS66 strain caused diarrhea (five/five calves) and intestinal lesions (one/two calves tested) in the proximal small intestine (duodenum and jejunum) of Gn calves, with lesions similar to, but less severe than, those described for the Newbury agent 2 (NA-2) and NB BEC. Viral capsid antigen was also detected in the jejunum of the proximal small intestine of one of two calves tested by immunohistochemistry. All inoculated calves shed virus in feces (five/five calves), and one/five had viremia. Antibodies and cytokine (proinflammatory, tumor necrosis factor alpha [TNF-α]; Th1, interleukin-12 [IL-12] and gamma interferon [IFN-γ]; Th2, IL-4; Th2/T-regulatory, IL-10) profiles were determined in serum, feces, and intestinal contents (IC) of the HuNoV-HS66-inoculated calves (\n            <jats:italic>n</jats:italic>\n            = 5) and controls (\n            <jats:italic>n</jats:italic>\n            = 4) by enzyme-linked immunosorbent assay in the acute (postinoculation day 3 [PID 3]) and convalescent (PID 28) stages of infection. The HuNoV-HS66-specific antibody and cytokine-secreting cells (CSCs) were quantitated by ELISPOT in mononuclear cells of local and systemic tissues at PID 28. Sixty-seven percent of the HuNoV-HS66-inoculated calves seroconverted, and 100% coproconverted with immunoglobulin A (IgA) and/or IgG antibodies to HuNoV-HS66, at low titers. The highest numbers of antibody-secreting cells (ASC), both IgA and IgG, were detected locally in intestine, but systemic IgA and IgG ASC responses also occurred in the HuNoV-HS66-inoculated calves. In serum, HuNoV-HS66 induced higher peaks of TNF-α and IFN-γ at PIDs 2, 7, and 10; of IL-4 and IL-10 at PID 4; and of IL-12 at PIDs 7 and 10, compared to controls. In feces, cytokines increased earlier (PID 1) than in serum and TNF-α and IL-10 were elevated acutely in the IC of the HS66-inoculated calves. Compared to controls, at PID 28 higher numbers of IFN-γ and TNF-α CSCs were detected in mesenteric lymph nodes (MLN) or spleen and Th2 (IL-4) CSCs were elevated in intestine; IL-10 CSCs were highest in spleen. Our study provides new data confirming HuNoV-HS66 replication and enteropathogenicity in Gn calves and reveals important and comprehensive aspects of the host's local (intestine and MLN) and systemic (spleen and blood) immune responses to HuNoV-HS66.\n          </jats:p>""",""" abstract \n \n we previously characterized the pathogenesis of two host-specific bovine enteric caliciviruses (bec), the giii.2 norovirus (nov) strain cv186-oh and the phylogenetically unassigned nb strain, in gnotobiotic (gn) calves. in this study we evaluated the gn calf as an alternative animal model to study the pathogenesis and host immune responses to the human norovirus (hunov) strain gii.4-hs66. the hunov hs66 strain caused diarrhea (five/five calves) and intestinal lesions (one/two calves tested) in the proximal small intestine (duodenum and jejunum) of gn calves, with lesions similar to, but less severe than, those described for the newbury agent 2 (na-2) and nb bec. viral capsid antigen was also detected in the jejunum of the proximal small intestine of one of two calves tested by immunohistochemistry. all inoculated calves shed virus in feces (five/five calves), and one/five had viremia. antibodies and cytokine (proinflammatory, tumor necrosis factor alpha [tnf-α]; th1, interleukin-12 [il-12] and gamma interferon [ifn-γ]; th2, il-4; th2/t-regulatory, il-10) profiles were determined in serum, feces, and intestinal contents (ic) of the hunov-hs66-inoculated calves (\n n \n = 5) and controls (\n n \n = 4) by enzyme-linked immunosorbent assay in the acute (postinoculation day 3 [pid 3]) and convalescent (pid 28) stages of infection. the hunov-hs66-specific antibody and cytokine-secreting cells (cscs) were quantitated by elispot in mononuclear cells of local and systemic tissues at pid 28. sixty-seven percent of the hunov-hs66-inoculated calves seroconverted, and 100% coproconverted with immunoglobulin a (iga) and/or igg antibodies to hunov-hs66, at low titers. the highest numbers of antibody-secreting cells (asc), both iga and igg, were detected locally in intestine, but systemic iga and igg asc responses also occurred in the hunov-hs66-inoculated calves. in serum, hunov-hs66 induced higher peaks of tnf-α and ifn-γ at pids 2, 7, and 10; of il-4 and il-10 at pid 4; and of il-12 at pids 7 and 10, compared to controls. in feces, cytokines increased earlier (pid 1) than in serum and tnf-α and il-10 were elevated acutely in the ic of the hs66-inoculated calves. compared to controls, at pid 28 higher numbers of ifn-γ and tnf-α cscs were detected in mesenteric lymph nodes (mln) or spleen and th2 (il-4) cscs were elevated in intestine; il-10 cscs were highest in spleen. our study provides new data confirming hunov-hs66 replication and enteropathogenicity in gn calves and reveals important and comprehensive aspects of the host's local (intestine and mln) and systemic (spleen and blood) immune responses to hunov-hs66.\n """
http://orkg.org/orkg/resource/R197233,Median infectious dose of human norovirus GII.4 in gnotobiotic pigs is decreased by simvastatin treatment and increased by age,10.1099/vir.0.054080-0,crossref,"<jats:p>Human noroviruses (NoVs), a major cause of viral gastroenteritis, are difficult to study due to the lack of a cell-culture and a small-animal model. Pigs share with humans the types A and H histo-blood group antigens on the intestinal epithelium and have been suggested as a potential model for studies of NoV pathogenesis, immunity and vaccines. In this study, the effects of age and a cholesterol-lowering drug, simvastatin, on the susceptibility of pigs to NoV infection were evaluated. The median infectious dose (ID<jats:sub>50</jats:sub>) of a genogroup II, genotype 4 (GII.4) 2006b variant was determined. The ID<jats:sub>50</jats:sub> in neonatal (4–5 days of age) pigs was ≤2.74×10<jats:sup>3</jats:sup> viral RNA copies. In older pigs (33–34 days of age), the ID<jats:sub>50</jats:sub> was 6.43×10<jats:sup>4</jats:sup> but decreased to &lt;2.74×10<jats:sup>3</jats:sup> in simvastatin-fed older pigs. Evidence of NoV infection was obtained by increased virus load in the intestinal contents, cytopathological changes in the small intestine, including irregular microvilli, necrosis and apoptosis, and detection of viral antigen in the tip of villi in duodenum. This GII.4 variant was isolated in 2008 from a patient from whom a large volume of stool was collected. GII.4 NoVs are continuously subjected to selective pressure by human immunity, and antigenically different GII.4 NoV variants emerge every 1–2 years. The determination of the ID<jats:sub>50</jats:sub> of this challenge virus is valuable for evaluation of protection against different GII.4 variants conferred by NoV vaccines in concurrence with other GII.4 variants in the gnotobiotic pig model.</jats:p>","human noroviruses (novs), a major cause of viral gastroenteritis, are difficult to study due to the lack of a cell-culture and a small-animal model. pigs share with humans the types a and h histo-blood group antigens on the intestinal epithelium and have been suggested as a potential model for studies of nov pathogenesis, immunity and vaccines. in this study, the effects of age and a cholesterol-lowering drug, simvastatin, on the susceptibility of pigs to nov infection were evaluated. the median infectious dose (id 50 ) of a genogroup ii, genotype 4 (gii.4) 2006b variant was determined. the id 50 in neonatal (4–5 days of age) pigs was ≤2.74×10 3 viral rna copies. in older pigs (33–34 days of age), the id 50 was 6.43×10 4 but decreased to &lt;2.74×10 3 in simvastatin-fed older pigs. evidence of nov infection was obtained by increased virus load in the intestinal contents, cytopathological changes in the small intestine, including irregular microvilli, necrosis and apoptosis, and detection of viral antigen in the tip of villi in duodenum. this gii.4 variant was isolated in 2008 from a patient from whom a large volume of stool was collected. gii.4 novs are continuously subjected to selective pressure by human immunity, and antigenically different gii.4 nov variants emerge every 1–2 years. the determination of the id 50 of this challenge virus is valuable for evaluation of protection against different gii.4 variants conferred by nov vaccines in concurrence with other gii.4 variants in the gnotobiotic pig model."
http://orkg.org/orkg/resource/R197236,A Mouse Model for Human Norovirus,10.1128/mbio.00450-13,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>\n            Human noroviruses (HuNoVs) cause significant morbidity and mortality worldwide. However, despite substantial efforts, a small-animal model for HuNoV has not been described to date. Since “humanized” mice have been successfully used to study human-tropic pathogens in the past, we challenged BALB/c mice deficient in recombination activation gene (Rag) 1 or 2 and common gamma chain (γc) (Rag-γc) engrafted with human CD34\n            <jats:sup>+</jats:sup>\n            hematopoietic stem cells, nonengrafted siblings, and immunocompetent wild-type controls with pooled stool isolates from patients positive for HuNoV. Surprisingly, both humanized and nonhumanized BALB/c Rag-γc-deficient mice supported replication of a GII.4 strain of HuNoV, as indicated by increased viral loads over input. In contrast, immunocompetent wild-type BALB/c mice were not infected. An intraperitoneal route of infection and the BALB/c genetic background were important for facilitating a subclinical HuNoV infection of Rag-γc-deficient mice. Expression of structural and nonstructural proteins was detected in cells with macrophage-like morphology in the spleens and livers of BALB/c Rag-γc-deficient mice, confirming the ability of HuNoV to replicate in a mouse model. In summary, HuNoV replication in BALB/c Rag-γc-deficient mice is dependent on the immune-deficient status of the host but not on the presence of human immune cells and provides the first genetically manipulable small-animal model for studying HuNoV infection.\n          </jats:p>\n          <jats:p>\n            <jats:bold>IMPORTANCE</jats:bold>\n            Human noroviruses are a significant cause of viral gastroenteritis worldwide, resulting in significant morbidity and mortality. Antivirals and vaccines are currently not available, in part due to the inability to study these viruses in a genetically manipulable, small-animal model. Herein, we report the first mouse model for human noroviruses. This model will accelerate our understanding of human norovirus biology and provide a useful resource for evaluating antiviral therapies.\n          </jats:p>","abstract \n \n human noroviruses (hunovs) cause significant morbidity and mortality worldwide. however, despite substantial efforts, a small-animal model for hunov has not been described to date. since “humanized” mice have been successfully used to study human-tropic pathogens in the past, we challenged balb/c mice deficient in recombination activation gene (rag) 1 or 2 and common gamma chain (γc) (rag-γc) engrafted with human cd34\n + \n hematopoietic stem cells, nonengrafted siblings, and immunocompetent wild-type controls with pooled stool isolates from patients positive for hunov. surprisingly, both humanized and nonhumanized balb/c rag-γc-deficient mice supported replication of a gii.4 strain of hunov, as indicated by increased viral loads over input. in contrast, immunocompetent wild-type balb/c mice were not infected. an intraperitoneal route of infection and the balb/c genetic background were important for facilitating a subclinical hunov infection of rag-γc-deficient mice. expression of structural and nonstructural proteins was detected in cells with macrophage-like morphology in the spleens and livers of balb/c rag-γc-deficient mice, confirming the ability of hunov to replicate in a mouse model. in summary, hunov replication in balb/c rag-γc-deficient mice is dependent on the immune-deficient status of the host but not on the presence of human immune cells and provides the first genetically manipulable small-animal model for studying hunov infection.\n \n \n importance \n human noroviruses are a significant cause of viral gastroenteritis worldwide, resulting in significant morbidity and mortality. antivirals and vaccines are currently not available, in part due to the inability to study these viruses in a genetically manipulable, small-animal model. herein, we report the first mouse model for human noroviruses. this model will accelerate our understanding of human norovirus biology and provide a useful resource for evaluating antiviral therapies.\n"
http://orkg.org/orkg/resource/R198000,Sentinel-2 Exposed Soil Composite for Soil Organic Carbon Prediction,10.3390/rs13091791,crossref,"<jats:p>Pilot studies have demonstrated the potential of remote sensing for soil organic carbon (SOC) mapping in exposed croplands. However, the use of remote sensing for SOC prediction is often hindered by disturbing factors at the soil surface, such as photosynthetic active and non-photosynthetic active vegetation, variation in soil moisture or surface roughness. With the increasing amount of freely available satellite data, recent studies have focused on stabilizing the soil reflectance by building image composites. These composites tend to minimize the disturbing effects by applying sets of criteria. Here, we aim to develop a robust method that allows selecting Sentinel-2 (S-2) pixels with minimal influence of the following disturbing factors: crop residues, surface roughness and soil moisture. We selected all S-2 cloud-free images covering the Belgian Loam Belt from January 2019 to December 2020 (in total 36 images). We then built nine exposed soil composites based on four sets of criteria: (1) lowest Normalized Burn Ratio (NBR2), (2) Normalized Difference Vegetation Index (NDVI) &lt; 0.25, (3–5) NDVI &lt; 0.25 and NBR2 &lt; threshold, (6) the ‘greening-up’ period of a crop and (7–9) the ‘greening-up’ period of a crop and NBR2 &lt; threshold. The ‘greening-up’ period was selected based on the NDVI timeline, where ‘greening-up’ is considered as the last date of acquisition where the soil is exposed (NDVI &lt; 0.25) before the crop develops (NDVI &gt; 0.25). We then built a partial least square regression (PLSR) model with 10-fold cross-validation to estimate the SOC content based on 137 georeferenced calibration samples on the nine composites. We obtained non-satisfactory results (R2 &lt; 0.30, RMSE &gt; 2.50 g C kg–1, and RPD &lt; 1.4, n &gt; 68) for all composites except for the composite in the ‘greening-up’ stage with a NBR2 &lt; 0.07 (R2 = 0.54 ± 0.12, RPD = 1.68 ± 0.45 and RMSE = 2.09 ± 0.39 g C kg–1, n = 49). Hence, the ‘greening-up’ method combined with a strict NBR2 threshold allows selecting the purest exposed soil pixels suitable for SOC prediction. The limit of this method might be its coverage of the total cropland area, which in a two-year period reached 62%, compared to 95% coverage if only the NDVI threshold is applied.</jats:p>","pilot studies have demonstrated the potential of remote sensing for soil organic carbon (soc) mapping in exposed croplands. however, the use of remote sensing for soc prediction is often hindered by disturbing factors at the soil surface, such as photosynthetic active and non-photosynthetic active vegetation, variation in soil moisture or surface roughness. with the increasing amount of freely available satellite data, recent studies have focused on stabilizing the soil reflectance by building image composites. these composites tend to minimize the disturbing effects by applying sets of criteria. here, we aim to develop a robust method that allows selecting sentinel-2 (s-2) pixels with minimal influence of the following disturbing factors: crop residues, surface roughness and soil moisture. we selected all s-2 cloud-free images covering the belgian loam belt from january 2019 to december 2020 (in total 36 images). we then built nine exposed soil composites based on four sets of criteria: (1) lowest normalized burn ratio (nbr2), (2) normalized difference vegetation index (ndvi) &lt; 0.25, (3–5) ndvi &lt; 0.25 and nbr2 &lt; threshold, (6) the ‘greening-up’ period of a crop and (7–9) the ‘greening-up’ period of a crop and nbr2 &lt; threshold. the ‘greening-up’ period was selected based on the ndvi timeline, where ‘greening-up’ is considered as the last date of acquisition where the soil is exposed (ndvi &lt; 0.25) before the crop develops (ndvi &gt; 0.25). we then built a partial least square regression (plsr) model with 10-fold cross-validation to estimate the soc content based on 137 georeferenced calibration samples on the nine composites. we obtained non-satisfactory results (r2 &lt; 0.30, rmse &gt; 2.50 g c kg–1, and rpd &lt; 1.4, n &gt; 68) for all composites except for the composite in the ‘greening-up’ stage with a nbr2 &lt; 0.07 (r2 = 0.54 ± 0.12, rpd = 1.68 ± 0.45 and rmse = 2.09 ± 0.39 g c kg–1, n = 49). hence, the ‘greening-up’ method combined with a strict nbr2 threshold allows selecting the purest exposed soil pixels suitable for soc prediction. the limit of this method might be its coverage of the total cropland area, which in a two-year period reached 62%, compared to 95% coverage if only the ndvi threshold is applied."
http://orkg.org/orkg/resource/R200014,Presence of Antibodies against Genogroup VI Norovirus in Humans,10.1186/1743-422x-10-176,crossref,"<jats:title>Abstract</jats:title>\n          <jats:sec>\n            <jats:title>Background</jats:title>\n            <jats:p>Noroviruses are important enteric pathogens in humans and animals. Recently, we reported a novel canine norovirus (CaNoV) in dogs with diarrhea belonging to a new genogroup (GVI). No data are available on exposure of humans to this virus.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Methods</jats:title>\n            <jats:p>Sera from 373 small animal veterinarians and 120 age-matched population controls were tested for IgG antibodies to CaNoV by a recombinant virus like particle based enzyme-linked immunosorbent assay.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Results</jats:title>\n            <jats:p>Antibodies to CaNoV were found in 22.3% of the veterinarians and 5.8% of the control group (p\u2009&lt;\u20090.001). Mean corrected OD<jats:sub>450</jats:sub> values for CaNoV antibodies were significantly higher in small animal veterinarians compared to the control group.</jats:p>\n          </jats:sec>\n          <jats:sec>\n            <jats:title>Conclusions</jats:title>\n            <jats:p>These findings suggest that CaNoV may infect humans and small animal veterinarians are at an increased risk for exposure to this virus. Additional studies are needed to assess if this virus is able to cause disease in humans.</jats:p>\n          </jats:sec>","abstract \n \n background \n noroviruses are important enteric pathogens in humans and animals. recently, we reported a novel canine norovirus (canov) in dogs with diarrhea belonging to a new genogroup (gvi). no data are available on exposure of humans to this virus. \n \n \n methods \n sera from 373 small animal veterinarians and 120 age-matched population controls were tested for igg antibodies to canov by a recombinant virus like particle based enzyme-linked immunosorbent assay. \n \n \n results \n antibodies to canov were found in 22.3% of the veterinarians and 5.8% of the control group (p\u2009&lt;\u20090.001). mean corrected od 450 values for canov antibodies were significantly higher in small animal veterinarians compared to the control group. \n \n \n conclusions \n these findings suggest that canov may infect humans and small animal veterinarians are at an increased risk for exposure to this virus. additional studies are needed to assess if this virus is able to cause disease in humans. \n"
http://orkg.org/orkg/resource/R200003,Detection of Antibodies against Norovirus Genogroup GIV in Carnivores,10.1128/cvi.00312-09,crossref,"<jats:title>ABSTRACT</jats:title>\n          <jats:p>Noroviruses (NoVs) resembling human NoV genotype GIV (Alphatron-like) have recently been detected in carnivores. By using an enzyme-linked immunosorbent assay based on baculovirus-expressed capsid protein VP1 of lion strain GGIV.2/Pistoia/387/06/ITA, NoV-specific antibodies were detected in cats (16.11%) and dogs (4.8%), demonstrating that these animals are exposed to infections caused by NoVs.</jats:p>","abstract \n noroviruses (novs) resembling human nov genotype giv (alphatron-like) have recently been detected in carnivores. by using an enzyme-linked immunosorbent assay based on baculovirus-expressed capsid protein vp1 of lion strain ggiv.2/pistoia/387/06/ita, nov-specific antibodies were detected in cats (16.11%) and dogs (4.8%), demonstrating that these animals are exposed to infections caused by novs."
http://orkg.org/orkg/resource/R199183,Exposure to Human and Bovine Noroviruses in a Birth Cohort in Southern India from 2002 to 2006,10.1128/jcm.01015-13,crossref,<jats:title>ABSTRACT</jats:title>\n          <jats:p>Human and bovine norovirus virus-like particles were used to evaluate antibodies in Indian children at ages 6 and 36 months and their mothers. Antibodies to genogroup II viruses were acquired early and were more prevalent than antibodies to genogroup I. Low levels of IgG antibodies against bovine noroviruses indicate possible zoonotic transmission.</jats:p>,abstract \n human and bovine norovirus virus-like particles were used to evaluate antibodies in indian children at ages 6 and 36 months and their mothers. antibodies to genogroup ii viruses were acquired early and were more prevalent than antibodies to genogroup i. low levels of igg antibodies against bovine noroviruses indicate possible zoonotic transmission.
http://orkg.org/orkg/resource/R201001,An Open-Source Platform for Human Pose Estimation and Tracking Using a Heterogeneous Multi-Sensor System,10.3390/s21072340,crossref,"<jats:p>Human pose estimation and tracking in real-time from multi-sensor systems is essential for many applications. Combining multiple heterogeneous sensors increases opportunities to improve human motion tracking. Using only a single sensor type, e.g., inertial sensors, human pose estimation accuracy is affected by sensor drift over longer periods. This paper proposes a human motion tracking system using lidar and inertial sensors to estimate 3D human pose in real-time. Human motion tracking includes human detection and estimation of height, skeletal parameters, position, and orientation by fusing lidar and inertial sensor data. Finally, the estimated data are reconstructed on a virtual 3D avatar. The proposed human pose tracking system was developed using open-source platform APIs. Experimental results verified the proposed human position tracking accuracy in real-time and were in good agreement with current multi-sensor systems.</jats:p>","human pose estimation and tracking in real-time from multi-sensor systems is essential for many applications. combining multiple heterogeneous sensors increases opportunities to improve human motion tracking. using only a single sensor type, e.g., inertial sensors, human pose estimation accuracy is affected by sensor drift over longer periods. this paper proposes a human motion tracking system using lidar and inertial sensors to estimate 3d human pose in real-time. human motion tracking includes human detection and estimation of height, skeletal parameters, position, and orientation by fusing lidar and inertial sensor data. finally, the estimated data are reconstructed on a virtual 3d avatar. the proposed human pose tracking system was developed using open-source platform apis. experimental results verified the proposed human position tracking accuracy in real-time and were in good agreement with current multi-sensor systems."
http://orkg.org/orkg/resource/R201004,Fusion of Multiple Lidars and Inertial Sensors for the Real-Time Pose Tracking of Human Motion,10.3390/s20185342,crossref,"<jats:p>Today, enhancement in sensing technology enables the use of multiple sensors to track human motion/activity precisely. Tracking human motion has various applications, such as fitness training, healthcare, rehabilitation, human-computer interaction, virtual reality, and activity recognition. Therefore, the fusion of multiple sensors creates new opportunities to develop and improve an existing system. This paper proposes a pose-tracking system by fusing multiple three-dimensional (3D) light detection and ranging (lidar) and inertial measurement unit (IMU) sensors. The initial step estimates the human skeletal parameters proportional to the target user’s height by extracting the point cloud from lidars. Next, IMUs are used to capture the orientation of each skeleton segment and estimate the respective joint positions. In the final stage, the displacement drift in the position is corrected by fusing the data from both sensors in real time. The installation setup is relatively effortless, flexible for sensor locations, and delivers results comparable to the state-of-the-art pose-tracking system. We evaluated the proposed system regarding its accuracy in the user’s height estimation, full-body joint position estimation, and reconstruction of the 3D avatar. We used a publicly available dataset for the experimental evaluation wherever possible. The results reveal that the accuracy of height and the position estimation is well within an acceptable range of ±3–5 cm. The reconstruction of the motion based on the publicly available dataset and our data is precise and realistic.</jats:p>","today, enhancement in sensing technology enables the use of multiple sensors to track human motion/activity precisely. tracking human motion has various applications, such as fitness training, healthcare, rehabilitation, human-computer interaction, virtual reality, and activity recognition. therefore, the fusion of multiple sensors creates new opportunities to develop and improve an existing system. this paper proposes a pose-tracking system by fusing multiple three-dimensional (3d) light detection and ranging (lidar) and inertial measurement unit (imu) sensors. the initial step estimates the human skeletal parameters proportional to the target user’s height by extracting the point cloud from lidars. next, imus are used to capture the orientation of each skeleton segment and estimate the respective joint positions. in the final stage, the displacement drift in the position is corrected by fusing the data from both sensors in real time. the installation setup is relatively effortless, flexible for sensor locations, and delivers results comparable to the state-of-the-art pose-tracking system. we evaluated the proposed system regarding its accuracy in the user’s height estimation, full-body joint position estimation, and reconstruction of the 3d avatar. we used a publicly available dataset for the experimental evaluation wherever possible. the results reveal that the accuracy of height and the position estimation is well within an acceptable range of ±3–5 cm. the reconstruction of the motion based on the publicly available dataset and our data is precise and realistic."
http://orkg.org/orkg/resource/R200146,Digital Soil Mapping of Soil Organic Matter with Deep Learning Algorithms,10.3390/ijgi11050299,crossref,"<jats:p>Digital soil mapping has emerged as a new method to describe the spatial distribution of soils economically and efficiently. In this study, a lightweight soil organic matter (SOM) mapping method based on a deep residual network, which we call LSM-ResNet, is proposed to make accurate predictions with background covariates. ResNet not only integrates spatial background information around the observed environmental covariates, but also reduces problems such as information loss, which undermines the integrity of information and reduces prediction uncertainty. To train the model, rectified linear units, mean squared error, and adaptive momentum estimation were used as the activation function, loss/cost function, and optimizer, respectively. The method was tested with Landsat5, the meteorological data from WorldClim, and the 1602 sampling points set from Xinxiang, China. The performance of the proposed LSM-ResNet was compared to a traditional machine learning algorithm, the random forest (RF) algorithm, and a training set (80%) and a test set (20%) were created to test both models. The results showed that the LSM-ResNet (RMSE = 6.40, R2 = 0.51) model outperformed the RF model in both the roots mean square error (RMSE) and coefficient of determination (R2), and the training accuracy was significantly improved compared to RF (RMSE = 6.81, R2 = 0.46). The trained LSM-ResNet model was used for SOM prediction in Xinxiang, a district of plain terrain in China. The prediction maps can be deemed an accurate reflection of the spatial variability of the SOM distribution.</jats:p>","digital soil mapping has emerged as a new method to describe the spatial distribution of soils economically and efficiently. in this study, a lightweight soil organic matter (som) mapping method based on a deep residual network, which we call lsm-resnet, is proposed to make accurate predictions with background covariates. resnet not only integrates spatial background information around the observed environmental covariates, but also reduces problems such as information loss, which undermines the integrity of information and reduces prediction uncertainty. to train the model, rectified linear units, mean squared error, and adaptive momentum estimation were used as the activation function, loss/cost function, and optimizer, respectively. the method was tested with landsat5, the meteorological data from worldclim, and the 1602 sampling points set from xinxiang, china. the performance of the proposed lsm-resnet was compared to a traditional machine learning algorithm, the random forest (rf) algorithm, and a training set (80%) and a test set (20%) were created to test both models. the results showed that the lsm-resnet (rmse = 6.40, r2 = 0.51) model outperformed the rf model in both the roots mean square error (rmse) and coefficient of determination (r2), and the training accuracy was significantly improved compared to rf (rmse = 6.81, r2 = 0.46). the trained lsm-resnet model was used for som prediction in xinxiang, a district of plain terrain in china. the prediction maps can be deemed an accurate reflection of the spatial variability of the som distribution."
http://orkg.org/orkg/resource/R200157,Mapping Regional Soil Organic Matter Based on Sentinel-2A and MODIS Imagery Using Machine Learning Algorithms and Google Earth Engine,10.3390/rs13152934,crossref,"<jats:p>Many studies have attempted to predict soil organic matter (SOM), whereas mapping high-precision and high-resolution SOM maps remains a challenge due to the difficulty of selecting appropriate satellite data sources and prediction algorithms. This study aimed to investigate the influence of different remotely sensed images and machine learning algorithms on SOM prediction. We constructed two comparative experiments, i.e., full-band and common-band variable datasets of Sentinel-2A and MODIS images using Google Earth Engine (GEE). The predictive performances of random forest (RF), artificial neural network (ANN), and support vector regression (SVR) algorithms were evaluated, and the SOM map was generated for the Songnen Plain. Results showed that the model based on the full-band Sentinel-2A dataset achieved the best performance. The application of Sentinel-2A data resulted in mean relative improvements (RIs) of 7.67% and 5.87%, respectively. The RF achieved a lower root mean squared error (RMSE = 0.68%) and a higher coefficient of determination (R2 = 0.67) in all of the predicted scenarios than ANN and SVR. The resultant SOM map accurately characterized the SOM spatial distribution. Therefore, the Sentinel-2A data have obvious advantages over MODIS due to their higher spectral and spatial resolutions, and the combination of the RF algorithm and GEE is an effective approach to SOM mapping.</jats:p>","many studies have attempted to predict soil organic matter (som), whereas mapping high-precision and high-resolution som maps remains a challenge due to the difficulty of selecting appropriate satellite data sources and prediction algorithms. this study aimed to investigate the influence of different remotely sensed images and machine learning algorithms on som prediction. we constructed two comparative experiments, i.e., full-band and common-band variable datasets of sentinel-2a and modis images using google earth engine (gee). the predictive performances of random forest (rf), artificial neural network (ann), and support vector regression (svr) algorithms were evaluated, and the som map was generated for the songnen plain. results showed that the model based on the full-band sentinel-2a dataset achieved the best performance. the application of sentinel-2a data resulted in mean relative improvements (ris) of 7.67% and 5.87%, respectively. the rf achieved a lower root mean squared error (rmse = 0.68%) and a higher coefficient of determination (r2 = 0.67) in all of the predicted scenarios than ann and svr. the resultant som map accurately characterized the som spatial distribution. therefore, the sentinel-2a data have obvious advantages over modis due to their higher spectral and spatial resolutions, and the combination of the rf algorithm and gee is an effective approach to som mapping."
http://orkg.org/orkg/resource/R201007,Motion-Sphere: Visual Representation of the Subtle Motion of Human Joints,10.3390/app10186462,crossref,"<jats:p>Understanding and differentiating subtle human motion over time as sequential data is challenging. We propose Motion-sphere, which is a novel trajectory-based visualization technique, to represent human motion on a unit sphere. Motion-sphere adopts a two-fold approach for human motion visualization, namely a three-dimensional (3D) avatar to reconstruct the target motion and an interactive 3D unit sphere, that enables users to perceive subtle human motion as swing trajectories and color-coded miniature 3D models for twist. This also allows for the simultaneous visual comparison of two motions. Therefore, the technique is applicable in a wide range of applications, including rehabilitation, choreography, and physical fitness training. The current work validates the effectiveness of the proposed work with a user study in comparison with existing motion visualization methods. Our study’s findings show that Motion-sphere is informative in terms of quantifying the swing and twist movements. The Motion-sphere is validated in threefold ways: validation of motion reconstruction on the avatar, accuracy of swing, twist, and speed visualization, and the usability and learnability of the Motion-sphere. Multiple range of motions from an online open database are selectively chosen, such that all joint segments are covered. In all fronts, Motion-sphere fares well. Visualization on the 3D unit sphere and the reconstructed 3D avatar make it intuitive to understand the nature of human motion.</jats:p>","understanding and differentiating subtle human motion over time as sequential data is challenging. we propose motion-sphere, which is a novel trajectory-based visualization technique, to represent human motion on a unit sphere. motion-sphere adopts a two-fold approach for human motion visualization, namely a three-dimensional (3d) avatar to reconstruct the target motion and an interactive 3d unit sphere, that enables users to perceive subtle human motion as swing trajectories and color-coded miniature 3d models for twist. this also allows for the simultaneous visual comparison of two motions. therefore, the technique is applicable in a wide range of applications, including rehabilitation, choreography, and physical fitness training. the current work validates the effectiveness of the proposed work with a user study in comparison with existing motion visualization methods. our study’s findings show that motion-sphere is informative in terms of quantifying the swing and twist movements. the motion-sphere is validated in threefold ways: validation of motion reconstruction on the avatar, accuracy of swing, twist, and speed visualization, and the usability and learnability of the motion-sphere. multiple range of motions from an online open database are selectively chosen, such that all joint segments are covered. in all fronts, motion-sphere fares well. visualization on the 3d unit sphere and the reconstructed 3d avatar make it intuitive to understand the nature of human motion."
http://orkg.org/orkg/resource/R201108,Effect of Road Traffic on Air Pollution. Experimental Evidence from COVID-19 Lockdown,10.3390/su12218984,crossref,"<jats:p>The increasing concentration of human activities in cities has been leading to a worsening in air quality, thus negatively affecting the lives and health of humans living in urban contexts. Transport is one of the main sources of pollution in such environments. Several local authorities have therefore implemented strict traffic-restriction measures. The aim of this paper is to evaluate the effectiveness and limitations of these interventions, by analyzing the relationship between traffic flows and air quality. The used dataset contains concentrations of NO, NO2, NOx and PM10, vehicle counts and meteorology, all collected during the COVID-19 lockdown in the city of Padova (Italy), in which severe limitations to contain the spread of the virus simulated long and large-scale traffic restrictions in normal conditions. In particular, statistical tests, correlation analyses and multivariate linear regression models were applied to non-rainy days in 2020, 2018 and 2017, in order to isolate the effect of traffic. Analysis indicated that vehicle flows significantly affect NO, NO2, and NOx concentrations, although no evidence of a relationship between traffic and PM10 was highlighted. According to this perspective, measures to limit traffic flows seem to be effective in improving air quality only in terms of reducing nitrogen oxide.</jats:p>","the increasing concentration of human activities in cities has been leading to a worsening in air quality, thus negatively affecting the lives and health of humans living in urban contexts. transport is one of the main sources of pollution in such environments. several local authorities have therefore implemented strict traffic-restriction measures. the aim of this paper is to evaluate the effectiveness and limitations of these interventions, by analyzing the relationship between traffic flows and air quality. the used dataset contains concentrations of no, no2, nox and pm10, vehicle counts and meteorology, all collected during the covid-19 lockdown in the city of padova (italy), in which severe limitations to contain the spread of the virus simulated long and large-scale traffic restrictions in normal conditions. in particular, statistical tests, correlation analyses and multivariate linear regression models were applied to non-rainy days in 2020, 2018 and 2017, in order to isolate the effect of traffic. analysis indicated that vehicle flows significantly affect no, no2, and nox concentrations, although no evidence of a relationship between traffic and pm10 was highlighted. according to this perspective, measures to limit traffic flows seem to be effective in improving air quality only in terms of reducing nitrogen oxide."
