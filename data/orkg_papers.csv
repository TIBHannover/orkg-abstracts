uri,title,doi,abstract,processed_abstract
http://orkg.org/orkg/resource/R1000,,,,
http://orkg.org/orkg/resource/R10015,MapSDI: A Scaled-up Semantic Data Integration Framework for Knowledge Graph Creation,10.1007/978-3-030-33246-4_4,,
http://orkg.org/orkg/resource/R10024,Reporting quality of travel and non-travel activities: A comparison of three different survey formats,10.1016/j.trpro.2018.10.057,,
http://orkg.org/orkg/resource/R1004,"A semi-automated, KNIME-based workflow for biofilm assays",10.1186/s12866-016-0676-9,"Background A current focus of biofilm research is the chemical interaction between microorganisms within the biofilms. Prerequisites for this research are bioassay systems which integrate reliable tools for the planning of experiments with robot-assisted measurements and with rapid data processing. Here, data structures that are both human- and machine readable may be particularly useful. Results In this report, we present several simplification and robotisation options for an assay of bacteria-induced biofilm formation by the freshwater diatom Achnanthidium minutissimum. We also tested several proof-of-concept robotisation methods for pipetting, as well as for measuring the biofilm absorbance directly in the multi-well plates. Furthermore, we exemplify the implementation of an improved data processing workflow for this assay using the Konstanz Information Miner (KNIME), a free and open source data analysis environment. The workflow integrates experiment planning files and absorbance read-out data, towards their automated processing for analysis. Conclusions Our workflow lead to a substantial reduction of the measurement and data processing workload, while still reproducing previously obtained results in the A. minutissimum biofilm assay. The methods, scripts and files we designed are described here, offering adaptable options for other medium-throughput biofilm screenings. ; published","background a current focus of biofilm research is the chemical interaction between microorganisms within the biofilms. prerequisites for this research are bioassay systems which integrate reliable tools for the planning of experiments with robot-assisted measurements and with rapid data processing. here, data structures that are both human- and machine readable may be particularly useful. results in this report, we present several simplification and robotisation options for an assay of bacteria-induced biofilm formation by the freshwater diatom achnanthidium minutissimum. we also tested several proof-of-concept robotisation methods for pipetting, as well as for measuring the biofilm absorbance directly in the multi-well plates. furthermore, we exemplify the implementation of an improved data processing workflow for this assay using the konstanz information miner (knime), a free and open source data analysis environment. the workflow integrates experiment planning files and absorbance read-out data, towards their automated processing for analysis. conclusions our workflow lead to a substantial reduction of the measurement and data processing workload, while still reproducing previously obtained results in the a. minutissimum biofilm assay. the methods, scripts and files we designed are described here, offering adaptable options for other medium-throughput biofilm screenings. ; published"
http://orkg.org/orkg/resource/R1013,Capsules of the diatomAchnanthidium minutissimumarise from fibrillar precursors and foster attachment of bacteria,10.7717/peerj.858,"Achnanthidium minutissimum is a benthic freshwater diatom that forms biofilms on submerged surfaces in aquatic environments. Within these biofilms, A. minutissimum cells produce extracellular structures which facilitate substrate adhesion, such as stalks and capsules. Both consist of extracellular polymeric substance (EPS), but the microstructure and development stages of the capsules are so far unknown, despite a number of hypotheses about their function, including attachment and protection. We coupled scanning electron microscopy (SEM) to bright-field microscopy (BFM) and found that A. minutissimum capsules mostly possess an unstructured surface. However, capsule material that was mechanically stressed by being stretched between or around cells displayed fibrillar substructures. Fibrils were also found on the frustules of non-encapsulated cells, implicating that A. minutissimum capsules may develop from fibrillar precursors. Energy-dispersive X-ray (EDX) spectroscopy revealed that the capsule material do not contain silicon, distinguishing it from the frustule material. We furthermore show that bacteria preferentially attach to capsules, instead of non-encapsulated A. minutissimum cells, which supports the idea that capsules mediate diatom-bacteria interactions.","achnanthidium minutissimum is a benthic freshwater diatom that forms biofilms on submerged surfaces in aquatic environments. within these biofilms, a. minutissimum cells produce extracellular structures which facilitate substrate adhesion, such as stalks and capsules. both consist of extracellular polymeric substance (eps), but the microstructure and development stages of the capsules are so far unknown, despite a number of hypotheses about their function, including attachment and protection. we coupled scanning electron microscopy (sem) to bright-field microscopy (bfm) and found that a. minutissimum capsules mostly possess an unstructured surface. however, capsule material that was mechanically stressed by being stretched between or around cells displayed fibrillar substructures. fibrils were also found on the frustules of non-encapsulated cells, implicating that a. minutissimum capsules may develop from fibrillar precursors. energy-dispersive x-ray (edx) spectroscopy revealed that the capsule material do not contain silicon, distinguishing it from the frustule material. we furthermore show that bacteria preferentially attach to capsules, instead of non-encapsulated a. minutissimum cells, which supports the idea that capsules mediate diatom-bacteria interactions."
http://orkg.org/orkg/resource/R1020,Open Research Knowledge Graph: Towards Machine Actionability in Scholarly Communication,,,
http://orkg.org/orkg/resource/R107613,Static analysis and optimization of semantic web queries,10.1145/2500130,,
http://orkg.org/orkg/resource/R107618,Instructional interface’s blueprint for guiding instructional-technological interactions’ research: the Big Bang shift in K-12,10.1007/s11423-020-09885-z,,
http://orkg.org/orkg/resource/R107637,Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets,10.1145/3165713,,
http://orkg.org/orkg/resource/R107647,LOD-a-lot: A Queryable Dump of the LOD Cloud,10.1007/978-3-319-68204-4_7,,
http://orkg.org/orkg/resource/R107655,LOD Lab: Experiments at LOD Scale,10.1007/978-3-319-25010-6_23,,
http://orkg.org/orkg/resource/R107663,Dimensions of transactional distance in the world wide web learning environment: a factor analysis,10.1111/1467-8535.00213,"<p>Moore's Theory of Transactional Distance hypothesizes that distance is a pedagogical, not geographic phenomenon. It is a distance of understandings and perceptions that might lead to a communication gap or a psychological space of potential misunderstandings between people. Moore also suggests that this distance has to be overcome if effective, deliberate, planned learning is to occur. However, the conceptualizations of transactional distance in a telecommunication era have not been systematically addressed. Investigating 71 learners' experiences with WorldWide Web, this study examined the postulate of Moore's theory and identified the dimensions (factors) constituting transactional distance in such learning environment. Exploratory factor analysis using a principal axis factor method was carried out. It was concluded that this concept represented multifaceted ideas. Transactional distance consisted of four dimensions—instructor‐learner, learner‐learner, learner‐content, and learner‐interface transactional distance. The results inform researchers and practitioners of Web‐based instruction concerning the factors of transactional distance that need to be taken into account and overcome in WWW courses.</p>","moore's theory of transactional distance hypothesizes that distance is a pedagogical, not geographic phenomenon. it is a distance of understandings and perceptions that might lead to a communication gap or a psychological space of potential misunderstandings between people. moore also suggests that this distance has to be overcome if effective, deliberate, planned learning is to occur. however, the conceptualizations of transactional distance in a telecommunication era have not been systematically addressed. investigating 71 learners' experiences with worldwide web, this study examined the postulate of moore's theory and identified the dimensions (factors) constituting transactional distance in such learning environment. exploratory factor analysis using a principal axis factor method was carried out. it was concluded that this concept represented multifaceted ideas. transactional distance consisted of four dimensions—instructor‐learner, learner‐learner, learner‐content, and learner‐interface transactional distance. the results inform researchers and practitioners of web‐based instruction concerning the factors of transactional distance that need to be taken into account and overcome in www courses."
http://orkg.org/orkg/resource/R107671,SPARQLES: Monitoring public SPARQL endpoints,10.3233/SW-170254,"<p>We describe SPARQLES: an online system that monitors the health of public SPARQL endpoints on the Web by probing them with custom-designed queries at regular intervals. We present the architecture of SPARQLES and the variety of analytics that it runs over public SPARQL endpoints, categorised by availability, discoverability, performance and interoperability. We also detail the interfaces that the system provides for human and software agents to learn more about the recent history and current state of an individual SPARQL endpoint or about overall trends concerning the maturity of all endpoints monitored by the system. We likewise present some details of the performance of the system and the impact it has had thus far.</p>","we describe sparqles: an online system that monitors the health of public sparql endpoints on the web by probing them with custom-designed queries at regular intervals. we present the architecture of sparqles and the variety of analytics that it runs over public sparql endpoints, categorised by availability, discoverability, performance and interoperability. we also detail the interfaces that the system provides for human and software agents to learn more about the recent history and current state of an individual sparql endpoint or about overall trends concerning the maturity of all endpoints monitored by the system. we likewise present some details of the performance of the system and the impact it has had thus far."
http://orkg.org/orkg/resource/R107696,The Influence of Message Framing on Intentions to Perform Health Behaviors,10.1006/jesp.1993.1019,"<section-title>Abstract</section-title><abstract-sec><simple-para>Prospect Theory proposes that people prefer taking risks to options that are certain when considering losses and prefer certainty to risk when considering gains (Kahneman & Tversky, 1979). As a result, individuals are expected to be persuaded to take risks when exposed to negatively framed messages. For instance, Meyerowitz and Chaiken (1987) demonstrated that exposure to negatively framed information promotes breast self-examination. However, the influence of message framing on other health behaviors has been inconsistent. Two studies examined the moderating effect of involvement with the health issue and type of target behavior on the influence of message framing on intentions to perform health behaviors relevant to preventing or detecting skin cancer. In our samples, women as compared to men were more concerned about sun tanning and skin cancer and therefore were considered to be more involved with this health issue. In Experiment 1, exposure to negatively framed versus positively framed messages differentially influenced the intentions of female (high involvement) and male (low involvement) subjects to obtain a skin cancer detection examination. In Experiment 2, women who read positively framed pamphlets were more likely than those who read negatively framed pamphlets to request sunscreen with an appropriate sun protection factor (a prevention behavior).</simple-para></abstract-sec>","abstract prospect theory proposes that people prefer taking risks to options that are certain when considering losses and prefer certainty to risk when considering gains (kahneman & tversky, 1979). as a result, individuals are expected to be persuaded to take risks when exposed to negatively framed messages. for instance, meyerowitz and chaiken (1987) demonstrated that exposure to negatively framed information promotes breast self-examination. however, the influence of message framing on other health behaviors has been inconsistent. two studies examined the moderating effect of involvement with the health issue and type of target behavior on the influence of message framing on intentions to perform health behaviors relevant to preventing or detecting skin cancer. in our samples, women as compared to men were more concerned about sun tanning and skin cancer and therefore were considered to be more involved with this health issue. in experiment 1, exposure to negatively framed versus positively framed messages differentially influenced the intentions of female (high involvement) and male (low involvement) subjects to obtain a skin cancer detection examination. in experiment 2, women who read positively framed pamphlets were more likely than those who read negatively framed pamphlets to request sunscreen with an appropriate sun protection factor (a prevention behavior)."
http://orkg.org/orkg/resource/R107739,Message framing and sunscreen use: Gain-framed messages motivate beach-goers.,10.1037/0278-6133.18.2.189,,
http://orkg.org/orkg/resource/R107744,The trade-off behaviours between virtual and physical activities during the first wave of the COVID-19 pandemic period,10.1186/s12544-021-00473-7,"Abstract Introduction The first wave of COVID-19 pandemic period has drastically changed people’s lives all over the world. To cope with the disruption, digital solutions have become more popular. However, the ability to adopt digitalised alternatives is different across socio-economic and socio-demographic groups. Objective This study investigates how individuals have changed their activity-travel patterns and internet usage during the first wave of the COVID-19 pandemicperiod, and which of these changes may be kept. Methods An empirical data collection was deployed through online forms. 781 responses from different countries (Italy, Sweden, India and others) have beencollected, and a series of multivariate analyses was carried out. Two linear regression models are presented, related to the change of travel activities andinternet usage, before and during the pandemic period. Furthermore, a binary regression model is used to examine the likelihood of the respondents to adoptand keep their behaviours beyond the pandemic period. Results The results show that the possibility to change the behaviour matter. External restrictions and personal characteristics are the driving factors of the reductionin ones' daily trips. However, the estimation results do not show a strong correlation between the countries' restriction policy and the respondents' likelihoodto adopt the new and online-based behaviours for any of the activities after the restriction period. Conclusion The acceptance and long-term adoption of the online alternatives for activities are correlated with the respondents' personality and socio-demographicgroup, highlighting the importance of promoting alternatives as a part of longer-term behavioural and lifestyle changes.","abstract introduction the first wave of covid-19 pandemic period has drastically changed people’s lives all over the world. to cope with the disruption, digital solutions have become more popular. however, the ability to adopt digitalised alternatives is different across socio-economic and socio-demographic groups. objective this study investigates how individuals have changed their activity-travel patterns and internet usage during the first wave of the covid-19 pandemicperiod, and which of these changes may be kept. methods an empirical data collection was deployed through online forms. 781 responses from different countries (italy, sweden, india and others) have beencollected, and a series of multivariate analyses was carried out. two linear regression models are presented, related to the change of travel activities andinternet usage, before and during the pandemic period. furthermore, a binary regression model is used to examine the likelihood of the respondents to adoptand keep their behaviours beyond the pandemic period. results the results show that the possibility to change the behaviour matter. external restrictions and personal characteristics are the driving factors of the reductionin ones' daily trips. however, the estimation results do not show a strong correlation between the countries' restriction policy and the respondents' likelihoodto adopt the new and online-based behaviours for any of the activities after the restriction period. conclusion the acceptance and long-term adoption of the online alternatives for activities are correlated with the respondents' personality and socio-demographicgroup, highlighting the importance of promoting alternatives as a part of longer-term behavioural and lifestyle changes."
http://orkg.org/orkg/resource/R107834,The future of learner-interface interaction. A vision from EdTech,10.32870/Ap.v12n2.1910,,
http://orkg.org/orkg/resource/R107843,Getting the Mix Right Again: An updated and theoretical rationale for interaction,10.19173/irrodl.v4i2.149 ,,
http://orkg.org/orkg/resource/R107855,"Phytochemical Screening, Analgesic and Anti-inflammatory Properties and Median Lethal Dose of Ethanol Leaf Extract of Wild Species of Eryngium foetidum L. on Albino Rats",,,
http://orkg.org/orkg/resource/R107858,HAEMOPOIETIC EFFECT OF Cucurbita pepo LINN. DIET PREPARATION IN ALBINO RATS,,,
http://orkg.org/orkg/resource/R107861,"INFLUENCE OF SESAMUM INDICUM L. ETHANOLIC LEAF EXTRACT ON HAEMATOLOGICAL PARAMETERS ON ALBINO RATS IN AKWA IBOM STATE, NIGERIA",,,
http://orkg.org/orkg/resource/R107864,"Phytochemical screening and effect of ethanolic root extract of Uvaria chamae on haematological parameters on albino rats in Akwa Ibom State, Nigeria",,,
http://orkg.org/orkg/resource/R107867,Haematopoietic Properties of Ethanolic Fruit Extract of Musa Acuminata on Albino Rats,,,
http://orkg.org/orkg/resource/R107870,ANTIDIABETIC EFFECT OF Dioscorea bulbifera ON ALLOXANINDUCED DIABETIC RATS,,,
http://orkg.org/orkg/resource/R107873,Evaluation of concentration of heavy metals in leaf tissues of three improved varieties of Manihot esculenta Crantz,,,
http://orkg.org/orkg/resource/R107876,"Influence of nutrient amendments of soil quality on germination, growth and yield components of two varieties of okra (Abelmoschus esculentus (L.) Moench) sown at University of Uyo botanical garden, Uyo, Akwa Ibom State",,,
http://orkg.org/orkg/resource/R107879,HAEMOPOIETIC EFFECTS OF ETHANOLIC EXTRACT OF Stellaria media (L.) LEAVES ON ALBINO MICE,,,
http://orkg.org/orkg/resource/R107882,PHYTOCHEMICAL SCREENING AND ANTIDIABETIC PROPERTIES OF Musa acuminata ETHANOLIC FRUIT EXTRACT ON ALLOXAN INDUCED DIABETIC ALBINO RATS. ,,,
http://orkg.org/orkg/resource/R107885,"Effect of Water Stress on Some Growth Aspects of Two Varieties of Cowpea: Vigna unguiculata (L.) Walp, Fabaceae",,,
http://orkg.org/orkg/resource/R107888,Antidiabetic Effect of Pleurotus ostreatus (Jacq. ex Fr) kumm. Mushroom on Alloxan-induced Diabetic Rats,,,
http://orkg.org/orkg/resource/R107891,Effects of Nutrient Amendments of Spent Engine Oil Polluted Soil on Some Growth Parameters of Abelmoschus esculentus (L.) Moench. in South-South Nigeria,,,
http://orkg.org/orkg/resource/R107894,Phytochemical screening and anti-inflammatory effect of ethanolic and aqueous extract of Nephrolepis biserrata leaf on Albino Wistar Mice,,,
http://orkg.org/orkg/resource/R107897,Phytochemical Screening and Anti-hyperglycaemic Activity of Ethanolic Extract of Terminalia ivorensis A. Chev. Leaves on Albino Wistar Rats.,,,
http://orkg.org/orkg/resource/R107900,Phytochemical Screening and Haemopoetic Study of the Ethanolic Root Extract of Baphia nitida Lodd on Albino Rats ,,,
http://orkg.org/orkg/resource/R107903,NUTRACEUTICAL POTENTIALS OF WILD AND NEGLECTED EDIBLE LEAFY VEGETABLES (Sterculia tragacantha and Sesamum indicum) IN AKWA IBOM STATE-NIGERIA,,,
http://orkg.org/orkg/resource/R107906,"Phytochemical and Physicochemical Properties of Leaf, Stem and Flowers of Luffa aegyptiaca (Johann Veslingius)",,,
http://orkg.org/orkg/resource/R107933,The Ontology-based Business Architecture Engineering Framework,10.3233/978-1-60750-831-1-233,,
http://orkg.org/orkg/resource/R108037,Bottom-up energy supply optimization of a national building stock,10.1016/j.enbuild.2019.109667,"<section-title>Abstract</section-title>
         <abstract-sec><simple-para>The installation and operation distributed energy resources (DER) and the electrification of the heat supply significantly changes the interaction of the residential building stock with the grid infrastructure. Evaluating the mass deployment of DER at the national level would require analyzing millions of individual buildings, entailing significant computational burden.</simple-para>
            <simple-para>To overcome this, this work proposes a novel bottom-up model that consists of an aggregation algorithm to create a spatially distributed set of typical residential buildings from census data. Each typical building is then optimized with a Mixed-Integer Linear Program to derive its cost optimal technology adoption and operation, determining its changing grid load in future scenarios.</simple-para>
            <simple-para>The model is validated for Germany, with 200 typical buildings considered to sufficiently represent the diversity of the residential building stock. In a future scenario for 2050, photovoltaic and heat pumps are predicted to be the most economically and ecologically robust supply solutions for the different building types. Nevertheless, their electricity generation and demand temporally do not match, resulting in a doubling of the peak electricity grid load in the rural areas during the winter. The urban areas can compensate this with efficient co-generation units, which are not cost-efficient in the rural areas.</simple-para></abstract-sec>","abstract the installation and operation distributed energy resources (der) and the electrification of the heat supply significantly changes the interaction of the residential building stock with the grid infrastructure. evaluating the mass deployment of der at the national level would require analyzing millions of individual buildings, entailing significant computational burden. to overcome this, this work proposes a novel bottom-up model that consists of an aggregation algorithm to create a spatially distributed set of typical residential buildings from census data. each typical building is then optimized with a mixed-integer linear program to derive its cost optimal technology adoption and operation, determining its changing grid load in future scenarios. the model is validated for germany, with 200 typical buildings considered to sufficiently represent the diversity of the residential building stock. in a future scenario for 2050, photovoltaic and heat pumps are predicted to be the most economically and ecologically robust supply solutions for the different building types. nevertheless, their electricity generation and demand temporally do not match, resulting in a doubling of the peak electricity grid load in the rural areas during the winter. the urban areas can compensate this with efficient co-generation units, which are not cost-efficient in the rural areas."
http://orkg.org/orkg/resource/R108050,The Architecture of the ArchiMate Language,10.1007/978-3-642-01862-6_30,,
http://orkg.org/orkg/resource/R108058,Defining Requirements for a Business Architecture Standard,,,
http://orkg.org/orkg/resource/R108100,Business Motivation Model (BMM) Specification,,,
http://orkg.org/orkg/resource/R108102,First impressions from the PRISMA  hyperspectral mission,10.18520/cs/v119/i8/1267-1281,,
http://orkg.org/orkg/resource/R108126,"The Performance of the Satellite-borne Hyperion Hyperspectral VNIR-SWIR Imaging System for Mineral Mapping at Mount Fitton, South Australia",10.1109/IGARSS.2001.976142,"Satellite-based hyperspectral imaging became a reality in November 2000 with the successful launch and operation of the Hyperion system on board the EO-1 platform. Hyperion is a pushbroom imager with 220 spectral bands in the 400-2500 nm wavelength range, a 30 meter pixel size and a 7.5 km swath. Pre-launch characterization of Hyperion measured low signal to noise (SNR<40:1) for the geologically significant shortwave infrared (SWIR) wavelength region (2000-2500 nm). The impact of this low SNR on Hyperion's capacity to resolve spectral detail was evaluated for the Mount Fitton test site in South Australia, which comprises a diverse range of minerals with narrow, diagnostic absorption bands in the SWIR. Following radiative transfer correction of the Hyperion radiance at sensor data to surface radiance (apparent reflectance), diagnostic spectral signatures were clearly apparent, including: green vegetation; talc; dolomite; chlorite; white mica and possibly tremolite. Even though the derived surface composition maps generated from these image endmembers were noisy (both random and column), they were nonetheless spatially coherent and correlated well with the known geology. In addition, the Hyperion data were used to measure and map spectral shifts of <10 nm in the SWIR related to white mica chemical variations.","satellite-based hyperspectral imaging became a reality in november 2000 with the successful launch and operation of the hyperion system on board the eo-1 platform. hyperion is a pushbroom imager with 220 spectral bands in the 400-2500 nm wavelength range, a 30 meter pixel size and a 7.5 km swath. pre-launch characterization of hyperion measured low signal to noise (snr<40:1) for the geologically significant shortwave infrared (swir) wavelength region (2000-2500 nm). the impact of this low snr on hyperion's capacity to resolve spectral detail was evaluated for the mount fitton test site in south australia, which comprises a diverse range of minerals with narrow, diagnostic absorption bands in the swir. following radiative transfer correction of the hyperion radiance at sensor data to surface radiance (apparent reflectance), diagnostic spectral signatures were clearly apparent, including: green vegetation; talc; dolomite; chlorite; white mica and possibly tremolite. even though the derived surface composition maps generated from these image endmembers were noisy (both random and column), they were nonetheless spatially coherent and correlated well with the known geology. in addition, the hyperion data were used to measure and map spectral shifts of <10 nm in the swir related to white mica chemical variations."
http://orkg.org/orkg/resource/R108129,Comparison of Airborne Hyperspectral Data and EO-1 Hyperion for Mineral Mapping,10.1109/TGRS.2003.812908,"Airborne hyperspectral data have been available to researchers since the early 1980s and their use for geologic applications is well documented. The launch of the National Aeronautics and Space Administration Earth Observing 1 Hyperion sensor in November 2000 marked the establishment of a test bed for spaceborne hyperspectral capabilities. Hyperion covers the 0.4-2.5-/spl mu/m range with 242 spectral bands at approximately 10-nm spectral resolution and 30-m spatial resolution. Analytical Imaging and Geophysics LLC and the Commonwealth Scientific and Industrial Research Organisation have been involved in efforts to evaluate, validate, and demonstrate Hyperions's utility for geologic mapping in a variety of sites in the United States and around the world. Initial results over several sites with established ground truth and years of airborne hyperspectral data show that Hyperion data from the shortwave infrared spectrometer can be used to produce useful geologic (mineralogic) information. Minerals mapped include carbonates, chlorite, epidote, kaolinite, alunite, buddingtonite, muscovite, hydrothermal silica, and zeolite. Hyperion data collected under optimum conditions (summer season, bright targets, well-exposed geology) indicate that Hyperion data meet prelaunch specifications and allow subtle distinctions such as determining the difference between calcite and dolomite and mapping solid solution differences in micas caused by substitution in octahedral molecular sites. Comparison of airborne hyperspectral data [from the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS)] to the Hyperion data establishes that Hyperion provides similar basic mineralogic information, with the principal limitation being limited mapping of fine spectral detail under less-than-optimum acquisition conditions (winter season, dark targets) based on lower signal-to-noise ratios. Case histories demonstrate the analysis methodologies and level of information available from the Hyperion data. They also show the viability of Hyperion as a means of extending hyperspectral mineral mapping to areas not accessible to aircraft sensors. The analysis results demonstrate that spaceborne hyperspectral sensors can produce useful mineralogic information, but also indicate that SNR improvements are required for future spaceborne sensors to allow the same level of mapping that is currently possible from airborne sensors such as AVIRIS.","airborne hyperspectral data have been available to researchers since the early 1980s and their use for geologic applications is well documented. the launch of the national aeronautics and space administration earth observing 1 hyperion sensor in november 2000 marked the establishment of a test bed for spaceborne hyperspectral capabilities. hyperion covers the 0.4-2.5-/spl mu/m range with 242 spectral bands at approximately 10-nm spectral resolution and 30-m spatial resolution. analytical imaging and geophysics llc and the commonwealth scientific and industrial research organisation have been involved in efforts to evaluate, validate, and demonstrate hyperions's utility for geologic mapping in a variety of sites in the united states and around the world. initial results over several sites with established ground truth and years of airborne hyperspectral data show that hyperion data from the shortwave infrared spectrometer can be used to produce useful geologic (mineralogic) information. minerals mapped include carbonates, chlorite, epidote, kaolinite, alunite, buddingtonite, muscovite, hydrothermal silica, and zeolite. hyperion data collected under optimum conditions (summer season, bright targets, well-exposed geology) indicate that hyperion data meet prelaunch specifications and allow subtle distinctions such as determining the difference between calcite and dolomite and mapping solid solution differences in micas caused by substitution in octahedral molecular sites. comparison of airborne hyperspectral data [from the airborne visible/infrared imaging spectrometer (aviris)] to the hyperion data establishes that hyperion provides similar basic mineralogic information, with the principal limitation being limited mapping of fine spectral detail under less-than-optimum acquisition conditions (winter season, dark targets) based on lower signal-to-noise ratios. case histories demonstrate the analysis methodologies and level of information available from the hyperion data. they also show the viability of hyperion as a means of extending hyperspectral mineral mapping to areas not accessible to aircraft sensors. the analysis results demonstrate that spaceborne hyperspectral sensors can produce useful mineralogic information, but also indicate that snr improvements are required for future spaceborne sensors to allow the same level of mapping that is currently possible from airborne sensors such as aviris."
http://orkg.org/orkg/resource/R108132,Analysis of spectral absorption features in hyperspectral imagery,10.1016/j.jag.2003.09.001,"<section-title>Abstract</section-title><abstract-sec><simple-para>Spectral reflectance in the visible and near-infrared wavelengths provides a rapid and inexpensive means for determining the mineralogy of samples and obtaining information on chemical composition. Absorption-band parameters such as the position, depth, width, and asymmetry of the feature have been used to quantitatively estimate composition of samples from hyperspectral field and laboratory reflectance data. The parameters have also been used to develop mapping methods for the analysis of hyperspectral image data. This has resulted in techniques providing surface mineralogical information (e.g., classification) using absorption-band depth and position. However, no attempt has been made to prepare images of the absorption-band parameters. In this paper, a simple linear interpolation technique is proposed in order to derive absorption-band position, depth and asymmetry from hyperspectral image data. AVIRIS data acquired in 1995 over the Cuprite mining area (Nevada, USA) are used to demonstrate the technique and to interpret the data in terms of the known alteration phases characterizing the area. A sensitivity analysis of the methods proposed shows that good results can be obtained for estimating the absorption wavelength position, however the estimated absorption-band-depth is sensitive to the input parameters chosen. The resulting parameter images (depth, position, asymmetry of the absorption) when carefully examined and interpreted by an experienced remote sensing geologist provide key information on surface mineralogy. The estimates of depth and position can be related to the chemistry of the samples and thus allow to bridge the gap between field geochemistry and remote sensing.</simple-para></abstract-sec>","abstract spectral reflectance in the visible and near-infrared wavelengths provides a rapid and inexpensive means for determining the mineralogy of samples and obtaining information on chemical composition. absorption-band parameters such as the position, depth, width, and asymmetry of the feature have been used to quantitatively estimate composition of samples from hyperspectral field and laboratory reflectance data. the parameters have also been used to develop mapping methods for the analysis of hyperspectral image data. this has resulted in techniques providing surface mineralogical information (e.g., classification) using absorption-band depth and position. however, no attempt has been made to prepare images of the absorption-band parameters. in this paper, a simple linear interpolation technique is proposed in order to derive absorption-band position, depth and asymmetry from hyperspectral image data. aviris data acquired in 1995 over the cuprite mining area (nevada, usa) are used to demonstrate the technique and to interpret the data in terms of the known alteration phases characterizing the area. a sensitivity analysis of the methods proposed shows that good results can be obtained for estimating the absorption wavelength position, however the estimated absorption-band-depth is sensitive to the input parameters chosen. the resulting parameter images (depth, position, asymmetry of the absorption) when carefully examined and interpreted by an experienced remote sensing geologist provide key information on surface mineralogy. the estimates of depth and position can be related to the chemistry of the samples and thus allow to bridge the gap between field geochemistry and remote sensing."
http://orkg.org/orkg/resource/R108135,"Mapping of hydrothermally altered rocks by the EO-1 Hyperion sensor, Northern Danakil Depression, Eritrea",10.1080/01431160701874587,,
http://orkg.org/orkg/resource/R108138,Mapping the wavelength position of deepest absorption features to explore mineral diversity in hyperspectral images,10.1016/j.pss.2014.06.009,"<section-title>Abstract</section-title>
         <abstract-sec><simple-para>A new method is presented for the exploratory analysis of hyperspectral OMEGA imagery of Mars. It involves mapping the wavelength position and depth of the deepest absorption feature in the range between 2.1 and 2.4<hsp></hsp>µm, where reflectance spectra of minerals such as phyllosilicates, carbonates and sulphates contain diagnostic absorption features. For each pixel of the image, the wavelength position maps display the wavelength position of the deepest absorption feature in color and its depth in intensity. This can be correlated with (groups of) minerals and their occurrences.</simple-para>
            <simple-para>To test the validity of the method, comparisons were made between wavelength position maps calculated from OMEGA images of the Nili Fossae area at two different spatial resolutions, of 0.95 and 2.2<hsp></hsp>km, and five CRISM images in targeted mode, at 18<hsp></hsp>m spatial resolution. The wavelength positions and their spatial patterns in the two OMEGA images were generally similar, except that the higher spatial resolution OMEGA image showed a larger diversity of wavelength positions and more spatial detail than the lower resolution OMEGA image. Patterns formed by groups of pixels with relatively deep absorption features between 2.250 and 2.350<hsp></hsp>µm in the OMEGA imagery were in agreement with the patterns calculated from the CRISM imagery. The wavelength positions of clusters of similar pixels in the wavelength position maps are consistent with groups of minerals that have been described elsewhere in the literature.</simple-para>
            <simple-para>We conclude that mapping the wavelength position of the deepest absorption features between 2.1 and 2.4<hsp></hsp>µm provides a useful method for exploratory analysis of the surface mineralogy of Mars with hyperspectral OMEGA imagery. The method provides a synoptic spatial view of the spectral diversity in one single image. It is complementary to the use of summary products, which many researchers have been using for assessment of the information content of OMEGA imagery. The results of the exploratory analysis can be used as input for the construction of surface mineralogical maps. The wavelength position mapping method itself is equally applicable to other terrestrial and planetary data sets and will be particular useful in areas where field validation is sparse and with imagery containing shallow spectral features.</simple-para></abstract-sec> <section-title>Highlights</section-title>
         <abstract-sec><simple-para><list><list-item><label>•</label>
                     <para>A new method is presented for exploring mineral diversity in hyperspectral images.</para></list-item>
                  <list-item><label>•</label>
                     <para>The method calculates the wavelength position of deepest absorption features.</para></list-item>
                  <list-item><label>•</label>
                     <para>The absorption features of image spectra between 2.1 and 2.4<hsp></hsp>μm were mapped.</para></list-item>
                  <list-item><label>•</label>
                     <para>Resulting wavelength position maps show spatial distributions of (groups of) minerals.</para></list-item>
                  <list-item><label>•</label>
                     <para>Results can be used to assess the surface mineralogy of Mars.</para></list-item></list></simple-para></abstract-sec>","abstract a new method is presented for the exploratory analysis of hyperspectral omega imagery of mars. it involves mapping the wavelength position and depth of the deepest absorption feature in the range between 2.1 and 2.4 µm, where reflectance spectra of minerals such as phyllosilicates, carbonates and sulphates contain diagnostic absorption features. for each pixel of the image, the wavelength position maps display the wavelength position of the deepest absorption feature in color and its depth in intensity. this can be correlated with (groups of) minerals and their occurrences. to test the validity of the method, comparisons were made between wavelength position maps calculated from omega images of the nili fossae area at two different spatial resolutions, of 0.95 and 2.2 km, and five crism images in targeted mode, at 18 m spatial resolution. the wavelength positions and their spatial patterns in the two omega images were generally similar, except that the higher spatial resolution omega image showed a larger diversity of wavelength positions and more spatial detail than the lower resolution omega image. patterns formed by groups of pixels with relatively deep absorption features between 2.250 and 2.350 µm in the omega imagery were in agreement with the patterns calculated from the crism imagery. the wavelength positions of clusters of similar pixels in the wavelength position maps are consistent with groups of minerals that have been described elsewhere in the literature. we conclude that mapping the wavelength position of the deepest absorption features between 2.1 and 2.4 µm provides a useful method for exploratory analysis of the surface mineralogy of mars with hyperspectral omega imagery. the method provides a synoptic spatial view of the spectral diversity in one single image. it is complementary to the use of summary products, which many researchers have been using for assessment of the information content of omega imagery. the results of the exploratory analysis can be used as input for the construction of surface mineralogical maps. the wavelength position mapping method itself is equally applicable to other terrestrial and planetary data sets and will be particular useful in areas where field validation is sparse and with imagery containing shallow spectral features. highlights • a new method is presented for exploring mineral diversity in hyperspectral images. • the method calculates the wavelength position of deepest absorption features. • the absorption features of image spectra between 2.1 and 2.4 μm were mapped. • resulting wavelength position maps show spatial distributions of (groups of) minerals. • results can be used to assess the surface mineralogy of mars."
http://orkg.org/orkg/resource/R108141,Iron Oxides Mapping from E0-1 Hyperion Data,10.1007/s12594-015-0364-7,,
http://orkg.org/orkg/resource/R108144,Mapping of Alteration Zones in Mineral Rich Belt of South-East Rajasthan Using Remote Sensing Techniques,10.5281/zenodo.1128817,"Remote sensing techniques have emerged as an asset for various geological studies. Satellite images obtained by different sensors contain plenty of information related to the terrain. Digital image processing further helps in customized ways for the prospecting of minerals. In this study, an attempt has been made to map the hydrothermally altered zones using multispectral and hyperspectral datasets of South East Rajasthan. Advanced Space-borne Thermal Emission and Reflection Radiometer (ASTER) and Hyperion (Level1R) dataset have been processed to generate different Band Ratio Composites (BRCs). For this study, ASTER derived BRCs were generated to delineate the alteration zones, gossans, abundant clays and host rocks. ASTER and Hyperion images were further processed to extract mineral end members and classified mineral maps have been produced using Spectral Angle Mapper (SAM) method. Results were validated with the geological map of the area which shows positive agreement with the image processing outputs. Thus, this study concludes that the band ratios and image processing in combination play significant role in demarcation of alteration zones which may provide pathfinders for mineral prospecting studies.","remote sensing techniques have emerged as an asset for various geological studies. satellite images obtained by different sensors contain plenty of information related to the terrain. digital image processing further helps in customized ways for the prospecting of minerals. in this study, an attempt has been made to map the hydrothermally altered zones using multispectral and hyperspectral datasets of south east rajasthan. advanced space-borne thermal emission and reflection radiometer (aster) and hyperion (level1r) dataset have been processed to generate different band ratio composites (brcs). for this study, aster derived brcs were generated to delineate the alteration zones, gossans, abundant clays and host rocks. aster and hyperion images were further processed to extract mineral end members and classified mineral maps have been produced using spectral angle mapper (sam) method. results were validated with the geological map of the area which shows positive agreement with the image processing outputs. thus, this study concludes that the band ratios and image processing in combination play significant role in demarcation of alteration zones which may provide pathfinders for mineral prospecting studies."
http://orkg.org/orkg/resource/R108147,Potential of airborne hyperspectral data for  geo-exploration over parts of different  geological/metallogenic provinces in India based on AVIRIS-NG observations,10.18520/cs/v116/i7/1143-1156,,
http://orkg.org/orkg/resource/R108150,Improved k-means and spectral matching for hyperspectral mineral mapping,10.1016/j.jag.2020.102154,"Mineral mapping is an important step for the development and utilization of mineral resources. The emergence of remote sensing technology, especially hyperspectral imagery, has paved a new approach to geological mapping. The k-means clustering algorithm is a classical approach to classifying hyperspectral imagery, but the influence of mixed pixels and noise mean that it usually has poor mineral mapping accuracy. In this study, the mapping accuracy of the k-means algorithm was improved in three ways: similarity measurement methods that are insensitive to dimensions are used instead of the Euclidean distance for clustering; the spectral absorption features of minerals are enhanced; and the mineral mapping results are combined as the number of cluster centers (K) is incremented from 1. The improved algorithm is used with combined spectral matching to match the clustering results with a spectral library. A case study on Cuprite, Nevada, demonstrated that the improved k-means algorithm can identify most minerals with the kappa value of over 0.8, which is 46% and 15% higher than the traditional k-means and spectral matching technology. New mineral types are more likely to be found with increasing K. When K is much greater than the number of mineral types, the accuracy is improved, and the mineral mapping results are independent of the similarity measurement method. The improved k-means algorithm can also effectively remove speckle noise from the mineral mapping results and be used to identify other objects.","mineral mapping is an important step for the development and utilization of mineral resources. the emergence of remote sensing technology, especially hyperspectral imagery, has paved a new approach to geological mapping. the k-means clustering algorithm is a classical approach to classifying hyperspectral imagery, but the influence of mixed pixels and noise mean that it usually has poor mineral mapping accuracy. in this study, the mapping accuracy of the k-means algorithm was improved in three ways: similarity measurement methods that are insensitive to dimensions are used instead of the euclidean distance for clustering; the spectral absorption features of minerals are enhanced; and the mineral mapping results are combined as the number of cluster centers (k) is incremented from 1. the improved algorithm is used with combined spectral matching to match the clustering results with a spectral library. a case study on cuprite, nevada, demonstrated that the improved k-means algorithm can identify most minerals with the kappa value of over 0.8, which is 46% and 15% higher than the traditional k-means and spectral matching technology. new mineral types are more likely to be found with increasing k. when k is much greater than the number of mineral types, the accuracy is improved, and the mineral mapping results are independent of the similarity measurement method. the improved k-means algorithm can also effectively remove speckle noise from the mineral mapping results and be used to identify other objects."
http://orkg.org/orkg/resource/R108153,Comparative analysis of mineral mapping for hyperspectral and multispectral imagery,10.1007/s12517-020-5148-8,,
http://orkg.org/orkg/resource/R108156,"Potential Use of Airborne Hyperspectral AVIRIS-NG Data for Mapping Proterozoic Metasediments in Banswara, India",10.1007/s12594-020-1404-5,,
http://orkg.org/orkg/resource/R108158,Phytochemical Studies and Antidiabetic Activities of Newbouldia laevis (P. Beauv) Ethanolic Leaves Extracts in Alloxan-Induced Diabetic Rats,,,
http://orkg.org/orkg/resource/R108161,THE INFLUENCE OF SALTS STRESS ON SOME GROWTH MORPHOLOGY OF (MELON PUMPKIN) Cucurbita maxima Duchesne GROWN IN-VITRO,,,
http://orkg.org/orkg/resource/R108164,Phytochemical Evaluation and Anti-bacterial Activity of Brachystegia eurycoma Harms. Ethanolic Seed Extract. ,,,
http://orkg.org/orkg/resource/R108167,RESPONSES ON SOME GROWTH MORPHOLOGY AND NUTRIENT COMPOSITIONS OF NORTHERN MELON (Cucurbita melo L.) IN DIFFERENT SALT STRESS IN-VITRO. ,,,
http://orkg.org/orkg/resource/R108170,Growth Morphology and Chlorophyll Synthesis of Vigna unguiculata (L). Walp Varieties (White and Brown) Grown Under Salinity Stress.,,,
http://orkg.org/orkg/resource/R108199,A Little Bird Told Me: Mining Tweets for Requirements and Software Evolution,10.1109/re.2017.88,"Twitter is one of the most popular social networks. Previous research found that users employ Twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. However, due to their large number---in the range of thousands per day for popular applications---a manual analysis is unfeasible.In this work we present ALERTme, an approach to automatically classify, group and rank tweets about software applications. We apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. We ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. Our results show that ALERTme is an effective approach for filtering, summarizing and ranking tweets about software applications. ALERTme enables the exploitation of Twitter as a feedback channel for information relevant to software evolution, including end-user requirements.","twitter is one of the most popular social networks. previous research found that users employ twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. however, due to their large number---in the range of thousands per day for popular applications---a manual analysis is unfeasible.in this work we present alertme, an approach to automatically classify, group and rank tweets about software applications. we apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. we ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. our results show that alertme is an effective approach for filtering, summarizing and ranking tweets about software applications. alertme enables the exploitation of twitter as a feedback channel for information relevant to software evolution, including end-user requirements."
http://orkg.org/orkg/resource/R108208,Facilitating developer-user interactions with mobile app review digests,10.1145/2468356.2468681,,
http://orkg.org/orkg/resource/R108212,TOGAF Version 9 - The Open Group Architecture Framework,,,
http://orkg.org/orkg/resource/R108224,Linking Business Goals to Process Models in Semantic Business Process Modeling,10.1109/edoc.2008.43,"Broad knowledge is required when a business process is modeled by a business analyst. We argue that existing Business Process Management methodologies do not consider business goals at the appropriate level. In this paper we present an approach to integrate business goals and business process models. We design a Business Goal Ontology for modeling business goals. Furthermore, we devise a modeling pattern for linking the goals to process models and show how the ontology can be used in query answering. In this way, we integrate the intentional perspective into our business process ontology framework, enriching the process description and enabling new types of business process analysis.","broad knowledge is required when a business process is modeled by a business analyst. we argue that existing business process management methodologies do not consider business goals at the appropriate level. in this paper we present an approach to integrate business goals and business process models. we design a business goal ontology for modeling business goals. furthermore, we devise a modeling pattern for linking the goals to process models and show how the ontology can be used in query answering. in this way, we integrate the intentional perspective into our business process ontology framework, enriching the process description and enabling new types of business process analysis."
http://orkg.org/orkg/resource/R108227,Exploiting abused trending topics to identify spam campaigns in Twitter,10.1007/s13278-016-0354-9,"<em>Abstract</em>
              <Para ID=""Par1""> Twitter is an online social network (OSN) with approximately 650 million users. It has been fairly characterized as one of the most influential OSNs since it includes public figures, organizations, news media and official authorities. Twitter has an inherent simple philosophy with short messages, friendship relations, hashtags and support for media sharing such as photos and short videos. Popular hashtags that emerge from users’ activity are displayed prominently in the platform as <Emphasis Type=""Italic"">Popular Trends</Emphasis>. Unfortunately, the capabilities of the platform can be also abused and exploited for distributing illicit content or boosting false information, and the consequences of such actions can be <Emphasis Type=""Italic"">really</Emphasis> severe: one false tweet was enough for making the stock market crash for a short period of time in 2013. In this study, we make an experimental analysis on a large dataset containing 150 million tweets. We delve into the dynamics of the popular trends as well as other Twitter features in regard to deliberate misuse. We investigate traditional spam techniques as well as an obfuscated way of spam campaigns that exploit trending topics and hides malicious URLs within Google search result links. We implement a simple and lightweight classifier for indentifying spam users as well as spam tweets. Finally, we visualize these spam campaigns and investigate their inner properties.</Para>","abstract twitter is an online social network (osn) with approximately 650 million users. it has been fairly characterized as one of the most influential osns since it includes public figures, organizations, news media and official authorities. twitter has an inherent simple philosophy with short messages, friendship relations, hashtags and support for media sharing such as photos and short videos. popular hashtags that emerge from users’ activity are displayed prominently in the platform as popular trends . unfortunately, the capabilities of the platform can be also abused and exploited for distributing illicit content or boosting false information, and the consequences of such actions can be really severe: one false tweet was enough for making the stock market crash for a short period of time in 2013. in this study, we make an experimental analysis on a large dataset containing 150 million tweets. we delve into the dynamics of the popular trends as well as other twitter features in regard to deliberate misuse. we investigate traditional spam techniques as well as an obfuscated way of spam campaigns that exploit trending topics and hides malicious urls within google search result links. we implement a simple and lightweight classifier for indentifying spam users as well as spam tweets. finally, we visualize these spam campaigns and investigate their inner properties."
http://orkg.org/orkg/resource/R108235,Suspended accounts in retrospect: an analysis of twitter spam,10.1145/2068816.2068840,,
http://orkg.org/orkg/resource/R108270,@spam: the underground on 140 characters or less,10.1145/1866307.1866311,,
http://orkg.org/orkg/resource/R108275,Consequences of Connectivity: Characterizing Account Hijacking on Twitter,10.1145/2660267.2660282,,
http://orkg.org/orkg/resource/R108285,Integriertes Wissensmanagement,10.1007/978-3-642-19003-2_19,"Die Veränderung ist die einzige Konstante, insbesondere in der Nutzung der Informationstechnik. Sie ist auch das zentrale Anliegen der zweiten Auflage dieses Werkes zum Business Engineering, das auf die wichtige Frage eingeht, wie sich ein Unternehmen des Industriezeitalters erfolgreich in ein Echtzeitunternehmen der vernetzten Wirtschaft transformiert: Welche Geschäftsstrategien sind zukünftig erfolgreich? Wie können diese Strategien in Prozessen umgesetzt werden? Welche Anforderung sind an die technische Realisierung in Informations- und Kommunikationssystemen zu stellen? Wie fördern wir eine Kultur des Wandels? Der erste Teil des Buches ist den Grundlagen des St. Galler Ansatzes des Business Engineering gewidmet. Der zweite Teil fasst verschiedene Ansätze aus der angewandten Forschung zum Business Engineering zusammen, analysiert Geschäftsmodelle des Informationszeitalters, identifiziert Treiber des Wandels und liefert Instrumente für Veränderungsprozesse. Das Buch zeigt ""Veränderern"" Gestaltungsmöglichkeiten und Handlungsoptionen für Unternehmenserfolg in der vernetzten Wirtschaft auf und möchte anregen, diesen Wandel aktiv mitzugestalten","die veränderung ist die einzige konstante, insbesondere in der nutzung der informationstechnik. sie ist auch das zentrale anliegen der zweiten auflage dieses werkes zum business engineering, das auf die wichtige frage eingeht, wie sich ein unternehmen des industriezeitalters erfolgreich in ein echtzeitunternehmen der vernetzten wirtschaft transformiert: welche geschäftsstrategien sind zukünftig erfolgreich? wie können diese strategien in prozessen umgesetzt werden? welche anforderung sind an die technische realisierung in informations- und kommunikationssystemen zu stellen? wie fördern wir eine kultur des wandels? der erste teil des buches ist den grundlagen des st. galler ansatzes des business engineering gewidmet. der zweite teil fasst verschiedene ansätze aus der angewandten forschung zum business engineering zusammen, analysiert geschäftsmodelle des informationszeitalters, identifiziert treiber des wandels und liefert instrumente für veränderungsprozesse. das buch zeigt ""veränderern"" gestaltungsmöglichkeiten und handlungsoptionen für unternehmenserfolg in der vernetzten wirtschaft auf und möchte anregen, diesen wandel aktiv mitzugestalten"
http://orkg.org/orkg/resource/R108289,Unsupervised collective-based framework for dynamic retraining of supervised real-time spam tweets detection model,https://doi.org/10.1016/j.eswa.2019.05.052,,
http://orkg.org/orkg/resource/R108292,Process Oriented Knowledge Management: A Service Based Approach,10.3217/JUCS-011-04-0565,,
http://orkg.org/orkg/resource/R108296,B-KIDE: a framework and a tool for business process-oriented knowledge infrastructure development,10.1002/kpm.227,"<title>Abstract</title>
<doi>10.1002/kpm.227.abs</doi>
<p>The need for an effective management of knowledge is gaining increasing recognition in today's economy. To acknowledge this fact, new promising and powerful technologies have emerged from industrial and academic research. With these innovations maturing, organizations are increasingly willing to adapt such new knowledge management technologies to improve their knowledge‐intensive businesses. However, the successful application in given business contexts is a complex, multidimensional challenge and a current research topic. Therefore, this contribution addresses this challenge and introduces a framework for the development of business process‐supportive, technological knowledge infrastructures. While business processes represent the organizational setting for the application of knowledge management technologies, knowledge infrastructures represent a concept that can enable knowledge management in organizations. The B‐KIDE Framework introduced in this work provides support for the development of knowledge infrastructures that comprise innovative knowledge management functionality and are visibly supportive of an organization's business processes. The developed B‐KIDE Tool eases the application of the B‐KIDE Framework for knowledge infrastructure developers. Three empirical studies that were conducted with industrial partners from heterogeneous industry sectors corroborate the relevance and viability of the introduced concepts. Copyright © 2005 John Wiley & Sons, Ltd.</p>","abstract 10.1002/kpm.227.abs the need for an effective management of knowledge is gaining increasing recognition in today's economy. to acknowledge this fact, new promising and powerful technologies have emerged from industrial and academic research. with these innovations maturing, organizations are increasingly willing to adapt such new knowledge management technologies to improve their knowledge‐intensive businesses. however, the successful application in given business contexts is a complex, multidimensional challenge and a current research topic. therefore, this contribution addresses this challenge and introduces a framework for the development of business process‐supportive, technological knowledge infrastructures. while business processes represent the organizational setting for the application of knowledge management technologies, knowledge infrastructures represent a concept that can enable knowledge management in organizations. the b‐kide framework introduced in this work provides support for the development of knowledge infrastructures that comprise innovative knowledge management functionality and are visibly supportive of an organization's business processes. the developed b‐kide tool eases the application of the b‐kide framework for knowledge infrastructure developers. three empirical studies that were conducted with industrial partners from heterogeneous industry sectors corroborate the relevance and viability of the introduced concepts. copyright © 2005 john wiley & sons, ltd."
http://orkg.org/orkg/resource/R108301,A notation for Knowledge-Intensive Processes,10.1109/cscwd.2013.6580961,"Business process modeling has become essential for managing organizational knowledge artifacts. However, this is not an easy task, especially when it comes to the so-called Knowledge-Intensive Processes (KIPs). A KIP comprises activities based on acquisition, sharing, storage, and (re)use of knowledge, as well as collaboration among participants, so that the amount of value added to the organization depends on process agents' knowledge. The previously developed Knowledge Intensive Process Ontology (KIPO) structures all the concepts (and relationships among them) to make a KIP explicit. Nevertheless, KIPO does not include a graphical notation, which is crucial for KIP stakeholders to reach a common understanding about it. This paper proposes the Knowledge Intensive Process Notation (KIPN), a notation for building knowledge-intensive processes graphical models.","business process modeling has become essential for managing organizational knowledge artifacts. however, this is not an easy task, especially when it comes to the so-called knowledge-intensive processes (kips). a kip comprises activities based on acquisition, sharing, storage, and (re)use of knowledge, as well as collaboration among participants, so that the amount of value added to the organization depends on process agents' knowledge. the previously developed knowledge intensive process ontology (kipo) structures all the concepts (and relationships among them) to make a kip explicit. nevertheless, kipo does not include a graphical notation, which is crucial for kip stakeholders to reach a common understanding about it. this paper proposes the knowledge intensive process notation (kipn), a notation for building knowledge-intensive processes graphical models."
http://orkg.org/orkg/resource/R108304,CATS: Characterizing automation of Twitter spammers,10.1109/COMSNETS.2013.6465541,"Twitter, with its rising popularity as a micro-blogging website, has inevitably attracted the attention of spammers. Spammers use myriad of techniques to evade security mechanisms and post spam messages, which are either unwelcome advertisements for the victim or lure victims in to clicking malicious URLs embedded in spam tweets. In this paper, we propose several novel features capable of distinguishing spam accounts from legitimate accounts. The features analyze the behavioral and content entropy, bait-techniques, and profile vectors characterizing spammers, which are then fed into supervised learning algorithms to generate models for our tool, CATS. Using our system on two real-world Twitter data sets, we observe a 96% detection rate with about 0.8% false positive rate beating state of the art detection approach. Our analysis reveals detection of more than 90% of spammers with less than five tweets and about half of the spammers detected with only a single tweet. Our feature computation has low latency and resource requirement making fast detection feasible. Additionally, we cluster the unknown spammers to identify and understand the prevalent spam campaigns on Twitter.","twitter, with its rising popularity as a micro-blogging website, has inevitably attracted the attention of spammers. spammers use myriad of techniques to evade security mechanisms and post spam messages, which are either unwelcome advertisements for the victim or lure victims in to clicking malicious urls embedded in spam tweets. in this paper, we propose several novel features capable of distinguishing spam accounts from legitimate accounts. the features analyze the behavioral and content entropy, bait-techniques, and profile vectors characterizing spammers, which are then fed into supervised learning algorithms to generate models for our tool, cats. using our system on two real-world twitter data sets, we observe a 96% detection rate with about 0.8% false positive rate beating state of the art detection approach. our analysis reveals detection of more than 90% of spammers with less than five tweets and about half of the spammers detected with only a single tweet. our feature computation has low latency and resource requirement making fast detection feasible. additionally, we cluster the unknown spammers to identify and understand the prevalent spam campaigns on twitter."
http://orkg.org/orkg/resource/R108307,Knowledge modelling in weakly‐structured business processes,10.1108/13673270310477261,"<p>In this paper we present a new approach for integrating knowledge management and business process management. We focus on the modelling of weakly-structured knowledge-intensive business processes. We develop a framework for modelling this type of processes that explicitly considers knowledge-related tasks and knowledge objects and present a workflow tool that is an implementation of our theoretical meta-model. As an example, we sketch one case study, the process for granting full old age pension as it is performed in the Greek Social Security Institution. Finally we briefly describe some related approaches and compare them to our work and draw the main conclusions and further research directions.</p>","in this paper we present a new approach for integrating knowledge management and business process management. we focus on the modelling of weakly-structured knowledge-intensive business processes. we develop a framework for modelling this type of processes that explicitly considers knowledge-related tasks and knowledge objects and present a workflow tool that is an implementation of our theoretical meta-model. as an example, we sketch one case study, the process for granting full old age pension as it is performed in the greek social security institution. finally we briefly describe some related approaches and compare them to our work and draw the main conclusions and further research directions."
http://orkg.org/orkg/resource/R108312,Rapid knowledge work visualization for organizations,10.1108/13673270710762747,"<sec><title>Purpose</title><x>–</x><p><italic>The purpose of this contribution is to motivate a new, rapid approach to modeling knowledge work in organizational settings and to introduce a software tool that demonstrates the viability of the envisioned concept.</italic></p></sec><sec><title>Design/methodology/approach</title><x>–</x><p><italic>Based on existing modeling structures, the KnowFlow toolset that aids knowledge analysts in rapidly conducting interviews and in conducting multi-perspective analysis of organizational knowledge work is introduced.</italic></p></sec><sec><title>Findings</title><x>–</x><p><italic>This article demonstrates how rapid knowledge work visualization can be conducted largely without human modelers by developing an interview structure that allows for self-service interviews. Two application scenarios illustrate the pressing need for and the potentials of rapid knowledge work visualizations in organizational settings.</italic></p></sec><sec><title>Research limitations/implications</title><x>–</x><p><italic>The efforts necessary for traditional modeling approaches in the area of knowledge management are often prohibitive. This contribution argues that future research needs to take economical constraints of organizational settings into account in order to be able to realize the full potential of knowledge work management.</italic></p></sec><sec><title>Practical implications</title><x>–</x><p><italic>This work picks up a problem identified in practice and proposes the novel concept of rapid knowledge work visualization for making knowledge work modeling in organizations more feasible.</italic></p></sec><sec><title>Originality/value</title><x>–</x><p><italic>This work develops a vision of rapid knowledge work visualization and introduces a tool-supported approach that addresses some of the identified challenges.</italic></p></sec>","purpose – the purpose of this contribution is to motivate a new, rapid approach to modeling knowledge work in organizational settings and to introduce a software tool that demonstrates the viability of the envisioned concept. design/methodology/approach – based on existing modeling structures, the knowflow toolset that aids knowledge analysts in rapidly conducting interviews and in conducting multi-perspective analysis of organizational knowledge work is introduced. findings – this article demonstrates how rapid knowledge work visualization can be conducted largely without human modelers by developing an interview structure that allows for self-service interviews. two application scenarios illustrate the pressing need for and the potentials of rapid knowledge work visualizations in organizational settings. research limitations/implications – the efforts necessary for traditional modeling approaches in the area of knowledge management are often prohibitive. this contribution argues that future research needs to take economical constraints of organizational settings into account in order to be able to realize the full potential of knowledge work management. practical implications – this work picks up a problem identified in practice and proposes the novel concept of rapid knowledge work visualization for making knowledge work modeling in organizations more feasible. originality/value – this work develops a vision of rapid knowledge work visualization and introduces a tool-supported approach that addresses some of the identified challenges."
http://orkg.org/orkg/resource/R108316,Analyzing Knowledge Transfer Effectiveness--An Agent-Oriented Modeling Approach,10.1109/hicss.2007.80,"Facilitating the transfer of knowledge between knowledge workers represents one of the main challenges of knowledge management. Knowledge transfer instruments, such as the experience factory concept, represent means for facilitating knowledge transfer in organizations. As past research has shown, effectiveness of knowledge transfer instruments strongly depends on their situational context, on the stakeholders involved in knowledge transfer, and on their acceptance, motivation and goals. In this paper, we introduce an agent-oriented modeling approach for analyzing the effectiveness of knowledge transfer instruments in the light of (potentially conflicting) stakeholders¿ goals. We apply this intentional approach to the experience factory concept and analyze under which conditions it can fail, and how adaptations to the Experience Factory can be explored in a structured way.","facilitating the transfer of knowledge between knowledge workers represents one of the main challenges of knowledge management. knowledge transfer instruments, such as the experience factory concept, represent means for facilitating knowledge transfer in organizations. as past research has shown, effectiveness of knowledge transfer instruments strongly depends on their situational context, on the stakeholders involved in knowledge transfer, and on their acceptance, motivation and goals. in this paper, we introduce an agent-oriented modeling approach for analyzing the effectiveness of knowledge transfer instruments in the light of (potentially conflicting) stakeholders¿ goals. we apply this intentional approach to the experience factory concept and analyze under which conditions it can fail, and how adaptations to the experience factory can be explored in a structured way."
http://orkg.org/orkg/resource/R108321,Modelling knowledge transfer: A knowledge dynamics perspective,10.1177/1063293x15592185,"The increasing complexity in design activities leads designers to collaborate and share knowledge within distributed teams. This makes designers use systems such as knowledge management systems to reach their goal. In this article, our aim is to investigate on improving the use of knowledge management systems by defining a framework for modelling knowledge transfer in such context. The proposed framework is partly based on reuse of existing models found in the literature and on a participant observation methodology. Then, we tested this framework through several case studies presented in this article. These investigations enable us to observe, define and model more finely the knowledge dynamics that occur between knowledge workers and knowledge management systems.","the increasing complexity in design activities leads designers to collaborate and share knowledge within distributed teams. this makes designers use systems such as knowledge management systems to reach their goal. in this article, our aim is to investigate on improving the use of knowledge management systems by defining a framework for modelling knowledge transfer in such context. the proposed framework is partly based on reuse of existing models found in the literature and on a participant observation methodology. then, we tested this framework through several case studies presented in this article. these investigations enable us to observe, define and model more finely the knowledge dynamics that occur between knowledge workers and knowledge management systems."
http://orkg.org/orkg/resource/R108325,Modeling Knowledge Work for the Design of Knowledge Infrastructures,10.3217/JUCS-011-04-0429,,
http://orkg.org/orkg/resource/R108328,Modeling Techniques for Knowledge Management: ,10.4018/978-1-59904-603-7.ch003,,
http://orkg.org/orkg/resource/R108333,Credibility in context: An analysis of feature distributions in twitter,,"Twitter is a major forum for rapid dissemination of user-provided content in real time. As such, a large proportion of the information it contains is not particularly relevant to many users and in fact is perceived as unwanted 'noise' by many. There has been increased research interest in predicting whether tweets are relevant, newsworthy or credible, using a variety of models and methods. In this paper, we focus on an analysis that highlights the utility of the individual features in Twitter such as hash tags, retweets and mentions for predicting credibility. We first describe a context-based evaluation of the utility of a set of features for predicting manually provided credibility assessments on a corpus of microblog tweets. This is followed by an evaluation of the distribution/presence of each feature across 8 diverse crawls of tweet data. Last, an analysis of feature distribution across dyadic pairs of tweets and retweet chains of various lengths is described. Our results show that the best indicators of credibility include URLs, mentions, retweets and tweet length and that features occur more prominently in data describing emergency and unrest situations.","twitter is a major forum for rapid dissemination of user-provided content in real time. as such, a large proportion of the information it contains is not particularly relevant to many users and in fact is perceived as unwanted 'noise' by many. there has been increased research interest in predicting whether tweets are relevant, newsworthy or credible, using a variety of models and methods. in this paper, we focus on an analysis that highlights the utility of the individual features in twitter such as hash tags, retweets and mentions for predicting credibility. we first describe a context-based evaluation of the utility of a set of features for predicting manually provided credibility assessments on a corpus of microblog tweets. this is followed by an evaluation of the distribution/presence of each feature across 8 diverse crawls of tweet data. last, an analysis of feature distribution across dyadic pairs of tweets and retweet chains of various lengths is described. our results show that the best indicators of credibility include urls, mentions, retweets and tweet length and that features occur more prominently in data describing emergency and unrest situations."
http://orkg.org/orkg/resource/R108336,Trafficking Fraudulent Accounts: The Role of the Underground Market in Twitter Spam and Abuse,,,
http://orkg.org/orkg/resource/R108341,Release planning of mobile apps based on user reviews,10.1145/2884781.2884818,,
http://orkg.org/orkg/resource/R108344,"How We Refactor, and How We Know It",10.1109/tse.2011.41,"Refactoring is widely practiced by developers, and considerable research and development effort has been invested in refactoring tools. However, little has been reported about the adoption of refactoring tools, and many assumptions about refactoring practice have little empirical support. In this paper, we examine refactoring tool usage and evaluate some of the assumptions made by other researchers. To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support. We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers. To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. Finally, we interviewed the Eclipse and Mylyn developers to help us understand why they did not use refactoring tools and to gather ideas for future research.","refactoring is widely practiced by developers, and considerable research and development effort has been invested in refactoring tools. however, little has been reported about the adoption of refactoring tools, and many assumptions about refactoring practice have little empirical support. in this paper, we examine refactoring tool usage and evaluate some of the assumptions made by other researchers. to measure tool usage, we randomly sampled code changes from four eclipse and eight mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support. we found that refactoring tools are seldom used: 11 percent by eclipse developers and 9 percent by mylyn developers. to understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits. using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. finally, we interviewed the eclipse and mylyn developers to help us understand why they did not use refactoring tools and to gather ideas for future research."
http://orkg.org/orkg/resource/R108347,The Usability (or Not) of Refactoring Tools,10.1109/saner50967.2021.00030,"Although software developers typically have access to numerous refactoring tools, most developers avoid using these tools despite their benefits. Researchers have identified many reasons for the disuse of refactoring tools, including a lack of awareness by the developers, a lack of predictability of the tools, and a lack of need for the tools. In this paper, we build on this earlier work and employ the ISO 9241-11 definition of usability to develop a theory of usability for refactoring tools. We investigate existing refactoring tools using this theory by analyzing how 17 developers experience refactoring tools in three software change tasks we asked them to perform. We analyze qualitatively the resulting interview transcripts based on our theory and report on a number of observations that can inform tool designers interested in improving the usability of refactoring tools. For instance, we found a desire for developers to guide how a refactoring tool changes the code and a need for refactoring tools to describe changes made to developers. Refactoring tools are currently expected to preserve program behavior. These observations indicate that it may be necessary to give developers more control over this property, including the ability to relax it, for the tools to be usable; that is, for the tools to be effective, efficient and satisfying for the developer to employ.","although software developers typically have access to numerous refactoring tools, most developers avoid using these tools despite their benefits. researchers have identified many reasons for the disuse of refactoring tools, including a lack of awareness by the developers, a lack of predictability of the tools, and a lack of need for the tools. in this paper, we build on this earlier work and employ the iso 9241-11 definition of usability to develop a theory of usability for refactoring tools. we investigate existing refactoring tools using this theory by analyzing how 17 developers experience refactoring tools in three software change tasks we asked them to perform. we analyze qualitatively the resulting interview transcripts based on our theory and report on a number of observations that can inform tool designers interested in improving the usability of refactoring tools. for instance, we found a desire for developers to guide how a refactoring tool changes the code and a need for refactoring tools to describe changes made to developers. refactoring tools are currently expected to preserve program behavior. these observations indicate that it may be necessary to give developers more control over this property, including the ability to relax it, for the tools to be usable; that is, for the tools to be effective, efficient and satisfying for the developer to employ."
http://orkg.org/orkg/resource/R108350,Improving Usability of Software Refactoring Tools,10.1109/aswec.2007.24,"Post-deployment maintenance and evolution can account for up to 75% of the cost of developing a software system. Software refactoring can reduce the costs associated with evolution by improving system quality. Although refactoring can yield benefits, the process includes potentially complex, error-prone, tedious and time-consuming tasks. It is these tasks that automated refactoring tools seek to address. However, although the refactoring process is well-defined, current refactoring tools do not support the full process. To develop better automated refactoring support, we have completed a usability study of software refactoring tools. In the study, we analysed the task of software refactoring using the ISO 9241-11 usability standard and Fitts¿ List of task allocation. Expanding on this analysis, we reviewed 11 collections of usability guidelines and combined these into a single list of 38 guidelines. From this list, we developed 81 usability requirements for refactoring tools. Using these requirements, the software refactoring tools Eclipse 3.2, Condenser 1.05, RefactorIT 2.5.1, and Eclipse 3.2 with the Simian UI 2.2.12 plugin were studied. Based on the analysis, we have selected a subset of the requirements that will be incorporated into a prototype refactoring tool intended to address the full refactoring process.","post-deployment maintenance and evolution can account for up to 75% of the cost of developing a software system. software refactoring can reduce the costs associated with evolution by improving system quality. although refactoring can yield benefits, the process includes potentially complex, error-prone, tedious and time-consuming tasks. it is these tasks that automated refactoring tools seek to address. however, although the refactoring process is well-defined, current refactoring tools do not support the full process. to develop better automated refactoring support, we have completed a usability study of software refactoring tools. in the study, we analysed the task of software refactoring using the iso 9241-11 usability standard and fitts¿ list of task allocation. expanding on this analysis, we reviewed 11 collections of usability guidelines and combined these into a single list of 38 guidelines. from this list, we developed 81 usability requirements for refactoring tools. using these requirements, the software refactoring tools eclipse 3.2, condenser 1.05, refactorit 2.5.1, and eclipse 3.2 with the simian ui 2.2.12 plugin were studied. based on the analysis, we have selected a subset of the requirements that will be incorporated into a prototype refactoring tool intended to address the full refactoring process."
http://orkg.org/orkg/resource/R108368,SDM-RDFizer: An RML Interpreter for the Efficient Creation of RDF Knowledge Graphs,,"In recent years, the amount of data has increased exponentially, and knowledge graphs have gained attention as data structures to integrate data and knowledge harvested from myriad data sources. However, data complexity issues like large volume, high-duplicate rate, and heterogeneity usually characterize these data sources, being required data management tools able to address the negative impact of these issues on the knowledge graph creation process. In this paper, we propose the SDM-RDFizer, an interpreter of the RDF Mapping Language (RML), to transform raw data in various formats into an RDF knowledge graph. SDM-RDFizer implements novel algorithms to execute the logical operators between mappings in RML, allowing thus to scale up to complex scenarios where data is not only broad but has a high-duplication rate. We empirically evaluate the SDM-RDFizer performance against diverse testbeds with diverse configurations of data volume, duplicates, and heterogeneity. The observed results indicate that SDM-RDFizer is two orders of magnitude faster than state of the art, thus, meaning that SDM-RDFizer an interoperable and scalable solution for knowledge graph creation. SDM-RDFizer is publicly available as a resource through a Github repository and a DOI.","in recent years, the amount of data has increased exponentially, and knowledge graphs have gained attention as data structures to integrate data and knowledge harvested from myriad data sources. however, data complexity issues like large volume, high-duplicate rate, and heterogeneity usually characterize these data sources, being required data management tools able to address the negative impact of these issues on the knowledge graph creation process. in this paper, we propose the sdm-rdfizer, an interpreter of the rdf mapping language (rml), to transform raw data in various formats into an rdf knowledge graph. sdm-rdfizer implements novel algorithms to execute the logical operators between mappings in rml, allowing thus to scale up to complex scenarios where data is not only broad but has a high-duplication rate. we empirically evaluate the sdm-rdfizer performance against diverse testbeds with diverse configurations of data volume, duplicates, and heterogeneity. the observed results indicate that sdm-rdfizer is two orders of magnitude faster than state of the art, thus, meaning that sdm-rdfizer an interoperable and scalable solution for knowledge graph creation. sdm-rdfizer is publicly available as a resource through a github repository and a doi."
http://orkg.org/orkg/resource/R108372,Simple and difficult mathematics in children. A Minimum Spanning Tree EEG network analysis,,,
http://orkg.org/orkg/resource/R108374,Genetic effects on source level phase-locked and phase-independent brain responses in a visual oddball task,,,
http://orkg.org/orkg/resource/R108376,Altered Cross-frequency Coupling in Resting-State MEG after Mild Traumatic Brain Injury,,,
http://orkg.org/orkg/resource/R108378,Reconfiguration of Dominant Coupling Modes in Mild Traumatic Brain Injury Mediated by _-band Activity: a Resting State MEG Study,,,
http://orkg.org/orkg/resource/R108380,Altered Rich-Club and Frequency-Dependent Subnetwork Organization in Mild Traumatic Brain Injury: A MEG Resting-State Study,,,
http://orkg.org/orkg/resource/R108382,Data-driven Topological Filtering based on Orthogonal Minimal Spanning Trees: Application to Multi-Group MEG Resting-State Connectivity,,,
http://orkg.org/orkg/resource/R108384,Source reconstruction of somatosensory P20/N20 component: comparison of stimulation types,,,
http://orkg.org/orkg/resource/R108386,Aberrant Whole-Brain Transitions and Dynamics of Spontaneous Network Microstates in Mild Traumatic Brain Injury,,,
http://orkg.org/orkg/resource/R108388,Inter-Subject Variability of Skull Conductivity and Thickness in Calibrated Realistic Head Models,,,
http://orkg.org/orkg/resource/R108390,A novel method for calibrating head models to account for variability in conductivity and its evaluation in a sphere model,,"<P>The accuracy in electroencephalography (EEG) and combined EEG and magnetoencephalography (MEG) source reconstructions as well as in optimized transcranial electric stimulation (TES) depends on the conductive properties assigned to the head model, and most importantly on individual skull conductivity. In this study, we present an automatic pipeline to calibrate head models with respect to skull conductivity based on the reconstruction of the P20/N20 response using somatosensory evoked potentials and fields. In order to validate in a well-controlled setup without interplay with numerical errors, we evaluate the accuracy of this algorithm in a 4-layer spherical head model using realistic noise levels as well as dipole sources at different eccentricities with strengths and orientations related to somatosensory experiments. Our results show that the reference skull conductivity can be reliably reconstructed for sources resembling the generator of the P20/N20 response. In case of erroneous assumptions on scalp conductivity, the resulting skull conductivity parameter counterbalances this effect, so that EEG source reconstructions using the fitted skull conductivity parameter result in lower errors than when using the standard value. We propose an automatized procedure to calibrate head models which only relies on non-invasive modalities that are available in a standard MEG laboratory, measures under <I>in vivo</I> conditions and in the low frequency range of interest. Calibrated head modeling can improve EEG and combined EEG/MEG source analysis as well as optimized TES.</P>","the accuracy in electroencephalography (eeg) and combined eeg and magnetoencephalography (meg) source reconstructions as well as in optimized transcranial electric stimulation (tes) depends on the conductive properties assigned to the head model, and most importantly on individual skull conductivity. in this study, we present an automatic pipeline to calibrate head models with respect to skull conductivity based on the reconstruction of the p20/n20 response using somatosensory evoked potentials and fields. in order to validate in a well-controlled setup without interplay with numerical errors, we evaluate the accuracy of this algorithm in a 4-layer spherical head model using realistic noise levels as well as dipole sources at different eccentricities with strengths and orientations related to somatosensory experiments. our results show that the reference skull conductivity can be reliably reconstructed for sources resembling the generator of the p20/n20 response. in case of erroneous assumptions on scalp conductivity, the resulting skull conductivity parameter counterbalances this effect, so that eeg source reconstructions using the fitted skull conductivity parameter result in lower errors than when using the standard value. we propose an automatized procedure to calibrate head models which only relies on non-invasive modalities that are available in a standard meg laboratory, measures under in vivo conditions and in the low frequency range of interest. calibrated head modeling can improve eeg and combined eeg/meg source analysis as well as optimized tes."
http://orkg.org/orkg/resource/R108392,Parametrizing the Conditionally Gaussian Prior Model for Source Localization with Reference to the P20/N20 Component of Median Nerve SEP/SEF,,,
http://orkg.org/orkg/resource/R108394,Synchronization coupling investigation using ICA cluster analysis in resting MEG signals in Reading Difficulties,,"The understanding of the mechanisms of human brain is a demanding issue for neuroscience research. Physiological studies acknowledge the usefulness of synchronization coupling in the study of dysfunctions associated with reading difficulties. Magnetoencephalogram (MEG) is a useful tool towards this direction having been assessed for its superior accuracy over other modalities. In this paper we consider synchronization features for identifying brain operations. Independent Component Analysis (ICA) is applied on MEG surface signals in controls and children with reading difficulties and are clustered to representative components. Then, coupling measures of mutual information and partial directed coherence are estimated in order to reveal dysfunction of cerebral networks and its related coordination.","the understanding of the mechanisms of human brain is a demanding issue for neuroscience research. physiological studies acknowledge the usefulness of synchronization coupling in the study of dysfunctions associated with reading difficulties. magnetoencephalogram (meg) is a useful tool towards this direction having been assessed for its superior accuracy over other modalities. in this paper we consider synchronization features for identifying brain operations. independent component analysis (ica) is applied on meg surface signals in controls and children with reading difficulties and are clustered to representative components. then, coupling measures of mutual information and partial directed coherence are estimated in order to reveal dysfunction of cerebral networks and its related coordination."
http://orkg.org/orkg/resource/R108396,A Minimal Spanning Tree Analysis of EEG Responses to Complex Visual Stimuli,,"Human brain is the most complicated network and its functional mechanism is a demanding concept in neuroscience research. Graph theory and forms an interesting tool for modeling the brain interactions and estimated brain parameters. In this paper, we consider synchronization features for modeling brain operations in electro-encephalogram (EEG) responses to kanizsa and fractal stimuli, using minimal spanning tree (MST) on a network of phase synchronization EEG channels. Graphs of phase-synchronization activity and MST structures are computed using these graphs. The proposed approach yields evidence that the fractal stimuli generate stronger energy response and synchronization of theta band in occipital lobe.","human brain is the most complicated network and its functional mechanism is a demanding concept in neuroscience research. graph theory and forms an interesting tool for modeling the brain interactions and estimated brain parameters. in this paper, we consider synchronization features for modeling brain operations in electro-encephalogram (eeg) responses to kanizsa and fractal stimuli, using minimal spanning tree (mst) on a network of phase synchronization eeg channels. graphs of phase-synchronization activity and mst structures are computed using these graphs. the proposed approach yields evidence that the fractal stimuli generate stronger energy response and synchronization of theta band in occipital lobe."
http://orkg.org/orkg/resource/R108398,Rich Club Differentiation of Phase Synchronization EEG Responses,,,
http://orkg.org/orkg/resource/R108400,Comparison of Brain Network Models using Cross-Frequency Coupling and Attack Strategies,,"Several neuroimaging studies have suggested that functional brain connectivity networks exhibit &#x201C;small-world&#x201D; characteristics, whereas recent studies based on structural data have proposed a &#x201C;rich-club&#x201D; organization of brain networks, whereby hubs of high connection density tend to connect among themselves compared to nodes of lower density. In this study, we adopted an &#x201C;attack strategy&#x201D; to compare the rich-club and small-world organizations and identify the model that describes best the topology of brain connectivity. We hypothesized that the highest reduction in global efficiency caused by a targeted attack on each model's hubs would reveal the organization that better describes the topology of the underlying brain networks. We applied this approach to magnetoencephalographic data obtained at rest from neurologically intact controls and mild traumatic brain injury patients. Functional connectivity networks were computed using phase-to-amplitude cross-frequency coupling between the &#x03B4; and &#x03B2; frequency bands. Our results suggest that resting state MEG connectivity networks follow a rich-club organization.","several neuroimaging studies have suggested that functional brain connectivity networks exhibit &#x201c;small-world&#x201d; characteristics, whereas recent studies based on structural data have proposed a &#x201c;rich-club&#x201d; organization of brain networks, whereby hubs of high connection density tend to connect among themselves compared to nodes of lower density. in this study, we adopted an &#x201c;attack strategy&#x201d; to compare the rich-club and small-world organizations and identify the model that describes best the topology of brain connectivity. we hypothesized that the highest reduction in global efficiency caused by a targeted attack on each model's hubs would reveal the organization that better describes the topology of the underlying brain networks. we applied this approach to magnetoencephalographic data obtained at rest from neurologically intact controls and mild traumatic brain injury patients. functional connectivity networks were computed using phase-to-amplitude cross-frequency coupling between the &#x03b4; and &#x03b2; frequency bands. our results suggest that resting state meg connectivity networks follow a rich-club organization."
http://orkg.org/orkg/resource/R108402,Color Characteristics for the Evaluation of Suspended Sediments,,"This study focuses on a significant issue of the environmental monitoring application area, which is the suspended sediment concentration estimation. More specifically, the purpose of the current work is to provide a new non-intrusive way to estimate the suspended sediment (SS) distribution. The proposed methodology uses the color characteristics of river flow images and provides a high correlation factor with the suspended sediment measurements. In our opinion, the importance of the current work derives from the fact that it provides an alternative and effective way of estimating SS distribution rather as opposed to the conventional method that requires human presence, especially if we consider the difficulty of taking measurements of the river pollution during flush flood events when the sediment distribution is increased and is directly related to water quality.","this study focuses on a significant issue of the environmental monitoring application area, which is the suspended sediment concentration estimation. more specifically, the purpose of the current work is to provide a new non-intrusive way to estimate the suspended sediment (ss) distribution. the proposed methodology uses the color characteristics of river flow images and provides a high correlation factor with the suspended sediment measurements. in our opinion, the importance of the current work derives from the fact that it provides an alternative and effective way of estimating ss distribution rather as opposed to the conventional method that requires human presence, especially if we consider the difficulty of taking measurements of the river pollution during flush flood events when the sediment distribution is increased and is directly related to water quality."
